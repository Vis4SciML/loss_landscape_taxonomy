{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON autoencoder loss analysis\n",
    "This notebook aims to show the effect of the hyperparameters on the ECON model which could be hide to the user due to the approximation of the EMD metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-11-21 16:54:36.903440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 16:54:37.766102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from common.metrics.gradient import Gradient\n",
    "import torch\n",
    "import torchinfo\n",
    "import pytorch_lightning as pl \n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import modules from ECON model\n",
    "module_path = os.path.abspath(os.path.join('../../workspace/models/econ/code/')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from q_autoencoder import AutoEncoder\n",
    "from autoencoder_datamodule import AutoEncoderDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the desired ECON models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/loss_landscape/checkpoint/different_knobs_subset_10/\"\n",
    "batch_size = [1024]\n",
    "learning_rate = [0.1, 0.05, 0.025, 0.0125, 0.00625, 0.003125, 0.0015625]\n",
    "precision = 8\n",
    "size = ['small', 'baseline', 'large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AutoEncoder                              [1, 1, 8, 8]              --\n",
       "├─QuantizedEncoder: 1-1                  [1, 16]                   --\n",
       "│    └─QuantAct: 2-1                     [1, 1, 8, 8]              --\n",
       "│    └─QuantConv2d: 2-2                  [1, 1, 4, 4]              20\n",
       "│    └─ReLU: 2-3                         [1, 1, 4, 4]              --\n",
       "│    └─QuantAct: 2-4                     [1, 1, 4, 4]              --\n",
       "│    └─Flatten: 2-5                      [1, 16]                   --\n",
       "│    └─QuantLinear: 2-6                  [1, 16]                   272\n",
       "│    └─ReLU: 2-7                         [1, 16]                   --\n",
       "├─Sequential: 1-2                        [1, 1, 8, 8]              --\n",
       "│    └─Linear: 2-8                       [1, 128]                  2,176\n",
       "│    └─ReLU: 2-9                         [1, 128]                  --\n",
       "│    └─Unflatten: 2-10                   [1, 8, 4, 4]              --\n",
       "│    └─ConvTranspose2d: 2-11             [1, 8, 8, 8]              584\n",
       "│    └─ReLU: 2-12                        [1, 8, 8, 8]              --\n",
       "│    └─ConvTranspose2d: 2-13             [1, 1, 8, 8]              73\n",
       "│    └─Sigmoid: 2-14                     [1, 1, 8, 8]              --\n",
       "==========================================================================================\n",
       "Total params: 3,125\n",
       "Trainable params: 3,125\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(path, batch_size, learning_rate, precision, size, exp=1):\n",
    "    model_path = path + f'bs{batch_size}_lr{learning_rate}/ECON_{precision}b/{size}/net_{exp}_best.pkl'\n",
    "    model = AutoEncoder(\n",
    "        quantize=(precision < 32),\n",
    "        precision=[\n",
    "            precision,\n",
    "            precision,\n",
    "            precision+3\n",
    "        ],\n",
    "        learning_rate=learning_rate,\n",
    "        econ_type=size\n",
    "    )\n",
    "    model(torch.randn((1, 1, 8, 8)))  # Update tensor shapes \n",
    "    model_param = torch.load(model_path)\n",
    "    model.load_state_dict(model_param['state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model(base_path, batch_size[0], learning_rate[0], precision, size[0])\n",
    "torchinfo.summary(model, input_size=(1, 1, 8, 8))  # (B, C, H, W)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/ECON/Elegun'\n",
    "processed_file = 'nELinks5.npy'\n",
    "\n",
    "def get_data_module(batch_size):\n",
    "    data_module = AutoEncoderDataModule(\n",
    "        data_dir=data_path,\n",
    "        data_file=os.path.join(data_path, processed_file),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "        )\n",
    "    # checek if we have processed the data\n",
    "    if not os.path.exists(os.path.join(data_path, processed_file)):\n",
    "        print('Processing the data...')\n",
    "        data_module.process_data(save=True)\n",
    "\n",
    "    data_module.setup(0)\n",
    "\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the real EMD without the flags of the empty matrix\n",
    "During the studies of the loss landscape, we discover a strange case in the test of the ECON model. The Econ model is an autoencoder used to compress the information coming from signals that are encoded in a sort of matrix. To evaluate the model we should use the EMD but it is not differentiable, so it is used only as test and the training and evaluation of the model is done with an approximate EMD loss function called Telescope.\n",
    "\n",
    "Looking at the models with big batch sizes (ex. 512 or 1024) we achieved negative EMD values during the test of the model, but it should not be possible (EMD >= 0). Looking at the implementation of the model we found in the computation of the real EMD thi flag, where if the output of the Encoder is zero set the emd as -0.5 which bias the final average EMD of the model, which in most of the case is actually really high.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/EMD_flag.png\"></img>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH SIZE: 1024 - LEARNING RATE 0.1 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': -0.19799108803272247}]\n",
      "AVG EMD:  [{'AVG_EMD': -0.5}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.05 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 6.228666305541992}]\n",
      "AVG EMD:  [{'AVG_EMD': 7.037817478179932}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.025 - SIZE small - PRECISION 8\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.1153912544250488}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.1363208293914795}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.00625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2486635446548462}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.254514455795288}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.003125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2883154153823853}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.318579077720642}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0015625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2461320161819458}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.3005080223083496}]\n"
     ]
    }
   ],
   "source": [
    "def print_old_EMD(size, precision, bs, lr):\n",
    "    print(f'BATCH SIZE: {bs} - LEARNING RATE {lr} - SIZE {size} - PRECISION {precision}')\n",
    "    for exp in range(1, 3):\n",
    "        file_path = base_path + f'bs{bs}_lr{lr}/ECON_{precision}b/{size}/{size}_emd_{exp}.txt'\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                print(\"AVG EMD: \", content)\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "for lr in learning_rate:              \n",
    "    print_old_EMD('small', 8, 1024, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n",
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 341/341 [02:08<00:00,  2.66it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD            13.577227592468262\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.1 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 13.577227592468262}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.1 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': -0.19799108803272247}]\n",
      "AVG EMD:  [{'AVG_EMD': -0.5}]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "def compute_real_EMD(size, precision, bs, lr):\n",
    "    # load the model \n",
    "    model = load_model(base_path, bs, lr, precision, size)\n",
    "    # get the data loader\n",
    "    data_module = get_data_module(bs)\n",
    "    _, val_sum = data_module.get_val_max_and_sum()\n",
    "    model.set_val_sum(val_sum)\n",
    "    data_module.setup(\"test\")\n",
    "    avg_emd = trainer.test(model, dataloaders=data_module.test_dataloader())\n",
    "    print(f'BATCH SIZE: {bs} - LEARNING RATE {lr} - SIZE {size} - PRECISION {precision}')\n",
    "    print(\"AVG EMD: \", avg_emd)\n",
    "    \n",
    "\n",
    "\n",
    "compute_real_EMD('small', 8, 1024, 0.1)\n",
    "print_old_EMD('small', 8, 1024, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Testing DataLoader 0: 100%|██████████| 341/341 [02:26<00:00,  2.33it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD             9.190462112426758\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.05 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 9.190462112426758}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.05 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 6.228666305541992}]\n",
      "AVG EMD:  [{'AVG_EMD': 7.037817478179932}]\n"
     ]
    }
   ],
   "source": [
    "compute_real_EMD('small', 8, 1024, 0.05)\n",
    "print_old_EMD('small', 8, 1024, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Testing DataLoader 0: 100%|██████████| 341/341 [05:28<00:00,  1.04it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD            1.1153733730316162\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.1153733730316162}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.1153912544250488}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.1363208293914795}]\n"
     ]
    }
   ],
   "source": [
    "compute_real_EMD('small', 8, 1024, 0.0125)\n",
    "print_old_EMD('small', 8, 1024, 0.0125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Testing DataLoader 0: 100%|██████████| 341/341 [05:29<00:00,  1.03it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD            1.2486552000045776\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.00625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2486552000045776}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.00625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2486635446548462}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.254514455795288}]\n"
     ]
    }
   ],
   "source": [
    "compute_real_EMD('small', 8, 1024, 0.00625)\n",
    "print_old_EMD('small', 8, 1024, 0.00625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Testing DataLoader 0: 100%|██████████| 341/341 [05:25<00:00,  1.05it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD            1.2884447574615479\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.003125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2884447574615479}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.003125 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2883154153823853}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.318579077720642}]\n"
     ]
    }
   ],
   "source": [
    "compute_real_EMD('small', 8, 1024, 0.003125)\n",
    "print_old_EMD('small', 8, 1024, 0.003125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Reading files ['5Elinks_data.csv', 'nELinks5.npy']\n",
      "Input data shape: (1740950, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepped shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data shape: (1740950, 1, 8, 8)\n",
      "Loaded shaped data datatype: float32\n",
      "Testing DataLoader 0: 100%|██████████| 341/341 [05:31<00:00,  1.03it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         AVG_EMD            1.2459907531738281\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0015625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2459907531738281}]\n",
      "BATCH SIZE: 1024 - LEARNING RATE 0.0015625 - SIZE small - PRECISION 8\n",
      "AVG EMD:  [{'AVG_EMD': 1.2461320161819458}]\n",
      "AVG EMD:  [{'AVG_EMD': 1.3005080223083496}]\n"
     ]
    }
   ],
   "source": [
    "compute_real_EMD('small', 8, 1024, 0.0015625)\n",
    "print_old_EMD('small', 8, 1024, 0.0015625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from those results, negative values of EMD corresponds to really high values of EMD. I assume that when the model have to deal with large batch size and learning rates it has to process really big and sparse matrix which it just compress as Zero-matrices ignoring all the signals with low intensity. This could not be a problem since we are aware of it, especially because this event is not detected by the telescope loss function which returns good value of EMD. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_landscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
