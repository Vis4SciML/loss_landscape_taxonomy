{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Analysis for ECON model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from common.metrics.gradient import Gradient\n",
    "import torch\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import modules from ECON model\n",
    "module_path = os.path.abspath(os.path.join('../../workspace/models/econ/code/')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from q_autoencoder import AutoEncoder\n",
    "from autoencoder_datamodule import AutoEncoderDataModule\n",
    "\n",
    "# import modules from common metrics\n",
    "module_path = os.path.abspath(os.path.join('../../workspace/common/metrics/')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from gradient import Gradient\n",
    "from hessian import Hessian\n",
    "from CKA import CKA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the desired ECON model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/data/tbaldi/checkpoint/\"\n",
    "batch_size = 1024\n",
    "learning_rate = 0.1\n",
    "precision = 8\n",
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, batch_size, learning_rate, precision, size):\n",
    "    model_path = path + f'bs{batch_size}_lr{learning_rate}/ECON_{precision}b/{size}/net_1_best.pkl'\n",
    "    model = AutoEncoder(\n",
    "        quantize=(precision < 32),\n",
    "        precision=[\n",
    "            precision,\n",
    "            precision,\n",
    "            precision+3\n",
    "        ],\n",
    "        learning_rate=learning_rate,\n",
    "        econ_type=size\n",
    "    )\n",
    "    \n",
    "    # to set the map location\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model(torch.randn((1, 1, 8, 8)))  # Update tensor shapes \n",
    "    print(model_path)\n",
    "    model_param = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(model_param['state_dict'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model(base_path, batch_size, learning_rate, precision, size)\n",
    "torchinfo.summary(model, input_size=(1, 1, 8, 8))  # (B, C, H, W)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/ECON/Elegun'\n",
    "processed_file = 'nELinks5.npy'\n",
    "\n",
    "def get_data_module(batch_size):\n",
    "    data_module = AutoEncoderDataModule(\n",
    "        data_dir=data_path,\n",
    "        data_file=os.path.join(data_path, processed_file),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "        )\n",
    "    # checek if we have processed the data\n",
    "    if not os.path.exists(os.path.join(data_path, processed_file)):\n",
    "        print('Processing the data...')\n",
    "        data_module.process_data(save=True)\n",
    "\n",
    "    data_module.setup(0)\n",
    "    return data_module\n",
    "\n",
    "data_module = get_data_module(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "# TODO: add the possibility to load the EMD from file\n",
    "def compute_EMD(size, precision, bs, lr, load=True):\n",
    "    # load the model \n",
    "    model = load_model(base_path, bs, lr, precision, size)\n",
    "    # get the data loader\n",
    "    data_module = get_data_module(bs)\n",
    "    _, val_sum = data_module.get_val_max_and_sum()\n",
    "    model.set_val_sum(val_sum)\n",
    "    data_module.setup(\"test\")\n",
    "    avg_emd = trainer.test(model, dataloaders=data_module.test_dataloader(), verbose=False)\n",
    "    print(f'BATCH SIZE: {bs} - LEARNING RATE {lr} - SIZE {size} - PRECISION {precision}')\n",
    "    print(\"AVG EMD: \", avg_emd)\n",
    "    \n",
    "\n",
    "# compute_EMD('small', 8, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from each layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(base_path, batch_size, learning_rate, 8, 'small')\n",
    "data_module = get_data_module(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = CKA(model, \n",
    "             data_module.test_dataloader(),\n",
    "             activation_layers=['encoder.relu1', 'encoder.relu2'])\n",
    "\n",
    "result = metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cka_heatmap(cka_values, title, x_label='Layers', y_label='Layers'):\n",
    "    # Create a heatmap\n",
    "    plt.imshow(cka_values.values, \n",
    "            cmap='Reds', \n",
    "            interpolation='nearest',\n",
    "            vmin=0,\n",
    "            vmax=1)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.colorbar(label='CKA distance')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    # Set axis ticks based on the dimensions\n",
    "    plt.xticks(np.arange(len(cka_values.columns)), cka_values.columns, rotation='vertical')\n",
    "    plt.yticks(np.arange(len(cka_values.index)), cka_values.index, rotation='horizontal')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "plot_cka_heatmap(result['cka_dist'], \n",
    "                 f\"ECON BS: {batch_size} LR: {learning_rate} Size: {size} bitwidt: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = load_model(base_path, 1024, learning_rate, 8, 'small')\n",
    "data_module_small = get_data_module(1024)\n",
    "\n",
    "model_baseline = load_model(base_path, 1024, learning_rate, 2, 'baseline')\n",
    "data_module_baseline = get_data_module(1024)\n",
    "\n",
    "activation_layers=['encoder.relu1', 'encoder.relu2']\n",
    "\n",
    "metric = CKA(model_small, \n",
    "             data_module_small.test_dataloader(),\n",
    "             activation_layers=activation_layers)\n",
    "\n",
    "result = metric.compare(model_baseline, \n",
    "                        data_module_baseline.test_dataloader(), \n",
    "                        activation_layers)\n",
    "\n",
    "\n",
    "plot_cka_heatmap(result['cka_dist'], \n",
    "                 f\"ECON BS: {batch_size} LR: {learning_rate} size: {size}\\n 8 bit VS 11 bit\",\n",
    "                 x_label='model 2',\n",
    "                 y_label='model 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# _, val_loader = data_module.dataloaders()\n",
    "\n",
    "# metric = Hessian(\n",
    "#     model=model,\n",
    "#     data_loader=val_loader,\n",
    "#     loss=model.loss\n",
    "# )\n",
    "\n",
    "# results = metric.compute()\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.save_on_file(path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = metric.load_from_file(path=\".\")\n",
    "\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_landscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
