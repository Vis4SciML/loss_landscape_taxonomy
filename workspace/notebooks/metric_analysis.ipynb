{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Analysis for ECON model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from common.metrics.gradient import Gradient\n",
    "import torch\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl \n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import modules from ECON model\n",
    "module_path = os.path.abspath(os.path.join('../../workspace/models/econ/code/')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from q_autoencoder import AutoEncoder\n",
    "from autoencoder_datamodule import AutoEncoderDataModule\n",
    "\n",
    "# import modules from common metrics\n",
    "module_path = os.path.abspath(os.path.join('../../workspace/common/metrics/')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "from gradient import Gradient\n",
    "from hessian import Hessian\n",
    "from CKA import CKA\n",
    "from neural_efficiency import NeuralEfficiency\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the desired ECON model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/jovyan/checkpoint/\"\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0015625\n",
    "precision = 8\n",
    "size = 'small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, batch_size, learning_rate, precision, size):\n",
    "    model_path = path + f'bs{batch_size}_lr{learning_rate}/ECON_{precision}b/{size}/net_1_best.pkl'\n",
    "    \n",
    "    # load the model\n",
    "    model = AutoEncoder(\n",
    "        quantize=(precision < 32),\n",
    "        precision=[\n",
    "            precision,\n",
    "            precision,\n",
    "            precision+3\n",
    "        ],\n",
    "        learning_rate=learning_rate,\n",
    "        econ_type=size\n",
    "    )\n",
    "    \n",
    "    # to set the map location\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model(torch.randn((1, 1, 8, 8)))  # Update tensor shapes \n",
    "    model_param = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(model_param['state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# model = load_model(base_path, batch_size, learning_rate, precision, size)\n",
    "# torchinfo.summary(model, input_size=(1, 1, 8, 8))  # (B, C, H, W)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/ECON/Elegun'\n",
    "processed_file = 'nELinks5.npy'\n",
    "\n",
    "def get_data_module(batch_size):\n",
    "    data_module = AutoEncoderDataModule(\n",
    "        data_dir=data_path,\n",
    "        data_file=os.path.join(data_path, processed_file),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "        )\n",
    "    # checek if we have processed the data\n",
    "    if not os.path.exists(os.path.join(data_path, processed_file)):\n",
    "        print('Processing the data...')\n",
    "        data_module.process_data(save=True)\n",
    "\n",
    "    data_module.setup(0)\n",
    "    return data_module\n",
    "\n",
    "data_module = get_data_module(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "def compute_EMD(bs, lr, size, precision, load=True):\n",
    "    if load:\n",
    "        print(\"Loading the EMD...\")\n",
    "        emd_path = base_path + f'bs{bs}_lr{lr}/ECON_{precision}b/{size}/{size}_emd_1.txt'\n",
    "        # load the achieved EMD of the model\n",
    "        emd = None\n",
    "        try:\n",
    "            emd_file = open(emd_path)\n",
    "            emd_text = emd_file.read()\n",
    "            emd = ast.literal_eval(emd_text)\n",
    "            emd = emd[0]['AVG_EMD']\n",
    "            emd_file.close()\n",
    "            return emd\n",
    "        except Exception as e:\n",
    "            print(\"ATTENTION: not able to open the file with the EMD content!\")\n",
    "        \n",
    "    print(\"Computing the EMD...\")\n",
    "    # load the model \n",
    "    model = load_model(base_path, bs, lr, precision, size)\n",
    "    # get the data loader\n",
    "    data_module = get_data_module(bs)\n",
    "    _, val_sum = data_module.get_val_max_and_sum()\n",
    "    model.set_val_sum(val_sum)\n",
    "    data_module.setup(\"test\")\n",
    "    avg_emd = trainer.test(model, dataloaders=data_module.test_dataloader(), verbose=False)\n",
    "    return avg_emd\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Estimated) Fisher trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, val_loader = data_module.dataloaders()\n",
    "\n",
    "# metric = Fisher(\n",
    "#     model=model,\n",
    "#     data_loader=val_loader,\n",
    "#     optimizer=model.configure_optimizers(),\n",
    "#     target_layers=['encoder.conv', 'encoder.enc_dense']\n",
    "# )\n",
    "\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# _, val_loader = data_module.dataloaders(max_batches=100)\n",
    "\n",
    "# print(len(val_loader))\n",
    "# metric = Hessian(\n",
    "#     model=model,\n",
    "#     data_loader=val_loader,\n",
    "#     loss=model.loss)\n",
    "\n",
    "# results = metric.compute()\n",
    "# len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKA similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from each layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = load_model(base_path, 1024, 0.1, 8, 'small')\n",
    "# model2 = load_model(base_path, 256, 0.003125, 2, 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = CKA(model1, layers=['encoder.conv', 'encoder.enc_dense'])\n",
    "\n",
    "# result = metric.compare(model2, layers=['encoder.conv', 'encoder.enc_dense'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_small = load_model(base_path, 1024, learning_rate, 8, 'small')\n",
    "# data_module_small = get_data_module(1024)\n",
    "\n",
    "# model_baseline = load_model(base_path, 1024, learning_rate, 2, 'small')\n",
    "# data_module_baseline = get_data_module(1024)\n",
    "\n",
    "# activation_layers=['encoder.relu1', 'encoder.relu2']\n",
    "\n",
    "# metric = CKA(model_small, \n",
    "#              data_module_small.test_dataloader(),\n",
    "#              activation_layers=activation_layers)\n",
    "\n",
    "# result = metric.compare(model_baseline, \n",
    "#                         data_module_baseline.test_dataloader(), \n",
    "#                         activation_layers)\n",
    "\n",
    "\n",
    "# plot_cka_heatmap(result['cka_dist'], \n",
    "#                  f\"ECON BS: {batch_size} LR: {learning_rate} size: {size}\\n 8 bit VS 2 bit\",\n",
    "#                  x_label='model 2',\n",
    "#                  y_label='model 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/checkpoint/bs128_lr0.003125/ECON_8b/small/net_1_best.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.003125\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_module \u001b[38;5;241m=\u001b[39m get_data_module(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# emd = compute_EMD(1024, 0.003125, 'small', 8)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path, batch_size, learning_rate, precision, size)\u001b[0m\n\u001b[1;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m model(torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)))  \u001b[38;5;66;03m# Update tensor shapes \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model_param \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/envs/loss_landscape/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/checkpoint/bs128_lr0.003125/ECON_8b/small/net_1_best.pkl'"
     ]
    }
   ],
   "source": [
    "model = load_model(base_path, 128, 0.003125, 8, 'small')\n",
    "data_module = get_data_module(1)\n",
    "# emd = compute_EMD(1024, 0.003125, 'small', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the Neural efficiency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/loss_landscape_taxonomy/workspace/common/metrics/neural_efficiency.py:112: UserWarning: Attention: the layer conv has tuple as features!\n",
      "  warnings.warn(f\"Attention: the layer \" + name + \" has tuple as features!\")\n",
      "/home/jovyan/loss_landscape_taxonomy/workspace/common/metrics/neural_efficiency.py:108: UserWarning: Attention: the layer conv.conv has None features!\n",
      "  warnings.warn(f\"Attention: the layer \" + name + \" has None features!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "100\n",
      "layers neural efficiency:\n",
      " {'conv': 0.08698144271506131, 'enc_dense': 0.002597523870257269}\n",
      "network neural efficiency:\n",
      " 0.015031180050873815\n",
      "aIQ\n",
      " None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/loss_landscape_taxonomy/workspace/common/metrics/neural_efficiency.py:185: UserWarning: Warning: you cannot compute the aIQ without the performance of the model (accuracy, EMD, MSE, ...).\n",
      "  warnings.warn(\"Warning: you cannot compute the aIQ without the performance of the model (accuracy, EMD, MSE, ...).\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'layers_efficiency': {'conv': 0.08698144271506131,\n",
       "  'enc_dense': 0.002597523870257269},\n",
       " 'network_efficiency': 0.015031180050873815,\n",
       " 'aIQ': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = NeuralEfficiency(model.encoder, data_module.test_dataloader(), performance=None, max_batches=1000)\n",
    "result = metric.compute(beta=-2)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_neural_efficiency_per_layer(neural_efficiency_values, \n",
    "                                     title, \n",
    "                                     x_label='Layers', \n",
    "                                     y_label='Efficiency'):\n",
    "    # Create a heatmap\n",
    "    plt.bar(neural_efficiency_values.keys(), \n",
    "            neural_efficiency_values.values())\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_neural_efficiency_per_layer(result['layers_efficiency'], 'ECON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loss_landscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
