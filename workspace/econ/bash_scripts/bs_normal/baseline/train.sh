## BATCHSIZE 16 ##
echo "Running BATCHSIZE 16"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 16 --test-bs 16 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_16/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 32 ##
echo "Running BATCHSIZE 32"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 32 --test-bs 32 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_32/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 64 ##
echo "Running BATCHSIZE 64"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 64 --test-bs 64 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_64/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 128 ##
echo "Running BATCHSIZE 128"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 128 --test-bs 128 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_128/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 256 ##
echo "Running BATCHSIZE 256"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 256 --test-bs 256 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_256/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 512 ##
echo "Running BATCHSIZE 512"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 512 --test-bs 512 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_512/normal/ECON_11b/err_baseline_2.txt >&2) 
## BATCHSIZE 1024 ##
echo "Running BATCHSIZE 1024"
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_6b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 6 --bias-precision 6 --act-precision 9  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_6b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_7b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 7 --bias-precision 7 --act-precision 10  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_7b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_8b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 8 --bias-precision 8 --act-precision 11  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_8b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_9b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 9 --bias-precision 9 --act-precision 12  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_9b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_10b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 10 --bias-precision 10 --act-precision 13  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_10b/err_baseline_2.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/ --file-prefix exp_0 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/err_baseline_0.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/ --file-prefix exp_1 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/err_baseline_1.txt >&2) 
python code/train.py --training-type normal --experiment_name baseline --arch ECON_11b --saving-folder ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/ --file-prefix exp_2 --mixup-alpha 16.0 --data-subset --subset 1.0 --data_dir ../../data/ECON/Elegun  --lr 0.0015625 --weight-decay 0.0005 --train-bs 1024 --test-bs 1024 --weight-precision 11 --bias-precision 11 --act-precision 14  --no-lr-decay --max_epochs 25 --save-early-stop --min-delta 0.0001 --patience 5 --save-best --train --ignore-incomplete-batch > >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/log_baseline_0.txt) 2> >(tee -a ../checkpoint/different_knobs_subset_10/bs_1024/normal/ECON_11b/err_baseline_2.txt >&2) 
