train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.4
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.4/lr_decay/JT_32b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/train/jetImage_4_100p_70000_80000.h5
Could not load file: ../../data/train/jetImage_1_100p_70000_80000.h5
Could not load file: ../../data/train/jetImage_3_100p_60000_70000.h5
Could not load file: ../../data/train/jetImage_5_100p_10000_20000.h5
Could not load file: ../../data/train/jetImage_5_100p_0_10000.h5
Could not load file: ../../data/train/jetImage_3_100p_10000_20000.h5
Could not load file: ../../data/train/jetImage_4_100p_10000_20000.h5
Could not load file: ../../data/train/jetImage_3_100p_70000_80000.h5
Could not load file: ../../data/test/jetImage_8_100p_50000_60000.h5
Could not load file: ../../data/test/jetImage_8_100p_40000_50000.h5
Could not load file: ../../data/test/jetImage_8_100p_10000_20000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.4
---------------------
Start epoch 0
---------------------
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.4
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.4/lr_decay/JT_32b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using quant model
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.4
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.4/lr_decay/JT_32b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using FP32 model
---------------------- Model -------------------------
three_layer_model(
  (dense_1): Linear(in_features=16, out_features=64, bias=True)
  (dense_2): Linear(in_features=64, out_features=32, bias=True)
  (dense_3): Linear(in_features=32, out_features=32, bias=True)
  (dense_4): Linear(in_features=32, out_features=5, bias=True)
  (act1): ReLU()
  (act2): ReLU()
  (act3): ReLU()
  (softmax): Softmax(dim=0)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/train/jetImage_2_100p_50000_60000.h5
Could not load file: ../../data/train/jetImage_0_100p_50000_60000.h5
Could not load file: ../../data/train/jetImage_2_100p_80000_90000.h5
Could not load file: ../../data/train/jetImage_5_100p_0_10000.h5
Could not load file: ../../data/train/jetImage_4_100p_10000_20000.h5
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.4
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.4/lr_decay/JT_32b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using FP32 model
---------------------- Model -------------------------
three_layer_model(
  (fc1): Linear(in_features=16, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=32, bias=True)
  (fc3): Linear(in_features=32, out_features=32, bias=True)
  (fc4): Linear(in_features=32, out_features=5, bias=True)
  (act1): ReLU()
  (act2): ReLU()
  (act3): ReLU()
  (softmax): Softmax(dim=0)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.4
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 1.3879e+00 (1.3879e+00)	Acc 0.158203 (0.158203)
tensor([1, 1, 2,  ..., 1, 4, 4], device='cuda:0')
tensor([2, 3, 4,  ..., 2, 2, 4], device='cuda:0')
Epoch: [0][ 50/616]	Loss 2.0059e+01 (1.9455e+01)	Acc 0.186523 (0.206323)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([0, 4, 0,  ..., 3, 1, 1], device='cuda:0')
Epoch: [0][100/616]	Loss 2.0059e+01 (1.9754e+01)	Acc 0.213867 (0.203415)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([2, 2, 3,  ..., 0, 1, 1], device='cuda:0')
Epoch: [0][150/616]	Loss 2.0059e+01 (1.9854e+01)	Acc 0.185547 (0.202795)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([3, 2, 0,  ..., 0, 4, 0], device='cuda:0')
Epoch: [0][200/616]	Loss 2.0059e+01 (1.9905e+01)	Acc 0.191406 (0.201930)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 4, 1,  ..., 0, 4, 2], device='cuda:0')
Epoch: [0][250/616]	Loss 2.0059e+01 (1.9936e+01)	Acc 0.203125 (0.201382)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 1, 3,  ..., 2, 2, 3], device='cuda:0')
Epoch: [0][300/616]	Loss 2.0059e+01 (1.9956e+01)	Acc 0.197266 (0.201532)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([0, 3, 2,  ..., 4, 3, 0], device='cuda:0')
Epoch: [0][350/616]	Loss 2.0059e+01 (1.9971e+01)	Acc 0.208984 (0.201361)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 4, 0,  ..., 0, 0, 4], device='cuda:0')
Epoch: [0][400/616]	Loss 2.0059e+01 (1.9982e+01)	Acc 0.209961 (0.201554)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([0, 2, 2,  ..., 4, 4, 4], device='cuda:0')
Epoch: [0][450/616]	Loss 2.0059e+01 (1.9990e+01)	Acc 0.208984 (0.201689)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([4, 2, 3,  ..., 0, 1, 4], device='cuda:0')
Epoch: [0][500/616]	Loss 2.0059e+01 (1.9997e+01)	Acc 0.216797 (0.201722)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([0, 0, 4,  ..., 0, 2, 2], device='cuda:0')
Epoch: [0][550/616]	Loss 2.0059e+01 (2.0003e+01)	Acc 0.186523 (0.201544)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([2, 3, 2,  ..., 0, 2, 4], device='cuda:0')
Epoch: [0][600/616]	Loss 2.0059e+01 (2.0007e+01)	Acc 0.208008 (0.201453)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([4, 3, 1,  ..., 0, 3, 0], device='cuda:0')
Neglect the last epoch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 0: 20.00840003742435
Training Acc of Epoch 0: 0.20148628048780487
Testing Acc of Epoch 0: 0.20121739130434782
Model with the best training loss saved! The loss is 0.0
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.0059e+01 (2.0059e+01)	Acc 0.230469 (0.230469)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([2, 2, 1,  ..., 2, 2, 4], device='cuda:0')
Epoch: [1][ 50/616]	Loss 2.0059e+01 (2.0059e+01)	Acc 0.203125 (0.201574)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([2, 0, 0,  ..., 4, 2, 4], device='cuda:0')
Epoch: [1][100/616]	Loss 2.0041e+01 (2.0058e+01)	Acc 0.188477 (0.200901)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([2, 1, 1,  ..., 3, 4, 1], device='cuda:0')
Epoch: [1][150/616]	Loss 2.0059e+01 (2.0058e+01)	Acc 0.193359 (0.200920)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 2, 0,  ..., 1, 2, 4], device='cuda:0')
Epoch: [1][200/616]	Loss 2.0059e+01 (2.0058e+01)	Acc 0.166992 (0.200594)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 4, 2,  ..., 3, 2, 3], device='cuda:0')
Epoch: [1][250/616]	Loss 2.0059e+01 (2.0058e+01)	Acc 0.174805 (0.200343)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([4, 1, 1,  ..., 4, 4, 4], device='cuda:0')
Epoch: [1][300/616]	Loss 2.0059e+01 (2.0058e+01)	Acc 0.210938 (0.200786)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 1, 4,  ..., 1, 1, 1], device='cuda:0')
Epoch: [1][350/616]	Loss 2.0059e+01 (2.0058e+01)	Acc 0.225586 (0.201275)
tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0')
tensor([1, 2, 3,  ..., 2, 3, 1], device='cuda:0')
