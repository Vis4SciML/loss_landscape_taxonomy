train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_32b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using FP32 model
---------------------- Model -------------------------
ThreeLayerMLP(
  (dense_1): Linear(in_features=16, out_features=64, bias=True)
  (dense_2): Linear(in_features=64, out_features=32, bias=True)
  (dense_3): Linear(in_features=32, out_features=32, bias=True)
  (dense_4): Linear(in_features=32, out_features=5, bias=True)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0271e-01 (5.0271e-01)	Acc 0.211914 (0.211914)
Epoch: [0][300/616]	Loss 2.5659e-01 (2.7521e-01)	Acc 0.732422 (0.708125)
Epoch: [0][600/616]	Loss 2.4185e-01 (2.6290e-01)	Acc 0.741211 (0.723561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.744235)
Training Loss of Epoch 0: 0.26262665727758794
Training Acc of Epoch 0: 0.7238630589430894
Testing Acc of Epoch 0: 0.7442347826086957
Model with the best training loss saved! The loss is 0.26262665727758794
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.5223e-01 (2.5223e-01)	Acc 0.734375 (0.734375)
Epoch: [1][300/616]	Loss 2.4946e-01 (2.4669e-01)	Acc 0.749023 (0.741587)
Epoch: [1][600/616]	Loss 2.3519e-01 (2.4601e-01)	Acc 0.771484 (0.743005)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739417)
Training Loss of Epoch 1: 0.24603661780919486
Training Acc of Epoch 1: 0.7429735137195121
Testing Acc of Epoch 1: 0.7394173913043478
Model with the best training loss saved! The loss is 0.24603661780919486
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5295e-01 (2.5295e-01)	Acc 0.739258 (0.739258)
Epoch: [2][300/616]	Loss 2.2965e-01 (2.4393e-01)	Acc 0.767578 (0.744426)
Epoch: [2][600/616]	Loss 2.4007e-01 (2.4425e-01)	Acc 0.747070 (0.744173)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748874)
Training Loss of Epoch 2: 0.2442960157142422
Training Acc of Epoch 2: 0.7440644054878048
Testing Acc of Epoch 2: 0.7488739130434783
Model with the best training loss saved! The loss is 0.2442960157142422
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3866e-01 (2.3866e-01)	Acc 0.746094 (0.746094)
Epoch: [3][300/616]	Loss 2.5146e-01 (2.4433e-01)	Acc 0.742188 (0.744449)
Epoch: [3][600/616]	Loss 2.3636e-01 (2.4498e-01)	Acc 0.750977 (0.743749)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.736609)
Training Loss of Epoch 3: 0.24504514964130836
Training Acc of Epoch 3: 0.7436182037601626
Testing Acc of Epoch 3: 0.7366086956521739
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5638e-01 (2.5638e-01)	Acc 0.731445 (0.731445)
Epoch: [4][300/616]	Loss 2.5569e-01 (2.4358e-01)	Acc 0.748047 (0.744540)
Epoch: [4][600/616]	Loss 2.5629e-01 (2.4407e-01)	Acc 0.722656 (0.743962)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745987)
Training Loss of Epoch 4: 0.24403318576211852
Training Acc of Epoch 4: 0.7440564659552845
Testing Acc of Epoch 4: 0.7459869565217391
Model with the best training loss saved! The loss is 0.24403318576211852
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3161e-01 (2.3161e-01)	Acc 0.750977 (0.750977)
Epoch: [5][300/616]	Loss 2.4315e-01 (2.4632e-01)	Acc 0.747070 (0.741649)
Epoch: [5][600/616]	Loss 2.7562e-01 (2.4569e-01)	Acc 0.714844 (0.742618)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.745565)
Training Loss of Epoch 5: 0.24574903235202883
Training Acc of Epoch 5: 0.7425892403455284
Testing Acc of Epoch 5: 0.7455652173913043
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4082e-01 (2.4082e-01)	Acc 0.746094 (0.746094)
Epoch: [6][300/616]	Loss 2.5192e-01 (2.4454e-01)	Acc 0.730469 (0.742989)
Epoch: [6][600/616]	Loss 2.5827e-01 (2.4447e-01)	Acc 0.733398 (0.743796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745483)
Training Loss of Epoch 6: 0.24460162210270642
Training Acc of Epoch 6: 0.7436833079268292
Testing Acc of Epoch 6: 0.7454826086956522
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2865e-01 (2.2865e-01)	Acc 0.770508 (0.770508)
Epoch: [7][300/616]	Loss 2.2486e-01 (2.4322e-01)	Acc 0.756836 (0.744634)
Epoch: [7][600/616]	Loss 2.5157e-01 (2.4385e-01)	Acc 0.725586 (0.744269)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744187)
Training Loss of Epoch 7: 0.24394082470637996
Training Acc of Epoch 7: 0.7442025533536586
Testing Acc of Epoch 7: 0.7441869565217392
Model with the best training loss saved! The loss is 0.24394082470637996
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4681e-01 (2.4681e-01)	Acc 0.739258 (0.739258)
Epoch: [8][300/616]	Loss 2.3693e-01 (2.4520e-01)	Acc 0.754883 (0.742905)
Epoch: [8][600/616]	Loss 2.3917e-01 (2.4488e-01)	Acc 0.752930 (0.743265)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.747204)
Training Loss of Epoch 8: 0.24482857306313707
Training Acc of Epoch 8: 0.7433292047764227
Testing Acc of Epoch 8: 0.7472043478260869
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4474e-01 (2.4474e-01)	Acc 0.747070 (0.747070)
Epoch: [9][300/616]	Loss 2.4528e-01 (2.4408e-01)	Acc 0.743164 (0.744485)
Epoch: [9][600/616]	Loss 2.4823e-01 (2.4416e-01)	Acc 0.733398 (0.743944)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744648)
Training Loss of Epoch 9: 0.24427931446854662
Training Acc of Epoch 9: 0.7438817962398374
Testing Acc of Epoch 9: 0.7446478260869566
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.4093e-01 (2.4093e-01)	Acc 0.755859 (0.755859)
Epoch: [10][300/616]	Loss 2.6285e-01 (2.4484e-01)	Acc 0.723633 (0.742960)
Epoch: [10][600/616]	Loss 2.4768e-01 (2.4429e-01)	Acc 0.725586 (0.743861)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748009)
Training Loss of Epoch 10: 0.24423989738390697
Training Acc of Epoch 10: 0.7438849720528455
Testing Acc of Epoch 10: 0.7480086956521739
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4071e-01 (2.4071e-01)	Acc 0.739258 (0.739258)
Epoch: [11][300/616]	Loss 2.3792e-01 (2.4467e-01)	Acc 0.740234 (0.743926)
Epoch: [11][600/616]	Loss 2.4506e-01 (2.4448e-01)	Acc 0.758789 (0.744035)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.626603 (0.648587)
Training Loss of Epoch 11: 0.24471659674877075
Training Acc of Epoch 11: 0.7437722306910569
Testing Acc of Epoch 11: 0.6485869565217391
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 3.3364e-01 (3.3364e-01)	Acc 0.658203 (0.658203)
Epoch: [12][300/616]	Loss 2.3969e-01 (2.6624e-01)	Acc 0.759766 (0.718192)
Epoch: [12][600/616]	Loss 2.4630e-01 (2.5573e-01)	Acc 0.755859 (0.730714)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.740648)
Training Loss of Epoch 12: 0.25556899532070004
Training Acc of Epoch 12: 0.7309721163617886
Testing Acc of Epoch 12: 0.7406478260869566
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3175e-01 (2.3175e-01)	Acc 0.763672 (0.763672)
Epoch: [13][300/616]	Loss 2.4196e-01 (2.4367e-01)	Acc 0.742188 (0.745059)
Epoch: [13][600/616]	Loss 2.4751e-01 (2.4500e-01)	Acc 0.747070 (0.743520)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747678)
Training Loss of Epoch 13: 0.2449468789304175
Training Acc of Epoch 13: 0.7435261051829268
Testing Acc of Epoch 13: 0.7476782608695652
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.4651e-01 (2.4651e-01)	Acc 0.732422 (0.732422)
Epoch: [14][300/616]	Loss 2.6197e-01 (2.4603e-01)	Acc 0.726562 (0.742421)
Epoch: [14][600/616]	Loss 2.3932e-01 (2.4518e-01)	Acc 0.750977 (0.743146)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.747326)
Training Loss of Epoch 14: 0.24506602001384023
Training Acc of Epoch 14: 0.743311737804878
Testing Acc of Epoch 14: 0.7473260869565217
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4924e-01 (2.4924e-01)	Acc 0.740234 (0.740234)
Epoch: [15][300/616]	Loss 2.4627e-01 (2.4467e-01)	Acc 0.738281 (0.743417)
Epoch: [15][600/616]	Loss 2.3480e-01 (2.4466e-01)	Acc 0.762695 (0.743691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744596)
Training Loss of Epoch 15: 0.24471343491620165
Training Acc of Epoch 15: 0.743530868902439
Testing Acc of Epoch 15: 0.744595652173913
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3773e-01 (2.3773e-01)	Acc 0.754883 (0.754883)
Epoch: [16][300/616]	Loss 2.4371e-01 (2.4421e-01)	Acc 0.743164 (0.744102)
Epoch: [16][600/616]	Loss 2.6646e-01 (2.4426e-01)	Acc 0.721680 (0.744074)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745800)
Training Loss of Epoch 16: 0.2441999946667896
Training Acc of Epoch 16: 0.7442517784552846
Testing Acc of Epoch 16: 0.7458
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.5208e-01 (2.5208e-01)	Acc 0.737305 (0.737305)
Epoch: [17][300/616]	Loss 2.4574e-01 (2.4362e-01)	Acc 0.730469 (0.744725)
Epoch: [17][600/616]	Loss 2.4742e-01 (2.4430e-01)	Acc 0.745117 (0.744329)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747213)
Training Loss of Epoch 17: 0.24425368161220862
Training Acc of Epoch 17: 0.7444645579268293
Testing Acc of Epoch 17: 0.7472130434782609
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3124e-01 (2.3124e-01)	Acc 0.753906 (0.753906)
Epoch: [18][300/616]	Loss 2.4550e-01 (2.4481e-01)	Acc 0.744141 (0.743125)
Epoch: [18][600/616]	Loss 2.4697e-01 (2.4452e-01)	Acc 0.737305 (0.743499)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.737874)
Training Loss of Epoch 18: 0.24467341517045246
Training Acc of Epoch 18: 0.7433673145325204
Testing Acc of Epoch 18: 0.7378739130434783
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.5337e-01 (2.5337e-01)	Acc 0.730469 (0.730469)
Epoch: [19][300/616]	Loss 2.3394e-01 (2.4626e-01)	Acc 0.750977 (0.741604)
Epoch: [19][600/616]	Loss 2.3740e-01 (2.4503e-01)	Acc 0.748047 (0.743112)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.744661)
Training Loss of Epoch 19: 0.244854328205915
Training Acc of Epoch 19: 0.7432910950203252
Testing Acc of Epoch 19: 0.7446608695652174
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4982e-01 (2.4982e-01)	Acc 0.725586 (0.725586)
Epoch: [20][300/616]	Loss 2.4702e-01 (2.4466e-01)	Acc 0.744141 (0.743641)
Epoch: [20][600/616]	Loss 2.3294e-01 (2.4436e-01)	Acc 0.750977 (0.743830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746987)
Training Loss of Epoch 20: 0.24431088666121165
Training Acc of Epoch 20: 0.7438817962398374
Testing Acc of Epoch 20: 0.7469869565217391
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4425e-01 (2.4425e-01)	Acc 0.724609 (0.724609)
Epoch: [21][300/616]	Loss 2.4146e-01 (2.4388e-01)	Acc 0.750000 (0.744656)
Epoch: [21][600/616]	Loss 2.3324e-01 (2.4525e-01)	Acc 0.761719 (0.743132)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.747339)
Training Loss of Epoch 21: 0.24519576849491614
Training Acc of Epoch 21: 0.7432641006097561
Testing Acc of Epoch 21: 0.7473391304347826
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4434e-01 (2.4434e-01)	Acc 0.740234 (0.740234)
Epoch: [22][300/616]	Loss 2.3916e-01 (2.4451e-01)	Acc 0.755859 (0.743777)
Epoch: [22][600/616]	Loss 2.4615e-01 (2.4411e-01)	Acc 0.753906 (0.743665)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746983)
Training Loss of Epoch 22: 0.2441412468266681
Training Acc of Epoch 22: 0.7436102642276423
Testing Acc of Epoch 22: 0.7469826086956521
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.5586e-01 (2.5586e-01)	Acc 0.732422 (0.732422)
Epoch: [23][300/616]	Loss 2.5005e-01 (2.4592e-01)	Acc 0.742188 (0.742645)
Epoch: [23][600/616]	Loss 2.3991e-01 (2.4494e-01)	Acc 0.754883 (0.743744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747465)
Training Loss of Epoch 23: 0.24494292796627293
Training Acc of Epoch 23: 0.7437103023373983
Testing Acc of Epoch 23: 0.7474652173913043
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.4634e-01 (2.4634e-01)	Acc 0.752930 (0.752930)
Epoch: [24][300/616]	Loss 2.2655e-01 (2.4544e-01)	Acc 0.764648 (0.742655)
Epoch: [24][600/616]	Loss 2.4338e-01 (2.4482e-01)	Acc 0.735352 (0.743473)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.746600)
Training Loss of Epoch 24: 0.24489698625677
Training Acc of Epoch 24: 0.7434292428861788
Testing Acc of Epoch 24: 0.7466
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4180e-01 (2.4180e-01)	Acc 0.752930 (0.752930)
Epoch: [25][300/616]	Loss 2.5729e-01 (2.4497e-01)	Acc 0.731445 (0.743498)
Epoch: [25][600/616]	Loss 2.3245e-01 (2.4448e-01)	Acc 0.768555 (0.744046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746739)
Training Loss of Epoch 25: 0.24437987949790024
Training Acc of Epoch 25: 0.7441231580284553
Testing Acc of Epoch 25: 0.7467391304347826
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4168e-01 (2.4168e-01)	Acc 0.742188 (0.742188)
Epoch: [26][300/616]	Loss 2.3633e-01 (2.4574e-01)	Acc 0.751953 (0.742577)
Epoch: [26][600/616]	Loss 2.3992e-01 (2.4516e-01)	Acc 0.744141 (0.743359)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.741865)
Training Loss of Epoch 26: 0.24503892066517496
Training Acc of Epoch 26: 0.743480055894309
Testing Acc of Epoch 26: 0.7418652173913044
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.5184e-01 (2.5184e-01)	Acc 0.740234 (0.740234)
Epoch: [27][300/616]	Loss 2.6263e-01 (2.4436e-01)	Acc 0.722656 (0.744433)
Epoch: [27][600/616]	Loss 2.3374e-01 (2.4445e-01)	Acc 0.765625 (0.744095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745665)
Training Loss of Epoch 27: 0.24442849549336162
Training Acc of Epoch 27: 0.7441580919715447
Testing Acc of Epoch 27: 0.7456652173913043
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4272e-01 (2.4272e-01)	Acc 0.743164 (0.743164)
Epoch: [28][300/616]	Loss 2.6273e-01 (2.4445e-01)	Acc 0.725586 (0.743910)
Epoch: [28][600/616]	Loss 2.6387e-01 (2.4544e-01)	Acc 0.722656 (0.742956)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.733461)
Training Loss of Epoch 28: 0.24568266832247013
Training Acc of Epoch 28: 0.7426686356707317
Testing Acc of Epoch 28: 0.7334608695652174
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4426e-01 (2.4426e-01)	Acc 0.742188 (0.742188)
Epoch: [29][300/616]	Loss 2.5710e-01 (2.4589e-01)	Acc 0.717773 (0.742632)
Epoch: [29][600/616]	Loss 2.4457e-01 (2.4478e-01)	Acc 0.750000 (0.743769)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747317)
Training Loss of Epoch 29: 0.24475896612900058
Training Acc of Epoch 29: 0.7437642911585366
Testing Acc of Epoch 29: 0.7473173913043478
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4429e-01 (2.4429e-01)	Acc 0.746094 (0.746094)
Epoch: [30][300/616]	Loss 2.3095e-01 (2.4343e-01)	Acc 0.758789 (0.744488)
Epoch: [30][600/616]	Loss 2.2600e-01 (2.4399e-01)	Acc 0.774414 (0.744535)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746296)
Training Loss of Epoch 30: 0.24395200287908073
Training Acc of Epoch 30: 0.7445217225609756
Testing Acc of Epoch 30: 0.746295652173913
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3081e-01 (2.3081e-01)	Acc 0.752930 (0.752930)
Epoch: [31][300/616]	Loss 2.5451e-01 (2.4432e-01)	Acc 0.732422 (0.744309)
Epoch: [31][600/616]	Loss 2.3163e-01 (2.4400e-01)	Acc 0.765625 (0.744414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.746352)
Training Loss of Epoch 31: 0.24396344047736346
Training Acc of Epoch 31: 0.7444645579268293
Testing Acc of Epoch 31: 0.7463521739130434
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4573e-01 (2.4573e-01)	Acc 0.752930 (0.752930)
Epoch: [32][300/616]	Loss 2.5700e-01 (2.4643e-01)	Acc 0.739258 (0.743219)
Epoch: [32][600/616]	Loss 2.5192e-01 (2.4579e-01)	Acc 0.730469 (0.743003)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744370)
Training Loss of Epoch 32: 0.2458759621876042
Training Acc of Epoch 32: 0.7429322281504065
Testing Acc of Epoch 32: 0.7443695652173913
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.5239e-01 (2.5239e-01)	Acc 0.723633 (0.723633)
Epoch: [33][300/616]	Loss 2.5086e-01 (2.4393e-01)	Acc 0.741211 (0.744085)
Epoch: [33][600/616]	Loss 2.4464e-01 (2.4462e-01)	Acc 0.744141 (0.743900)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.737791)
Training Loss of Epoch 33: 0.24474257276794775
Training Acc of Epoch 33: 0.7437960492886179
Testing Acc of Epoch 33: 0.7377913043478261
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4502e-01 (2.4502e-01)	Acc 0.734375 (0.734375)
Epoch: [34][300/616]	Loss 2.5013e-01 (2.4523e-01)	Acc 0.725586 (0.743109)
Epoch: [34][600/616]	Loss 2.5651e-01 (2.4491e-01)	Acc 0.737305 (0.743385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746309)
Training Loss of Epoch 34: 0.24495766439088962
Training Acc of Epoch 34: 0.7433149136178862
Testing Acc of Epoch 34: 0.746308695652174
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4762e-01 (2.4762e-01)	Acc 0.739258 (0.739258)
Epoch: [35][300/616]	Loss 2.3926e-01 (2.4480e-01)	Acc 0.749023 (0.743845)
Epoch: [35][600/616]	Loss 2.4122e-01 (2.4461e-01)	Acc 0.759766 (0.743860)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744117)
Training Loss of Epoch 35: 0.24461145863785008
Training Acc of Epoch 35: 0.743845274390244
Testing Acc of Epoch 35: 0.7441173913043478
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.4086e-01 (2.4086e-01)	Acc 0.738281 (0.738281)
Epoch: [36][300/616]	Loss 2.3985e-01 (2.4509e-01)	Acc 0.745117 (0.742908)
Epoch: [36][600/616]	Loss 2.4423e-01 (2.4400e-01)	Acc 0.750977 (0.744405)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.740774)
Training Loss of Epoch 36: 0.24402270346153074
Training Acc of Epoch 36: 0.7443883384146341
Testing Acc of Epoch 36: 0.7407739130434783
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.6007e-01 (2.6007e-01)	Acc 0.716797 (0.716797)
Epoch: [37][300/616]	Loss 2.4930e-01 (2.4494e-01)	Acc 0.751953 (0.743508)
Epoch: [37][600/616]	Loss 2.4082e-01 (2.4501e-01)	Acc 0.748047 (0.743383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.741096)
Training Loss of Epoch 37: 0.24511147437056874
Training Acc of Epoch 37: 0.7432466336382114
Testing Acc of Epoch 37: 0.741095652173913
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.5313e-01 (2.5313e-01)	Acc 0.746094 (0.746094)
Epoch: [38][300/616]	Loss 2.5258e-01 (2.4381e-01)	Acc 0.736328 (0.744439)
Epoch: [38][600/616]	Loss 2.4457e-01 (2.4426e-01)	Acc 0.733398 (0.743920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741435)
Training Loss of Epoch 38: 0.24420904362104773
Training Acc of Epoch 38: 0.7440024771341464
Testing Acc of Epoch 38: 0.7414347826086957
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.5650e-01 (2.5650e-01)	Acc 0.722656 (0.722656)
Epoch: [39][300/616]	Loss 2.4181e-01 (2.4500e-01)	Acc 0.757812 (0.743664)
Epoch: [39][600/616]	Loss 2.3982e-01 (2.4503e-01)	Acc 0.745117 (0.743808)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.735965)
Training Loss of Epoch 39: 0.24495757459624995
Training Acc of Epoch 39: 0.7438738567073171
Testing Acc of Epoch 39: 0.7359652173913044
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.5328e-01 (2.5328e-01)	Acc 0.739258 (0.739258)
Epoch: [40][300/616]	Loss 2.4227e-01 (2.4481e-01)	Acc 0.745117 (0.743294)
Epoch: [40][600/616]	Loss 2.3488e-01 (2.4430e-01)	Acc 0.752930 (0.743630)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.745257)
Training Loss of Epoch 40: 0.24433570278853906
Training Acc of Epoch 40: 0.7435658028455284
Testing Acc of Epoch 40: 0.7452565217391305
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.5113e-01 (2.5113e-01)	Acc 0.728516 (0.728516)
Epoch: [41][300/616]	Loss 2.4265e-01 (2.4494e-01)	Acc 0.743164 (0.743394)
Epoch: [41][600/616]	Loss 2.4108e-01 (2.4454e-01)	Acc 0.753906 (0.743783)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.739630)
Training Loss of Epoch 41: 0.24456931025516696
Training Acc of Epoch 41: 0.7437928734756097
Testing Acc of Epoch 41: 0.7396304347826087
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.4939e-01 (2.4939e-01)	Acc 0.740234 (0.740234)
Epoch: [42][300/616]	Loss 2.3405e-01 (2.4450e-01)	Acc 0.745117 (0.744257)
Epoch: [42][600/616]	Loss 2.4040e-01 (2.4433e-01)	Acc 0.747070 (0.744256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739852)
Training Loss of Epoch 42: 0.24430040920652996
Training Acc of Epoch 42: 0.7442755970528455
Testing Acc of Epoch 42: 0.7398521739130435
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3247e-01 (2.3247e-01)	Acc 0.765625 (0.765625)
Epoch: [43][300/616]	Loss 2.3446e-01 (2.4482e-01)	Acc 0.750977 (0.742989)
Epoch: [43][600/616]	Loss 2.3397e-01 (2.4474e-01)	Acc 0.742188 (0.743624)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742900)
Training Loss of Epoch 43: 0.24477030150289458
Training Acc of Epoch 43: 0.7436372586382114
Testing Acc of Epoch 43: 0.7429
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.5442e-01 (2.5442e-01)	Acc 0.730469 (0.730469)
Epoch: [44][300/616]	Loss 2.3850e-01 (2.4459e-01)	Acc 0.749023 (0.743459)
Epoch: [44][600/616]	Loss 2.3934e-01 (2.4450e-01)	Acc 0.751953 (0.743585)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746717)
Training Loss of Epoch 44: 0.24450275556343357
Training Acc of Epoch 44: 0.7435340447154472
Testing Acc of Epoch 44: 0.7467173913043478
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3828e-01 (2.3828e-01)	Acc 0.748047 (0.748047)
Epoch: [45][300/616]	Loss 2.4773e-01 (2.4482e-01)	Acc 0.743164 (0.743531)
Epoch: [45][600/616]	Loss 2.5034e-01 (2.4569e-01)	Acc 0.743164 (0.742309)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.724722)
Training Loss of Epoch 45: 0.24621431027970664
Training Acc of Epoch 45: 0.7417587652439024
Testing Acc of Epoch 45: 0.7247217391304348
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4399e-01 (2.4399e-01)	Acc 0.737305 (0.737305)
Epoch: [46][300/616]	Loss 2.2816e-01 (2.4384e-01)	Acc 0.762695 (0.744682)
Epoch: [46][600/616]	Loss 2.3313e-01 (2.4398e-01)	Acc 0.750000 (0.744492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744891)
Training Loss of Epoch 46: 0.24397122392324896
Training Acc of Epoch 46: 0.7444502667682927
Testing Acc of Epoch 46: 0.7448913043478261
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.3901e-01 (2.3901e-01)	Acc 0.748047 (0.748047)
Epoch: [47][300/616]	Loss 2.6756e-01 (2.4507e-01)	Acc 0.727539 (0.743122)
Epoch: [47][600/616]	Loss 2.3756e-01 (2.4464e-01)	Acc 0.749023 (0.743806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742930)
Training Loss of Epoch 47: 0.24449755227178094
Training Acc of Epoch 47: 0.743946900406504
Testing Acc of Epoch 47: 0.7429304347826087
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4361e-01 (2.4361e-01)	Acc 0.747070 (0.747070)
Epoch: [48][300/616]	Loss 2.2536e-01 (2.4422e-01)	Acc 0.773438 (0.744241)
Epoch: [48][600/616]	Loss 2.4920e-01 (2.4446e-01)	Acc 0.734375 (0.743795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.747191)
Training Loss of Epoch 48: 0.24435505430872848
Training Acc of Epoch 48: 0.743967543191057
Testing Acc of Epoch 48: 0.7471913043478261
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4387e-01 (2.4387e-01)	Acc 0.738281 (0.738281)
Epoch: [49][300/616]	Loss 2.5783e-01 (2.4526e-01)	Acc 0.724609 (0.742450)
Epoch: [49][600/616]	Loss 2.4983e-01 (2.4436e-01)	Acc 0.726562 (0.743370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745222)
Training Loss of Epoch 49: 0.2442900003456488
Training Acc of Epoch 49: 0.7435769181910569
Testing Acc of Epoch 49: 0.7452217391304348
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.3433e-01 (2.3433e-01)	Acc 0.775391 (0.775391)
Epoch: [50][300/616]	Loss 2.4677e-01 (2.4554e-01)	Acc 0.748047 (0.742489)
Epoch: [50][600/616]	Loss 2.3207e-01 (2.4483e-01)	Acc 0.756836 (0.743622)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.744943)
Training Loss of Epoch 50: 0.2447694224555318
Training Acc of Epoch 50: 0.7437261814024391
Testing Acc of Epoch 50: 0.7449434782608696
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3140e-01 (2.3140e-01)	Acc 0.769531 (0.769531)
Epoch: [51][300/616]	Loss 2.3680e-01 (2.4325e-01)	Acc 0.754883 (0.745578)
Epoch: [51][600/616]	Loss 2.2825e-01 (2.4389e-01)	Acc 0.772461 (0.744401)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743148)
Training Loss of Epoch 51: 0.24391818211330632
Training Acc of Epoch 51: 0.7443613440040651
Testing Acc of Epoch 51: 0.7431478260869565
Model with the best training loss saved! The loss is 0.24391818211330632
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.3625e-01 (2.3625e-01)	Acc 0.741211 (0.741211)
Epoch: [52][300/616]	Loss 2.3735e-01 (2.4463e-01)	Acc 0.753906 (0.743797)
Epoch: [52][600/616]	Loss 2.4882e-01 (2.4513e-01)	Acc 0.749023 (0.743231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746348)
Training Loss of Epoch 52: 0.24500673151597743
Training Acc of Epoch 52: 0.743408600101626
Testing Acc of Epoch 52: 0.7463478260869565
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5327e-01 (2.5327e-01)	Acc 0.737305 (0.737305)
Epoch: [53][300/616]	Loss 2.6725e-01 (2.4426e-01)	Acc 0.707031 (0.744656)
Epoch: [53][600/616]	Loss 2.3799e-01 (2.4421e-01)	Acc 0.758789 (0.743889)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744091)
Training Loss of Epoch 53: 0.2441686821177723
Training Acc of Epoch 53: 0.7438881478658537
Testing Acc of Epoch 53: 0.7440913043478261
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.5362e-01 (2.5362e-01)	Acc 0.744141 (0.744141)
Epoch: [54][300/616]	Loss 2.3653e-01 (2.4378e-01)	Acc 0.757812 (0.744533)
Epoch: [54][600/616]	Loss 2.3449e-01 (2.4414e-01)	Acc 0.752930 (0.744531)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.742196)
Training Loss of Epoch 54: 0.2443786150798565
Training Acc of Epoch 54: 0.7442835365853658
Testing Acc of Epoch 54: 0.742195652173913
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.5612e-01 (2.5612e-01)	Acc 0.735352 (0.735352)
Epoch: [55][300/616]	Loss 2.5749e-01 (2.4527e-01)	Acc 0.725586 (0.742635)
Epoch: [55][600/616]	Loss 2.4667e-01 (2.4506e-01)	Acc 0.753906 (0.743431)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.743996)
Training Loss of Epoch 55: 0.24508501855823084
Training Acc of Epoch 55: 0.743505462398374
Testing Acc of Epoch 55: 0.7439956521739131
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.3094e-01 (2.3094e-01)	Acc 0.761719 (0.761719)
Epoch: [56][300/616]	Loss 2.5000e-01 (2.4565e-01)	Acc 0.738281 (0.742632)
Epoch: [56][600/616]	Loss 2.4170e-01 (2.4476e-01)	Acc 0.742188 (0.743608)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747922)
Training Loss of Epoch 56: 0.24471945714175217
Training Acc of Epoch 56: 0.7436340828252033
Testing Acc of Epoch 56: 0.7479217391304348
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.2779e-01 (2.2779e-01)	Acc 0.761719 (0.761719)
Epoch: [57][300/616]	Loss 2.3699e-01 (2.4477e-01)	Acc 0.758789 (0.743693)
Epoch: [57][600/616]	Loss 2.3335e-01 (2.4450e-01)	Acc 0.753906 (0.743897)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.745687)
Training Loss of Epoch 57: 0.24441914936391318
Training Acc of Epoch 57: 0.7440072408536585
Testing Acc of Epoch 57: 0.7456869565217391
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.4064e-01 (2.4064e-01)	Acc 0.748047 (0.748047)
Epoch: [58][300/616]	Loss 2.2757e-01 (2.4472e-01)	Acc 0.765625 (0.743343)
Epoch: [58][600/616]	Loss 2.5199e-01 (2.4473e-01)	Acc 0.733398 (0.743619)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.739739)
Training Loss of Epoch 58: 0.24474331874672958
Training Acc of Epoch 58: 0.7436340828252033
Testing Acc of Epoch 58: 0.7397391304347826
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3813e-01 (2.3813e-01)	Acc 0.752930 (0.752930)
Epoch: [59][300/616]	Loss 2.2993e-01 (2.4463e-01)	Acc 0.762695 (0.744296)
Epoch: [59][600/616]	Loss 2.4151e-01 (2.4480e-01)	Acc 0.746094 (0.743549)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746183)
Training Loss of Epoch 59: 0.2446761486733832
Training Acc of Epoch 59: 0.7436229674796748
Testing Acc of Epoch 59: 0.7461826086956522
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3055e-01 (2.3055e-01)	Acc 0.754883 (0.754883)
Epoch: [60][300/616]	Loss 2.6254e-01 (2.4379e-01)	Acc 0.736328 (0.744413)
Epoch: [60][600/616]	Loss 2.4276e-01 (2.4398e-01)	Acc 0.749023 (0.744467)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.740283)
Training Loss of Epoch 60: 0.24394860231294865
Training Acc of Epoch 60: 0.7445296620934959
Testing Acc of Epoch 60: 0.7402826086956522
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.4106e-01 (2.4106e-01)	Acc 0.758789 (0.758789)
Epoch: [61][300/616]	Loss 2.4513e-01 (2.4465e-01)	Acc 0.735352 (0.744196)
Epoch: [61][600/616]	Loss 2.3457e-01 (2.4472e-01)	Acc 0.757812 (0.743834)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744735)
Training Loss of Epoch 61: 0.24467679882437232
Training Acc of Epoch 61: 0.7438897357723577
Testing Acc of Epoch 61: 0.7447347826086956
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4412e-01 (2.4412e-01)	Acc 0.749023 (0.749023)
Epoch: [62][300/616]	Loss 2.4018e-01 (2.4359e-01)	Acc 0.746094 (0.744848)
Epoch: [62][600/616]	Loss 2.4234e-01 (2.4436e-01)	Acc 0.742188 (0.744241)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.742183)
Training Loss of Epoch 62: 0.24440630054086204
Training Acc of Epoch 62: 0.744069169207317
Testing Acc of Epoch 62: 0.7421826086956522
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.6404e-01 (2.6404e-01)	Acc 0.738281 (0.738281)
Epoch: [63][300/616]	Loss 3.1953e+01 (1.2728e+01)	Acc 0.201172 (0.512585)
Epoch: [63][600/616]	Loss 3.2422e+01 (2.2478e+01)	Acc 0.189453 (0.353298)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 63: 22.70042470230804
Training Acc of Epoch 63: 0.3496681275406504
Testing Acc of Epoch 63: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 3.3164e+01 (3.3164e+01)	Acc 0.170898 (0.170898)
Epoch: [64][300/616]	Loss 3.2266e+01 (3.2221e+01)	Acc 0.193359 (0.194485)
Epoch: [64][600/616]	Loss 3.2109e+01 (3.2247e+01)	Acc 0.197266 (0.193816)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 64: 32.247395833333336
Training Acc of Epoch 64: 0.19381510416666667
Testing Acc of Epoch 64: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [65][300/616]	Loss 3.2500e+01 (3.2262e+01)	Acc 0.187500 (0.193453)
Epoch: [65][600/616]	Loss 3.2422e+01 (3.2252e+01)	Acc 0.189453 (0.193707)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 65: 32.24720528455285
Training Acc of Epoch 65: 0.19381986788617886
Testing Acc of Epoch 65: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [66][300/616]	Loss 3.2227e+01 (3.2261e+01)	Acc 0.194336 (0.193486)
Epoch: [66][600/616]	Loss 3.2148e+01 (3.2248e+01)	Acc 0.196289 (0.193788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 66: 32.247141768292686
Training Acc of Epoch 66: 0.19382145579268292
Testing Acc of Epoch 66: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [67][300/616]	Loss 3.2656e+01 (3.2239e+01)	Acc 0.183594 (0.194037)
Epoch: [67][600/616]	Loss 3.2812e+01 (3.2247e+01)	Acc 0.179688 (0.193826)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 67: 32.24803099593496
Training Acc of Epoch 67: 0.193799225101626
Testing Acc of Epoch 67: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [68][300/616]	Loss 3.2344e+01 (3.2287e+01)	Acc 0.191406 (0.192831)
Epoch: [68][600/616]	Loss 3.2383e+01 (3.2249e+01)	Acc 0.190430 (0.193780)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 68: 32.24803099593496
Training Acc of Epoch 68: 0.193799225101626
Testing Acc of Epoch 68: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [69][300/616]	Loss 3.2031e+01 (3.2251e+01)	Acc 0.199219 (0.193720)
Epoch: [69][600/616]	Loss 3.1953e+01 (3.2247e+01)	Acc 0.201172 (0.193814)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 69: 32.24701473577236
Training Acc of Epoch 69: 0.19382463160569105
Testing Acc of Epoch 69: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [70][300/616]	Loss 3.1836e+01 (3.2250e+01)	Acc 0.204102 (0.193762)
Epoch: [70][600/616]	Loss 3.1055e+01 (3.2250e+01)	Acc 0.223633 (0.193741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 70: 32.24784044715447
Training Acc of Epoch 70: 0.19380398882113822
Testing Acc of Epoch 70: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [71][300/616]	Loss 3.2656e+01 (3.2244e+01)	Acc 0.183594 (0.193911)
Epoch: [71][600/616]	Loss 3.2148e+01 (3.2243e+01)	Acc 0.196289 (0.193920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 71: 32.2474593495935
Training Acc of Epoch 71: 0.1938135162601626
Testing Acc of Epoch 71: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [72][300/616]	Loss 3.1484e+01 (3.2251e+01)	Acc 0.212891 (0.193723)
Epoch: [72][600/616]	Loss 3.2578e+01 (3.2250e+01)	Acc 0.185547 (0.193743)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 72: 32.24720528455285
Training Acc of Epoch 72: 0.19381986788617886
Testing Acc of Epoch 72: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.3047e+01 (3.3047e+01)	Acc 0.173828 (0.173828)
Epoch: [73][300/616]	Loss 3.1875e+01 (3.2255e+01)	Acc 0.203125 (0.193625)
Epoch: [73][600/616]	Loss 3.2227e+01 (3.2248e+01)	Acc 0.194336 (0.193790)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 73: 32.24669715447155
Training Acc of Epoch 73: 0.1938325711382114
Testing Acc of Epoch 73: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [74][300/616]	Loss 3.2109e+01 (3.2241e+01)	Acc 0.197266 (0.193963)
Epoch: [74][600/616]	Loss 3.1211e+01 (3.2248e+01)	Acc 0.219727 (0.193805)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 74: 32.24676067073171
Training Acc of Epoch 74: 0.1938309832317073
Testing Acc of Epoch 74: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 3.3047e+01 (3.3047e+01)	Acc 0.173828 (0.173828)
Epoch: [75][300/616]	Loss 3.2344e+01 (3.2211e+01)	Acc 0.191406 (0.194719)
Epoch: [75][600/616]	Loss 3.1836e+01 (3.2253e+01)	Acc 0.204102 (0.193679)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 75: 32.24726880081301
Training Acc of Epoch 75: 0.1938182799796748
Testing Acc of Epoch 75: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [76][300/616]	Loss 3.2773e+01 (3.2235e+01)	Acc 0.180664 (0.194132)
Epoch: [76][600/616]	Loss 3.2383e+01 (3.2252e+01)	Acc 0.190430 (0.193705)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 76: 32.24771341463415
Training Acc of Epoch 76: 0.19380716463414635
Testing Acc of Epoch 76: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [77][300/616]	Loss 3.2461e+01 (3.2241e+01)	Acc 0.188477 (0.193966)
Epoch: [77][600/616]	Loss 3.1680e+01 (3.2247e+01)	Acc 0.208008 (0.193837)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 77: 32.24777693089431
Training Acc of Epoch 77: 0.1938055767276423
Testing Acc of Epoch 77: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [78][300/616]	Loss 3.2227e+01 (3.2263e+01)	Acc 0.194336 (0.193424)
Epoch: [78][600/616]	Loss 3.2344e+01 (3.2248e+01)	Acc 0.191406 (0.193800)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 78: 32.2469512195122
Training Acc of Epoch 78: 0.1938262195121951
Testing Acc of Epoch 78: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [79][300/616]	Loss 3.2617e+01 (3.2251e+01)	Acc 0.184570 (0.193736)
Epoch: [79][600/616]	Loss 3.2148e+01 (3.2243e+01)	Acc 0.196289 (0.193933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 79: 32.24784044715447
Training Acc of Epoch 79: 0.19380398882113822
Testing Acc of Epoch 79: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [80][300/616]	Loss 3.2266e+01 (3.2238e+01)	Acc 0.193359 (0.194054)
Epoch: [80][600/616]	Loss 3.2031e+01 (3.2249e+01)	Acc 0.199219 (0.193779)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 80: 32.247395833333336
Training Acc of Epoch 80: 0.19381510416666667
Testing Acc of Epoch 80: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [81][300/616]	Loss 3.1875e+01 (3.2234e+01)	Acc 0.203125 (0.194141)
Epoch: [81][600/616]	Loss 3.2617e+01 (3.2253e+01)	Acc 0.184570 (0.193670)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 81: 32.248412093495936
Training Acc of Epoch 81: 0.19378969766260162
Testing Acc of Epoch 81: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [82][300/616]	Loss 3.3281e+01 (3.2241e+01)	Acc 0.167969 (0.193966)
Epoch: [82][600/616]	Loss 3.2305e+01 (3.2249e+01)	Acc 0.192383 (0.193780)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 82: 32.24752286585366
Training Acc of Epoch 82: 0.19381192835365854
Testing Acc of Epoch 82: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 3.2578e+01 (3.2578e+01)	Acc 0.185547 (0.185547)
Epoch: [83][300/616]	Loss 3.2812e+01 (3.2272e+01)	Acc 0.179688 (0.193204)
Epoch: [83][600/616]	Loss 3.2344e+01 (3.2245e+01)	Acc 0.191406 (0.193886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 83: 32.247903963414636
Training Acc of Epoch 83: 0.19380240091463416
Testing Acc of Epoch 83: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [84][300/616]	Loss 3.2656e+01 (3.2276e+01)	Acc 0.183594 (0.193100)
Epoch: [84][600/616]	Loss 3.2266e+01 (3.2250e+01)	Acc 0.193359 (0.193746)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 84: 32.24853912601626
Training Acc of Epoch 84: 0.1937865218495935
Testing Acc of Epoch 84: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 3.2852e+01 (3.2852e+01)	Acc 0.178711 (0.178711)
Epoch: [85][300/616]	Loss 3.3203e+01 (3.2273e+01)	Acc 0.169922 (0.193168)
Epoch: [85][600/616]	Loss 3.2500e+01 (3.2248e+01)	Acc 0.187500 (0.193790)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 85: 32.248412093495936
Training Acc of Epoch 85: 0.19378969766260162
Testing Acc of Epoch 85: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [86][300/616]	Loss 3.2852e+01 (3.2246e+01)	Acc 0.178711 (0.193856)
Epoch: [86][600/616]	Loss 3.2188e+01 (3.2245e+01)	Acc 0.195312 (0.193874)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 86: 32.24752286585366
Training Acc of Epoch 86: 0.19381192835365854
Testing Acc of Epoch 86: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.191406 (0.191406)
Epoch: [87][300/616]	Loss 3.2031e+01 (3.2237e+01)	Acc 0.199219 (0.194073)
Epoch: [87][600/616]	Loss 3.2539e+01 (3.2243e+01)	Acc 0.186523 (0.193931)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 87: 32.24758638211382
Training Acc of Epoch 87: 0.19381034044715448
Testing Acc of Epoch 87: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [88][300/616]	Loss 3.1875e+01 (3.2265e+01)	Acc 0.203125 (0.193376)
Epoch: [88][600/616]	Loss 3.2485e+01 (3.2246e+01)	Acc 0.187500 (0.193832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 88: 32.24567204297073
Training Acc of Epoch 88: 0.1938309832317073
Testing Acc of Epoch 88: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.1830e+01 (3.1830e+01)	Acc 0.204102 (0.204102)
Epoch: [89][300/616]	Loss 3.2061e+01 (3.2186e+01)	Acc 0.196289 (0.194073)
Epoch: [89][600/616]	Loss 3.2142e+01 (3.2068e+01)	Acc 0.180664 (0.193829)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 89: 32.059971606246826
Training Acc of Epoch 89: 0.1938055767276423
Testing Acc of Epoch 89: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 3.1619e+01 (3.1619e+01)	Acc 0.191406 (0.191406)
Epoch: [90][300/616]	Loss 2.9952e+01 (3.0943e+01)	Acc 0.197266 (0.193405)
Epoch: [90][600/616]	Loss 3.0793e+01 (3.0477e+01)	Acc 0.166992 (0.193744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 90: 30.461322269594767
Training Acc of Epoch 90: 0.1938182799796748
Testing Acc of Epoch 90: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.9819e+01 (2.9819e+01)	Acc 0.191406 (0.191406)
Epoch: [91][300/616]	Loss 2.9893e+01 (2.9814e+01)	Acc 0.190430 (0.194362)
Epoch: [91][600/616]	Loss 3.0087e+01 (2.9813e+01)	Acc 0.185547 (0.193766)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 91: 29.810124110400192
Training Acc of Epoch 91: 0.19381510416666667
Testing Acc of Epoch 91: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 3.0408e+01 (3.0408e+01)	Acc 0.184570 (0.184570)
Epoch: [92][300/616]	Loss 2.9577e+01 (2.9778e+01)	Acc 0.200195 (0.193642)
Epoch: [92][600/616]	Loss 3.0462e+01 (2.9757e+01)	Acc 0.172852 (0.193832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 92: 29.757622707956205
Training Acc of Epoch 92: 0.19380240091463416
Testing Acc of Epoch 92: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.9439e+01 (2.9439e+01)	Acc 0.203125 (0.203125)
Epoch: [93][300/616]	Loss 3.0315e+01 (2.9731e+01)	Acc 0.188477 (0.193960)
Epoch: [93][600/616]	Loss 2.9808e+01 (2.9730e+01)	Acc 0.188477 (0.193897)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 93: 29.731597925201665
Training Acc of Epoch 93: 0.19381192835365854
Testing Acc of Epoch 93: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.9990e+01 (2.9990e+01)	Acc 0.189453 (0.189453)
Epoch: [94][300/616]	Loss 3.2266e+01 (3.0990e+01)	Acc 0.193359 (0.194923)
Epoch: [94][600/616]	Loss 3.2188e+01 (3.1629e+01)	Acc 0.195312 (0.194091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 94: 31.645524136613055
Training Acc of Epoch 94: 0.19402153201219513
Testing Acc of Epoch 94: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [95][300/616]	Loss 3.2070e+01 (3.2225e+01)	Acc 0.198242 (0.194381)
Epoch: [95][600/616]	Loss 3.2305e+01 (3.2248e+01)	Acc 0.192383 (0.193806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 95: 32.247649898373986
Training Acc of Epoch 95: 0.19380875254065041
Testing Acc of Epoch 95: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [96][300/616]	Loss 3.1914e+01 (3.2243e+01)	Acc 0.202148 (0.193927)
Epoch: [96][600/616]	Loss 3.2617e+01 (3.2248e+01)	Acc 0.184570 (0.193801)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 96: 32.24733231707317
Training Acc of Epoch 96: 0.19381669207317073
Testing Acc of Epoch 96: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.191406 (0.191406)
Epoch: [97][300/616]	Loss 3.1914e+01 (3.2246e+01)	Acc 0.202148 (0.193846)
Epoch: [97][600/616]	Loss 3.1523e+01 (3.2248e+01)	Acc 0.211914 (0.193798)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 97: 32.248602642276424
Training Acc of Epoch 97: 0.19378493394308943
Testing Acc of Epoch 97: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [98][300/616]	Loss 3.3125e+01 (3.2251e+01)	Acc 0.171875 (0.193723)
Epoch: [98][600/616]	Loss 3.2148e+01 (3.2251e+01)	Acc 0.196289 (0.193717)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 98: 32.248158028455286
Training Acc of Epoch 98: 0.19379604928861788
Testing Acc of Epoch 98: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [99][300/616]	Loss 3.2109e+01 (3.2264e+01)	Acc 0.197266 (0.193389)
Epoch: [99][600/616]	Loss 3.1680e+01 (3.2252e+01)	Acc 0.208008 (0.193705)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 99: 32.24758638211382
Training Acc of Epoch 99: 0.19381034044715448
Testing Acc of Epoch 99: 0.19464782608695652
Early stopping not satisfied.
