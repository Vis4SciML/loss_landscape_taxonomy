train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_32b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using FP32 model
---------------------- Model -------------------------
ThreeLayerMLP(
  (dense_1): Linear(in_features=16, out_features=64, bias=True)
  (dense_2): Linear(in_features=64, out_features=32, bias=True)
  (dense_3): Linear(in_features=32, out_features=32, bias=True)
  (dense_4): Linear(in_features=32, out_features=5, bias=True)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0051e-01 (5.0051e-01)	Acc 0.194336 (0.194336)
Epoch: [0][300/616]	Loss 2.5362e-01 (2.7210e-01)	Acc 0.728516 (0.711242)
Epoch: [0][600/616]	Loss 2.4721e-01 (2.6156e-01)	Acc 0.747070 (0.724535)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.743774)
Training Loss of Epoch 0: 0.2611966451493705
Training Acc of Epoch 0: 0.7249126651422764
Testing Acc of Epoch 0: 0.7437739130434783
Model with the best training loss saved! The loss is 0.2611966451493705
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.2623e-01 (2.2623e-01)	Acc 0.778320 (0.778320)
Epoch: [1][300/616]	Loss 2.4144e-01 (2.4542e-01)	Acc 0.745117 (0.743466)
Epoch: [1][600/616]	Loss 2.5719e-01 (2.4535e-01)	Acc 0.728516 (0.743314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.744557)
Training Loss of Epoch 1: 0.24529649419028585
Training Acc of Epoch 1: 0.7433212652439024
Testing Acc of Epoch 1: 0.7445565217391305
Model with the best training loss saved! The loss is 0.24529649419028585
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3501e-01 (2.3501e-01)	Acc 0.761719 (0.761719)
Epoch: [2][300/616]	Loss 2.5231e-01 (2.4416e-01)	Acc 0.733398 (0.744293)
Epoch: [2][600/616]	Loss 2.6908e-01 (2.4436e-01)	Acc 0.711914 (0.744128)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747413)
Training Loss of Epoch 2: 0.2443718521091027
Training Acc of Epoch 2: 0.7441009273373984
Testing Acc of Epoch 2: 0.7474130434782609
Model with the best training loss saved! The loss is 0.2443718521091027
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3630e-01 (2.3630e-01)	Acc 0.752930 (0.752930)
Epoch: [3][300/616]	Loss 2.6254e-01 (2.4405e-01)	Acc 0.720703 (0.744235)
Epoch: [3][600/616]	Loss 2.5259e-01 (2.4365e-01)	Acc 0.750977 (0.744576)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.739443)
Training Loss of Epoch 3: 0.24380178495151242
Training Acc of Epoch 3: 0.744482024898374
Testing Acc of Epoch 3: 0.7394434782608695
Model with the best training loss saved! The loss is 0.24380178495151242
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6342e-01 (2.6342e-01)	Acc 0.718750 (0.718750)
Epoch: [4][300/616]	Loss 2.2381e-01 (2.4293e-01)	Acc 0.773438 (0.744929)
Epoch: [4][600/616]	Loss 2.3795e-01 (2.4275e-01)	Acc 0.767578 (0.745312)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.744322)
Training Loss of Epoch 4: 0.24283320215659412
Training Acc of Epoch 4: 0.7451949949186992
Testing Acc of Epoch 4: 0.7443217391304348
Model with the best training loss saved! The loss is 0.24283320215659412
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.5327e-01 (2.5327e-01)	Acc 0.732422 (0.732422)
Epoch: [5][300/616]	Loss 2.3829e-01 (2.4410e-01)	Acc 0.758789 (0.743969)
Epoch: [5][600/616]	Loss 2.5996e-01 (2.4403e-01)	Acc 0.719727 (0.744163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742665)
Training Loss of Epoch 5: 0.24386235931055333
Training Acc of Epoch 5: 0.7442740091463415
Testing Acc of Epoch 5: 0.7426652173913043
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.2982e-01 (2.2982e-01)	Acc 0.762695 (0.762695)
Epoch: [6][300/616]	Loss 2.3186e-01 (2.4591e-01)	Acc 0.751953 (0.742473)
Epoch: [6][600/616]	Loss 2.4093e-01 (2.4428e-01)	Acc 0.742188 (0.744085)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734700)
Training Loss of Epoch 6: 0.24423518091197904
Training Acc of Epoch 6: 0.7440818724593496
Testing Acc of Epoch 6: 0.7347
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5361e-01 (2.5361e-01)	Acc 0.720703 (0.720703)
Epoch: [7][300/616]	Loss 2.5287e-01 (2.4551e-01)	Acc 0.718750 (0.742272)
Epoch: [7][600/616]	Loss 2.4846e-01 (2.4424e-01)	Acc 0.745117 (0.743723)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747574)
Training Loss of Epoch 7: 0.2443181233192847
Training Acc of Epoch 7: 0.7437325330284553
Testing Acc of Epoch 7: 0.7475739130434783
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4520e-01 (2.4520e-01)	Acc 0.745117 (0.745117)
Epoch: [8][300/616]	Loss 2.5619e-01 (2.4387e-01)	Acc 0.733398 (0.744206)
Epoch: [8][600/616]	Loss 2.3849e-01 (2.4420e-01)	Acc 0.758789 (0.743838)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.749278)
Training Loss of Epoch 8: 0.24415633254419497
Training Acc of Epoch 8: 0.7438817962398374
Testing Acc of Epoch 8: 0.7492782608695652
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3839e-01 (2.3839e-01)	Acc 0.746094 (0.746094)
Epoch: [9][300/616]	Loss 2.3109e-01 (2.4256e-01)	Acc 0.758789 (0.745247)
Epoch: [9][600/616]	Loss 2.3706e-01 (2.4395e-01)	Acc 0.750000 (0.744402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.749543)
Training Loss of Epoch 9: 0.2439214970765075
Training Acc of Epoch 9: 0.7443232342479674
Testing Acc of Epoch 9: 0.7495434782608695
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3606e-01 (2.3606e-01)	Acc 0.757812 (0.757812)
Epoch: [10][300/616]	Loss 2.3827e-01 (2.4356e-01)	Acc 0.750977 (0.744072)
Epoch: [10][600/616]	Loss 2.5382e-01 (2.4382e-01)	Acc 0.723633 (0.743730)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.748674)
Training Loss of Epoch 10: 0.24378967607409005
Training Acc of Epoch 10: 0.7437055386178861
Testing Acc of Epoch 10: 0.7486739130434783
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3036e-01 (2.3036e-01)	Acc 0.759766 (0.759766)
Epoch: [11][300/616]	Loss 2.4345e-01 (2.4330e-01)	Acc 0.748047 (0.744767)
Epoch: [11][600/616]	Loss 2.5821e-01 (2.4463e-01)	Acc 0.720703 (0.743538)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747452)
Training Loss of Epoch 11: 0.24452809085690877
Training Acc of Epoch 11: 0.743626143292683
Testing Acc of Epoch 11: 0.7474521739130435
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4486e-01 (2.4486e-01)	Acc 0.748047 (0.748047)
Epoch: [12][300/616]	Loss 2.5486e-01 (2.4280e-01)	Acc 0.728516 (0.745584)
Epoch: [12][600/616]	Loss 2.4343e-01 (2.4311e-01)	Acc 0.755859 (0.744961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.746130)
Training Loss of Epoch 12: 0.24312372903029125
Training Acc of Epoch 12: 0.744944105691057
Testing Acc of Epoch 12: 0.7461304347826087
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4605e-01 (2.4605e-01)	Acc 0.741211 (0.741211)
Epoch: [13][300/616]	Loss 2.4412e-01 (2.4329e-01)	Acc 0.741211 (0.744004)
Epoch: [13][600/616]	Loss 2.3743e-01 (2.4388e-01)	Acc 0.754883 (0.743765)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.746652)
Training Loss of Epoch 13: 0.24401870134884748
Training Acc of Epoch 13: 0.743580094004065
Testing Acc of Epoch 13: 0.7466521739130435
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.4318e-01 (2.4318e-01)	Acc 0.756836 (0.756836)
Epoch: [14][300/616]	Loss 2.2666e-01 (2.4377e-01)	Acc 0.764648 (0.744157)
Epoch: [14][600/616]	Loss 2.4387e-01 (2.4477e-01)	Acc 0.755859 (0.743507)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747217)
Training Loss of Epoch 14: 0.2448761073069844
Training Acc of Epoch 14: 0.7434467098577235
Testing Acc of Epoch 14: 0.7472173913043478
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4365e-01 (2.4365e-01)	Acc 0.745117 (0.745117)
Epoch: [15][300/616]	Loss 3.8015e-01 (2.6972e-01)	Acc 0.439453 (0.708744)
Epoch: [15][600/616]	Loss 2.5222e-01 (2.8563e-01)	Acc 0.731445 (0.685259)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735587)
Training Loss of Epoch 15: 0.2848643169412768
Training Acc of Epoch 15: 0.6864789761178862
Testing Acc of Epoch 15: 0.7355869565217391
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4658e-01 (2.4658e-01)	Acc 0.743164 (0.743164)
Epoch: [16][300/616]	Loss 2.4870e-01 (2.5067e-01)	Acc 0.733398 (0.737675)
Epoch: [16][600/616]	Loss 2.4979e-01 (2.4923e-01)	Acc 0.740234 (0.740038)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.742874)
Training Loss of Epoch 16: 0.2491751034327639
Training Acc of Epoch 16: 0.7400676448170732
Testing Acc of Epoch 16: 0.7428739130434783
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3671e-01 (2.3671e-01)	Acc 0.754883 (0.754883)
Epoch: [17][300/616]	Loss 2.4443e-01 (2.4551e-01)	Acc 0.742188 (0.743028)
Epoch: [17][600/616]	Loss 2.4709e-01 (2.4566e-01)	Acc 0.750977 (0.742714)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745513)
Training Loss of Epoch 17: 0.2456347762326884
Training Acc of Epoch 17: 0.7427416793699188
Testing Acc of Epoch 17: 0.7455130434782609
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3725e-01 (2.3725e-01)	Acc 0.754883 (0.754883)
Epoch: [18][300/616]	Loss 2.4595e-01 (2.4539e-01)	Acc 0.750977 (0.743910)
Epoch: [18][600/616]	Loss 2.4601e-01 (2.4555e-01)	Acc 0.738281 (0.743172)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746074)
Training Loss of Epoch 18: 0.2454405442001374
Training Acc of Epoch 18: 0.7432545731707317
Testing Acc of Epoch 18: 0.7460739130434783
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.5408e-01 (2.5408e-01)	Acc 0.735352 (0.735352)
Epoch: [19][300/616]	Loss 2.5089e-01 (2.4680e-01)	Acc 0.748047 (0.741889)
Epoch: [19][600/616]	Loss 2.4677e-01 (2.4563e-01)	Acc 0.751953 (0.742889)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745713)
Training Loss of Epoch 19: 0.2455503437334929
Training Acc of Epoch 19: 0.7429798653455284
Testing Acc of Epoch 19: 0.7457130434782608
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.2959e-01 (2.2959e-01)	Acc 0.755859 (0.755859)
Epoch: [20][300/616]	Loss 2.3084e-01 (2.4520e-01)	Acc 0.758789 (0.742457)
Epoch: [20][600/616]	Loss 2.6219e-01 (2.4543e-01)	Acc 0.713867 (0.742444)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746148)
Training Loss of Epoch 20: 0.2454213283895477
Training Acc of Epoch 20: 0.7424971417682927
Testing Acc of Epoch 20: 0.7461478260869565
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4458e-01 (2.4458e-01)	Acc 0.750000 (0.750000)
Epoch: [21][300/616]	Loss 2.4447e-01 (2.4580e-01)	Acc 0.741211 (0.742457)
Epoch: [21][600/616]	Loss 2.2618e-01 (2.4508e-01)	Acc 0.779297 (0.743434)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.742591)
Training Loss of Epoch 21: 0.24507429180591087
Training Acc of Epoch 21: 0.7434308307926829
Testing Acc of Epoch 21: 0.742591304347826
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.2512e-01 (2.2512e-01)	Acc 0.771484 (0.771484)
Epoch: [22][300/616]	Loss 2.3834e-01 (2.4504e-01)	Acc 0.747070 (0.743086)
Epoch: [22][600/616]	Loss 2.4353e-01 (2.4590e-01)	Acc 0.747070 (0.742381)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.746265)
Training Loss of Epoch 22: 0.24592251026533485
Training Acc of Epoch 22: 0.7423526422764227
Testing Acc of Epoch 22: 0.7462652173913044
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.6268e-01 (2.6268e-01)	Acc 0.723633 (0.723633)
Epoch: [23][300/616]	Loss 2.4004e-01 (2.4591e-01)	Acc 0.753906 (0.742544)
Epoch: [23][600/616]	Loss 2.5282e-01 (2.4536e-01)	Acc 0.727539 (0.743203)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740939)
Training Loss of Epoch 23: 0.24534032793064428
Training Acc of Epoch 23: 0.743262512703252
Testing Acc of Epoch 23: 0.7409391304347827
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.3660e-01 (2.3660e-01)	Acc 0.744141 (0.744141)
Epoch: [24][300/616]	Loss 2.3605e-01 (2.4496e-01)	Acc 0.755859 (0.743592)
Epoch: [24][600/616]	Loss 2.3448e-01 (2.4507e-01)	Acc 0.756836 (0.743260)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748135)
Training Loss of Epoch 24: 0.2450886819178496
Training Acc of Epoch 24: 0.7433149136178862
Testing Acc of Epoch 24: 0.7481347826086957
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4648e-01 (2.4648e-01)	Acc 0.735352 (0.735352)
Epoch: [25][300/616]	Loss 2.4124e-01 (2.4379e-01)	Acc 0.745117 (0.744374)
Epoch: [25][600/616]	Loss 2.4646e-01 (2.4415e-01)	Acc 0.746094 (0.744162)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.749874)
Training Loss of Epoch 25: 0.244098727780629
Training Acc of Epoch 25: 0.744239075203252
Testing Acc of Epoch 25: 0.7498739130434783
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4588e-01 (2.4588e-01)	Acc 0.738281 (0.738281)
Epoch: [26][300/616]	Loss 2.3601e-01 (2.4423e-01)	Acc 0.757812 (0.744167)
Epoch: [26][600/616]	Loss 2.3531e-01 (2.4397e-01)	Acc 0.760742 (0.744344)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747235)
Training Loss of Epoch 26: 0.24402071014652407
Training Acc of Epoch 26: 0.7442486026422764
Testing Acc of Epoch 26: 0.7472347826086957
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4222e-01 (2.4222e-01)	Acc 0.735352 (0.735352)
Epoch: [27][300/616]	Loss 2.3612e-01 (2.4563e-01)	Acc 0.748047 (0.742905)
Epoch: [27][600/616]	Loss 2.5053e-01 (2.4785e-01)	Acc 0.738281 (0.740870)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.745648)
Training Loss of Epoch 27: 0.24776936517498357
Training Acc of Epoch 27: 0.7409139989837399
Testing Acc of Epoch 27: 0.7456478260869566
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.5092e-01 (2.5092e-01)	Acc 0.730469 (0.730469)
Epoch: [28][300/616]	Loss 2.4400e-01 (2.4629e-01)	Acc 0.747070 (0.741753)
Epoch: [28][600/616]	Loss 2.4233e-01 (2.4535e-01)	Acc 0.752930 (0.742839)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.735843)
Training Loss of Epoch 28: 0.2452695388861788
Training Acc of Epoch 28: 0.7429751016260162
Testing Acc of Epoch 28: 0.7358434782608696
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.6068e-01 (2.6068e-01)	Acc 0.722656 (0.722656)
Epoch: [29][300/616]	Loss 2.3954e-01 (2.4357e-01)	Acc 0.746094 (0.744102)
Epoch: [29][600/616]	Loss 2.3418e-01 (2.4637e-01)	Acc 0.750977 (0.741760)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.745948)
Training Loss of Epoch 29: 0.2462307173789032
Training Acc of Epoch 29: 0.7419112042682927
Testing Acc of Epoch 29: 0.7459478260869565
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4250e-01 (2.4250e-01)	Acc 0.748047 (0.748047)
Epoch: [30][300/616]	Loss 2.4472e-01 (2.4294e-01)	Acc 0.739258 (0.744916)
Epoch: [30][600/616]	Loss 2.4420e-01 (2.4364e-01)	Acc 0.748047 (0.744318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743783)
Training Loss of Epoch 30: 0.24371608125000466
Training Acc of Epoch 30: 0.7442073170731708
Testing Acc of Epoch 30: 0.7437826086956522
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3114e-01 (2.3114e-01)	Acc 0.764648 (0.764648)
Epoch: [31][300/616]	Loss 2.3915e-01 (2.4543e-01)	Acc 0.742188 (0.742340)
Epoch: [31][600/616]	Loss 2.6237e-01 (2.4484e-01)	Acc 0.732422 (0.743133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.744300)
Training Loss of Epoch 31: 0.2449237656060273
Training Acc of Epoch 31: 0.7429814532520326
Testing Acc of Epoch 31: 0.7443
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.2972e-01 (2.2972e-01)	Acc 0.773438 (0.773438)
Epoch: [32][300/616]	Loss 2.4047e-01 (2.4414e-01)	Acc 0.748047 (0.744523)
Epoch: [32][600/616]	Loss 2.4732e-01 (2.4469e-01)	Acc 0.738281 (0.743421)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.742670)
Training Loss of Epoch 32: 0.2446689904947591
Training Acc of Epoch 32: 0.7434641768292682
Testing Acc of Epoch 32: 0.7426695652173914
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.3975e-01 (2.3975e-01)	Acc 0.738281 (0.738281)
Epoch: [33][300/616]	Loss 2.5543e-01 (2.4547e-01)	Acc 0.724609 (0.743300)
Epoch: [33][600/616]	Loss 2.4545e-01 (2.4465e-01)	Acc 0.733398 (0.743728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.743417)
Training Loss of Epoch 33: 0.24458573630185632
Training Acc of Epoch 33: 0.7437357088414634
Testing Acc of Epoch 33: 0.7434173913043478
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3742e-01 (2.3742e-01)	Acc 0.754883 (0.754883)
Epoch: [34][300/616]	Loss 2.4289e-01 (2.5666e-01)	Acc 0.748047 (0.731987)
Epoch: [34][600/616]	Loss 2.5261e-01 (2.5109e-01)	Acc 0.729492 (0.737519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744591)
Training Loss of Epoch 34: 0.25096149909787063
Training Acc of Epoch 34: 0.7376651422764228
Testing Acc of Epoch 34: 0.744591304347826
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2946e-01 (2.2946e-01)	Acc 0.759766 (0.759766)
Epoch: [35][300/616]	Loss 2.4145e-01 (2.4603e-01)	Acc 0.747070 (0.742363)
Epoch: [35][600/616]	Loss 2.3262e-01 (2.4583e-01)	Acc 0.761719 (0.742212)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.747604)
Training Loss of Epoch 35: 0.24567866817237885
Training Acc of Epoch 35: 0.7423240599593496
Testing Acc of Epoch 35: 0.747604347826087
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.5141e-01 (2.5141e-01)	Acc 0.750977 (0.750977)
Epoch: [36][300/616]	Loss 2.3212e-01 (2.4418e-01)	Acc 0.762695 (0.744689)
Epoch: [36][600/616]	Loss 2.3054e-01 (2.4418e-01)	Acc 0.762695 (0.744318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743096)
Training Loss of Epoch 36: 0.24420346164606452
Training Acc of Epoch 36: 0.7442978277439024
Testing Acc of Epoch 36: 0.743095652173913
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3556e-01 (2.3556e-01)	Acc 0.742188 (0.742188)
Epoch: [37][300/616]	Loss 2.2806e-01 (2.4404e-01)	Acc 0.763672 (0.744572)
Epoch: [37][600/616]	Loss 2.4493e-01 (2.4395e-01)	Acc 0.736328 (0.744368)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748452)
Training Loss of Epoch 37: 0.2439060252129547
Training Acc of Epoch 37: 0.744410569105691
Testing Acc of Epoch 37: 0.7484521739130435
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3107e-01 (2.3107e-01)	Acc 0.752930 (0.752930)
Epoch: [38][300/616]	Loss 2.4974e-01 (2.7748e-01)	Acc 0.734375 (0.704241)
Epoch: [38][600/616]	Loss 2.4166e-01 (2.6281e-01)	Acc 0.754883 (0.722726)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745809)
Training Loss of Epoch 38: 0.26246811199963577
Training Acc of Epoch 38: 0.7231516768292683
Testing Acc of Epoch 38: 0.7458086956521739
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.6165e-01 (2.6165e-01)	Acc 0.726562 (0.726562)
Epoch: [39][300/616]	Loss 2.3983e-01 (2.4683e-01)	Acc 0.763672 (0.741730)
Epoch: [39][600/616]	Loss 2.5254e-01 (2.4591e-01)	Acc 0.738281 (0.743175)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742783)
Training Loss of Epoch 39: 0.24593427433231013
Training Acc of Epoch 39: 0.7431100736788618
Testing Acc of Epoch 39: 0.7427826086956522
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4363e-01 (2.4363e-01)	Acc 0.733398 (0.733398)
Epoch: [40][300/616]	Loss 2.6060e-01 (2.4633e-01)	Acc 0.730469 (0.742243)
Epoch: [40][600/616]	Loss 2.4779e-01 (2.4596e-01)	Acc 0.726562 (0.742589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.745757)
Training Loss of Epoch 40: 0.2460677424824335
Training Acc of Epoch 40: 0.7425717733739837
Testing Acc of Epoch 40: 0.7457565217391304
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4927e-01 (2.4927e-01)	Acc 0.744141 (0.744141)
Epoch: [41][300/616]	Loss 2.4361e-01 (2.4565e-01)	Acc 0.758789 (0.742613)
Epoch: [41][600/616]	Loss 2.6672e-01 (2.4640e-01)	Acc 0.738281 (0.741963)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.731978)
Training Loss of Epoch 41: 0.24687086498834254
Training Acc of Epoch 41: 0.7414189532520326
Testing Acc of Epoch 41: 0.7319782608695652
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.6533e-01 (2.6533e-01)	Acc 0.728516 (0.728516)
Epoch: [42][300/616]	Loss 2.4026e-01 (2.4760e-01)	Acc 0.734375 (0.740283)
Epoch: [42][600/616]	Loss 2.3325e-01 (2.4689e-01)	Acc 0.763672 (0.741437)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744226)
Training Loss of Epoch 42: 0.24678649597051666
Training Acc of Epoch 42: 0.7416476117886179
Testing Acc of Epoch 42: 0.7442260869565217
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3763e-01 (2.3763e-01)	Acc 0.743164 (0.743164)
Epoch: [43][300/616]	Loss 2.4382e-01 (2.4721e-01)	Acc 0.757812 (0.742032)
Epoch: [43][600/616]	Loss 2.3143e-01 (2.4671e-01)	Acc 0.752930 (0.742071)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739809)
Training Loss of Epoch 43: 0.24670268285080668
Training Acc of Epoch 43: 0.7420779344512195
Testing Acc of Epoch 43: 0.7398086956521739
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4343e-01 (2.4343e-01)	Acc 0.750977 (0.750977)
Epoch: [44][300/616]	Loss 2.5995e-01 (2.4551e-01)	Acc 0.720703 (0.743453)
Epoch: [44][600/616]	Loss 2.5538e-01 (2.4708e-01)	Acc 0.730469 (0.741310)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.744683)
Training Loss of Epoch 44: 0.24705969146112117
Training Acc of Epoch 44: 0.7413268546747968
Testing Acc of Epoch 44: 0.7446826086956522
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.5629e-01 (2.5629e-01)	Acc 0.727539 (0.727539)
Epoch: [45][300/616]	Loss 2.5606e-01 (2.4547e-01)	Acc 0.733398 (0.743051)
Epoch: [45][600/616]	Loss 2.4109e-01 (2.4565e-01)	Acc 0.744141 (0.743026)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744087)
Training Loss of Epoch 45: 0.24563485820118974
Training Acc of Epoch 45: 0.7430433816056911
Testing Acc of Epoch 45: 0.7440869565217392
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4628e-01 (2.4628e-01)	Acc 0.729492 (0.729492)
Epoch: [46][300/616]	Loss 2.4419e-01 (2.4607e-01)	Acc 0.749023 (0.742557)
Epoch: [46][600/616]	Loss 2.4654e-01 (2.4584e-01)	Acc 0.742188 (0.742620)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744239)
Training Loss of Epoch 46: 0.24592810692341346
Training Acc of Epoch 46: 0.7425844766260162
Testing Acc of Epoch 46: 0.7442391304347826
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.5165e-01 (2.5165e-01)	Acc 0.725586 (0.725586)
Epoch: [47][300/616]	Loss 2.5774e-01 (2.4602e-01)	Acc 0.739258 (0.742522)
Epoch: [47][600/616]	Loss 2.5020e-01 (2.4560e-01)	Acc 0.744141 (0.742982)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740470)
Training Loss of Epoch 47: 0.24574341034986139
Training Acc of Epoch 47: 0.7427813770325203
Testing Acc of Epoch 47: 0.7404695652173913
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.5977e-01 (2.5977e-01)	Acc 0.722656 (0.722656)
Epoch: [48][300/616]	Loss 2.4064e-01 (2.4576e-01)	Acc 0.751953 (0.743330)
Epoch: [48][600/616]	Loss 2.4451e-01 (2.4669e-01)	Acc 0.744141 (0.742360)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.747043)
Training Loss of Epoch 48: 0.24671371942613182
Training Acc of Epoch 48: 0.7423288236788618
Testing Acc of Epoch 48: 0.7470434782608696
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.5185e-01 (2.5185e-01)	Acc 0.732422 (0.732422)
Epoch: [49][300/616]	Loss 2.4226e-01 (2.4690e-01)	Acc 0.753906 (0.741750)
Epoch: [49][600/616]	Loss 2.4312e-01 (2.4593e-01)	Acc 0.738281 (0.742719)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.746530)
Training Loss of Epoch 49: 0.245881795713572
Training Acc of Epoch 49: 0.7427400914634147
Testing Acc of Epoch 49: 0.7465304347826087
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.7016e-01 (2.7016e-01)	Acc 0.710938 (0.710938)
Epoch: [50][300/616]	Loss 2.4895e-01 (2.4569e-01)	Acc 0.748047 (0.742937)
Epoch: [50][600/616]	Loss 2.3391e-01 (2.4701e-01)	Acc 0.750977 (0.741518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.743352)
Training Loss of Epoch 50: 0.24688109891201424
Training Acc of Epoch 50: 0.7416523755081301
Testing Acc of Epoch 50: 0.7433521739130434
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.5706e-01 (2.5706e-01)	Acc 0.720703 (0.720703)
Epoch: [51][300/616]	Loss 2.4591e-01 (2.4709e-01)	Acc 0.745117 (0.740938)
Epoch: [51][600/616]	Loss 2.4149e-01 (2.4651e-01)	Acc 0.733398 (0.741812)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.743057)
Training Loss of Epoch 51: 0.2464960972225763
Training Acc of Epoch 51: 0.7417428861788617
Testing Acc of Epoch 51: 0.7430565217391304
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4639e-01 (2.4639e-01)	Acc 0.740234 (0.740234)
Epoch: [52][300/616]	Loss 2.2847e-01 (2.4729e-01)	Acc 0.757812 (0.741386)
Epoch: [52][600/616]	Loss 2.4057e-01 (2.4691e-01)	Acc 0.746094 (0.742163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.746148)
Training Loss of Epoch 52: 0.24676705102125804
Training Acc of Epoch 52: 0.7423002413617886
Testing Acc of Epoch 52: 0.7461478260869565
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3795e-01 (2.3795e-01)	Acc 0.755859 (0.755859)
Epoch: [53][300/616]	Loss 2.3567e-01 (2.4548e-01)	Acc 0.752930 (0.743667)
Epoch: [53][600/616]	Loss 2.6031e-01 (2.4573e-01)	Acc 0.726562 (0.743185)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.743317)
Training Loss of Epoch 53: 0.24576806944560228
Training Acc of Epoch 53: 0.7431354801829269
Testing Acc of Epoch 53: 0.7433173913043478
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.4543e-01 (2.4543e-01)	Acc 0.753906 (0.753906)
Epoch: [54][300/616]	Loss 2.5244e-01 (2.4623e-01)	Acc 0.739258 (0.742298)
Epoch: [54][600/616]	Loss 2.2714e-01 (2.4579e-01)	Acc 0.772461 (0.742732)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.741478)
Training Loss of Epoch 54: 0.24574608654995275
Training Acc of Epoch 54: 0.7427623221544716
Testing Acc of Epoch 54: 0.7414782608695653
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.5009e-01 (2.5009e-01)	Acc 0.728516 (0.728516)
Epoch: [55][300/616]	Loss 2.3609e-01 (2.4632e-01)	Acc 0.758789 (0.742694)
Epoch: [55][600/616]	Loss 2.4804e-01 (2.4679e-01)	Acc 0.736328 (0.742340)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744135)
Training Loss of Epoch 55: 0.24677158494790394
Training Acc of Epoch 55: 0.7423574059959349
Testing Acc of Epoch 55: 0.7441347826086957
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5158e-01 (2.5158e-01)	Acc 0.735352 (0.735352)
Epoch: [56][300/616]	Loss 2.4216e-01 (2.4552e-01)	Acc 0.751953 (0.743365)
Epoch: [56][600/616]	Loss 2.4757e-01 (2.4574e-01)	Acc 0.744141 (0.742670)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.745500)
Training Loss of Epoch 56: 0.24573250551049303
Training Acc of Epoch 56: 0.7426130589430894
Testing Acc of Epoch 56: 0.7455
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3868e-01 (2.3868e-01)	Acc 0.748047 (0.748047)
Epoch: [57][300/616]	Loss 2.4240e-01 (2.4557e-01)	Acc 0.750000 (0.742859)
Epoch: [57][600/616]	Loss 2.4013e-01 (2.4578e-01)	Acc 0.750977 (0.742781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745526)
Training Loss of Epoch 57: 0.2457342671911891
Training Acc of Epoch 57: 0.7427702616869919
Testing Acc of Epoch 57: 0.7455260869565218
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3604e-01 (2.3604e-01)	Acc 0.760742 (0.760742)
Epoch: [58][300/616]	Loss 2.5777e-01 (2.4548e-01)	Acc 0.714844 (0.743407)
Epoch: [58][600/616]	Loss 2.5828e-01 (2.4639e-01)	Acc 0.742188 (0.742049)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.737283)
Training Loss of Epoch 58: 0.24652224464145134
Training Acc of Epoch 58: 0.7418508638211382
Testing Acc of Epoch 58: 0.7372826086956522
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.5206e-01 (2.5206e-01)	Acc 0.744141 (0.744141)
Epoch: [59][300/616]	Loss 2.4418e-01 (2.4615e-01)	Acc 0.740234 (0.742973)
Epoch: [59][600/616]	Loss 2.2068e-01 (2.4639e-01)	Acc 0.764648 (0.742477)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.736735)
Training Loss of Epoch 59: 0.24649565500941703
Training Acc of Epoch 59: 0.7423081808943089
Testing Acc of Epoch 59: 0.7367347826086956
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5147e-01 (2.5147e-01)	Acc 0.741211 (0.741211)
Epoch: [60][300/616]	Loss 2.4256e-01 (2.4566e-01)	Acc 0.744141 (0.743239)
Epoch: [60][600/616]	Loss 2.6963e-01 (2.4626e-01)	Acc 0.718750 (0.742363)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746400)
Training Loss of Epoch 60: 0.2462061041254338
Training Acc of Epoch 60: 0.7424288617886179
Testing Acc of Epoch 60: 0.7464
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.5490e-01 (2.5490e-01)	Acc 0.732422 (0.732422)
Epoch: [61][300/616]	Loss 2.4793e-01 (2.4637e-01)	Acc 0.738281 (0.741886)
Epoch: [61][600/616]	Loss 2.4229e-01 (2.4805e-01)	Acc 0.742188 (0.740467)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736757)
Training Loss of Epoch 61: 0.24800213827350276
Training Acc of Epoch 61: 0.7404630335365854
Testing Acc of Epoch 61: 0.7367565217391304
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.6075e-01 (2.6075e-01)	Acc 0.723633 (0.723633)
Epoch: [62][300/616]	Loss 2.4036e-01 (2.4627e-01)	Acc 0.751953 (0.742457)
Epoch: [62][600/616]	Loss 2.4517e-01 (2.4623e-01)	Acc 0.742188 (0.742400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.738730)
Training Loss of Epoch 62: 0.2462348756024508
Training Acc of Epoch 62: 0.742360581808943
Testing Acc of Epoch 62: 0.7387304347826087
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4521e-01 (2.4521e-01)	Acc 0.748047 (0.748047)
Epoch: [63][300/616]	Loss 2.4174e-01 (2.4614e-01)	Acc 0.750977 (0.741986)
Epoch: [63][600/616]	Loss 2.4166e-01 (2.4579e-01)	Acc 0.745117 (0.742837)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745174)
Training Loss of Epoch 63: 0.24572071418529604
Training Acc of Epoch 63: 0.7428845909552846
Testing Acc of Epoch 63: 0.7451739130434782
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5184e-01 (2.5184e-01)	Acc 0.729492 (0.729492)
Epoch: [64][300/616]	Loss 2.5138e-01 (2.4772e-01)	Acc 0.729492 (0.740526)
Epoch: [64][600/616]	Loss 2.4975e-01 (2.4694e-01)	Acc 0.754883 (0.741785)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743857)
Training Loss of Epoch 64: 0.2468460489094742
Training Acc of Epoch 64: 0.7418429242886179
Testing Acc of Epoch 64: 0.7438565217391304
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3347e-01 (2.3347e-01)	Acc 0.766602 (0.766602)
Epoch: [65][300/616]	Loss 2.5226e-01 (2.4495e-01)	Acc 0.744141 (0.743375)
Epoch: [65][600/616]	Loss 2.4209e-01 (2.4571e-01)	Acc 0.746094 (0.742870)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.743913)
Training Loss of Epoch 65: 0.2456291049961152
Training Acc of Epoch 65: 0.7430100355691057
Testing Acc of Epoch 65: 0.7439130434782608
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4317e-01 (2.4317e-01)	Acc 0.736328 (0.736328)
Epoch: [66][300/616]	Loss 2.2108e-01 (2.4593e-01)	Acc 0.781250 (0.742356)
Epoch: [66][600/616]	Loss 2.4640e-01 (2.4646e-01)	Acc 0.736328 (0.742301)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744178)
Training Loss of Epoch 66: 0.24641418692057696
Training Acc of Epoch 66: 0.7423796366869919
Testing Acc of Epoch 66: 0.7441782608695652
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.5285e-01 (2.5285e-01)	Acc 0.728516 (0.728516)
Epoch: [67][300/616]	Loss 2.4405e-01 (2.4547e-01)	Acc 0.740234 (0.743300)
Epoch: [67][600/616]	Loss 2.2991e-01 (2.4592e-01)	Acc 0.755859 (0.742642)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.743148)
Training Loss of Epoch 67: 0.24584526727354625
Training Acc of Epoch 67: 0.7426892784552845
Testing Acc of Epoch 67: 0.7431478260869565
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3310e-01 (2.3310e-01)	Acc 0.762695 (0.762695)
Epoch: [68][300/616]	Loss 2.4129e-01 (2.4560e-01)	Acc 0.755859 (0.743063)
Epoch: [68][600/616]	Loss 2.5412e-01 (2.4580e-01)	Acc 0.731445 (0.742672)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.746074)
Training Loss of Epoch 68: 0.24588584415311734
Training Acc of Epoch 68: 0.7426178226626017
Testing Acc of Epoch 68: 0.7460739130434783
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4069e-01 (2.4069e-01)	Acc 0.754883 (0.754883)
Epoch: [69][300/616]	Loss 2.5289e-01 (2.6884e-01)	Acc 0.751953 (0.716018)
Epoch: [69][600/616]	Loss 2.5598e-01 (2.5725e-01)	Acc 0.723633 (0.729505)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.744483)
Training Loss of Epoch 69: 0.25698091974103354
Training Acc of Epoch 69: 0.7298367632113821
Testing Acc of Epoch 69: 0.7444826086956522
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4720e-01 (2.4720e-01)	Acc 0.740234 (0.740234)
Epoch: [70][300/616]	Loss 2.4323e-01 (2.4613e-01)	Acc 0.739258 (0.742467)
Epoch: [70][600/616]	Loss 2.6451e-01 (2.4658e-01)	Acc 0.730469 (0.742152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.739483)
Training Loss of Epoch 70: 0.24664120405185513
Training Acc of Epoch 70: 0.7421573297764228
Testing Acc of Epoch 70: 0.7394826086956522
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5814e-01 (2.5814e-01)	Acc 0.728516 (0.728516)
Epoch: [71][300/616]	Loss 2.4477e-01 (2.4570e-01)	Acc 0.738281 (0.742943)
Epoch: [71][600/616]	Loss 2.4748e-01 (2.4617e-01)	Acc 0.746094 (0.742683)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.739448)
Training Loss of Epoch 71: 0.24625832154983426
Training Acc of Epoch 71: 0.7425971798780487
Testing Acc of Epoch 71: 0.7394478260869565
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4129e-01 (2.4129e-01)	Acc 0.736328 (0.736328)
Epoch: [72][300/616]	Loss 2.2054e-01 (2.4751e-01)	Acc 0.776367 (0.741386)
Epoch: [72][600/616]	Loss 2.4406e-01 (2.4701e-01)	Acc 0.751953 (0.741632)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745570)
Training Loss of Epoch 72: 0.24698256881741004
Training Acc of Epoch 72: 0.7416523755081301
Testing Acc of Epoch 72: 0.7455695652173913
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4124e-01 (2.4124e-01)	Acc 0.748047 (0.748047)
Epoch: [73][300/616]	Loss 2.3496e-01 (2.4552e-01)	Acc 0.752930 (0.743145)
Epoch: [73][600/616]	Loss 2.3327e-01 (2.4586e-01)	Acc 0.765625 (0.742857)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748174)
Training Loss of Epoch 73: 0.245927490378783
Training Acc of Epoch 73: 0.7426940421747967
Testing Acc of Epoch 73: 0.7481739130434782
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3998e-01 (2.3998e-01)	Acc 0.750000 (0.750000)
Epoch: [74][300/616]	Loss 2.5043e-01 (2.4690e-01)	Acc 0.734375 (0.741211)
Epoch: [74][600/616]	Loss 2.5611e-01 (2.4615e-01)	Acc 0.732422 (0.742330)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745643)
Training Loss of Epoch 74: 0.24625560803626612
Training Acc of Epoch 74: 0.7422002032520325
Testing Acc of Epoch 74: 0.7456434782608695
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.3001e-01 (2.3001e-01)	Acc 0.768555 (0.768555)
Epoch: [75][300/616]	Loss 2.4456e-01 (2.3923e-01)	Acc 0.735352 (0.748456)
Epoch: [75][600/616]	Loss 2.2903e-01 (2.3888e-01)	Acc 0.767578 (0.748846)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750735)
Training Loss of Epoch 75: 0.23880065337913792
Training Acc of Epoch 75: 0.748974212398374
Testing Acc of Epoch 75: 0.7507347826086956
Model with the best training loss saved! The loss is 0.23880065337913792
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2712e-01 (2.2712e-01)	Acc 0.757812 (0.757812)
Epoch: [76][300/616]	Loss 2.3546e-01 (2.3770e-01)	Acc 0.749023 (0.749906)
Epoch: [76][600/616]	Loss 2.4009e-01 (2.3750e-01)	Acc 0.746094 (0.749730)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.749887)
Training Loss of Epoch 76: 0.23747973861248511
Training Acc of Epoch 76: 0.7497745172764227
Testing Acc of Epoch 76: 0.7498869565217391
Model with the best training loss saved! The loss is 0.23747973861248511
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4307e-01 (2.4307e-01)	Acc 0.732422 (0.732422)
Epoch: [77][300/616]	Loss 2.3586e-01 (2.3627e-01)	Acc 0.746094 (0.751087)
Epoch: [77][600/616]	Loss 2.3776e-01 (2.3667e-01)	Acc 0.748047 (0.750286)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.750635)
Training Loss of Epoch 77: 0.23666428385711297
Training Acc of Epoch 77: 0.7503429878048781
Testing Acc of Epoch 77: 0.7506347826086956
Model with the best training loss saved! The loss is 0.23666428385711297
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3566e-01 (2.3566e-01)	Acc 0.753906 (0.753906)
Epoch: [78][300/616]	Loss 2.3300e-01 (2.3676e-01)	Acc 0.759766 (0.749685)
Epoch: [78][600/616]	Loss 2.4453e-01 (2.3638e-01)	Acc 0.735352 (0.750421)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.751596)
Training Loss of Epoch 78: 0.23638795333180002
Training Acc of Epoch 78: 0.750366806402439
Testing Acc of Epoch 78: 0.751595652173913
Model with the best training loss saved! The loss is 0.23638795333180002
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2164e-01 (2.2164e-01)	Acc 0.770508 (0.770508)
Epoch: [79][300/616]	Loss 2.2869e-01 (2.3685e-01)	Acc 0.759766 (0.749575)
Epoch: [79][600/616]	Loss 2.5261e-01 (2.3612e-01)	Acc 0.723633 (0.750600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.752470)
Training Loss of Epoch 79: 0.23602531583813147
Training Acc of Epoch 79: 0.7507669588414634
Testing Acc of Epoch 79: 0.7524695652173913
Model with the best training loss saved! The loss is 0.23602531583813147
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3427e-01 (2.3427e-01)	Acc 0.750977 (0.750977)
Epoch: [80][300/616]	Loss 2.2225e-01 (2.3518e-01)	Acc 0.777344 (0.751843)
Epoch: [80][600/616]	Loss 2.2650e-01 (2.3583e-01)	Acc 0.751953 (0.750752)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.751996)
Training Loss of Epoch 80: 0.23581652299660008
Training Acc of Epoch 80: 0.7507907774390243
Testing Acc of Epoch 80: 0.7519956521739131
Model with the best training loss saved! The loss is 0.23581652299660008
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2118e-01 (2.2118e-01)	Acc 0.769531 (0.769531)
Epoch: [81][300/616]	Loss 2.2467e-01 (2.3596e-01)	Acc 0.765625 (0.750866)
Epoch: [81][600/616]	Loss 2.4633e-01 (2.3574e-01)	Acc 0.743164 (0.750874)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751570)
Training Loss of Epoch 81: 0.2357019645654089
Training Acc of Epoch 81: 0.7509654471544716
Testing Acc of Epoch 81: 0.7515695652173913
Model with the best training loss saved! The loss is 0.2357019645654089
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3158e-01 (2.3158e-01)	Acc 0.753906 (0.753906)
Epoch: [82][300/616]	Loss 2.4057e-01 (2.3517e-01)	Acc 0.745117 (0.751593)
Epoch: [82][600/616]	Loss 2.3368e-01 (2.3542e-01)	Acc 0.755859 (0.751271)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752717)
Training Loss of Epoch 82: 0.2354353394450211
Training Acc of Epoch 82: 0.7511845782520326
Testing Acc of Epoch 82: 0.7527173913043478
Model with the best training loss saved! The loss is 0.2354353394450211
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4677e-01 (2.4677e-01)	Acc 0.728516 (0.728516)
Epoch: [83][300/616]	Loss 2.1383e-01 (2.3550e-01)	Acc 0.782227 (0.751382)
Epoch: [83][600/616]	Loss 2.3982e-01 (2.3522e-01)	Acc 0.728516 (0.751566)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750391)
Training Loss of Epoch 83: 0.23530398742939398
Training Acc of Epoch 83: 0.7514338795731708
Testing Acc of Epoch 83: 0.7503913043478261
Model with the best training loss saved! The loss is 0.23530398742939398
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3912e-01 (2.3912e-01)	Acc 0.751953 (0.751953)
Epoch: [84][300/616]	Loss 2.5065e-01 (2.3505e-01)	Acc 0.732422 (0.751622)
Epoch: [84][600/616]	Loss 2.3951e-01 (2.3520e-01)	Acc 0.744141 (0.751428)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.751352)
Training Loss of Epoch 84: 0.23517960220333037
Training Acc of Epoch 84: 0.751368775406504
Testing Acc of Epoch 84: 0.7513521739130434
Model with the best training loss saved! The loss is 0.23517960220333037
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3950e-01 (2.3950e-01)	Acc 0.749023 (0.749023)
Epoch: [85][300/616]	Loss 2.4577e-01 (2.3545e-01)	Acc 0.742188 (0.750399)
Epoch: [85][600/616]	Loss 2.3023e-01 (2.3524e-01)	Acc 0.750000 (0.751165)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752817)
Training Loss of Epoch 85: 0.23521063453782864
Training Acc of Epoch 85: 0.7512528582317073
Testing Acc of Epoch 85: 0.7528173913043478
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3436e-01 (2.3436e-01)	Acc 0.750977 (0.750977)
Epoch: [86][300/616]	Loss 2.3127e-01 (2.3461e-01)	Acc 0.758789 (0.751979)
Epoch: [86][600/616]	Loss 2.4582e-01 (2.3517e-01)	Acc 0.741211 (0.751185)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.751161)
Training Loss of Epoch 86: 0.23512467087768926
Training Acc of Epoch 86: 0.7512655614837398
Testing Acc of Epoch 86: 0.7511608695652174
Model with the best training loss saved! The loss is 0.23512467087768926
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.4064e-01 (2.4064e-01)	Acc 0.747070 (0.747070)
Epoch: [87][300/616]	Loss 2.2671e-01 (2.3533e-01)	Acc 0.778320 (0.751145)
Epoch: [87][600/616]	Loss 2.1157e-01 (2.3498e-01)	Acc 0.776367 (0.751627)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.752470)
Training Loss of Epoch 87: 0.2349948566861269
Training Acc of Epoch 87: 0.7515974339430894
Testing Acc of Epoch 87: 0.7524695652173913
Model with the best training loss saved! The loss is 0.2349948566861269
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3648e-01 (2.3648e-01)	Acc 0.744141 (0.744141)
Epoch: [88][300/616]	Loss 2.3855e-01 (2.3505e-01)	Acc 0.748047 (0.751097)
Epoch: [88][600/616]	Loss 2.4000e-01 (2.3504e-01)	Acc 0.751953 (0.751321)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752843)
Training Loss of Epoch 88: 0.23491963293494247
Training Acc of Epoch 88: 0.7514338795731708
Testing Acc of Epoch 88: 0.7528434782608696
Model with the best training loss saved! The loss is 0.23491963293494247
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2390e-01 (2.2390e-01)	Acc 0.765625 (0.765625)
Epoch: [89][300/616]	Loss 2.4139e-01 (2.3466e-01)	Acc 0.749023 (0.751561)
Epoch: [89][600/616]	Loss 2.3937e-01 (2.3480e-01)	Acc 0.734375 (0.751376)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753052)
Training Loss of Epoch 89: 0.2347281845604501
Training Acc of Epoch 89: 0.7515894944105691
Testing Acc of Epoch 89: 0.7530521739130435
Model with the best training loss saved! The loss is 0.2347281845604501
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3833e-01 (2.3833e-01)	Acc 0.748047 (0.748047)
Epoch: [90][300/616]	Loss 2.4085e-01 (2.3496e-01)	Acc 0.749023 (0.751979)
Epoch: [90][600/616]	Loss 2.3096e-01 (2.3463e-01)	Acc 0.765625 (0.751974)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753270)
Training Loss of Epoch 90: 0.2346108232814122
Training Acc of Epoch 90: 0.7519610645325203
Testing Acc of Epoch 90: 0.7532695652173913
Model with the best training loss saved! The loss is 0.2346108232814122
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2623e-01 (2.2623e-01)	Acc 0.767578 (0.767578)
Epoch: [91][300/616]	Loss 2.2168e-01 (2.3428e-01)	Acc 0.770508 (0.751976)
Epoch: [91][600/616]	Loss 2.4955e-01 (2.3419e-01)	Acc 0.731445 (0.751802)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752343)
Training Loss of Epoch 91: 0.2341586477145916
Training Acc of Epoch 91: 0.7518149771341464
Testing Acc of Epoch 91: 0.7523434782608696
Model with the best training loss saved! The loss is 0.2341586477145916
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3467e-01 (2.3467e-01)	Acc 0.744141 (0.744141)
Epoch: [92][300/616]	Loss 2.4409e-01 (2.3374e-01)	Acc 0.729492 (0.752287)
Epoch: [92][600/616]	Loss 2.4457e-01 (2.3381e-01)	Acc 0.738281 (0.752065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753126)
Training Loss of Epoch 92: 0.2338082451645921
Training Acc of Epoch 92: 0.7520356961382114
Testing Acc of Epoch 92: 0.7531260869565217
Model with the best training loss saved! The loss is 0.2338082451645921
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2779e-01 (2.2779e-01)	Acc 0.772461 (0.772461)
Epoch: [93][300/616]	Loss 2.4026e-01 (2.3393e-01)	Acc 0.752930 (0.751904)
Epoch: [93][600/616]	Loss 2.4577e-01 (2.3367e-01)	Acc 0.730469 (0.752390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.752096)
Training Loss of Epoch 93: 0.23365700574425177
Training Acc of Epoch 93: 0.7523548653455284
Testing Acc of Epoch 93: 0.752095652173913
Model with the best training loss saved! The loss is 0.23365700574425177
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3584e-01 (2.3584e-01)	Acc 0.750977 (0.750977)
Epoch: [94][300/616]	Loss 2.2633e-01 (2.3383e-01)	Acc 0.761719 (0.751713)
Epoch: [94][600/616]	Loss 2.2019e-01 (2.3353e-01)	Acc 0.769531 (0.752176)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753235)
Training Loss of Epoch 94: 0.2335362474365932
Training Acc of Epoch 94: 0.7522135416666667
Testing Acc of Epoch 94: 0.7532347826086957
Model with the best training loss saved! The loss is 0.2335362474365932
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3060e-01 (2.3060e-01)	Acc 0.750000 (0.750000)
Epoch: [95][300/616]	Loss 2.3400e-01 (2.3283e-01)	Acc 0.744141 (0.752706)
Epoch: [95][600/616]	Loss 2.4275e-01 (2.3350e-01)	Acc 0.727539 (0.751956)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753752)
Training Loss of Epoch 95: 0.23346921263671502
Training Acc of Epoch 95: 0.7520483993902439
Testing Acc of Epoch 95: 0.7537521739130435
Model with the best training loss saved! The loss is 0.23346921263671502
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2432e-01 (2.2432e-01)	Acc 0.756836 (0.756836)
Epoch: [96][300/616]	Loss 2.4128e-01 (2.3357e-01)	Acc 0.749023 (0.751788)
Epoch: [96][600/616]	Loss 2.3561e-01 (2.3348e-01)	Acc 0.732422 (0.752259)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752387)
Training Loss of Epoch 96: 0.2335502359198361
Training Acc of Epoch 96: 0.7521706681910569
Testing Acc of Epoch 96: 0.7523869565217391
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.4523e-01 (2.4523e-01)	Acc 0.741211 (0.741211)
Epoch: [97][300/616]	Loss 2.4557e-01 (2.3394e-01)	Acc 0.736328 (0.751820)
Epoch: [97][600/616]	Loss 2.3616e-01 (2.3331e-01)	Acc 0.749023 (0.752621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754330)
Training Loss of Epoch 97: 0.2332487587764011
Training Acc of Epoch 97: 0.7526740345528455
Testing Acc of Epoch 97: 0.7543304347826087
Model with the best training loss saved! The loss is 0.2332487587764011
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3239e-01 (2.3239e-01)	Acc 0.757812 (0.757812)
Epoch: [98][300/616]	Loss 2.3114e-01 (2.3282e-01)	Acc 0.754883 (0.752991)
Epoch: [98][600/616]	Loss 2.3660e-01 (2.3332e-01)	Acc 0.746094 (0.752478)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753891)
Training Loss of Epoch 98: 0.233361247666483
Training Acc of Epoch 98: 0.7524025025406504
Testing Acc of Epoch 98: 0.7538913043478261
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3858e-01 (2.3858e-01)	Acc 0.742188 (0.742188)
Epoch: [99][300/616]	Loss 2.4288e-01 (2.3346e-01)	Acc 0.739258 (0.752070)
Epoch: [99][600/616]	Loss 2.4091e-01 (2.3320e-01)	Acc 0.749023 (0.752460)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753939)
Training Loss of Epoch 99: 0.2332574175867608
Training Acc of Epoch 99: 0.7523929751016261
Testing Acc of Epoch 99: 0.7539391304347826
Early stopping not satisfied.
