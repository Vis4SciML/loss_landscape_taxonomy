train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_6b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0020e-01 (5.0020e-01)	Acc 0.196289 (0.196289)
Epoch: [0][300/616]	Loss 2.6828e-01 (2.8758e-01)	Acc 0.722656 (0.691238)
Epoch: [0][600/616]	Loss 2.5920e-01 (2.7244e-01)	Acc 0.726562 (0.711212)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.735974)
Training Loss of Epoch 0: 0.27200098803372885
Training Acc of Epoch 0: 0.7118759527439025
Testing Acc of Epoch 0: 0.7359739130434783
Model with the best training loss saved! The loss is 0.27200098803372885
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.5630e-01 (2.5630e-01)	Acc 0.727539 (0.727539)
Epoch: [1][300/616]	Loss 2.5055e-01 (2.5688e-01)	Acc 0.733398 (0.732581)
Epoch: [1][600/616]	Loss 2.5779e-01 (2.5494e-01)	Acc 0.742188 (0.734250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.725874)
Training Loss of Epoch 1: 0.2549231036649487
Training Acc of Epoch 1: 0.7342162093495935
Testing Acc of Epoch 1: 0.7258739130434783
Model with the best training loss saved! The loss is 0.2549231036649487
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.6226e-01 (2.6226e-01)	Acc 0.741211 (0.741211)
Epoch: [2][300/616]	Loss 2.4180e-01 (2.5306e-01)	Acc 0.744141 (0.736377)
Epoch: [2][600/616]	Loss 2.6947e-01 (2.5202e-01)	Acc 0.707031 (0.736924)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.730448)
Training Loss of Epoch 2: 0.2525132441908363
Training Acc of Epoch 2: 0.736521849593496
Testing Acc of Epoch 2: 0.7304478260869566
Model with the best training loss saved! The loss is 0.2525132441908363
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.6578e-01 (2.6578e-01)	Acc 0.721680 (0.721680)
Epoch: [3][300/616]	Loss 2.6089e-01 (2.5359e-01)	Acc 0.724609 (0.734732)
Epoch: [3][600/616]	Loss 2.4077e-01 (2.5333e-01)	Acc 0.765625 (0.735313)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.731157)
Training Loss of Epoch 3: 0.2533303263710766
Training Acc of Epoch 3: 0.7353134527439025
Testing Acc of Epoch 3: 0.7311565217391305
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5611e-01 (2.5611e-01)	Acc 0.731445 (0.731445)
Epoch: [4][300/616]	Loss 2.3571e-01 (2.5295e-01)	Acc 0.772461 (0.734972)
Epoch: [4][600/616]	Loss 2.4308e-01 (2.5257e-01)	Acc 0.742188 (0.735993)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.740191)
Training Loss of Epoch 4: 0.25252659979874525
Training Acc of Epoch 4: 0.7361756859756098
Testing Acc of Epoch 4: 0.7401913043478261
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.4167e-01 (2.4167e-01)	Acc 0.741211 (0.741211)
Epoch: [5][300/616]	Loss 2.4599e-01 (2.5240e-01)	Acc 0.741211 (0.736004)
Epoch: [5][600/616]	Loss 2.4837e-01 (2.5305e-01)	Acc 0.760742 (0.735417)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740065)
Training Loss of Epoch 5: 0.2531436128829553
Training Acc of Epoch 5: 0.7353452108739837
Testing Acc of Epoch 5: 0.7400652173913044
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4911e-01 (2.4911e-01)	Acc 0.726562 (0.726562)
Epoch: [6][300/616]	Loss 2.6299e-01 (2.5436e-01)	Acc 0.736328 (0.734333)
Epoch: [6][600/616]	Loss 2.4568e-01 (2.5444e-01)	Acc 0.747070 (0.734390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.738439)
Training Loss of Epoch 6: 0.2543246173519429
Training Acc of Epoch 6: 0.7344940929878049
Testing Acc of Epoch 6: 0.7384391304347826
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5106e-01 (2.5106e-01)	Acc 0.726562 (0.726562)
Epoch: [7][300/616]	Loss 2.4966e-01 (2.5482e-01)	Acc 0.747070 (0.733330)
Epoch: [7][600/616]	Loss 2.3391e-01 (2.5515e-01)	Acc 0.756836 (0.733017)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.732074)
Training Loss of Epoch 7: 0.2550707983776806
Training Acc of Epoch 7: 0.7331634273373984
Testing Acc of Epoch 7: 0.7320739130434782
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.6277e-01 (2.6277e-01)	Acc 0.725586 (0.725586)
Epoch: [8][300/616]	Loss 2.6478e-01 (2.5436e-01)	Acc 0.720703 (0.734187)
Epoch: [8][600/616]	Loss 2.5801e-01 (2.5623e-01)	Acc 0.742188 (0.732415)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.735148)
Training Loss of Epoch 8: 0.25651747489847787
Training Acc of Epoch 8: 0.7321185848577236
Testing Acc of Epoch 8: 0.7351478260869565
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5347e-01 (2.5347e-01)	Acc 0.737305 (0.737305)
Epoch: [9][300/616]	Loss 2.7360e-01 (2.5911e-01)	Acc 0.702148 (0.730316)
Epoch: [9][600/616]	Loss 2.7069e-01 (2.5765e-01)	Acc 0.722656 (0.731107)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738530)
Training Loss of Epoch 9: 0.25751227918194564
Training Acc of Epoch 9: 0.7312150660569106
Testing Acc of Epoch 9: 0.7385304347826087
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5412e-01 (2.5412e-01)	Acc 0.737305 (0.737305)
Epoch: [10][300/616]	Loss 2.5139e-01 (2.5886e-01)	Acc 0.742188 (0.729184)
Epoch: [10][600/616]	Loss 2.3180e-01 (2.5871e-01)	Acc 0.771484 (0.729356)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.721443)
Training Loss of Epoch 10: 0.25873952352903723
Training Acc of Epoch 10: 0.7293492759146342
Testing Acc of Epoch 10: 0.7214434782608695
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5474e-01 (2.5474e-01)	Acc 0.728516 (0.728516)
Epoch: [11][300/616]	Loss 2.4530e-01 (2.5981e-01)	Acc 0.735352 (0.728207)
Epoch: [11][600/616]	Loss 2.4798e-01 (2.6005e-01)	Acc 0.741211 (0.727955)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735352)
Training Loss of Epoch 11: 0.2600208350071093
Training Acc of Epoch 11: 0.7279487423780487
Testing Acc of Epoch 11: 0.7353521739130435
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.5489e-01 (2.5489e-01)	Acc 0.731445 (0.731445)
Epoch: [12][300/616]	Loss 2.4613e-01 (2.5764e-01)	Acc 0.748047 (0.731059)
Epoch: [12][600/616]	Loss 2.5452e-01 (2.5776e-01)	Acc 0.717773 (0.730474)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.725226)
Training Loss of Epoch 12: 0.25774663713404805
Training Acc of Epoch 12: 0.7305036839430894
Testing Acc of Epoch 12: 0.7252260869565217
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.6570e-01 (2.6570e-01)	Acc 0.722656 (0.722656)
Epoch: [13][300/616]	Loss 2.5721e-01 (2.6019e-01)	Acc 0.737305 (0.728081)
Epoch: [13][600/616]	Loss 2.5006e-01 (2.5862e-01)	Acc 0.741211 (0.729237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.732222)
Training Loss of Epoch 13: 0.25860926869923506
Training Acc of Epoch 13: 0.7293048145325203
Testing Acc of Epoch 13: 0.7322217391304348
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.4595e-01 (2.4595e-01)	Acc 0.733398 (0.733398)
Epoch: [14][300/616]	Loss 2.6014e-01 (2.6143e-01)	Acc 0.729492 (0.726381)
Epoch: [14][600/616]	Loss 2.6966e-01 (2.5979e-01)	Acc 0.723633 (0.728353)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.728196)
Training Loss of Epoch 14: 0.2597158846574101
Training Acc of Epoch 14: 0.7283361915650407
Testing Acc of Epoch 14: 0.728195652173913
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.7875e-01 (2.7875e-01)	Acc 0.715820 (0.715820)
Epoch: [15][300/616]	Loss 2.6132e-01 (2.5804e-01)	Acc 0.713867 (0.730504)
Epoch: [15][600/616]	Loss 2.5541e-01 (2.5845e-01)	Acc 0.724609 (0.730110)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736713)
Training Loss of Epoch 15: 0.2583449334148469
Training Acc of Epoch 15: 0.730225800304878
Testing Acc of Epoch 15: 0.7367130434782608
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4652e-01 (2.4652e-01)	Acc 0.748047 (0.748047)
Epoch: [16][300/616]	Loss 2.4607e-01 (2.5978e-01)	Acc 0.739258 (0.728937)
Epoch: [16][600/616]	Loss 2.3456e-01 (2.5946e-01)	Acc 0.752930 (0.729099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.737717)
Training Loss of Epoch 16: 0.25952524654264375
Training Acc of Epoch 16: 0.729030106707317
Testing Acc of Epoch 16: 0.7377173913043479
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4371e-01 (2.4371e-01)	Acc 0.739258 (0.739258)
Epoch: [17][300/616]	Loss 2.3881e-01 (2.6053e-01)	Acc 0.756836 (0.727409)
Epoch: [17][600/616]	Loss 2.5285e-01 (2.6056e-01)	Acc 0.737305 (0.727528)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.728252)
Training Loss of Epoch 17: 0.26041599802854587
Training Acc of Epoch 17: 0.7276327489837399
Testing Acc of Epoch 17: 0.7282521739130435
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.5766e-01 (2.5766e-01)	Acc 0.718750 (0.718750)
Epoch: [18][300/616]	Loss 2.6797e-01 (2.5828e-01)	Acc 0.713867 (0.730073)
Epoch: [18][600/616]	Loss 2.5898e-01 (2.5727e-01)	Acc 0.716797 (0.731309)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.720687)
Training Loss of Epoch 18: 0.2573464731133081
Training Acc of Epoch 18: 0.7312039507113821
Testing Acc of Epoch 18: 0.7206869565217391
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.6159e-01 (2.6159e-01)	Acc 0.726562 (0.726562)
Epoch: [19][300/616]	Loss 2.7109e-01 (2.6028e-01)	Acc 0.690430 (0.727912)
Epoch: [19][600/616]	Loss 2.5298e-01 (2.5889e-01)	Acc 0.740234 (0.729398)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.726617)
Training Loss of Epoch 19: 0.25881023969107525
Training Acc of Epoch 19: 0.7294254954268292
Testing Acc of Epoch 19: 0.7266173913043479
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6788e-01 (2.6788e-01)	Acc 0.713867 (0.713867)
Epoch: [20][300/616]	Loss 2.5107e-01 (2.6259e-01)	Acc 0.735352 (0.725745)
Epoch: [20][600/616]	Loss 2.4416e-01 (2.6030e-01)	Acc 0.738281 (0.727999)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.723678)
Training Loss of Epoch 20: 0.2601532667148404
Training Acc of Epoch 20: 0.7281186483739838
Testing Acc of Epoch 20: 0.7236782608695652
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.7149e-01 (2.7149e-01)	Acc 0.707031 (0.707031)
Epoch: [21][300/616]	Loss 2.7500e-01 (2.5742e-01)	Acc 0.711914 (0.730446)
Epoch: [21][600/616]	Loss 2.5432e-01 (2.5910e-01)	Acc 0.737305 (0.728917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.733278)
Training Loss of Epoch 21: 0.2590568772176417
Training Acc of Epoch 21: 0.7290174034552845
Testing Acc of Epoch 21: 0.7332782608695653
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.6694e-01 (2.6694e-01)	Acc 0.725586 (0.725586)
Epoch: [22][300/616]	Loss 2.5575e-01 (2.5885e-01)	Acc 0.727539 (0.729165)
Epoch: [22][600/616]	Loss 2.8637e-01 (2.5860e-01)	Acc 0.704102 (0.729703)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735639)
Training Loss of Epoch 22: 0.2585533835054413
Training Acc of Epoch 22: 0.7297938897357723
Testing Acc of Epoch 22: 0.7356391304347826
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.5809e-01 (2.5809e-01)	Acc 0.731445 (0.731445)
Epoch: [23][300/616]	Loss 2.5405e-01 (2.5991e-01)	Acc 0.750000 (0.728418)
Epoch: [23][600/616]	Loss 2.4559e-01 (2.6027e-01)	Acc 0.733398 (0.727999)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.702448)
Training Loss of Epoch 23: 0.2601824636139521
Training Acc of Epoch 23: 0.7280980055894309
Testing Acc of Epoch 23: 0.7024478260869566
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.8267e-01 (2.8267e-01)	Acc 0.693359 (0.693359)
Epoch: [24][300/616]	Loss 2.5029e-01 (2.5869e-01)	Acc 0.739258 (0.729489)
Epoch: [24][600/616]	Loss 2.6020e-01 (2.5920e-01)	Acc 0.720703 (0.728585)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.735996)
Training Loss of Epoch 24: 0.2590517516785521
Training Acc of Epoch 24: 0.7287744537601626
Testing Acc of Epoch 24: 0.735995652173913
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4324e-01 (2.4324e-01)	Acc 0.756836 (0.756836)
Epoch: [25][300/616]	Loss 2.5034e-01 (2.5773e-01)	Acc 0.748047 (0.730732)
Epoch: [25][600/616]	Loss 2.4625e-01 (2.5777e-01)	Acc 0.747070 (0.730691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.731491)
Training Loss of Epoch 25: 0.25772869664479076
Training Acc of Epoch 25: 0.7306910569105691
Testing Acc of Epoch 25: 0.731491304347826
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.5658e-01 (2.5658e-01)	Acc 0.724609 (0.724609)
Epoch: [26][300/616]	Loss 2.4318e-01 (2.5770e-01)	Acc 0.752930 (0.729849)
Epoch: [26][600/616]	Loss 2.5135e-01 (2.5778e-01)	Acc 0.735352 (0.730225)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.725791)
Training Loss of Epoch 26: 0.2578343988434086
Training Acc of Epoch 26: 0.7301368775406504
Testing Acc of Epoch 26: 0.7257913043478261
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.7556e-01 (2.7556e-01)	Acc 0.705078 (0.705078)
Epoch: [27][300/616]	Loss 2.7955e-01 (2.6088e-01)	Acc 0.689453 (0.727789)
Epoch: [27][600/616]	Loss 2.5384e-01 (2.6078e-01)	Acc 0.742188 (0.727731)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.732491)
Training Loss of Epoch 27: 0.2606987618818516
Training Acc of Epoch 27: 0.727834413109756
Testing Acc of Epoch 27: 0.732491304347826
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4814e-01 (2.4814e-01)	Acc 0.749023 (0.749023)
Epoch: [28][300/616]	Loss 2.7888e-01 (2.6089e-01)	Acc 0.705078 (0.726478)
Epoch: [28][600/616]	Loss 2.6561e-01 (2.6080e-01)	Acc 0.730469 (0.727193)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.736400)
Training Loss of Epoch 28: 0.2608327957915097
Training Acc of Epoch 28: 0.7272008384146341
Testing Acc of Epoch 28: 0.7364
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4869e-01 (2.4869e-01)	Acc 0.747070 (0.747070)
Epoch: [29][300/616]	Loss 2.8873e-01 (2.6172e-01)	Acc 0.698242 (0.726621)
Epoch: [29][600/616]	Loss 2.5886e-01 (2.6151e-01)	Acc 0.733398 (0.726562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737874)
Training Loss of Epoch 29: 0.2615106807976234
Training Acc of Epoch 29: 0.726587906504065
Testing Acc of Epoch 29: 0.7378739130434783
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.3937e-01 (2.3937e-01)	Acc 0.752930 (0.752930)
Epoch: [30][300/616]	Loss 2.6434e-01 (2.5831e-01)	Acc 0.733398 (0.729716)
Epoch: [30][600/616]	Loss 2.6351e-01 (2.6074e-01)	Acc 0.738281 (0.727183)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.730878)
Training Loss of Epoch 30: 0.26076540343645144
Training Acc of Epoch 30: 0.727075393800813
Testing Acc of Epoch 30: 0.7308782608695652
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.6411e-01 (2.6411e-01)	Acc 0.720703 (0.720703)
Epoch: [31][300/616]	Loss 2.6243e-01 (2.6006e-01)	Acc 0.724609 (0.726861)
Epoch: [31][600/616]	Loss 2.8152e-01 (2.5957e-01)	Acc 0.706055 (0.728098)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.711357)
Training Loss of Epoch 31: 0.2598233855109874
Training Acc of Epoch 31: 0.727782012195122
Testing Acc of Epoch 31: 0.7113565217391304
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.7099e-01 (2.7099e-01)	Acc 0.721680 (0.721680)
Epoch: [32][300/616]	Loss 2.5748e-01 (2.6196e-01)	Acc 0.736328 (0.726008)
Epoch: [32][600/616]	Loss 2.8277e-01 (2.6087e-01)	Acc 0.704102 (0.726722)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.731570)
Training Loss of Epoch 32: 0.2608299093517831
Training Acc of Epoch 32: 0.7267784552845529
Testing Acc of Epoch 32: 0.7315695652173912
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4551e-01 (2.4551e-01)	Acc 0.754883 (0.754883)
Epoch: [33][300/616]	Loss 2.5077e-01 (2.5994e-01)	Acc 0.734375 (0.728363)
Epoch: [33][600/616]	Loss 2.7559e-01 (2.5974e-01)	Acc 0.699219 (0.728304)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.732617)
Training Loss of Epoch 33: 0.2597733000429665
Training Acc of Epoch 33: 0.7283250762195121
Testing Acc of Epoch 33: 0.7326173913043478
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4674e-01 (2.4674e-01)	Acc 0.730469 (0.730469)
Epoch: [34][300/616]	Loss 2.7520e-01 (2.5911e-01)	Acc 0.708008 (0.728590)
Epoch: [34][600/616]	Loss 2.7497e-01 (2.5927e-01)	Acc 0.698242 (0.728189)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.733891)
Training Loss of Epoch 34: 0.2594007207610743
Training Acc of Epoch 34: 0.728053544207317
Testing Acc of Epoch 34: 0.7338913043478261
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4656e-01 (2.4656e-01)	Acc 0.737305 (0.737305)
Epoch: [35][300/616]	Loss 2.7597e-01 (2.5892e-01)	Acc 0.706055 (0.728396)
Epoch: [35][600/616]	Loss 2.5136e-01 (2.5816e-01)	Acc 0.734375 (0.729401)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.734878)
Training Loss of Epoch 35: 0.25804996228799587
Training Acc of Epoch 35: 0.7295064786585366
Testing Acc of Epoch 35: 0.7348782608695652
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.8085e-01 (2.8085e-01)	Acc 0.716797 (0.716797)
Epoch: [36][300/616]	Loss 2.5306e-01 (2.5799e-01)	Acc 0.732422 (0.729969)
Epoch: [36][600/616]	Loss 2.5177e-01 (2.5889e-01)	Acc 0.732422 (0.728831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733409)
Training Loss of Epoch 36: 0.25896602437748173
Training Acc of Epoch 36: 0.7287061737804879
Testing Acc of Epoch 36: 0.7334086956521739
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4710e-01 (2.4710e-01)	Acc 0.733398 (0.733398)
Epoch: [37][300/616]	Loss 2.5977e-01 (2.6308e-01)	Acc 0.726562 (0.724921)
Epoch: [37][600/616]	Loss 2.5301e-01 (2.6039e-01)	Acc 0.740234 (0.727838)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.720883)
Training Loss of Epoch 37: 0.26067178523152823
Training Acc of Epoch 37: 0.7275104801829269
Testing Acc of Epoch 37: 0.7208826086956521
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.6881e-01 (2.6881e-01)	Acc 0.716797 (0.716797)
Epoch: [38][300/616]	Loss 2.5524e-01 (2.6005e-01)	Acc 0.720703 (0.728016)
Epoch: [38][600/616]	Loss 2.5092e-01 (2.5979e-01)	Acc 0.738281 (0.728080)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.721935)
Training Loss of Epoch 38: 0.25997682141094675
Training Acc of Epoch 38: 0.7278534679878049
Testing Acc of Epoch 38: 0.7219347826086957
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.7172e-01 (2.7172e-01)	Acc 0.718750 (0.718750)
Epoch: [39][300/616]	Loss 2.5880e-01 (2.5817e-01)	Acc 0.714844 (0.731049)
Epoch: [39][600/616]	Loss 2.5623e-01 (2.6081e-01)	Acc 0.739258 (0.727578)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734491)
Training Loss of Epoch 39: 0.2610161990654178
Training Acc of Epoch 39: 0.7273040523373984
Testing Acc of Epoch 39: 0.7344913043478261
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.7125e-01 (2.7125e-01)	Acc 0.713867 (0.713867)
Epoch: [40][300/616]	Loss 2.4721e-01 (2.5982e-01)	Acc 0.753906 (0.727649)
Epoch: [40][600/616]	Loss 2.6789e-01 (2.5921e-01)	Acc 0.717773 (0.728581)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.736387)
Training Loss of Epoch 40: 0.2591286068282476
Training Acc of Epoch 40: 0.7286934705284552
Testing Acc of Epoch 40: 0.7363869565217391
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.6311e-01 (2.6311e-01)	Acc 0.733398 (0.733398)
Epoch: [41][300/616]	Loss 2.5085e-01 (2.5941e-01)	Acc 0.742188 (0.728603)
Epoch: [41][600/616]	Loss 2.6642e-01 (2.5965e-01)	Acc 0.727539 (0.728080)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.733909)
Training Loss of Epoch 41: 0.2597464524148925
Training Acc of Epoch 41: 0.7280186102642277
Testing Acc of Epoch 41: 0.7339086956521739
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.6037e-01 (2.6037e-01)	Acc 0.710938 (0.710938)
Epoch: [42][300/616]	Loss 2.6663e-01 (2.5939e-01)	Acc 0.708008 (0.728337)
Epoch: [42][600/616]	Loss 2.4562e-01 (2.5943e-01)	Acc 0.734375 (0.728348)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.730165)
Training Loss of Epoch 42: 0.25934308638902215
Training Acc of Epoch 42: 0.7284092352642276
Testing Acc of Epoch 42: 0.7301652173913044
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.5941e-01 (2.5941e-01)	Acc 0.725586 (0.725586)
Epoch: [43][300/616]	Loss 2.3659e-01 (2.6008e-01)	Acc 0.762695 (0.728587)
Epoch: [43][600/616]	Loss 2.4108e-01 (2.6023e-01)	Acc 0.746094 (0.727871)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.717943)
Training Loss of Epoch 43: 0.26029227090075735
Training Acc of Epoch 43: 0.7277994791666667
Testing Acc of Epoch 43: 0.7179434782608696
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.5832e-01 (2.5832e-01)	Acc 0.732422 (0.732422)
Epoch: [44][300/616]	Loss 2.5246e-01 (2.6062e-01)	Acc 0.731445 (0.727062)
Epoch: [44][600/616]	Loss 2.6346e-01 (2.6113e-01)	Acc 0.729492 (0.726858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734778)
Training Loss of Epoch 44: 0.261067270093817
Training Acc of Epoch 44: 0.7269499491869919
Testing Acc of Epoch 44: 0.7347782608695652
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.4099e-01 (2.4099e-01)	Acc 0.747070 (0.747070)
Epoch: [45][300/616]	Loss 2.5708e-01 (2.5988e-01)	Acc 0.736328 (0.728519)
Epoch: [45][600/616]	Loss 2.4920e-01 (2.6117e-01)	Acc 0.744141 (0.726946)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733396)
Training Loss of Epoch 45: 0.2612650505653242
Training Acc of Epoch 45: 0.7267498729674797
Testing Acc of Epoch 45: 0.733395652173913
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4060e-01 (2.4060e-01)	Acc 0.751953 (0.751953)
Epoch: [46][300/616]	Loss 2.6775e-01 (2.6171e-01)	Acc 0.713867 (0.726154)
Epoch: [46][600/616]	Loss 2.7256e-01 (2.6030e-01)	Acc 0.704102 (0.727515)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.727422)
Training Loss of Epoch 46: 0.2605092269860632
Training Acc of Epoch 46: 0.7272738821138212
Testing Acc of Epoch 46: 0.7274217391304347
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.6409e-01 (2.6409e-01)	Acc 0.719727 (0.719727)
Epoch: [47][300/616]	Loss 2.7776e-01 (2.6086e-01)	Acc 0.691406 (0.727737)
Epoch: [47][600/616]	Loss 2.7255e-01 (2.6109e-01)	Acc 0.709961 (0.727081)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.737235)
Training Loss of Epoch 47: 0.2609920319018325
Training Acc of Epoch 47: 0.7271404979674797
Testing Acc of Epoch 47: 0.7372347826086957
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.5153e-01 (2.5153e-01)	Acc 0.741211 (0.741211)
Epoch: [48][300/616]	Loss 2.7647e-01 (2.6039e-01)	Acc 0.719727 (0.728081)
Epoch: [48][600/616]	Loss 2.7430e-01 (2.6029e-01)	Acc 0.707031 (0.727923)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740348)
Training Loss of Epoch 48: 0.2602317775894956
Training Acc of Epoch 48: 0.7279757367886179
Testing Acc of Epoch 48: 0.7403478260869565
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.6354e-01 (2.6354e-01)	Acc 0.723633 (0.723633)
Epoch: [49][300/616]	Loss 2.6018e-01 (2.5808e-01)	Acc 0.717773 (0.730177)
Epoch: [49][600/616]	Loss 2.5800e-01 (2.5837e-01)	Acc 0.723633 (0.729406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.735509)
Training Loss of Epoch 49: 0.2585625970266699
Training Acc of Epoch 49: 0.7291206173780488
Testing Acc of Epoch 49: 0.7355086956521739
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.7510e-01 (2.7510e-01)	Acc 0.718750 (0.718750)
Epoch: [50][300/616]	Loss 2.5453e-01 (2.5996e-01)	Acc 0.737305 (0.728444)
Epoch: [50][600/616]	Loss 2.3748e-01 (2.5945e-01)	Acc 0.753906 (0.728675)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.717187)
Training Loss of Epoch 50: 0.25938593609546257
Training Acc of Epoch 50: 0.7288141514227642
Testing Acc of Epoch 50: 0.7171869565217391
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.7829e-01 (2.7829e-01)	Acc 0.697266 (0.697266)
Epoch: [51][300/616]	Loss 2.4599e-01 (2.5983e-01)	Acc 0.744141 (0.728324)
Epoch: [51][600/616]	Loss 2.4758e-01 (2.5914e-01)	Acc 0.739258 (0.729172)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.687500 (0.697883)
Training Loss of Epoch 51: 0.25916622969193187
Training Acc of Epoch 51: 0.7291809578252032
Testing Acc of Epoch 51: 0.6978826086956522
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.7654e-01 (2.7654e-01)	Acc 0.709961 (0.709961)
Epoch: [52][300/616]	Loss 2.5982e-01 (2.5994e-01)	Acc 0.713867 (0.727841)
Epoch: [52][600/616]	Loss 2.6214e-01 (2.6155e-01)	Acc 0.721680 (0.725960)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.737604)
Training Loss of Epoch 52: 0.2615020593249701
Training Acc of Epoch 52: 0.7260353150406504
Testing Acc of Epoch 52: 0.737604347826087
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.4931e-01 (2.4931e-01)	Acc 0.748047 (0.748047)
Epoch: [53][300/616]	Loss 2.4881e-01 (2.5788e-01)	Acc 0.738281 (0.730047)
Epoch: [53][600/616]	Loss 2.6337e-01 (2.5839e-01)	Acc 0.723633 (0.729131)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734222)
Training Loss of Epoch 53: 0.25837908413836624
Training Acc of Epoch 53: 0.729196836890244
Testing Acc of Epoch 53: 0.7342217391304348
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.5328e-01 (2.5328e-01)	Acc 0.726562 (0.726562)
Epoch: [54][300/616]	Loss 2.5376e-01 (2.5900e-01)	Acc 0.720703 (0.728934)
Epoch: [54][600/616]	Loss 2.3394e-01 (2.5962e-01)	Acc 0.750000 (0.728246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.729283)
Training Loss of Epoch 54: 0.25959113669104694
Training Acc of Epoch 54: 0.7282028074186991
Testing Acc of Epoch 54: 0.7292826086956522
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.5430e-01 (2.5430e-01)	Acc 0.738281 (0.738281)
Epoch: [55][300/616]	Loss 2.5151e-01 (2.5743e-01)	Acc 0.735352 (0.730608)
Epoch: [55][600/616]	Loss 2.6690e-01 (2.5877e-01)	Acc 0.725586 (0.729140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733709)
Training Loss of Epoch 55: 0.25886410612885546
Training Acc of Epoch 55: 0.729004700203252
Testing Acc of Epoch 55: 0.7337086956521739
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5230e-01 (2.5230e-01)	Acc 0.729492 (0.729492)
Epoch: [56][300/616]	Loss 2.4595e-01 (2.5943e-01)	Acc 0.733398 (0.728220)
Epoch: [56][600/616]	Loss 2.7410e-01 (2.5970e-01)	Acc 0.703125 (0.728631)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736813)
Training Loss of Epoch 56: 0.25972347530892226
Training Acc of Epoch 56: 0.7286442454268293
Testing Acc of Epoch 56: 0.7368130434782608
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.6679e-01 (2.6679e-01)	Acc 0.705078 (0.705078)
Epoch: [57][300/616]	Loss 2.7296e-01 (2.6016e-01)	Acc 0.715820 (0.727552)
Epoch: [57][600/616]	Loss 2.6001e-01 (2.5947e-01)	Acc 0.717773 (0.728824)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.737874)
Training Loss of Epoch 57: 0.2594559651322481
Training Acc of Epoch 57: 0.7287395198170732
Testing Acc of Epoch 57: 0.7378739130434783
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.5397e-01 (2.5397e-01)	Acc 0.745117 (0.745117)
Epoch: [58][300/616]	Loss 2.5671e-01 (2.5691e-01)	Acc 0.734375 (0.731056)
Epoch: [58][600/616]	Loss 2.5477e-01 (2.5809e-01)	Acc 0.731445 (0.729970)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.738378)
Training Loss of Epoch 58: 0.25820304157772683
Training Acc of Epoch 58: 0.7298986915650406
Testing Acc of Epoch 58: 0.7383782608695653
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.6144e-01 (2.6144e-01)	Acc 0.712891 (0.712891)
Epoch: [59][300/616]	Loss 2.5789e-01 (2.5879e-01)	Acc 0.728516 (0.729846)
Epoch: [59][600/616]	Loss 2.6934e-01 (2.5815e-01)	Acc 0.714844 (0.730545)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.733230)
Training Loss of Epoch 59: 0.25808729614184156
Training Acc of Epoch 59: 0.730616425304878
Testing Acc of Epoch 59: 0.7332304347826087
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5730e-01 (2.5730e-01)	Acc 0.729492 (0.729492)
Epoch: [60][300/616]	Loss 2.5268e-01 (2.5801e-01)	Acc 0.749023 (0.729839)
Epoch: [60][600/616]	Loss 2.7158e-01 (2.5896e-01)	Acc 0.707031 (0.729058)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.737878)
Training Loss of Epoch 60: 0.2589316779278158
Training Acc of Epoch 60: 0.7290936229674797
Testing Acc of Epoch 60: 0.7378782608695652
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.4961e-01 (2.4961e-01)	Acc 0.734375 (0.734375)
Epoch: [61][300/616]	Loss 2.4714e-01 (2.5926e-01)	Acc 0.733398 (0.728337)
Epoch: [61][600/616]	Loss 2.4814e-01 (2.5848e-01)	Acc 0.750000 (0.729687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.735683)
Training Loss of Epoch 61: 0.2584429182899677
Training Acc of Epoch 61: 0.7297287855691057
Testing Acc of Epoch 61: 0.7356826086956522
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4809e-01 (2.4809e-01)	Acc 0.739258 (0.739258)
Epoch: [62][300/616]	Loss 2.4499e-01 (2.5528e-01)	Acc 0.738281 (0.732367)
Epoch: [62][600/616]	Loss 2.4396e-01 (2.5652e-01)	Acc 0.740234 (0.731626)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.739717)
Training Loss of Epoch 62: 0.2564823527403963
Training Acc of Epoch 62: 0.7316310975609757
Testing Acc of Epoch 62: 0.7397173913043478
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4008e-01 (2.4008e-01)	Acc 0.750000 (0.750000)
Epoch: [63][300/616]	Loss 2.7011e-01 (2.5716e-01)	Acc 0.720703 (0.730783)
Epoch: [63][600/616]	Loss 2.5118e-01 (2.5804e-01)	Acc 0.736328 (0.730665)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.736500)
Training Loss of Epoch 63: 0.2578938379035733
Training Acc of Epoch 63: 0.7308292047764228
Testing Acc of Epoch 63: 0.7365
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.4888e-01 (2.4888e-01)	Acc 0.734375 (0.734375)
Epoch: [64][300/616]	Loss 3.0035e-01 (2.5906e-01)	Acc 0.694336 (0.728593)
Epoch: [64][600/616]	Loss 2.5732e-01 (2.5826e-01)	Acc 0.732422 (0.729577)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733800)
Training Loss of Epoch 64: 0.2582596913101227
Training Acc of Epoch 64: 0.7295985772357724
Testing Acc of Epoch 64: 0.7338
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.5970e-01 (2.5970e-01)	Acc 0.727539 (0.727539)
Epoch: [65][300/616]	Loss 2.7018e-01 (2.5844e-01)	Acc 0.710938 (0.729917)
Epoch: [65][600/616]	Loss 2.6828e-01 (2.5896e-01)	Acc 0.730469 (0.729326)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739213)
Training Loss of Epoch 65: 0.25893407987385264
Training Acc of Epoch 65: 0.729346100101626
Testing Acc of Epoch 65: 0.7392130434782609
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.5183e-01 (2.5183e-01)	Acc 0.741211 (0.741211)
Epoch: [66][300/616]	Loss 2.5739e-01 (2.5771e-01)	Acc 0.739258 (0.730446)
Epoch: [66][600/616]	Loss 2.5461e-01 (2.5804e-01)	Acc 0.722656 (0.729830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.725700)
Training Loss of Epoch 66: 0.25814948983308744
Training Acc of Epoch 66: 0.7297494283536585
Testing Acc of Epoch 66: 0.7257
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.7606e-01 (2.7606e-01)	Acc 0.706055 (0.706055)
Epoch: [67][300/616]	Loss 2.7035e-01 (2.5748e-01)	Acc 0.699219 (0.730154)
Epoch: [67][600/616]	Loss 2.6542e-01 (2.5883e-01)	Acc 0.724609 (0.729040)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.729013)
Training Loss of Epoch 67: 0.25892091643034926
Training Acc of Epoch 67: 0.728931656504065
Testing Acc of Epoch 67: 0.7290130434782609
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.6404e-01 (2.6404e-01)	Acc 0.710938 (0.710938)
Epoch: [68][300/616]	Loss 2.5854e-01 (2.5958e-01)	Acc 0.729492 (0.728756)
Epoch: [68][600/616]	Loss 2.5506e-01 (2.5809e-01)	Acc 0.739258 (0.730269)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.737565)
Training Loss of Epoch 68: 0.25812605604892824
Training Acc of Epoch 68: 0.7302115091463415
Testing Acc of Epoch 68: 0.7375652173913043
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5411e-01 (2.5411e-01)	Acc 0.725586 (0.725586)
Epoch: [69][300/616]	Loss 2.4439e-01 (2.5797e-01)	Acc 0.747070 (0.730832)
Epoch: [69][600/616]	Loss 2.3481e-01 (2.5884e-01)	Acc 0.747070 (0.729911)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.719852)
Training Loss of Epoch 69: 0.25873525641798006
Training Acc of Epoch 69: 0.7299749110772358
Testing Acc of Epoch 69: 0.7198521739130435
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.9694e-01 (2.9694e-01)	Acc 0.684570 (0.684570)
Epoch: [70][300/616]	Loss 2.7232e-01 (2.6036e-01)	Acc 0.712891 (0.727221)
Epoch: [70][600/616]	Loss 2.7720e-01 (2.5991e-01)	Acc 0.704102 (0.727921)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734726)
Training Loss of Epoch 70: 0.2598782356192426
Training Acc of Epoch 70: 0.7279598577235772
Testing Acc of Epoch 70: 0.7347260869565218
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5455e-01 (2.5455e-01)	Acc 0.739258 (0.739258)
Epoch: [71][300/616]	Loss 2.4894e-01 (2.5816e-01)	Acc 0.729492 (0.730553)
Epoch: [71][600/616]	Loss 2.5584e-01 (2.5919e-01)	Acc 0.728516 (0.728870)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.734587)
Training Loss of Epoch 71: 0.2593699595065621
Training Acc of Epoch 71: 0.7287220528455285
Testing Acc of Epoch 71: 0.7345869565217391
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.5808e-01 (2.5808e-01)	Acc 0.740234 (0.740234)
Epoch: [72][300/616]	Loss 2.5528e-01 (2.5856e-01)	Acc 0.753906 (0.729716)
Epoch: [72][600/616]	Loss 2.5799e-01 (2.5893e-01)	Acc 0.721680 (0.729052)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.734787)
Training Loss of Epoch 72: 0.25886729715800866
Training Acc of Epoch 72: 0.7291571392276422
Testing Acc of Epoch 72: 0.7347869565217391
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.5430e-01 (2.5430e-01)	Acc 0.720703 (0.720703)
Epoch: [73][300/616]	Loss 2.5597e-01 (2.6078e-01)	Acc 0.726562 (0.727354)
Epoch: [73][600/616]	Loss 2.7747e-01 (2.5954e-01)	Acc 0.704102 (0.728329)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.729078)
Training Loss of Epoch 73: 0.2594897431329014
Training Acc of Epoch 73: 0.7283568343495935
Testing Acc of Epoch 73: 0.7290782608695652
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.6588e-01 (2.6588e-01)	Acc 0.722656 (0.722656)
Epoch: [74][300/616]	Loss 2.8236e-01 (2.6208e-01)	Acc 0.706055 (0.726371)
Epoch: [74][600/616]	Loss 2.6010e-01 (2.6089e-01)	Acc 0.731445 (0.727237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.721987)
Training Loss of Epoch 74: 0.2608128587889477
Training Acc of Epoch 74: 0.7272992886178862
Testing Acc of Epoch 74: 0.7219869565217392
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5913e-01 (2.5913e-01)	Acc 0.739258 (0.739258)
Epoch: [75][300/616]	Loss 2.4127e-01 (2.4829e-01)	Acc 0.743164 (0.738940)
Epoch: [75][600/616]	Loss 2.4352e-01 (2.4869e-01)	Acc 0.737305 (0.738886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738996)
Training Loss of Epoch 75: 0.24864753765788505
Training Acc of Epoch 75: 0.7390180386178862
Testing Acc of Epoch 75: 0.738995652173913
Model with the best training loss saved! The loss is 0.24864753765788505
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.5880e-01 (2.5880e-01)	Acc 0.735352 (0.735352)
Epoch: [76][300/616]	Loss 2.6315e-01 (2.4974e-01)	Acc 0.716797 (0.737580)
Epoch: [76][600/616]	Loss 2.5393e-01 (2.4893e-01)	Acc 0.712891 (0.738657)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.735843)
Training Loss of Epoch 76: 0.2489685367035672
Training Acc of Epoch 76: 0.7386131224593496
Testing Acc of Epoch 76: 0.7358434782608696
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4546e-01 (2.4546e-01)	Acc 0.742188 (0.742188)
Epoch: [77][300/616]	Loss 2.4924e-01 (2.4818e-01)	Acc 0.739258 (0.738375)
Epoch: [77][600/616]	Loss 2.5086e-01 (2.4783e-01)	Acc 0.733398 (0.739529)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736983)
Training Loss of Epoch 77: 0.24780566474286522
Training Acc of Epoch 77: 0.7396817835365853
Testing Acc of Epoch 77: 0.7369826086956521
Model with the best training loss saved! The loss is 0.24780566474286522
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4337e-01 (2.4337e-01)	Acc 0.748047 (0.748047)
Epoch: [78][300/616]	Loss 2.3258e-01 (2.4807e-01)	Acc 0.752930 (0.739521)
Epoch: [78][600/616]	Loss 2.4144e-01 (2.4929e-01)	Acc 0.748047 (0.738341)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.738017)
Training Loss of Epoch 78: 0.24932517305137666
Training Acc of Epoch 78: 0.7383955792682927
Testing Acc of Epoch 78: 0.7380173913043478
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.4660e-01 (2.4660e-01)	Acc 0.747070 (0.747070)
Epoch: [79][300/616]	Loss 2.4030e-01 (2.4929e-01)	Acc 0.739258 (0.738576)
Epoch: [79][600/616]	Loss 2.5862e-01 (2.5012e-01)	Acc 0.724609 (0.737357)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.741830)
Training Loss of Epoch 79: 0.25022973355239
Training Acc of Epoch 79: 0.7372506986788618
Testing Acc of Epoch 79: 0.7418304347826087
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3995e-01 (2.3995e-01)	Acc 0.741211 (0.741211)
Epoch: [80][300/616]	Loss 2.5091e-01 (2.5009e-01)	Acc 0.737305 (0.737454)
Epoch: [80][600/616]	Loss 2.4409e-01 (2.5029e-01)	Acc 0.749023 (0.737209)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.734422)
Training Loss of Epoch 80: 0.2502756164083636
Training Acc of Epoch 80: 0.7371855945121951
Testing Acc of Epoch 80: 0.7344217391304347
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.5501e-01 (2.5501e-01)	Acc 0.737305 (0.737305)
Epoch: [81][300/616]	Loss 2.4527e-01 (2.4888e-01)	Acc 0.744141 (0.738336)
Epoch: [81][600/616]	Loss 2.5051e-01 (2.4897e-01)	Acc 0.731445 (0.738270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.741630)
Training Loss of Epoch 81: 0.24906600570775628
Training Acc of Epoch 81: 0.7381637449186992
Testing Acc of Epoch 81: 0.7416304347826087
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3408e-01 (2.3408e-01)	Acc 0.760742 (0.760742)
Epoch: [82][300/616]	Loss 2.6399e-01 (2.5025e-01)	Acc 0.716797 (0.736825)
Epoch: [82][600/616]	Loss 2.4111e-01 (2.5012e-01)	Acc 0.749023 (0.736848)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.743474)
Training Loss of Epoch 82: 0.24999284312977055
Training Acc of Epoch 82: 0.7369998094512196
Testing Acc of Epoch 82: 0.7434739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.5026e-01 (2.5026e-01)	Acc 0.742188 (0.742188)
Epoch: [83][300/616]	Loss 2.4939e-01 (2.5022e-01)	Acc 0.727539 (0.736734)
Epoch: [83][600/616]	Loss 2.2668e-01 (2.5021e-01)	Acc 0.776367 (0.737080)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.723083)
Training Loss of Epoch 83: 0.2502538566182299
Training Acc of Epoch 83: 0.7371458968495935
Testing Acc of Epoch 83: 0.7230826086956522
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.8262e-01 (2.8262e-01)	Acc 0.699219 (0.699219)
Epoch: [84][300/616]	Loss 2.5297e-01 (2.4969e-01)	Acc 0.728516 (0.737451)
Epoch: [84][600/616]	Loss 2.4146e-01 (2.4964e-01)	Acc 0.748047 (0.737206)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.738070)
Training Loss of Epoch 84: 0.2496056078168435
Training Acc of Epoch 84: 0.7372570503048781
Testing Acc of Epoch 84: 0.7380695652173913
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3641e-01 (2.3641e-01)	Acc 0.758789 (0.758789)
Epoch: [85][300/616]	Loss 2.6483e-01 (2.4870e-01)	Acc 0.714844 (0.738388)
Epoch: [85][600/616]	Loss 2.6043e-01 (2.4878e-01)	Acc 0.718750 (0.738488)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.732013)
Training Loss of Epoch 85: 0.24886580780754244
Training Acc of Epoch 85: 0.73833206300813
Testing Acc of Epoch 85: 0.7320130434782609
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3458e-01 (2.3458e-01)	Acc 0.751953 (0.751953)
Epoch: [86][300/616]	Loss 2.4717e-01 (2.5050e-01)	Acc 0.732422 (0.736390)
Epoch: [86][600/616]	Loss 2.5149e-01 (2.5062e-01)	Acc 0.729492 (0.736471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.743109)
Training Loss of Epoch 86: 0.2505294686410485
Training Acc of Epoch 86: 0.7365567835365854
Testing Acc of Epoch 86: 0.743108695652174
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.4187e-01 (2.4187e-01)	Acc 0.726562 (0.726562)
Epoch: [87][300/616]	Loss 2.5797e-01 (2.5044e-01)	Acc 0.720703 (0.736322)
Epoch: [87][600/616]	Loss 2.4607e-01 (2.5043e-01)	Acc 0.735352 (0.736754)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.732430)
Training Loss of Epoch 87: 0.25042122326246125
Training Acc of Epoch 87: 0.7367044588414634
Testing Acc of Epoch 87: 0.7324304347826087
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4384e-01 (2.4384e-01)	Acc 0.743164 (0.743164)
Epoch: [88][300/616]	Loss 2.4926e-01 (2.5032e-01)	Acc 0.747070 (0.737097)
Epoch: [88][600/616]	Loss 2.6644e-01 (2.4985e-01)	Acc 0.723633 (0.737420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.737626)
Training Loss of Epoch 88: 0.24990750192626704
Training Acc of Epoch 88: 0.7373126270325203
Testing Acc of Epoch 88: 0.7376260869565218
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.5384e-01 (2.5384e-01)	Acc 0.723633 (0.723633)
Epoch: [89][300/616]	Loss 2.3952e-01 (2.5006e-01)	Acc 0.745117 (0.737301)
Epoch: [89][600/616]	Loss 2.6713e-01 (2.5100e-01)	Acc 0.707031 (0.735872)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.741143)
Training Loss of Epoch 89: 0.2509561761850264
Training Acc of Epoch 89: 0.7358819232723577
Testing Acc of Epoch 89: 0.7411434782608696
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4241e-01 (2.4241e-01)	Acc 0.742188 (0.742188)
Epoch: [90][300/616]	Loss 2.4855e-01 (2.5038e-01)	Acc 0.740234 (0.736409)
Epoch: [90][600/616]	Loss 2.4913e-01 (2.5093e-01)	Acc 0.746094 (0.736203)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.735200)
Training Loss of Epoch 90: 0.25095966807225856
Training Acc of Epoch 90: 0.7361534552845529
Testing Acc of Epoch 90: 0.7352
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.5531e-01 (2.5531e-01)	Acc 0.744141 (0.744141)
Epoch: [91][300/616]	Loss 2.4382e-01 (2.5126e-01)	Acc 0.750977 (0.735339)
Epoch: [91][600/616]	Loss 2.3551e-01 (2.5070e-01)	Acc 0.767578 (0.736003)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.737504)
Training Loss of Epoch 91: 0.250726290446956
Training Acc of Epoch 91: 0.7360153074186991
Testing Acc of Epoch 91: 0.737504347826087
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.5234e-01 (2.5234e-01)	Acc 0.735352 (0.735352)
Epoch: [92][300/616]	Loss 2.6656e-01 (2.5187e-01)	Acc 0.707031 (0.735339)
Epoch: [92][600/616]	Loss 2.4894e-01 (2.5079e-01)	Acc 0.737305 (0.736296)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.738526)
Training Loss of Epoch 92: 0.2507537224913031
Training Acc of Epoch 92: 0.7363344766260163
Testing Acc of Epoch 92: 0.7385260869565218
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.5444e-01 (2.5444e-01)	Acc 0.738281 (0.738281)
Epoch: [93][300/616]	Loss 2.5774e-01 (2.4967e-01)	Acc 0.721680 (0.737798)
Epoch: [93][600/616]	Loss 2.4800e-01 (2.5005e-01)	Acc 0.729492 (0.737077)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.733870)
Training Loss of Epoch 93: 0.2501404976457115
Training Acc of Epoch 93: 0.7369251778455285
Testing Acc of Epoch 93: 0.7338695652173913
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.5454e-01 (2.5454e-01)	Acc 0.738281 (0.738281)
Epoch: [94][300/616]	Loss 2.7280e-01 (2.4894e-01)	Acc 0.708008 (0.737824)
Epoch: [94][600/616]	Loss 2.7542e-01 (2.4920e-01)	Acc 0.710938 (0.737691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.720909)
Training Loss of Epoch 94: 0.24946555366845635
Training Acc of Epoch 94: 0.7373221544715447
Testing Acc of Epoch 94: 0.7209086956521739
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.7093e-01 (2.7093e-01)	Acc 0.715820 (0.715820)
Epoch: [95][300/616]	Loss 2.6245e-01 (2.5026e-01)	Acc 0.715820 (0.736406)
Epoch: [95][600/616]	Loss 2.4161e-01 (2.4957e-01)	Acc 0.740234 (0.737682)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739865)
Training Loss of Epoch 95: 0.24971079932964915
Training Acc of Epoch 95: 0.7375444613821138
Testing Acc of Epoch 95: 0.7398652173913044
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.6967e-01 (2.6967e-01)	Acc 0.710938 (0.710938)
Epoch: [96][300/616]	Loss 2.4011e-01 (2.4833e-01)	Acc 0.747070 (0.739345)
Epoch: [96][600/616]	Loss 2.3610e-01 (2.4912e-01)	Acc 0.766602 (0.738296)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.732752)
Training Loss of Epoch 96: 0.2490546622654287
Training Acc of Epoch 96: 0.7383336509146341
Testing Acc of Epoch 96: 0.7327521739130435
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.6147e-01 (2.6147e-01)	Acc 0.726562 (0.726562)
Epoch: [97][300/616]	Loss 2.5190e-01 (2.4869e-01)	Acc 0.726562 (0.738106)
Epoch: [97][600/616]	Loss 2.4979e-01 (2.4946e-01)	Acc 0.727539 (0.737583)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.736439)
Training Loss of Epoch 97: 0.24950049495309348
Training Acc of Epoch 97: 0.7375428734756098
Testing Acc of Epoch 97: 0.7364391304347826
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.5528e-01 (2.5528e-01)	Acc 0.728516 (0.728516)
Epoch: [98][300/616]	Loss 2.7732e-01 (2.4956e-01)	Acc 0.709961 (0.737824)
Epoch: [98][600/616]	Loss 2.6501e-01 (2.5009e-01)	Acc 0.721680 (0.737056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.743813)
Training Loss of Epoch 98: 0.250058466462585
Training Acc of Epoch 98: 0.7370903201219512
Testing Acc of Epoch 98: 0.7438130434782608
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.5852e-01 (2.5852e-01)	Acc 0.726562 (0.726562)
Epoch: [99][300/616]	Loss 2.6762e-01 (2.5092e-01)	Acc 0.732422 (0.736438)
Epoch: [99][600/616]	Loss 2.4535e-01 (2.5082e-01)	Acc 0.745117 (0.736276)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.740943)
Training Loss of Epoch 99: 0.2508993116820731
Training Acc of Epoch 99: 0.7362407901422764
Testing Acc of Epoch 99: 0.7409434782608696
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_6b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0204e-01 (5.0204e-01)	Acc 0.122070 (0.122070)
Epoch: [0][300/616]	Loss 2.8085e-01 (2.8880e-01)	Acc 0.720703 (0.690034)
Epoch: [0][600/616]	Loss 2.4742e-01 (2.7412e-01)	Acc 0.744141 (0.709945)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733078)
Training Loss of Epoch 0: 0.2736404037814799
Training Acc of Epoch 0: 0.7106119791666666
Testing Acc of Epoch 0: 0.7330782608695652
Model with the best training loss saved! The loss is 0.2736404037814799
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.5391e-01 (2.5391e-01)	Acc 0.729492 (0.729492)
Epoch: [1][300/616]	Loss 2.8967e-01 (2.5844e-01)	Acc 0.690430 (0.730871)
Epoch: [1][600/616]	Loss 2.6670e-01 (2.5958e-01)	Acc 0.731445 (0.730046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.730913)
Training Loss of Epoch 1: 0.2594881829449801
Training Acc of Epoch 1: 0.7300479547764228
Testing Acc of Epoch 1: 0.7309130434782609
Model with the best training loss saved! The loss is 0.2594881829449801
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5725e-01 (2.5725e-01)	Acc 0.723633 (0.723633)
Epoch: [2][300/616]	Loss 2.5278e-01 (2.5889e-01)	Acc 0.718750 (0.730008)
Epoch: [2][600/616]	Loss 2.4410e-01 (2.5805e-01)	Acc 0.739258 (0.730977)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.734887)
Training Loss of Epoch 2: 0.25784601445605115
Training Acc of Epoch 2: 0.7312118902439024
Testing Acc of Epoch 2: 0.7348869565217391
Model with the best training loss saved! The loss is 0.25784601445605115
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4713e-01 (2.4713e-01)	Acc 0.741211 (0.741211)
Epoch: [3][300/616]	Loss 2.6690e-01 (2.5850e-01)	Acc 0.703125 (0.730112)
Epoch: [3][600/616]	Loss 2.4271e-01 (2.5793e-01)	Acc 0.753906 (0.730916)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.737148)
Training Loss of Epoch 3: 0.2579257929470481
Training Acc of Epoch 3: 0.7309911712398374
Testing Acc of Epoch 3: 0.7371478260869565
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5915e-01 (2.5915e-01)	Acc 0.724609 (0.724609)
Epoch: [4][300/616]	Loss 2.3872e-01 (2.5547e-01)	Acc 0.752930 (0.733298)
Epoch: [4][600/616]	Loss 2.4950e-01 (2.5757e-01)	Acc 0.740234 (0.730846)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.729800)
Training Loss of Epoch 4: 0.2574164010887224
Training Acc of Epoch 4: 0.7309864075203252
Testing Acc of Epoch 4: 0.7298
Model with the best training loss saved! The loss is 0.2574164010887224
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.6210e-01 (2.6210e-01)	Acc 0.712891 (0.712891)
Epoch: [5][300/616]	Loss 2.7224e-01 (2.5846e-01)	Acc 0.724609 (0.729946)
Epoch: [5][600/616]	Loss 2.6743e-01 (2.5717e-01)	Acc 0.712891 (0.730956)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.739230)
Training Loss of Epoch 5: 0.25708446347616554
Training Acc of Epoch 5: 0.7310038744918699
Testing Acc of Epoch 5: 0.7392304347826087
Model with the best training loss saved! The loss is 0.25708446347616554
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.6148e-01 (2.6148e-01)	Acc 0.713867 (0.713867)
Epoch: [6][300/616]	Loss 2.4905e-01 (2.5991e-01)	Acc 0.745117 (0.728464)
Epoch: [6][600/616]	Loss 2.7013e-01 (2.5969e-01)	Acc 0.726562 (0.728408)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.732578)
Training Loss of Epoch 6: 0.25967846626673285
Training Acc of Epoch 6: 0.7284203506097561
Testing Acc of Epoch 6: 0.7325782608695652
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5313e-01 (2.5313e-01)	Acc 0.727539 (0.727539)
Epoch: [7][300/616]	Loss 2.8498e-01 (2.5832e-01)	Acc 0.700195 (0.729784)
Epoch: [7][600/616]	Loss 2.4416e-01 (2.6094e-01)	Acc 0.756836 (0.727146)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.727326)
Training Loss of Epoch 7: 0.2609812332847254
Training Acc of Epoch 7: 0.727223069105691
Testing Acc of Epoch 7: 0.7273260869565218
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3645e-01 (2.3645e-01)	Acc 0.749023 (0.749023)
Epoch: [8][300/616]	Loss 2.7533e-01 (2.5866e-01)	Acc 0.714844 (0.729401)
Epoch: [8][600/616]	Loss 2.3815e-01 (2.5870e-01)	Acc 0.753906 (0.729562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.733443)
Training Loss of Epoch 8: 0.2585952666716847
Training Acc of Epoch 8: 0.7296589176829268
Testing Acc of Epoch 8: 0.7334434782608695
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3924e-01 (2.3924e-01)	Acc 0.764648 (0.764648)
Epoch: [9][300/616]	Loss 2.5772e-01 (2.6077e-01)	Acc 0.745117 (0.727597)
Epoch: [9][600/616]	Loss 2.7792e-01 (2.5996e-01)	Acc 0.704102 (0.727931)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.732283)
Training Loss of Epoch 9: 0.26013543654263505
Training Acc of Epoch 9: 0.7276978531504065
Testing Acc of Epoch 9: 0.7322826086956522
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.7328e-01 (2.7328e-01)	Acc 0.724609 (0.724609)
Epoch: [10][300/616]	Loss 2.7860e-01 (2.5933e-01)	Acc 0.703125 (0.728113)
Epoch: [10][600/616]	Loss 2.6016e-01 (2.5959e-01)	Acc 0.727539 (0.728577)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.728835)
Training Loss of Epoch 10: 0.25961282296878535
Training Acc of Epoch 10: 0.7285521468495935
Testing Acc of Epoch 10: 0.7288347826086956
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5209e-01 (2.5209e-01)	Acc 0.734375 (0.734375)
Epoch: [11][300/616]	Loss 2.8011e-01 (2.5738e-01)	Acc 0.691406 (0.730566)
Epoch: [11][600/616]	Loss 2.6379e-01 (2.5854e-01)	Acc 0.732422 (0.729310)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.714700)
Training Loss of Epoch 11: 0.2587415194850627
Training Acc of Epoch 11: 0.7291222052845528
Testing Acc of Epoch 11: 0.7147
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6868e-01 (2.6868e-01)	Acc 0.708984 (0.708984)
Epoch: [12][300/616]	Loss 2.6125e-01 (2.5871e-01)	Acc 0.721680 (0.728918)
Epoch: [12][600/616]	Loss 2.6232e-01 (2.5851e-01)	Acc 0.729492 (0.729377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.693887)
Training Loss of Epoch 12: 0.25856004529367616
Training Acc of Epoch 12: 0.7293254573170732
Testing Acc of Epoch 12: 0.6938869565217392
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.8830e-01 (2.8830e-01)	Acc 0.690430 (0.690430)
Epoch: [13][300/616]	Loss 2.6568e-01 (2.6000e-01)	Acc 0.715820 (0.728827)
Epoch: [13][600/616]	Loss 2.6386e-01 (2.5996e-01)	Acc 0.724609 (0.728213)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.734409)
Training Loss of Epoch 13: 0.25970021098609863
Training Acc of Epoch 13: 0.728541031504065
Testing Acc of Epoch 13: 0.7344086956521739
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6579e-01 (2.6579e-01)	Acc 0.728516 (0.728516)
Epoch: [14][300/616]	Loss 2.5921e-01 (2.5945e-01)	Acc 0.713867 (0.728155)
Epoch: [14][600/616]	Loss 2.6968e-01 (2.5990e-01)	Acc 0.709961 (0.728088)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.728696)
Training Loss of Epoch 14: 0.2599257771804081
Training Acc of Epoch 14: 0.7280948297764228
Testing Acc of Epoch 14: 0.7286956521739131
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.6570e-01 (2.6570e-01)	Acc 0.717773 (0.717773)
Epoch: [15][300/616]	Loss 2.4880e-01 (2.5950e-01)	Acc 0.745117 (0.727909)
Epoch: [15][600/616]	Loss 2.5730e-01 (2.5974e-01)	Acc 0.734375 (0.727965)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.732861)
Training Loss of Epoch 15: 0.2597010588258263
Training Acc of Epoch 15: 0.728028137703252
Testing Acc of Epoch 15: 0.7328608695652173
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.5867e-01 (2.5867e-01)	Acc 0.725586 (0.725586)
Epoch: [16][300/616]	Loss 2.5489e-01 (2.5857e-01)	Acc 0.737305 (0.729369)
Epoch: [16][600/616]	Loss 2.4535e-01 (2.5923e-01)	Acc 0.749023 (0.729219)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.729748)
Training Loss of Epoch 16: 0.2592779584047271
Training Acc of Epoch 16: 0.7291793699186991
Testing Acc of Epoch 16: 0.7297478260869565
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.5765e-01 (2.5765e-01)	Acc 0.738281 (0.738281)
Epoch: [17][300/616]	Loss 2.5020e-01 (2.5936e-01)	Acc 0.733398 (0.728795)
Epoch: [17][600/616]	Loss 2.7215e-01 (2.6003e-01)	Acc 0.718750 (0.728049)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.730517)
Training Loss of Epoch 17: 0.259891160741085
Training Acc of Epoch 17: 0.7281599339430894
Testing Acc of Epoch 17: 0.7305173913043478
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.5814e-01 (2.5814e-01)	Acc 0.739258 (0.739258)
Epoch: [18][300/616]	Loss 2.6320e-01 (2.6025e-01)	Acc 0.727539 (0.727296)
Epoch: [18][600/616]	Loss 2.4019e-01 (2.5935e-01)	Acc 0.756836 (0.728400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.729887)
Training Loss of Epoch 18: 0.25921254252515186
Training Acc of Epoch 18: 0.7285076854674797
Testing Acc of Epoch 18: 0.7298869565217392
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4332e-01 (2.4332e-01)	Acc 0.751953 (0.751953)
Epoch: [19][300/616]	Loss 2.7110e-01 (2.6111e-01)	Acc 0.716797 (0.726417)
Epoch: [19][600/616]	Loss 2.7362e-01 (2.5935e-01)	Acc 0.706055 (0.728483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.732430)
Training Loss of Epoch 19: 0.2595430793316384
Training Acc of Epoch 19: 0.728221862296748
Testing Acc of Epoch 19: 0.7324304347826087
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6515e-01 (2.6515e-01)	Acc 0.711914 (0.711914)
Epoch: [20][300/616]	Loss 2.7417e-01 (2.5879e-01)	Acc 0.710938 (0.729525)
Epoch: [20][600/616]	Loss 2.7151e-01 (2.5975e-01)	Acc 0.703125 (0.727906)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.725417)
Training Loss of Epoch 20: 0.25986053180403823
Training Acc of Epoch 20: 0.7277534298780488
Testing Acc of Epoch 20: 0.7254173913043478
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4745e-01 (2.4745e-01)	Acc 0.725586 (0.725586)
Epoch: [21][300/616]	Loss 2.4857e-01 (2.5845e-01)	Acc 0.737305 (0.729187)
Epoch: [21][600/616]	Loss 2.8141e-01 (2.5931e-01)	Acc 0.702148 (0.728386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736143)
Training Loss of Epoch 21: 0.2594539473211862
Training Acc of Epoch 21: 0.728220274390244
Testing Acc of Epoch 21: 0.7361434782608696
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4796e-01 (2.4796e-01)	Acc 0.755859 (0.755859)
Epoch: [22][300/616]	Loss 2.4909e-01 (2.6091e-01)	Acc 0.750000 (0.727195)
Epoch: [22][600/616]	Loss 2.6555e-01 (2.6053e-01)	Acc 0.735352 (0.727526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.727596)
Training Loss of Epoch 22: 0.26072597077222376
Training Acc of Epoch 22: 0.7272786458333333
Testing Acc of Epoch 22: 0.7275956521739131
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.5970e-01 (2.5970e-01)	Acc 0.728516 (0.728516)
Epoch: [23][300/616]	Loss 2.5814e-01 (2.5973e-01)	Acc 0.733398 (0.728584)
Epoch: [23][600/616]	Loss 2.5250e-01 (2.6023e-01)	Acc 0.744141 (0.727617)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.730522)
Training Loss of Epoch 23: 0.2603528239378115
Training Acc of Epoch 23: 0.7275279471544716
Testing Acc of Epoch 23: 0.7305217391304348
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.5148e-01 (2.5148e-01)	Acc 0.727539 (0.727539)
Epoch: [24][300/616]	Loss 2.5783e-01 (2.5877e-01)	Acc 0.730469 (0.730102)
Epoch: [24][600/616]	Loss 2.6189e-01 (2.5975e-01)	Acc 0.719727 (0.728587)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.719796)
Training Loss of Epoch 24: 0.2597786857829831
Training Acc of Epoch 24: 0.7285966082317074
Testing Acc of Epoch 24: 0.7197956521739131
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4455e-01 (2.4455e-01)	Acc 0.744141 (0.744141)
Epoch: [25][300/616]	Loss 2.5565e-01 (2.6018e-01)	Acc 0.734375 (0.727701)
Epoch: [25][600/616]	Loss 2.6049e-01 (2.6105e-01)	Acc 0.739258 (0.727169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.735013)
Training Loss of Epoch 25: 0.2609604551055567
Training Acc of Epoch 25: 0.727319931402439
Testing Acc of Epoch 25: 0.7350130434782609
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.6343e-01 (2.6343e-01)	Acc 0.720703 (0.720703)
Epoch: [26][300/616]	Loss 2.5862e-01 (2.5968e-01)	Acc 0.722656 (0.728006)
Epoch: [26][600/616]	Loss 2.6545e-01 (2.5967e-01)	Acc 0.707031 (0.727736)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.736104)
Training Loss of Epoch 26: 0.2597292109475872
Training Acc of Epoch 26: 0.7276248094512195
Testing Acc of Epoch 26: 0.7361043478260869
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4413e-01 (2.4413e-01)	Acc 0.732422 (0.732422)
Epoch: [27][300/616]	Loss 2.4161e-01 (2.6037e-01)	Acc 0.757812 (0.727448)
Epoch: [27][600/616]	Loss 2.5799e-01 (2.6013e-01)	Acc 0.725586 (0.727414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.735087)
Training Loss of Epoch 27: 0.25997409042788716
Training Acc of Epoch 27: 0.727588287601626
Testing Acc of Epoch 27: 0.7350869565217392
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4344e-01 (2.4344e-01)	Acc 0.745117 (0.745117)
Epoch: [28][300/616]	Loss 2.5692e-01 (2.6162e-01)	Acc 0.728516 (0.726316)
Epoch: [28][600/616]	Loss 2.6619e-01 (2.6076e-01)	Acc 0.715820 (0.727481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.737574)
Training Loss of Epoch 28: 0.2607973779604687
Training Acc of Epoch 28: 0.7274263211382114
Testing Acc of Epoch 28: 0.7375739130434783
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.6361e-01 (2.6361e-01)	Acc 0.725586 (0.725586)
Epoch: [29][300/616]	Loss 2.6923e-01 (2.5894e-01)	Acc 0.701172 (0.729382)
Epoch: [29][600/616]	Loss 2.7385e-01 (2.5938e-01)	Acc 0.706055 (0.728519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.730474)
Training Loss of Epoch 29: 0.2594309184851685
Training Acc of Epoch 29: 0.7285743775406504
Testing Acc of Epoch 29: 0.7304739130434783
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.3582e-01 (2.3582e-01)	Acc 0.766602 (0.766602)
Epoch: [30][300/616]	Loss 2.5083e-01 (2.5842e-01)	Acc 0.748047 (0.729664)
Epoch: [30][600/616]	Loss 2.5187e-01 (2.5951e-01)	Acc 0.737305 (0.728231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.718204)
Training Loss of Epoch 30: 0.2596309929601545
Training Acc of Epoch 30: 0.7280725990853658
Testing Acc of Epoch 30: 0.718204347826087
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.7799e-01 (2.7799e-01)	Acc 0.702148 (0.702148)
Epoch: [31][300/616]	Loss 2.4727e-01 (2.5958e-01)	Acc 0.747070 (0.728512)
Epoch: [31][600/616]	Loss 2.3909e-01 (2.5948e-01)	Acc 0.755859 (0.728407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.730013)
Training Loss of Epoch 31: 0.25948533091118664
Training Acc of Epoch 31: 0.7282885543699187
Testing Acc of Epoch 31: 0.7300130434782609
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4518e-01 (2.4518e-01)	Acc 0.742188 (0.742188)
Epoch: [32][300/616]	Loss 2.5922e-01 (2.5792e-01)	Acc 0.737305 (0.730563)
Epoch: [32][600/616]	Loss 2.4371e-01 (2.6005e-01)	Acc 0.743164 (0.727966)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.736470)
Training Loss of Epoch 32: 0.26012615084163543
Training Acc of Epoch 32: 0.7279090447154472
Testing Acc of Epoch 32: 0.7364695652173913
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4975e-01 (2.4975e-01)	Acc 0.739258 (0.739258)
Epoch: [33][300/616]	Loss 2.5682e-01 (2.5903e-01)	Acc 0.742188 (0.728217)
Epoch: [33][600/616]	Loss 2.4485e-01 (2.5876e-01)	Acc 0.739258 (0.729234)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.735883)
Training Loss of Epoch 33: 0.25866936295013115
Training Acc of Epoch 33: 0.7293222815040651
Testing Acc of Epoch 33: 0.7358826086956521
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.5092e-01 (2.5092e-01)	Acc 0.741211 (0.741211)
Epoch: [34][300/616]	Loss 2.4856e-01 (2.5987e-01)	Acc 0.737305 (0.727445)
Epoch: [34][600/616]	Loss 2.5876e-01 (2.5918e-01)	Acc 0.733398 (0.728540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.727361)
Training Loss of Epoch 34: 0.25914369782781216
Training Acc of Epoch 34: 0.7285997840447155
Testing Acc of Epoch 34: 0.7273608695652174
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.5672e-01 (2.5672e-01)	Acc 0.718750 (0.718750)
Epoch: [35][300/616]	Loss 2.6297e-01 (2.5902e-01)	Acc 0.710938 (0.728808)
Epoch: [35][600/616]	Loss 2.6971e-01 (2.5963e-01)	Acc 0.718750 (0.728283)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.727391)
Training Loss of Epoch 35: 0.25970190925811365
Training Acc of Epoch 35: 0.7281091209349594
Testing Acc of Epoch 35: 0.7273913043478261
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.6847e-01 (2.6847e-01)	Acc 0.709961 (0.709961)
Epoch: [36][300/616]	Loss 2.6705e-01 (2.5919e-01)	Acc 0.705078 (0.728396)
Epoch: [36][600/616]	Loss 2.6168e-01 (2.5908e-01)	Acc 0.724609 (0.729154)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.728600)
Training Loss of Epoch 36: 0.25900138795860417
Training Acc of Epoch 36: 0.7291333206300813
Testing Acc of Epoch 36: 0.7286
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4665e-01 (2.4665e-01)	Acc 0.745117 (0.745117)
Epoch: [37][300/616]	Loss 2.4773e-01 (2.6177e-01)	Acc 0.749023 (0.726595)
Epoch: [37][600/616]	Loss 2.6669e-01 (2.6123e-01)	Acc 0.722656 (0.727004)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.677885 (0.699235)
Training Loss of Epoch 37: 0.2611097109511616
Training Acc of Epoch 37: 0.7270626905487805
Testing Acc of Epoch 37: 0.6992347826086956
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.8135e-01 (2.8135e-01)	Acc 0.695312 (0.695312)
Epoch: [38][300/616]	Loss 2.4492e-01 (2.5711e-01)	Acc 0.755859 (0.731030)
Epoch: [38][600/616]	Loss 2.5692e-01 (2.5881e-01)	Acc 0.724609 (0.728941)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.729826)
Training Loss of Epoch 38: 0.25888261186882733
Training Acc of Epoch 38: 0.7289268927845528
Testing Acc of Epoch 38: 0.7298260869565217
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.6017e-01 (2.6017e-01)	Acc 0.727539 (0.727539)
Epoch: [39][300/616]	Loss 2.4839e-01 (2.5791e-01)	Acc 0.745117 (0.730083)
Epoch: [39][600/616]	Loss 2.5403e-01 (2.5886e-01)	Acc 0.747070 (0.729180)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.724878)
Training Loss of Epoch 39: 0.2588212196904469
Training Acc of Epoch 39: 0.7292143038617886
Testing Acc of Epoch 39: 0.7248782608695652
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.8785e-01 (2.8785e-01)	Acc 0.685547 (0.685547)
Epoch: [40][300/616]	Loss 2.6655e-01 (2.6140e-01)	Acc 0.707031 (0.727153)
Epoch: [40][600/616]	Loss 2.6182e-01 (2.6031e-01)	Acc 0.723633 (0.727559)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.737591)
Training Loss of Epoch 40: 0.2602316859291821
Training Acc of Epoch 40: 0.7276295731707317
Testing Acc of Epoch 40: 0.737591304347826
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.3959e-01 (2.3959e-01)	Acc 0.765625 (0.765625)
Epoch: [41][300/616]	Loss 2.6162e-01 (2.6226e-01)	Acc 0.738281 (0.725820)
Epoch: [41][600/616]	Loss 2.4659e-01 (2.6055e-01)	Acc 0.745117 (0.727606)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.728283)
Training Loss of Epoch 41: 0.2605489852467204
Training Acc of Epoch 41: 0.7276787982723577
Testing Acc of Epoch 41: 0.7282826086956522
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.7067e-01 (2.7067e-01)	Acc 0.714844 (0.714844)
Epoch: [42][300/616]	Loss 2.6472e-01 (2.5959e-01)	Acc 0.723633 (0.728327)
Epoch: [42][600/616]	Loss 2.5199e-01 (2.5961e-01)	Acc 0.749023 (0.728363)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.734922)
Training Loss of Epoch 42: 0.25957447239538517
Training Acc of Epoch 42: 0.7284743394308943
Testing Acc of Epoch 42: 0.7349217391304348
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.4730e-01 (2.4730e-01)	Acc 0.735352 (0.735352)
Epoch: [43][300/616]	Loss 2.7150e-01 (2.5915e-01)	Acc 0.713867 (0.728739)
Epoch: [43][600/616]	Loss 2.4754e-01 (2.5899e-01)	Acc 0.743164 (0.728787)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.732700)
Training Loss of Epoch 43: 0.2589569217063547
Training Acc of Epoch 43: 0.7287966844512195
Testing Acc of Epoch 43: 0.7327
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4673e-01 (2.4673e-01)	Acc 0.732422 (0.732422)
Epoch: [44][300/616]	Loss 2.5408e-01 (2.6190e-01)	Acc 0.750000 (0.726063)
Epoch: [44][600/616]	Loss 2.7210e-01 (2.6112e-01)	Acc 0.710938 (0.727185)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.727457)
Training Loss of Epoch 44: 0.26111375883342774
Training Acc of Epoch 44: 0.7272500635162602
Testing Acc of Epoch 44: 0.7274565217391304
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3544e-01 (2.3544e-01)	Acc 0.753906 (0.753906)
Epoch: [45][300/616]	Loss 2.5572e-01 (2.6026e-01)	Acc 0.750977 (0.728107)
Epoch: [45][600/616]	Loss 2.5881e-01 (2.6045e-01)	Acc 0.715820 (0.727404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.729078)
Training Loss of Epoch 45: 0.260474388483094
Training Acc of Epoch 45: 0.7274533155487805
Testing Acc of Epoch 45: 0.7290782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4305e-01 (2.4305e-01)	Acc 0.749023 (0.749023)
Epoch: [46][300/616]	Loss 2.8991e-01 (2.6228e-01)	Acc 0.685547 (0.726238)
Epoch: [46][600/616]	Loss 2.5672e-01 (2.6166e-01)	Acc 0.743164 (0.726291)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.723822)
Training Loss of Epoch 46: 0.26159402604510146
Training Acc of Epoch 46: 0.7262846163617886
Testing Acc of Epoch 46: 0.7238217391304348
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4491e-01 (2.4491e-01)	Acc 0.746094 (0.746094)
Epoch: [47][300/616]	Loss 2.7202e-01 (2.6110e-01)	Acc 0.705078 (0.726329)
Epoch: [47][600/616]	Loss 2.6308e-01 (2.6030e-01)	Acc 0.730469 (0.727581)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.698139)
Training Loss of Epoch 47: 0.2602923839072871
Training Acc of Epoch 47: 0.727513655995935
Testing Acc of Epoch 47: 0.6981391304347826
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.8912e-01 (2.8912e-01)	Acc 0.688477 (0.688477)
Epoch: [48][300/616]	Loss 2.7502e-01 (2.6190e-01)	Acc 0.705078 (0.726235)
Epoch: [48][600/616]	Loss 2.7145e-01 (2.6080e-01)	Acc 0.700195 (0.727563)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.730126)
Training Loss of Epoch 48: 0.26080280801629635
Training Acc of Epoch 48: 0.7275581173780488
Testing Acc of Epoch 48: 0.7301260869565217
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.5635e-01 (2.5635e-01)	Acc 0.729492 (0.729492)
Epoch: [49][300/616]	Loss 2.7523e-01 (2.5833e-01)	Acc 0.706055 (0.729476)
Epoch: [49][600/616]	Loss 2.5671e-01 (2.5903e-01)	Acc 0.731445 (0.729068)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735835)
Training Loss of Epoch 49: 0.25911794527759396
Training Acc of Epoch 49: 0.7289951727642277
Testing Acc of Epoch 49: 0.7358347826086956
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4705e-01 (2.4705e-01)	Acc 0.748047 (0.748047)
Epoch: [50][300/616]	Loss 2.6357e-01 (2.6088e-01)	Acc 0.722656 (0.727526)
Epoch: [50][600/616]	Loss 2.5985e-01 (2.6186e-01)	Acc 0.724609 (0.726455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.734839)
Training Loss of Epoch 50: 0.2617688933039099
Training Acc of Epoch 50: 0.7265450330284553
Testing Acc of Epoch 50: 0.7348391304347827
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.4177e-01 (2.4177e-01)	Acc 0.747070 (0.747070)
Epoch: [51][300/616]	Loss 2.6354e-01 (2.6021e-01)	Acc 0.725586 (0.727597)
Epoch: [51][600/616]	Loss 2.5482e-01 (2.6007e-01)	Acc 0.724609 (0.727633)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.725574)
Training Loss of Epoch 51: 0.25993271702673376
Training Acc of Epoch 51: 0.727782012195122
Testing Acc of Epoch 51: 0.7255739130434783
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.5406e-01 (2.5406e-01)	Acc 0.739258 (0.739258)
Epoch: [52][300/616]	Loss 2.5237e-01 (2.5862e-01)	Acc 0.730469 (0.729979)
Epoch: [52][600/616]	Loss 2.4476e-01 (2.5966e-01)	Acc 0.742188 (0.728662)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.731257)
Training Loss of Epoch 52: 0.259876265467667
Training Acc of Epoch 52: 0.7284457571138211
Testing Acc of Epoch 52: 0.7312565217391305
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5482e-01 (2.5482e-01)	Acc 0.738281 (0.738281)
Epoch: [53][300/616]	Loss 2.6444e-01 (2.5876e-01)	Acc 0.721680 (0.728811)
Epoch: [53][600/616]	Loss 2.5995e-01 (2.5922e-01)	Acc 0.719727 (0.728678)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.726891)
Training Loss of Epoch 53: 0.259079794937033
Training Acc of Epoch 53: 0.7288840193089431
Testing Acc of Epoch 53: 0.7268913043478261
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.6007e-01 (2.6007e-01)	Acc 0.729492 (0.729492)
Epoch: [54][300/616]	Loss 2.6859e-01 (2.6109e-01)	Acc 0.729492 (0.726433)
Epoch: [54][600/616]	Loss 2.5079e-01 (2.5983e-01)	Acc 0.750977 (0.728088)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.731035)
Training Loss of Epoch 54: 0.25973444360543074
Training Acc of Epoch 54: 0.7282552083333333
Testing Acc of Epoch 54: 0.7310347826086957
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.4984e-01 (2.4984e-01)	Acc 0.746094 (0.746094)
Epoch: [55][300/616]	Loss 2.7497e-01 (2.5861e-01)	Acc 0.708984 (0.729732)
Epoch: [55][600/616]	Loss 2.6251e-01 (2.5929e-01)	Acc 0.730469 (0.728972)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.732643)
Training Loss of Epoch 55: 0.2592902379791911
Training Acc of Epoch 55: 0.7289411839430894
Testing Acc of Epoch 55: 0.7326434782608696
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4359e-01 (2.4359e-01)	Acc 0.746094 (0.746094)
Epoch: [56][300/616]	Loss 2.7212e-01 (2.5961e-01)	Acc 0.709961 (0.728765)
Epoch: [56][600/616]	Loss 2.4263e-01 (2.6084e-01)	Acc 0.742188 (0.727802)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.727622)
Training Loss of Epoch 56: 0.26068443101111466
Training Acc of Epoch 56: 0.7278995172764228
Testing Acc of Epoch 56: 0.7276217391304348
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5514e-01 (2.5514e-01)	Acc 0.731445 (0.731445)
Epoch: [57][300/616]	Loss 2.5250e-01 (2.5953e-01)	Acc 0.735352 (0.729528)
Epoch: [57][600/616]	Loss 2.5029e-01 (2.6028e-01)	Acc 0.735352 (0.728171)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.727739)
Training Loss of Epoch 57: 0.2603172230768979
Training Acc of Epoch 57: 0.7280598958333333
Testing Acc of Epoch 57: 0.7277391304347826
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.6909e-01 (2.6909e-01)	Acc 0.713867 (0.713867)
Epoch: [58][300/616]	Loss 2.6316e-01 (2.6166e-01)	Acc 0.732422 (0.726167)
Epoch: [58][600/616]	Loss 2.6897e-01 (2.6170e-01)	Acc 0.721680 (0.726194)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.732552)
Training Loss of Epoch 58: 0.2616025478132372
Training Acc of Epoch 58: 0.7263211382113821
Testing Acc of Epoch 58: 0.7325521739130435
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3916e-01 (2.3916e-01)	Acc 0.758789 (0.758789)
Epoch: [59][300/616]	Loss 2.6143e-01 (2.5973e-01)	Acc 0.727539 (0.728272)
Epoch: [59][600/616]	Loss 2.5886e-01 (2.6047e-01)	Acc 0.735352 (0.727737)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.737230)
Training Loss of Epoch 59: 0.26039594482115613
Training Acc of Epoch 59: 0.7279249237804878
Testing Acc of Epoch 59: 0.7372304347826087
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5813e-01 (2.5813e-01)	Acc 0.733398 (0.733398)
Epoch: [60][300/616]	Loss 2.6408e-01 (2.6049e-01)	Acc 0.723633 (0.727221)
Epoch: [60][600/616]	Loss 2.6206e-01 (2.5918e-01)	Acc 0.734375 (0.728711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.737496)
Training Loss of Epoch 60: 0.2591683167994507
Training Acc of Epoch 60: 0.7287411077235773
Testing Acc of Epoch 60: 0.737495652173913
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.5239e-01 (2.5239e-01)	Acc 0.730469 (0.730469)
Epoch: [61][300/616]	Loss 2.5327e-01 (2.5990e-01)	Acc 0.742188 (0.727867)
Epoch: [61][600/616]	Loss 2.6430e-01 (2.6048e-01)	Acc 0.719727 (0.727667)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.731839)
Training Loss of Epoch 61: 0.26059762430869465
Training Acc of Epoch 61: 0.7275215955284553
Testing Acc of Epoch 61: 0.7318391304347827
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.5981e-01 (2.5981e-01)	Acc 0.731445 (0.731445)
Epoch: [62][300/616]	Loss 2.6914e-01 (2.6153e-01)	Acc 0.715820 (0.726381)
Epoch: [62][600/616]	Loss 2.3935e-01 (2.6152e-01)	Acc 0.750000 (0.726601)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.731357)
Training Loss of Epoch 62: 0.2614433267009937
Training Acc of Epoch 62: 0.7266164888211382
Testing Acc of Epoch 62: 0.7313565217391305
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.5153e-01 (2.5153e-01)	Acc 0.726562 (0.726562)
Epoch: [63][300/616]	Loss 2.5484e-01 (2.5931e-01)	Acc 0.730469 (0.728772)
Epoch: [63][600/616]	Loss 2.4027e-01 (2.5892e-01)	Acc 0.753906 (0.728990)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.735626)
Training Loss of Epoch 63: 0.2589986567817083
Training Acc of Epoch 63: 0.7288919588414634
Testing Acc of Epoch 63: 0.7356260869565218
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.6301e-01 (2.6301e-01)	Acc 0.725586 (0.725586)
Epoch: [64][300/616]	Loss 2.7368e-01 (2.6025e-01)	Acc 0.704102 (0.728240)
Epoch: [64][600/616]	Loss 2.5060e-01 (2.5852e-01)	Acc 0.737305 (0.729370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.733061)
Training Loss of Epoch 64: 0.25848334460723693
Training Acc of Epoch 64: 0.7293985010162601
Testing Acc of Epoch 64: 0.7330608695652174
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.6455e-01 (2.6455e-01)	Acc 0.708008 (0.708008)
Epoch: [65][300/616]	Loss 2.8139e-01 (2.6071e-01)	Acc 0.703125 (0.728240)
Epoch: [65][600/616]	Loss 2.5782e-01 (2.6053e-01)	Acc 0.716797 (0.728111)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734822)
Training Loss of Epoch 65: 0.2604615066109634
Training Acc of Epoch 65: 0.7282425050813008
Testing Acc of Epoch 65: 0.7348217391304348
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.5175e-01 (2.5175e-01)	Acc 0.721680 (0.721680)
Epoch: [66][300/616]	Loss 2.5234e-01 (2.6093e-01)	Acc 0.735352 (0.726540)
Epoch: [66][600/616]	Loss 2.6119e-01 (2.6006e-01)	Acc 0.722656 (0.727771)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.733370)
Training Loss of Epoch 66: 0.2600383952865756
Training Acc of Epoch 66: 0.727712144308943
Testing Acc of Epoch 66: 0.7333695652173913
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.4589e-01 (2.4589e-01)	Acc 0.735352 (0.735352)
Epoch: [67][300/616]	Loss 2.6793e-01 (2.5997e-01)	Acc 0.722656 (0.727533)
Epoch: [67][600/616]	Loss 2.4841e-01 (2.6008e-01)	Acc 0.733398 (0.727953)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.714739)
Training Loss of Epoch 67: 0.2600054988773858
Training Acc of Epoch 67: 0.7279677972560976
Testing Acc of Epoch 67: 0.7147391304347827
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.6635e-01 (2.6635e-01)	Acc 0.729492 (0.729492)
Epoch: [68][300/616]	Loss 2.5823e-01 (2.6006e-01)	Acc 0.740234 (0.728464)
Epoch: [68][600/616]	Loss 2.5571e-01 (2.5981e-01)	Acc 0.737305 (0.728452)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735791)
Training Loss of Epoch 68: 0.25961053584649313
Training Acc of Epoch 68: 0.7286982342479674
Testing Acc of Epoch 68: 0.7357913043478261
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5053e-01 (2.5053e-01)	Acc 0.739258 (0.739258)
Epoch: [69][300/616]	Loss 2.5973e-01 (2.5999e-01)	Acc 0.719727 (0.727666)
Epoch: [69][600/616]	Loss 2.4898e-01 (2.6102e-01)	Acc 0.750000 (0.726840)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.725943)
Training Loss of Epoch 69: 0.2609779512252265
Training Acc of Epoch 69: 0.7269070757113821
Testing Acc of Epoch 69: 0.7259434782608696
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.7219e-01 (2.7219e-01)	Acc 0.719727 (0.719727)
Epoch: [70][300/616]	Loss 2.5002e-01 (2.5904e-01)	Acc 0.746094 (0.729061)
Epoch: [70][600/616]	Loss 2.4642e-01 (2.5869e-01)	Acc 0.746094 (0.729695)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.731265)
Training Loss of Epoch 70: 0.25879955250557846
Training Acc of Epoch 70: 0.729516006097561
Testing Acc of Epoch 70: 0.7312652173913043
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.6205e-01 (2.6205e-01)	Acc 0.726562 (0.726562)
Epoch: [71][300/616]	Loss 2.5945e-01 (2.5860e-01)	Acc 0.730469 (0.729849)
Epoch: [71][600/616]	Loss 2.5293e-01 (2.5862e-01)	Acc 0.740234 (0.729481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.725943)
Training Loss of Epoch 71: 0.25866512976041656
Training Acc of Epoch 71: 0.7293969131097561
Testing Acc of Epoch 71: 0.7259434782608696
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.6275e-01 (2.6275e-01)	Acc 0.738281 (0.738281)
Epoch: [72][300/616]	Loss 2.5106e-01 (2.5876e-01)	Acc 0.738281 (0.729710)
Epoch: [72][600/616]	Loss 2.6939e-01 (2.5846e-01)	Acc 0.721680 (0.729676)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.719235)
Training Loss of Epoch 72: 0.25848434596526915
Training Acc of Epoch 72: 0.7296875
Testing Acc of Epoch 72: 0.7192347826086957
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.7513e-01 (2.7513e-01)	Acc 0.705078 (0.705078)
Epoch: [73][300/616]	Loss 2.6100e-01 (2.5974e-01)	Acc 0.717773 (0.728428)
Epoch: [73][600/616]	Loss 2.7331e-01 (2.5974e-01)	Acc 0.709961 (0.728347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.719843)
Training Loss of Epoch 73: 0.2596704013948518
Training Acc of Epoch 73: 0.7283838287601626
Testing Acc of Epoch 73: 0.7198434782608696
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.7446e-01 (2.7446e-01)	Acc 0.705078 (0.705078)
Epoch: [74][300/616]	Loss 2.6461e-01 (2.5945e-01)	Acc 0.728516 (0.729242)
Epoch: [74][600/616]	Loss 2.5946e-01 (2.5928e-01)	Acc 0.721680 (0.729349)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.727661)
Training Loss of Epoch 74: 0.2592159165357186
Training Acc of Epoch 74: 0.7293540396341464
Testing Acc of Epoch 74: 0.7276608695652174
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5047e-01 (2.5047e-01)	Acc 0.745117 (0.745117)
Epoch: [75][300/616]	Loss 2.4245e-01 (2.4838e-01)	Acc 0.743164 (0.739251)
Epoch: [75][600/616]	Loss 2.4935e-01 (2.4871e-01)	Acc 0.741211 (0.739110)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.743096)
Training Loss of Epoch 75: 0.24872463483635973
Training Acc of Epoch 75: 0.7390021595528455
Testing Acc of Epoch 75: 0.743095652173913
Model with the best training loss saved! The loss is 0.24872463483635973
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3332e-01 (2.3332e-01)	Acc 0.749023 (0.749023)
Epoch: [76][300/616]	Loss 2.5783e-01 (2.5018e-01)	Acc 0.725586 (0.736909)
Epoch: [76][600/616]	Loss 2.7927e-01 (2.4985e-01)	Acc 0.700195 (0.737540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.735296)
Training Loss of Epoch 76: 0.2500331794585639
Training Acc of Epoch 76: 0.7373920223577236
Testing Acc of Epoch 76: 0.735295652173913
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4678e-01 (2.4678e-01)	Acc 0.736328 (0.736328)
Epoch: [77][300/616]	Loss 2.6752e-01 (2.4958e-01)	Acc 0.717773 (0.737350)
Epoch: [77][600/616]	Loss 2.4841e-01 (2.5037e-01)	Acc 0.724609 (0.736858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740904)
Training Loss of Epoch 77: 0.25042015677545126
Training Acc of Epoch 77: 0.7368076727642277
Testing Acc of Epoch 77: 0.7409043478260869
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4598e-01 (2.4598e-01)	Acc 0.731445 (0.731445)
Epoch: [78][300/616]	Loss 2.4033e-01 (2.5091e-01)	Acc 0.747070 (0.736539)
Epoch: [78][600/616]	Loss 2.5928e-01 (2.5026e-01)	Acc 0.721680 (0.737043)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736865)
Training Loss of Epoch 78: 0.25040207829901845
Training Acc of Epoch 78: 0.7368378429878049
Testing Acc of Epoch 78: 0.7368652173913044
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.5367e-01 (2.5367e-01)	Acc 0.728516 (0.728516)
Epoch: [79][300/616]	Loss 2.4491e-01 (2.4921e-01)	Acc 0.738281 (0.737999)
Epoch: [79][600/616]	Loss 2.6177e-01 (2.5104e-01)	Acc 0.743164 (0.736430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734204)
Training Loss of Epoch 79: 0.2510421738391969
Training Acc of Epoch 79: 0.7364376905487805
Testing Acc of Epoch 79: 0.734204347826087
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4707e-01 (2.4707e-01)	Acc 0.753906 (0.753906)
Epoch: [80][300/616]	Loss 2.6286e-01 (2.5114e-01)	Acc 0.724609 (0.736085)
Epoch: [80][600/616]	Loss 2.3920e-01 (2.4991e-01)	Acc 0.747070 (0.737063)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743600)
Training Loss of Epoch 80: 0.24977083259481725
Training Acc of Epoch 80: 0.7372506986788618
Testing Acc of Epoch 80: 0.7436
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3191e-01 (2.3191e-01)	Acc 0.769531 (0.769531)
Epoch: [81][300/616]	Loss 2.5270e-01 (2.5005e-01)	Acc 0.728516 (0.737288)
Epoch: [81][600/616]	Loss 2.4886e-01 (2.4964e-01)	Acc 0.746094 (0.737651)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.738061)
Training Loss of Epoch 81: 0.2497106328727753
Training Acc of Epoch 81: 0.7375698678861788
Testing Acc of Epoch 81: 0.7380608695652174
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.6885e-01 (2.6885e-01)	Acc 0.723633 (0.723633)
Epoch: [82][300/616]	Loss 2.5419e-01 (2.4878e-01)	Acc 0.726562 (0.737986)
Epoch: [82][600/616]	Loss 2.5360e-01 (2.4965e-01)	Acc 0.718750 (0.737636)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.743570)
Training Loss of Epoch 82: 0.24968116956997694
Training Acc of Epoch 82: 0.7375381097560976
Testing Acc of Epoch 82: 0.7435695652173913
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4622e-01 (2.4622e-01)	Acc 0.747070 (0.747070)
Epoch: [83][300/616]	Loss 2.5185e-01 (2.4999e-01)	Acc 0.730469 (0.737081)
Epoch: [83][600/616]	Loss 2.4111e-01 (2.4962e-01)	Acc 0.750977 (0.737592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.736439)
Training Loss of Epoch 83: 0.2496134538233765
Training Acc of Epoch 83: 0.7376127413617887
Testing Acc of Epoch 83: 0.7364391304347826
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.6656e-01 (2.6656e-01)	Acc 0.713867 (0.713867)
Epoch: [84][300/616]	Loss 2.4117e-01 (2.4935e-01)	Acc 0.743164 (0.737353)
Epoch: [84][600/616]	Loss 2.4666e-01 (2.4938e-01)	Acc 0.730469 (0.737685)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.737939)
Training Loss of Epoch 84: 0.24930015853265436
Training Acc of Epoch 84: 0.7376841971544715
Testing Acc of Epoch 84: 0.7379391304347827
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.5021e-01 (2.5021e-01)	Acc 0.727539 (0.727539)
Epoch: [85][300/616]	Loss 2.2368e-01 (2.4927e-01)	Acc 0.772461 (0.737850)
Epoch: [85][600/616]	Loss 2.3645e-01 (2.4923e-01)	Acc 0.750000 (0.737867)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.739587)
Training Loss of Epoch 85: 0.2491158813480439
Training Acc of Epoch 85: 0.7380589430894309
Testing Acc of Epoch 85: 0.7395869565217391
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3438e-01 (2.3438e-01)	Acc 0.754883 (0.754883)
Epoch: [86][300/616]	Loss 2.4499e-01 (2.4792e-01)	Acc 0.737305 (0.739219)
Epoch: [86][600/616]	Loss 2.6633e-01 (2.4919e-01)	Acc 0.730469 (0.738041)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.740465)
Training Loss of Epoch 86: 0.24912845366369418
Training Acc of Epoch 86: 0.7380573551829268
Testing Acc of Epoch 86: 0.7404652173913043
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3449e-01 (2.3449e-01)	Acc 0.746094 (0.746094)
Epoch: [87][300/616]	Loss 2.2592e-01 (2.5024e-01)	Acc 0.768555 (0.736753)
Epoch: [87][600/616]	Loss 2.5673e-01 (2.5018e-01)	Acc 0.726562 (0.736547)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.738426)
Training Loss of Epoch 87: 0.25001754448181246
Training Acc of Epoch 87: 0.736815612296748
Testing Acc of Epoch 87: 0.7384260869565218
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3293e-01 (2.3293e-01)	Acc 0.741211 (0.741211)
Epoch: [88][300/616]	Loss 2.7377e-01 (2.5075e-01)	Acc 0.708984 (0.736604)
Epoch: [88][600/616]	Loss 2.5828e-01 (2.5073e-01)	Acc 0.727539 (0.736322)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.733548)
Training Loss of Epoch 88: 0.25057342866571936
Training Acc of Epoch 88: 0.7365170858739838
Testing Acc of Epoch 88: 0.7335478260869566
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.6388e-01 (2.6388e-01)	Acc 0.738281 (0.738281)
Epoch: [89][300/616]	Loss 2.5485e-01 (2.5065e-01)	Acc 0.740234 (0.735971)
Epoch: [89][600/616]	Loss 2.6306e-01 (2.5039e-01)	Acc 0.718750 (0.736294)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.728996)
Training Loss of Epoch 89: 0.25058506077867215
Training Acc of Epoch 89: 0.7360788236788618
Testing Acc of Epoch 89: 0.728995652173913
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.6938e-01 (2.6938e-01)	Acc 0.708984 (0.708984)
Epoch: [90][300/616]	Loss 2.6047e-01 (2.5109e-01)	Acc 0.736328 (0.735978)
Epoch: [90][600/616]	Loss 2.7254e-01 (2.5082e-01)	Acc 0.706055 (0.736247)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.738896)
Training Loss of Epoch 90: 0.25074981993776024
Training Acc of Epoch 90: 0.7363979928861789
Testing Acc of Epoch 90: 0.7388956521739131
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.5192e-01 (2.5192e-01)	Acc 0.729492 (0.729492)
Epoch: [91][300/616]	Loss 2.4284e-01 (2.5060e-01)	Acc 0.739258 (0.736494)
Epoch: [91][600/616]	Loss 2.6193e-01 (2.5150e-01)	Acc 0.720703 (0.735449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739222)
Training Loss of Epoch 91: 0.2514720177505074
Training Acc of Epoch 91: 0.7354817708333333
Testing Acc of Epoch 91: 0.7392217391304348
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3847e-01 (2.3847e-01)	Acc 0.747070 (0.747070)
Epoch: [92][300/616]	Loss 2.3768e-01 (2.5213e-01)	Acc 0.749023 (0.735374)
Epoch: [92][600/616]	Loss 2.6440e-01 (2.5253e-01)	Acc 0.719727 (0.734126)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.736765)
Training Loss of Epoch 92: 0.252570619936881
Training Acc of Epoch 92: 0.7340748856707318
Testing Acc of Epoch 92: 0.7367652173913043
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.6020e-01 (2.6020e-01)	Acc 0.722656 (0.722656)
Epoch: [93][300/616]	Loss 2.6258e-01 (2.5162e-01)	Acc 0.719727 (0.735011)
Epoch: [93][600/616]	Loss 2.6426e-01 (2.5166e-01)	Acc 0.720703 (0.735132)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.743109)
Training Loss of Epoch 93: 0.2516111847588686
Training Acc of Epoch 93: 0.7352372332317073
Testing Acc of Epoch 93: 0.743108695652174
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.5282e-01 (2.5282e-01)	Acc 0.745117 (0.745117)
Epoch: [94][300/616]	Loss 2.4812e-01 (2.5302e-01)	Acc 0.756836 (0.734268)
Epoch: [94][600/616]	Loss 2.5159e-01 (2.5180e-01)	Acc 0.744141 (0.735330)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740513)
Training Loss of Epoch 94: 0.25176968240156405
Training Acc of Epoch 94: 0.7353372713414634
Testing Acc of Epoch 94: 0.7405130434782609
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.4647e-01 (2.4647e-01)	Acc 0.728516 (0.728516)
Epoch: [95][300/616]	Loss 2.5552e-01 (2.5209e-01)	Acc 0.731445 (0.734891)
Epoch: [95][600/616]	Loss 2.5829e-01 (2.5161e-01)	Acc 0.728516 (0.735358)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737691)
Training Loss of Epoch 95: 0.25172226315106805
Training Acc of Epoch 95: 0.7352197662601626
Testing Acc of Epoch 95: 0.737691304347826
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.4698e-01 (2.4698e-01)	Acc 0.739258 (0.739258)
Epoch: [96][300/616]	Loss 2.3859e-01 (2.5113e-01)	Acc 0.770508 (0.735705)
Epoch: [96][600/616]	Loss 2.5676e-01 (2.5142e-01)	Acc 0.735352 (0.735719)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.735930)
Training Loss of Epoch 96: 0.25145680703283324
Training Acc of Epoch 96: 0.7356723196138212
Testing Acc of Epoch 96: 0.7359304347826087
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.5091e-01 (2.5091e-01)	Acc 0.727539 (0.727539)
Epoch: [97][300/616]	Loss 2.5113e-01 (2.5120e-01)	Acc 0.732422 (0.735890)
Epoch: [97][600/616]	Loss 2.6735e-01 (2.5199e-01)	Acc 0.717773 (0.735025)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.732661)
Training Loss of Epoch 97: 0.25203945929926586
Training Acc of Epoch 97: 0.7349482342479675
Testing Acc of Epoch 97: 0.7326608695652174
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4967e-01 (2.4967e-01)	Acc 0.742188 (0.742188)
Epoch: [98][300/616]	Loss 2.5752e-01 (2.5206e-01)	Acc 0.734375 (0.734654)
Epoch: [98][600/616]	Loss 2.4315e-01 (2.5183e-01)	Acc 0.748047 (0.734828)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.733109)
Training Loss of Epoch 98: 0.2518676866118501
Training Acc of Epoch 98: 0.7348529598577236
Testing Acc of Epoch 98: 0.733108695652174
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.5836e-01 (2.5836e-01)	Acc 0.715820 (0.715820)
Epoch: [99][300/616]	Loss 2.4106e-01 (2.5092e-01)	Acc 0.748047 (0.736315)
Epoch: [99][600/616]	Loss 2.4753e-01 (2.5147e-01)	Acc 0.736328 (0.735587)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736861)
Training Loss of Epoch 99: 0.2512918823375935
Training Acc of Epoch 99: 0.735767594004065
Testing Acc of Epoch 99: 0.7368608695652173
Early stopping not satisfied.
