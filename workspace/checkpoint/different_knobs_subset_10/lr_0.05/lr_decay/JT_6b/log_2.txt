train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_6b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0083e-01 (5.0083e-01)	Acc 0.191406 (0.191406)
Epoch: [0][300/616]	Loss 2.7099e-01 (2.8229e-01)	Acc 0.736328 (0.698073)
Epoch: [0][600/616]	Loss 2.4702e-01 (2.7036e-01)	Acc 0.753906 (0.714959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.723648)
Training Loss of Epoch 0: 0.27001372917396266
Training Acc of Epoch 0: 0.7154360391260163
Testing Acc of Epoch 0: 0.7236478260869565
Model with the best training loss saved! The loss is 0.27001372917396266
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6072e-01 (2.6072e-01)	Acc 0.730469 (0.730469)
Epoch: [1][300/616]	Loss 2.7903e-01 (2.5803e-01)	Acc 0.708008 (0.731724)
Epoch: [1][600/616]	Loss 2.6417e-01 (2.6002e-01)	Acc 0.728516 (0.729278)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.731743)
Training Loss of Epoch 1: 0.2600551646172516
Training Acc of Epoch 1: 0.729176194105691
Testing Acc of Epoch 1: 0.7317434782608696
Model with the best training loss saved! The loss is 0.2600551646172516
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5064e-01 (2.5064e-01)	Acc 0.739258 (0.739258)
Epoch: [2][300/616]	Loss 2.5680e-01 (2.6244e-01)	Acc 0.737305 (0.727267)
Epoch: [2][600/616]	Loss 2.8271e-01 (2.6077e-01)	Acc 0.696289 (0.728317)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739417)
Training Loss of Epoch 2: 0.2607554491215605
Training Acc of Epoch 2: 0.7282869664634146
Testing Acc of Epoch 2: 0.7394173913043478
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4858e-01 (2.4858e-01)	Acc 0.730469 (0.730469)
Epoch: [3][300/616]	Loss 2.5410e-01 (2.5851e-01)	Acc 0.743164 (0.731296)
Epoch: [3][600/616]	Loss 2.4830e-01 (2.5952e-01)	Acc 0.737305 (0.729778)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.728487)
Training Loss of Epoch 3: 0.2594596550474322
Training Acc of Epoch 3: 0.7297684832317073
Testing Acc of Epoch 3: 0.7284869565217391
Model with the best training loss saved! The loss is 0.2594596550474322
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6391e-01 (2.6391e-01)	Acc 0.736328 (0.736328)
Epoch: [4][300/616]	Loss 2.5630e-01 (2.5543e-01)	Acc 0.739258 (0.732866)
Epoch: [4][600/616]	Loss 2.6319e-01 (2.5562e-01)	Acc 0.721680 (0.732729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.734357)
Training Loss of Epoch 4: 0.2558141995251663
Training Acc of Epoch 4: 0.7326171875
Testing Acc of Epoch 4: 0.7343565217391305
Model with the best training loss saved! The loss is 0.2558141995251663
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.6586e-01 (2.6586e-01)	Acc 0.734375 (0.734375)
Epoch: [5][300/616]	Loss 2.5105e-01 (2.5616e-01)	Acc 0.734375 (0.732454)
Epoch: [5][600/616]	Loss 2.5942e-01 (2.5633e-01)	Acc 0.735352 (0.732540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.735217)
Training Loss of Epoch 5: 0.2562531715001517
Training Acc of Epoch 5: 0.7326251270325204
Testing Acc of Epoch 5: 0.7352173913043478
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.6461e-01 (2.6461e-01)	Acc 0.714844 (0.714844)
Epoch: [6][300/616]	Loss 2.7905e-01 (2.5522e-01)	Acc 0.709961 (0.732740)
Epoch: [6][600/616]	Loss 2.4908e-01 (2.5631e-01)	Acc 0.730469 (0.732038)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.737613)
Training Loss of Epoch 6: 0.2561568179508535
Training Acc of Epoch 6: 0.7321487550813008
Testing Acc of Epoch 6: 0.7376130434782608
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4034e-01 (2.4034e-01)	Acc 0.747070 (0.747070)
Epoch: [7][300/616]	Loss 2.3581e-01 (2.5374e-01)	Acc 0.751953 (0.735105)
Epoch: [7][600/616]	Loss 2.4470e-01 (2.5502e-01)	Acc 0.743164 (0.733267)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.736891)
Training Loss of Epoch 7: 0.25515038053679273
Training Acc of Epoch 7: 0.7330887957317073
Testing Acc of Epoch 7: 0.7368913043478261
Model with the best training loss saved! The loss is 0.25515038053679273
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5931e-01 (2.5931e-01)	Acc 0.731445 (0.731445)
Epoch: [8][300/616]	Loss 2.8375e-01 (2.5679e-01)	Acc 0.701172 (0.731462)
Epoch: [8][600/616]	Loss 2.4610e-01 (2.5779e-01)	Acc 0.738281 (0.730392)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.730665)
Training Loss of Epoch 8: 0.2577824394392773
Training Acc of Epoch 8: 0.730492568597561
Testing Acc of Epoch 8: 0.7306652173913043
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5050e-01 (2.5050e-01)	Acc 0.722656 (0.722656)
Epoch: [9][300/616]	Loss 2.6586e-01 (2.5694e-01)	Acc 0.719727 (0.731608)
Epoch: [9][600/616]	Loss 2.6027e-01 (2.5701e-01)	Acc 0.724609 (0.731431)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736735)
Training Loss of Epoch 9: 0.2568212648959664
Training Acc of Epoch 9: 0.7317057291666667
Testing Acc of Epoch 9: 0.7367347826086956
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.6200e-01 (2.6200e-01)	Acc 0.728516 (0.728516)
Epoch: [10][300/616]	Loss 2.6594e-01 (2.5736e-01)	Acc 0.718750 (0.730631)
Epoch: [10][600/616]	Loss 2.6408e-01 (2.5738e-01)	Acc 0.717773 (0.730779)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740678)
Training Loss of Epoch 10: 0.2571579533863843
Training Acc of Epoch 10: 0.7310261051829269
Testing Acc of Epoch 10: 0.7406782608695652
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5507e-01 (2.5507e-01)	Acc 0.729492 (0.729492)
Epoch: [11][300/616]	Loss 2.4547e-01 (2.5543e-01)	Acc 0.748047 (0.733126)
Epoch: [11][600/616]	Loss 2.4781e-01 (2.5528e-01)	Acc 0.743164 (0.733010)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737996)
Training Loss of Epoch 11: 0.25528229237571964
Training Acc of Epoch 11: 0.733031631097561
Testing Acc of Epoch 11: 0.737995652173913
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4017e-01 (2.4017e-01)	Acc 0.745117 (0.745117)
Epoch: [12][300/616]	Loss 2.5070e-01 (2.5440e-01)	Acc 0.738281 (0.733415)
Epoch: [12][600/616]	Loss 2.4604e-01 (2.5453e-01)	Acc 0.740234 (0.733826)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.737613)
Training Loss of Epoch 12: 0.2545644803987286
Training Acc of Epoch 12: 0.7338398755081301
Testing Acc of Epoch 12: 0.7376130434782608
Model with the best training loss saved! The loss is 0.2545644803987286
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3634e-01 (2.3634e-01)	Acc 0.747070 (0.747070)
Epoch: [13][300/616]	Loss 2.6898e-01 (2.5979e-01)	Acc 0.708984 (0.728418)
Epoch: [13][600/616]	Loss 2.4729e-01 (2.5619e-01)	Acc 0.744141 (0.732336)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.730248)
Training Loss of Epoch 13: 0.2562301881914216
Training Acc of Epoch 13: 0.7323551829268292
Testing Acc of Epoch 13: 0.7302478260869565
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6066e-01 (2.6066e-01)	Acc 0.732422 (0.732422)
Epoch: [14][300/616]	Loss 2.3311e-01 (2.5431e-01)	Acc 0.772461 (0.733989)
Epoch: [14][600/616]	Loss 2.7003e-01 (2.5625e-01)	Acc 0.718750 (0.732068)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.723270)
Training Loss of Epoch 14: 0.2564937860015931
Training Acc of Epoch 14: 0.7318311737804878
Testing Acc of Epoch 14: 0.7232695652173913
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.7159e-01 (2.7159e-01)	Acc 0.714844 (0.714844)
Epoch: [15][300/616]	Loss 2.6774e-01 (2.5849e-01)	Acc 0.717773 (0.730154)
Epoch: [15][600/616]	Loss 2.6305e-01 (2.5798e-01)	Acc 0.721680 (0.730548)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.730987)
Training Loss of Epoch 15: 0.25797679438823606
Training Acc of Epoch 15: 0.7305751397357724
Testing Acc of Epoch 15: 0.7309869565217392
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.6834e-01 (2.6834e-01)	Acc 0.712891 (0.712891)
Epoch: [16][300/616]	Loss 2.5007e-01 (2.6011e-01)	Acc 0.747070 (0.728350)
Epoch: [16][600/616]	Loss 2.5634e-01 (2.5997e-01)	Acc 0.743164 (0.728300)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.733874)
Training Loss of Epoch 16: 0.25991046612824853
Training Acc of Epoch 16: 0.7283711255081301
Testing Acc of Epoch 16: 0.7338739130434783
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4980e-01 (2.4980e-01)	Acc 0.747070 (0.747070)
Epoch: [17][300/616]	Loss 2.5590e-01 (2.5707e-01)	Acc 0.741211 (0.730504)
Epoch: [17][600/616]	Loss 2.4594e-01 (2.5807e-01)	Acc 0.745117 (0.729304)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.727352)
Training Loss of Epoch 17: 0.2579812561593405
Training Acc of Epoch 17: 0.7294540777439025
Testing Acc of Epoch 17: 0.7273521739130435
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4321e-01 (2.4321e-01)	Acc 0.746094 (0.746094)
Epoch: [18][300/616]	Loss 2.5375e-01 (2.5926e-01)	Acc 0.732422 (0.728529)
Epoch: [18][600/616]	Loss 2.7964e-01 (2.5979e-01)	Acc 0.701172 (0.728001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.728426)
Training Loss of Epoch 18: 0.2599712687779248
Training Acc of Epoch 18: 0.727734375
Testing Acc of Epoch 18: 0.7284260869565218
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.7354e-01 (2.7354e-01)	Acc 0.716797 (0.716797)
Epoch: [19][300/616]	Loss 2.5293e-01 (2.5986e-01)	Acc 0.728516 (0.727760)
Epoch: [19][600/616]	Loss 2.5149e-01 (2.5928e-01)	Acc 0.736328 (0.728402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.733857)
Training Loss of Epoch 19: 0.2591272698185308
Training Acc of Epoch 19: 0.7285505589430894
Testing Acc of Epoch 19: 0.7338565217391304
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4249e-01 (2.4249e-01)	Acc 0.750000 (0.750000)
Epoch: [20][300/616]	Loss 2.6106e-01 (2.5910e-01)	Acc 0.719727 (0.728032)
Epoch: [20][600/616]	Loss 2.8907e-01 (2.5995e-01)	Acc 0.693359 (0.727419)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.732639)
Training Loss of Epoch 20: 0.2599038287876098
Training Acc of Epoch 20: 0.7275358866869919
Testing Acc of Epoch 20: 0.7326391304347826
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3853e-01 (2.3853e-01)	Acc 0.762695 (0.762695)
Epoch: [21][300/616]	Loss 2.5990e-01 (2.6020e-01)	Acc 0.734375 (0.727649)
Epoch: [21][600/616]	Loss 2.7398e-01 (2.6056e-01)	Acc 0.714844 (0.726930)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.727635)
Training Loss of Epoch 21: 0.26052602347804277
Training Acc of Epoch 21: 0.726878493394309
Testing Acc of Epoch 21: 0.7276347826086956
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4431e-01 (2.4431e-01)	Acc 0.743164 (0.743164)
Epoch: [22][300/616]	Loss 2.5086e-01 (2.5768e-01)	Acc 0.730469 (0.729852)
Epoch: [22][600/616]	Loss 2.5885e-01 (2.5883e-01)	Acc 0.728516 (0.728841)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.730826)
Training Loss of Epoch 22: 0.25891537041198914
Training Acc of Epoch 22: 0.728661712398374
Testing Acc of Epoch 22: 0.7308260869565217
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.6120e-01 (2.6120e-01)	Acc 0.721680 (0.721680)
Epoch: [23][300/616]	Loss 2.5154e-01 (2.5877e-01)	Acc 0.739258 (0.728765)
Epoch: [23][600/616]	Loss 2.5146e-01 (2.5974e-01)	Acc 0.730469 (0.727910)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.719926)
Training Loss of Epoch 23: 0.2596575339635213
Training Acc of Epoch 23: 0.7279932037601626
Testing Acc of Epoch 23: 0.7199260869565217
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.7519e-01 (2.7519e-01)	Acc 0.715820 (0.715820)
Epoch: [24][300/616]	Loss 2.5445e-01 (2.5958e-01)	Acc 0.737305 (0.729515)
Epoch: [24][600/616]	Loss 2.4145e-01 (2.5908e-01)	Acc 0.743164 (0.729245)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.723074)
Training Loss of Epoch 24: 0.2589632850594637
Training Acc of Epoch 24: 0.7293064024390243
Testing Acc of Epoch 24: 0.7230739130434782
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.6636e-01 (2.6636e-01)	Acc 0.709961 (0.709961)
Epoch: [25][300/616]	Loss 2.6862e-01 (2.5852e-01)	Acc 0.716797 (0.729596)
Epoch: [25][600/616]	Loss 3.1753e-01 (2.5877e-01)	Acc 0.658203 (0.729240)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.694796)
Training Loss of Epoch 25: 0.25915088479111836
Training Acc of Epoch 25: 0.7288792555894309
Testing Acc of Epoch 25: 0.694795652173913
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.8494e-01 (2.8494e-01)	Acc 0.703125 (0.703125)
Epoch: [26][300/616]	Loss 2.7384e-01 (2.5715e-01)	Acc 0.714844 (0.730440)
Epoch: [26][600/616]	Loss 2.5412e-01 (2.5819e-01)	Acc 0.721680 (0.729809)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.735739)
Training Loss of Epoch 26: 0.2581670223455119
Training Acc of Epoch 26: 0.7297573678861788
Testing Acc of Epoch 26: 0.7357391304347826
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.6476e-01 (2.6476e-01)	Acc 0.711914 (0.711914)
Epoch: [27][300/616]	Loss 2.6085e-01 (2.5838e-01)	Acc 0.735352 (0.729473)
Epoch: [27][600/616]	Loss 2.7453e-01 (2.5959e-01)	Acc 0.691406 (0.728343)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.706474)
Training Loss of Epoch 27: 0.2598861221617799
Training Acc of Epoch 27: 0.7280440167682927
Testing Acc of Epoch 27: 0.7064739130434783
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.7908e-01 (2.7908e-01)	Acc 0.706055 (0.706055)
Epoch: [28][300/616]	Loss 2.5241e-01 (2.5776e-01)	Acc 0.734375 (0.729697)
Epoch: [28][600/616]	Loss 2.5180e-01 (2.5926e-01)	Acc 0.740234 (0.729097)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.734991)
Training Loss of Epoch 28: 0.2592675395370499
Training Acc of Epoch 28: 0.7290650406504066
Testing Acc of Epoch 28: 0.7349913043478261
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4499e-01 (2.4499e-01)	Acc 0.748047 (0.748047)
Epoch: [29][300/616]	Loss 2.5396e-01 (2.6126e-01)	Acc 0.744141 (0.727215)
Epoch: [29][600/616]	Loss 2.4555e-01 (2.6141e-01)	Acc 0.742188 (0.726831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.726678)
Training Loss of Epoch 29: 0.26121772925059
Training Acc of Epoch 29: 0.7269753556910569
Testing Acc of Epoch 29: 0.7266782608695652
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.7300e-01 (2.7300e-01)	Acc 0.697266 (0.697266)
Epoch: [30][300/616]	Loss 2.4372e-01 (2.5993e-01)	Acc 0.761719 (0.728366)
Epoch: [30][600/616]	Loss 2.7192e-01 (2.6011e-01)	Acc 0.712891 (0.727885)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.728774)
Training Loss of Epoch 30: 0.2603225641376604
Training Acc of Epoch 30: 0.7276343368902439
Testing Acc of Epoch 30: 0.7287739130434783
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.5036e-01 (2.5036e-01)	Acc 0.744141 (0.744141)
Epoch: [31][300/616]	Loss 2.5500e-01 (2.5928e-01)	Acc 0.736328 (0.729388)
Epoch: [31][600/616]	Loss 2.4167e-01 (2.5984e-01)	Acc 0.745117 (0.728352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.728135)
Training Loss of Epoch 31: 0.2598535499921659
Training Acc of Epoch 31: 0.7282663236788618
Testing Acc of Epoch 31: 0.7281347826086957
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.6630e-01 (2.6630e-01)	Acc 0.713867 (0.713867)
Epoch: [32][300/616]	Loss 2.5221e-01 (2.6158e-01)	Acc 0.746094 (0.727030)
Epoch: [32][600/616]	Loss 2.4457e-01 (2.6090e-01)	Acc 0.742188 (0.727500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.737565)
Training Loss of Epoch 32: 0.26089468508716523
Training Acc of Epoch 32: 0.7274231453252032
Testing Acc of Epoch 32: 0.7375652173913043
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2579e-01 (2.2579e-01)	Acc 0.773438 (0.773438)
Epoch: [33][300/616]	Loss 2.6902e-01 (2.6008e-01)	Acc 0.713867 (0.726708)
Epoch: [33][600/616]	Loss 2.6614e-01 (2.5959e-01)	Acc 0.717773 (0.727900)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.731691)
Training Loss of Epoch 33: 0.25950253840869036
Training Acc of Epoch 33: 0.7279852642276423
Testing Acc of Epoch 33: 0.731691304347826
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.5379e-01 (2.5379e-01)	Acc 0.728516 (0.728516)
Epoch: [34][300/616]	Loss 2.7687e-01 (2.5856e-01)	Acc 0.709961 (0.729002)
Epoch: [34][600/616]	Loss 2.2706e-01 (2.5944e-01)	Acc 0.773438 (0.728555)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.733843)
Training Loss of Epoch 34: 0.25933497191929233
Training Acc of Epoch 34: 0.7286426575203252
Testing Acc of Epoch 34: 0.7338434782608696
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.7024e-01 (2.7024e-01)	Acc 0.713867 (0.713867)
Epoch: [35][300/616]	Loss 2.6045e-01 (2.6143e-01)	Acc 0.743164 (0.727033)
Epoch: [35][600/616]	Loss 2.4707e-01 (2.6077e-01)	Acc 0.733398 (0.727299)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.728178)
Training Loss of Epoch 35: 0.26063246751219277
Training Acc of Epoch 35: 0.7275358866869919
Testing Acc of Epoch 35: 0.7281782608695652
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.6928e-01 (2.6928e-01)	Acc 0.718750 (0.718750)
Epoch: [36][300/616]	Loss 2.5738e-01 (2.5934e-01)	Acc 0.740234 (0.728895)
Epoch: [36][600/616]	Loss 2.5467e-01 (2.5991e-01)	Acc 0.743164 (0.728067)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.739065)
Training Loss of Epoch 36: 0.25972139060012694
Training Acc of Epoch 36: 0.7282774390243902
Testing Acc of Epoch 36: 0.7390652173913044
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.7600e-01 (2.7600e-01)	Acc 0.704102 (0.704102)
Epoch: [37][300/616]	Loss 2.7001e-01 (2.5873e-01)	Acc 0.722656 (0.729856)
Epoch: [37][600/616]	Loss 2.6387e-01 (2.6039e-01)	Acc 0.728516 (0.727602)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.730809)
Training Loss of Epoch 37: 0.26034524101551954
Training Acc of Epoch 37: 0.7275676448170731
Testing Acc of Epoch 37: 0.7308086956521739
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.6534e-01 (2.6534e-01)	Acc 0.731445 (0.731445)
Epoch: [38][300/616]	Loss 2.6818e-01 (2.6178e-01)	Acc 0.720703 (0.725904)
Epoch: [38][600/616]	Loss 2.5584e-01 (2.6064e-01)	Acc 0.745117 (0.727154)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.730561)
Training Loss of Epoch 38: 0.26050903387670593
Training Acc of Epoch 38: 0.7272738821138212
Testing Acc of Epoch 38: 0.7305608695652174
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.7019e-01 (2.7019e-01)	Acc 0.716797 (0.716797)
Epoch: [39][300/616]	Loss 2.6986e-01 (2.6185e-01)	Acc 0.718750 (0.725450)
Epoch: [39][600/616]	Loss 2.5945e-01 (2.6151e-01)	Acc 0.721680 (0.726252)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.725309)
Training Loss of Epoch 39: 0.26163432409123677
Training Acc of Epoch 39: 0.7260337271341464
Testing Acc of Epoch 39: 0.7253086956521739
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.6501e-01 (2.6501e-01)	Acc 0.723633 (0.723633)
Epoch: [40][300/616]	Loss 2.7044e-01 (2.5833e-01)	Acc 0.717773 (0.729787)
Epoch: [40][600/616]	Loss 2.5213e-01 (2.5913e-01)	Acc 0.725586 (0.728706)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.720239)
Training Loss of Epoch 40: 0.25922274972365156
Training Acc of Epoch 40: 0.7286061356707317
Testing Acc of Epoch 40: 0.7202391304347826
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.8280e-01 (2.8280e-01)	Acc 0.720703 (0.720703)
Epoch: [41][300/616]	Loss 2.5152e-01 (2.6025e-01)	Acc 0.735352 (0.727296)
Epoch: [41][600/616]	Loss 2.5114e-01 (2.6019e-01)	Acc 0.742188 (0.727778)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.728648)
Training Loss of Epoch 41: 0.2600289886802193
Training Acc of Epoch 41: 0.7279265116869919
Testing Acc of Epoch 41: 0.7286478260869566
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.6538e-01 (2.6538e-01)	Acc 0.715820 (0.715820)
Epoch: [42][300/616]	Loss 2.5140e-01 (2.6313e-01)	Acc 0.740234 (0.725313)
Epoch: [42][600/616]	Loss 2.4577e-01 (2.6179e-01)	Acc 0.736328 (0.726514)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.717983)
Training Loss of Epoch 42: 0.26167480410114535
Training Acc of Epoch 42: 0.7265942581300813
Testing Acc of Epoch 42: 0.7179826086956522
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.5774e-01 (2.5774e-01)	Acc 0.732422 (0.732422)
Epoch: [43][300/616]	Loss 2.4918e-01 (2.5824e-01)	Acc 0.740234 (0.729869)
Epoch: [43][600/616]	Loss 2.6906e-01 (2.5880e-01)	Acc 0.709961 (0.728980)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.735491)
Training Loss of Epoch 43: 0.25878267394817944
Training Acc of Epoch 43: 0.7291015625
Testing Acc of Epoch 43: 0.7354913043478261
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.5769e-01 (2.5769e-01)	Acc 0.728516 (0.728516)
Epoch: [44][300/616]	Loss 2.7336e-01 (2.6000e-01)	Acc 0.701172 (0.727692)
Epoch: [44][600/616]	Loss 2.4943e-01 (2.5921e-01)	Acc 0.740234 (0.728933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.736304)
Training Loss of Epoch 44: 0.2591649449695417
Training Acc of Epoch 44: 0.7289173653455284
Testing Acc of Epoch 44: 0.736304347826087
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.5091e-01 (2.5091e-01)	Acc 0.730469 (0.730469)
Epoch: [45][300/616]	Loss 2.6007e-01 (2.5836e-01)	Acc 0.715820 (0.729379)
Epoch: [45][600/616]	Loss 2.5536e-01 (2.5837e-01)	Acc 0.743164 (0.729387)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.727783)
Training Loss of Epoch 45: 0.25834489367841706
Training Acc of Epoch 45: 0.7294096163617886
Testing Acc of Epoch 45: 0.7277826086956521
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.5088e-01 (2.5088e-01)	Acc 0.736328 (0.736328)
Epoch: [46][300/616]	Loss 2.5637e-01 (2.5824e-01)	Acc 0.727539 (0.730086)
Epoch: [46][600/616]	Loss 2.6161e-01 (2.5934e-01)	Acc 0.722656 (0.728597)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.723674)
Training Loss of Epoch 46: 0.2592473945966581
Training Acc of Epoch 46: 0.7287712779471545
Testing Acc of Epoch 46: 0.7236739130434783
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.6576e-01 (2.6576e-01)	Acc 0.717773 (0.717773)
Epoch: [47][300/616]	Loss 2.6333e-01 (2.5932e-01)	Acc 0.718750 (0.728843)
Epoch: [47][600/616]	Loss 2.3989e-01 (2.6057e-01)	Acc 0.754883 (0.727354)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.733743)
Training Loss of Epoch 47: 0.26044953477576493
Training Acc of Epoch 47: 0.7274850736788618
Testing Acc of Epoch 47: 0.7337434782608696
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4452e-01 (2.4452e-01)	Acc 0.742188 (0.742188)
Epoch: [48][300/616]	Loss 2.4583e-01 (2.6119e-01)	Acc 0.748047 (0.727159)
Epoch: [48][600/616]	Loss 2.6101e-01 (2.6054e-01)	Acc 0.717773 (0.727481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.725957)
Training Loss of Epoch 48: 0.2606529616001176
Training Acc of Epoch 48: 0.7273246951219512
Testing Acc of Epoch 48: 0.7259565217391304
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.5263e-01 (2.5263e-01)	Acc 0.748047 (0.748047)
Epoch: [49][300/616]	Loss 2.5010e-01 (2.5992e-01)	Acc 0.728516 (0.728337)
Epoch: [49][600/616]	Loss 2.6115e-01 (2.5991e-01)	Acc 0.726562 (0.727858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.728574)
Training Loss of Epoch 49: 0.2600194422937021
Training Acc of Epoch 49: 0.7277311991869919
Testing Acc of Epoch 49: 0.7285739130434783
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.6103e-01 (2.6103e-01)	Acc 0.731445 (0.731445)
Epoch: [50][300/616]	Loss 2.4169e-01 (2.5969e-01)	Acc 0.752930 (0.728227)
Epoch: [50][600/616]	Loss 2.4629e-01 (2.5987e-01)	Acc 0.751953 (0.727672)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.701826)
Training Loss of Epoch 50: 0.2600671933918465
Training Acc of Epoch 50: 0.7275263592479675
Testing Acc of Epoch 50: 0.7018260869565217
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.8311e-01 (2.8311e-01)	Acc 0.706055 (0.706055)
Epoch: [51][300/616]	Loss 2.9361e-01 (2.5888e-01)	Acc 0.670898 (0.728665)
Epoch: [51][600/616]	Loss 2.4934e-01 (2.5826e-01)	Acc 0.726562 (0.729479)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.713943)
Training Loss of Epoch 51: 0.25831936699588126
Training Acc of Epoch 51: 0.7294477261178862
Testing Acc of Epoch 51: 0.7139434782608696
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.9225e-01 (2.9225e-01)	Acc 0.678711 (0.678711)
Epoch: [52][300/616]	Loss 2.5414e-01 (2.6056e-01)	Acc 0.731445 (0.726809)
Epoch: [52][600/616]	Loss 2.6476e-01 (2.5832e-01)	Acc 0.709961 (0.729016)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.726435)
Training Loss of Epoch 52: 0.2583382995632606
Training Acc of Epoch 52: 0.728953887195122
Testing Acc of Epoch 52: 0.7264347826086957
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.6399e-01 (2.6399e-01)	Acc 0.721680 (0.721680)
Epoch: [53][300/616]	Loss 2.7208e-01 (2.5693e-01)	Acc 0.708984 (0.731144)
Epoch: [53][600/616]	Loss 2.4644e-01 (2.5833e-01)	Acc 0.749023 (0.729130)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.730143)
Training Loss of Epoch 53: 0.2584210975625651
Training Acc of Epoch 53: 0.7290459857723577
Testing Acc of Epoch 53: 0.7301434782608696
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.5135e-01 (2.5135e-01)	Acc 0.750977 (0.750977)
Epoch: [54][300/616]	Loss 2.3800e-01 (2.5886e-01)	Acc 0.750977 (0.728804)
Epoch: [54][600/616]	Loss 2.6038e-01 (2.5826e-01)	Acc 0.708008 (0.729791)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.740296)
Training Loss of Epoch 54: 0.2584420925475718
Training Acc of Epoch 54: 0.7295969893292683
Testing Acc of Epoch 54: 0.740295652173913
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.6266e-01 (2.6266e-01)	Acc 0.745117 (0.745117)
Epoch: [55][300/616]	Loss 2.5885e-01 (2.5744e-01)	Acc 0.736328 (0.730599)
Epoch: [55][600/616]	Loss 2.7342e-01 (2.5851e-01)	Acc 0.704102 (0.729055)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.732261)
Training Loss of Epoch 55: 0.2584045281982034
Training Acc of Epoch 55: 0.7291110899390244
Testing Acc of Epoch 55: 0.7322608695652174
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4074e-01 (2.4074e-01)	Acc 0.745117 (0.745117)
Epoch: [56][300/616]	Loss 2.5798e-01 (2.5783e-01)	Acc 0.731445 (0.730355)
Epoch: [56][600/616]	Loss 2.9734e-01 (2.5751e-01)	Acc 0.685547 (0.730197)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.719313)
Training Loss of Epoch 56: 0.25786282252974624
Training Acc of Epoch 56: 0.7298224720528456
Testing Acc of Epoch 56: 0.7193130434782609
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.6388e-01 (2.6388e-01)	Acc 0.717773 (0.717773)
Epoch: [57][300/616]	Loss 2.3798e-01 (2.5773e-01)	Acc 0.747070 (0.729904)
Epoch: [57][600/616]	Loss 2.4575e-01 (2.5870e-01)	Acc 0.752930 (0.729133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.733122)
Training Loss of Epoch 57: 0.2586108648195499
Training Acc of Epoch 57: 0.7292143038617886
Testing Acc of Epoch 57: 0.7331217391304348
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.6046e-01 (2.6046e-01)	Acc 0.725586 (0.725586)
Epoch: [58][300/616]	Loss 2.6348e-01 (2.5777e-01)	Acc 0.728516 (0.730868)
Epoch: [58][600/616]	Loss 2.8268e-01 (2.5807e-01)	Acc 0.700195 (0.729793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.736678)
Training Loss of Epoch 58: 0.2580904440908897
Training Acc of Epoch 58: 0.7297049669715447
Testing Acc of Epoch 58: 0.7366782608695652
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.5587e-01 (2.5587e-01)	Acc 0.733398 (0.733398)
Epoch: [59][300/616]	Loss 2.7216e-01 (2.5848e-01)	Acc 0.698242 (0.728665)
Epoch: [59][600/616]	Loss 2.5961e-01 (2.5953e-01)	Acc 0.733398 (0.728082)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.736904)
Training Loss of Epoch 59: 0.2594145915372585
Training Acc of Epoch 59: 0.7282091590447154
Testing Acc of Epoch 59: 0.7369043478260869
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5342e-01 (2.5342e-01)	Acc 0.739258 (0.739258)
Epoch: [60][300/616]	Loss 2.6336e-01 (2.5847e-01)	Acc 0.721680 (0.729019)
Epoch: [60][600/616]	Loss 2.5521e-01 (2.5809e-01)	Acc 0.737305 (0.729638)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.731835)
Training Loss of Epoch 60: 0.25815115076739614
Training Acc of Epoch 60: 0.72954300050813
Testing Acc of Epoch 60: 0.7318347826086956
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.6266e-01 (2.6266e-01)	Acc 0.725586 (0.725586)
Epoch: [61][300/616]	Loss 2.3895e-01 (2.6005e-01)	Acc 0.751953 (0.727977)
Epoch: [61][600/616]	Loss 2.4122e-01 (2.5939e-01)	Acc 0.754883 (0.728561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.712835)
Training Loss of Epoch 61: 0.2593856406405689
Training Acc of Epoch 61: 0.7285775533536586
Testing Acc of Epoch 61: 0.7128347826086957
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.6798e-01 (2.6798e-01)	Acc 0.729492 (0.729492)
Epoch: [62][300/616]	Loss 2.5601e-01 (2.5936e-01)	Acc 0.726562 (0.729012)
Epoch: [62][600/616]	Loss 2.7847e-01 (2.5851e-01)	Acc 0.713867 (0.729500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.736996)
Training Loss of Epoch 62: 0.2585146711609228
Training Acc of Epoch 62: 0.7295144181910569
Testing Acc of Epoch 62: 0.736995652173913
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4255e-01 (2.4255e-01)	Acc 0.745117 (0.745117)
Epoch: [63][300/616]	Loss 3.1001e-01 (2.5983e-01)	Acc 0.657227 (0.727727)
Epoch: [63][600/616]	Loss 2.6967e-01 (2.5919e-01)	Acc 0.717773 (0.728556)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.729574)
Training Loss of Epoch 63: 0.2594889431222668
Training Acc of Epoch 63: 0.728272675304878
Testing Acc of Epoch 63: 0.7295739130434783
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5054e-01 (2.5054e-01)	Acc 0.739258 (0.739258)
Epoch: [64][300/616]	Loss 2.7416e-01 (2.5947e-01)	Acc 0.692383 (0.727805)
Epoch: [64][600/616]	Loss 2.5423e-01 (2.5893e-01)	Acc 0.728516 (0.728686)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735865)
Training Loss of Epoch 64: 0.2591212045371048
Training Acc of Epoch 64: 0.7284743394308943
Testing Acc of Epoch 64: 0.7358652173913044
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.6040e-01 (2.6040e-01)	Acc 0.724609 (0.724609)
Epoch: [65][300/616]	Loss 2.5600e-01 (2.5762e-01)	Acc 0.730469 (0.730138)
Epoch: [65][600/616]	Loss 2.7429e-01 (2.5829e-01)	Acc 0.718750 (0.729261)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.722648)
Training Loss of Epoch 65: 0.25844112901183647
Training Acc of Epoch 65: 0.7290094639227642
Testing Acc of Epoch 65: 0.7226478260869565
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.6351e-01 (2.6351e-01)	Acc 0.733398 (0.733398)
Epoch: [66][300/616]	Loss 2.6066e-01 (2.5929e-01)	Acc 0.725586 (0.728529)
Epoch: [66][600/616]	Loss 2.7036e-01 (2.5953e-01)	Acc 0.714844 (0.728589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734543)
Training Loss of Epoch 66: 0.2595187598369955
Training Acc of Epoch 66: 0.7286537728658536
Testing Acc of Epoch 66: 0.7345434782608695
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.6481e-01 (2.6481e-01)	Acc 0.723633 (0.723633)
Epoch: [67][300/616]	Loss 2.5900e-01 (2.5845e-01)	Acc 0.748047 (0.729784)
Epoch: [67][600/616]	Loss 2.7677e-01 (2.5898e-01)	Acc 0.713867 (0.729304)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.731957)
Training Loss of Epoch 67: 0.25916204559124584
Training Acc of Epoch 67: 0.7290698043699188
Testing Acc of Epoch 67: 0.7319565217391304
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.6284e-01 (2.6284e-01)	Acc 0.712891 (0.712891)
Epoch: [68][300/616]	Loss 2.6609e-01 (2.5890e-01)	Acc 0.721680 (0.729242)
Epoch: [68][600/616]	Loss 2.4281e-01 (2.5899e-01)	Acc 0.750977 (0.729019)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.736830)
Training Loss of Epoch 68: 0.25892049682334184
Training Acc of Epoch 68: 0.7291460238821138
Testing Acc of Epoch 68: 0.7368304347826087
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5069e-01 (2.5069e-01)	Acc 0.734375 (0.734375)
Epoch: [69][300/616]	Loss 2.5175e-01 (2.5952e-01)	Acc 0.734375 (0.728600)
Epoch: [69][600/616]	Loss 2.8491e-01 (2.5955e-01)	Acc 0.705078 (0.728741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.719578)
Training Loss of Epoch 69: 0.25966632945266194
Training Acc of Epoch 69: 0.7286505970528455
Testing Acc of Epoch 69: 0.7195782608695652
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.8142e-01 (2.8142e-01)	Acc 0.714844 (0.714844)
Epoch: [70][300/616]	Loss 2.5214e-01 (2.5845e-01)	Acc 0.747070 (0.730469)
Epoch: [70][600/616]	Loss 2.3535e-01 (2.5989e-01)	Acc 0.751953 (0.728404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.665064 (0.692091)
Training Loss of Epoch 70: 0.26025377339948486
Training Acc of Epoch 70: 0.7279709730691057
Testing Acc of Epoch 70: 0.6920913043478261
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.8873e-01 (2.8873e-01)	Acc 0.676758 (0.676758)
Epoch: [71][300/616]	Loss 2.4820e-01 (2.5816e-01)	Acc 0.740234 (0.729982)
Epoch: [71][600/616]	Loss 2.5369e-01 (2.5926e-01)	Acc 0.743164 (0.728792)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.730835)
Training Loss of Epoch 71: 0.2591501793725704
Training Acc of Epoch 71: 0.7288490853658537
Testing Acc of Epoch 71: 0.7308347826086956
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.7466e-01 (2.7466e-01)	Acc 0.712891 (0.712891)
Epoch: [72][300/616]	Loss 2.6388e-01 (2.5938e-01)	Acc 0.731445 (0.728856)
Epoch: [72][600/616]	Loss 2.6583e-01 (2.5931e-01)	Acc 0.720703 (0.728483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.738313)
Training Loss of Epoch 72: 0.259230024955137
Training Acc of Epoch 72: 0.7285791412601627
Testing Acc of Epoch 72: 0.7383130434782609
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4310e-01 (2.4310e-01)	Acc 0.745117 (0.745117)
Epoch: [73][300/616]	Loss 2.5521e-01 (2.6111e-01)	Acc 0.730469 (0.727276)
Epoch: [73][600/616]	Loss 2.6501e-01 (2.5959e-01)	Acc 0.724609 (0.728348)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.728778)
Training Loss of Epoch 73: 0.259587677659058
Training Acc of Epoch 73: 0.7284060594512195
Testing Acc of Epoch 73: 0.7287782608695652
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.6499e-01 (2.6499e-01)	Acc 0.726562 (0.726562)
Epoch: [74][300/616]	Loss 2.5303e-01 (2.5770e-01)	Acc 0.741211 (0.730618)
Epoch: [74][600/616]	Loss 2.7565e-01 (2.5860e-01)	Acc 0.723633 (0.729170)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.724783)
Training Loss of Epoch 74: 0.2587046958566681
Training Acc of Epoch 74: 0.7290856834349594
Testing Acc of Epoch 74: 0.7247826086956521
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4932e-01 (2.4932e-01)	Acc 0.746094 (0.746094)
Epoch: [75][300/616]	Loss 2.4299e-01 (2.4802e-01)	Acc 0.740234 (0.739595)
Epoch: [75][600/616]	Loss 2.5681e-01 (2.4979e-01)	Acc 0.736328 (0.737768)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.729183)
Training Loss of Epoch 75: 0.24992027442629744
Training Acc of Epoch 75: 0.7377445376016261
Testing Acc of Epoch 75: 0.7291826086956522
Model with the best training loss saved! The loss is 0.24992027442629744
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.5575e-01 (2.5575e-01)	Acc 0.727539 (0.727539)
Epoch: [76][300/616]	Loss 2.4874e-01 (2.5189e-01)	Acc 0.737305 (0.735679)
Epoch: [76][600/616]	Loss 2.4829e-01 (2.5159e-01)	Acc 0.734375 (0.735511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740339)
Training Loss of Epoch 76: 0.251560892637183
Training Acc of Epoch 76: 0.735546875
Testing Acc of Epoch 76: 0.7403391304347826
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.5013e-01 (2.5013e-01)	Acc 0.740234 (0.740234)
Epoch: [77][300/616]	Loss 2.5202e-01 (2.5196e-01)	Acc 0.732422 (0.735595)
Epoch: [77][600/616]	Loss 2.4610e-01 (2.5222e-01)	Acc 0.733398 (0.734898)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.736691)
Training Loss of Epoch 77: 0.25223619479958603
Training Acc of Epoch 77: 0.7348815421747967
Testing Acc of Epoch 77: 0.736691304347826
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4381e-01 (2.4381e-01)	Acc 0.749023 (0.749023)
Epoch: [78][300/616]	Loss 2.5262e-01 (2.5245e-01)	Acc 0.740234 (0.734356)
Epoch: [78][600/616]	Loss 2.6027e-01 (2.5147e-01)	Acc 0.727539 (0.735586)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.729561)
Training Loss of Epoch 78: 0.2515851533994442
Training Acc of Epoch 78: 0.735423018292683
Testing Acc of Epoch 78: 0.7295608695652174
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.6784e-01 (2.6784e-01)	Acc 0.710938 (0.710938)
Epoch: [79][300/616]	Loss 2.4058e-01 (2.5096e-01)	Acc 0.752930 (0.735845)
Epoch: [79][600/616]	Loss 2.3146e-01 (2.5147e-01)	Acc 0.761719 (0.735844)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.727870)
Training Loss of Epoch 79: 0.2516215670641845
Training Acc of Epoch 79: 0.7355738694105691
Testing Acc of Epoch 79: 0.7278695652173913
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4807e-01 (2.4807e-01)	Acc 0.735352 (0.735352)
Epoch: [80][300/616]	Loss 2.3152e-01 (2.5182e-01)	Acc 0.767578 (0.734320)
Epoch: [80][600/616]	Loss 2.6811e-01 (2.5229e-01)	Acc 0.703125 (0.734217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.723043)
Training Loss of Epoch 80: 0.25220022019816607
Training Acc of Epoch 80: 0.7343400660569106
Testing Acc of Epoch 80: 0.7230434782608696
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.7485e-01 (2.7485e-01)	Acc 0.706055 (0.706055)
Epoch: [81][300/616]	Loss 2.4517e-01 (2.5136e-01)	Acc 0.744141 (0.735254)
Epoch: [81][600/616]	Loss 2.3687e-01 (2.5040e-01)	Acc 0.751953 (0.736161)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.732183)
Training Loss of Epoch 81: 0.2505949240632174
Training Acc of Epoch 81: 0.7359930767276422
Testing Acc of Epoch 81: 0.7321826086956522
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.5814e-01 (2.5814e-01)	Acc 0.730469 (0.730469)
Epoch: [82][300/616]	Loss 2.5383e-01 (2.4995e-01)	Acc 0.720703 (0.737425)
Epoch: [82][600/616]	Loss 2.4395e-01 (2.5044e-01)	Acc 0.732422 (0.736288)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.738170)
Training Loss of Epoch 82: 0.250417601359569
Training Acc of Epoch 82: 0.7362582571138211
Testing Acc of Epoch 82: 0.7381695652173913
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.5340e-01 (2.5340e-01)	Acc 0.721680 (0.721680)
Epoch: [83][300/616]	Loss 2.4610e-01 (2.5077e-01)	Acc 0.740234 (0.736412)
Epoch: [83][600/616]	Loss 2.5445e-01 (2.5176e-01)	Acc 0.733398 (0.735028)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.741939)
Training Loss of Epoch 83: 0.25170666672349945
Training Acc of Epoch 83: 0.7351244918699187
Testing Acc of Epoch 83: 0.7419391304347827
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.5302e-01 (2.5302e-01)	Acc 0.723633 (0.723633)
Epoch: [84][300/616]	Loss 2.4571e-01 (2.5110e-01)	Acc 0.750000 (0.735796)
Epoch: [84][600/616]	Loss 2.3865e-01 (2.5195e-01)	Acc 0.742188 (0.734793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740696)
Training Loss of Epoch 84: 0.2519648193101573
Training Acc of Epoch 84: 0.7347529217479675
Testing Acc of Epoch 84: 0.7406956521739131
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4093e-01 (2.4093e-01)	Acc 0.754883 (0.754883)
Epoch: [85][300/616]	Loss 2.4375e-01 (2.5126e-01)	Acc 0.749023 (0.734430)
Epoch: [85][600/616]	Loss 2.6528e-01 (2.5215e-01)	Acc 0.717773 (0.733925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734617)
Training Loss of Epoch 85: 0.2521742159273566
Training Acc of Epoch 85: 0.7338891006097561
Testing Acc of Epoch 85: 0.7346173913043478
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.4828e-01 (2.4828e-01)	Acc 0.756836 (0.756836)
Epoch: [86][300/616]	Loss 2.5674e-01 (2.5301e-01)	Acc 0.737305 (0.733768)
Epoch: [86][600/616]	Loss 2.4439e-01 (2.5344e-01)	Acc 0.749023 (0.732966)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.740709)
Training Loss of Epoch 86: 0.25343777360470315
Training Acc of Epoch 86: 0.7329569994918699
Testing Acc of Epoch 86: 0.7407086956521739
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.6503e-01 (2.6503e-01)	Acc 0.708984 (0.708984)
Epoch: [87][300/616]	Loss 2.4450e-01 (2.5387e-01)	Acc 0.757812 (0.732493)
Epoch: [87][600/616]	Loss 2.5719e-01 (2.5275e-01)	Acc 0.736328 (0.733517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.734352)
Training Loss of Epoch 87: 0.25271107190992775
Training Acc of Epoch 87: 0.733592162093496
Testing Acc of Epoch 87: 0.7343521739130435
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4841e-01 (2.4841e-01)	Acc 0.728516 (0.728516)
Epoch: [88][300/616]	Loss 2.5749e-01 (2.5177e-01)	Acc 0.721680 (0.734463)
Epoch: [88][600/616]	Loss 2.4753e-01 (2.5212e-01)	Acc 0.747070 (0.734060)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.732843)
Training Loss of Epoch 88: 0.25231957057627236
Training Acc of Epoch 88: 0.7338382876016261
Testing Acc of Epoch 88: 0.7328434782608696
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.5993e-01 (2.5993e-01)	Acc 0.707031 (0.707031)
Epoch: [89][300/616]	Loss 2.5644e-01 (2.5110e-01)	Acc 0.724609 (0.735429)
Epoch: [89][600/616]	Loss 2.5207e-01 (2.5215e-01)	Acc 0.739258 (0.734258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.734400)
Training Loss of Epoch 89: 0.252253131095956
Training Acc of Epoch 89: 0.7341796875
Testing Acc of Epoch 89: 0.7344
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4941e-01 (2.4941e-01)	Acc 0.735352 (0.735352)
Epoch: [90][300/616]	Loss 2.5654e-01 (2.5199e-01)	Acc 0.718750 (0.734407)
Epoch: [90][600/616]	Loss 2.5067e-01 (2.5242e-01)	Acc 0.736328 (0.734065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.733113)
Training Loss of Epoch 90: 0.252445688746809
Training Acc of Epoch 90: 0.7340526549796748
Testing Acc of Epoch 90: 0.7331130434782609
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.6651e-01 (2.6651e-01)	Acc 0.710938 (0.710938)
Epoch: [91][300/616]	Loss 2.6678e-01 (2.5211e-01)	Acc 0.734375 (0.734012)
Epoch: [91][600/616]	Loss 2.5509e-01 (2.5117e-01)	Acc 0.732422 (0.735675)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734713)
Training Loss of Epoch 91: 0.2511921330438397
Training Acc of Epoch 91: 0.7356103912601626
Testing Acc of Epoch 91: 0.7347130434782608
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.6380e-01 (2.6380e-01)	Acc 0.717773 (0.717773)
Epoch: [92][300/616]	Loss 2.5363e-01 (2.5042e-01)	Acc 0.732422 (0.736059)
Epoch: [92][600/616]	Loss 2.5047e-01 (2.5077e-01)	Acc 0.729492 (0.735691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.736696)
Training Loss of Epoch 92: 0.2508957017001098
Training Acc of Epoch 92: 0.7355865726626016
Testing Acc of Epoch 92: 0.7366956521739131
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.4649e-01 (2.4649e-01)	Acc 0.744141 (0.744141)
Epoch: [93][300/616]	Loss 2.5302e-01 (2.5250e-01)	Acc 0.746094 (0.734664)
Epoch: [93][600/616]	Loss 2.4491e-01 (2.5277e-01)	Acc 0.732422 (0.733762)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.728400)
Training Loss of Epoch 93: 0.252886544810078
Training Acc of Epoch 93: 0.733714430894309
Testing Acc of Epoch 93: 0.7284
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.4785e-01 (2.4785e-01)	Acc 0.740234 (0.740234)
Epoch: [94][300/616]	Loss 2.2962e-01 (2.5346e-01)	Acc 0.759766 (0.733619)
Epoch: [94][600/616]	Loss 2.5495e-01 (2.5303e-01)	Acc 0.741211 (0.733215)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.712330)
Training Loss of Epoch 94: 0.2529971153513203
Training Acc of Epoch 94: 0.7332872840447154
Testing Acc of Epoch 94: 0.7123304347826087
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.5817e-01 (2.5817e-01)	Acc 0.728516 (0.728516)
Epoch: [95][300/616]	Loss 2.5444e-01 (2.5276e-01)	Acc 0.731445 (0.733389)
Epoch: [95][600/616]	Loss 2.4656e-01 (2.5303e-01)	Acc 0.739258 (0.733093)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.739126)
Training Loss of Epoch 95: 0.2530305005670563
Training Acc of Epoch 95: 0.7331269054878049
Testing Acc of Epoch 95: 0.7391260869565217
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.5199e-01 (2.5199e-01)	Acc 0.743164 (0.743164)
Epoch: [96][300/616]	Loss 2.5354e-01 (2.5224e-01)	Acc 0.735352 (0.734297)
Epoch: [96][600/616]	Loss 2.6164e-01 (2.5182e-01)	Acc 0.723633 (0.734601)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.736148)
Training Loss of Epoch 96: 0.2517952467367901
Training Acc of Epoch 96: 0.7345274390243902
Testing Acc of Epoch 96: 0.7361478260869565
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.5927e-01 (2.5927e-01)	Acc 0.734375 (0.734375)
Epoch: [97][300/616]	Loss 2.5142e-01 (2.5323e-01)	Acc 0.732422 (0.733324)
Epoch: [97][600/616]	Loss 2.6648e-01 (2.5307e-01)	Acc 0.713867 (0.733382)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.732600)
Training Loss of Epoch 97: 0.25313494382350427
Training Acc of Epoch 97: 0.7332571138211382
Testing Acc of Epoch 97: 0.7326
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4559e-01 (2.4559e-01)	Acc 0.735352 (0.735352)
Epoch: [98][300/616]	Loss 2.3831e-01 (2.5216e-01)	Acc 0.748047 (0.733652)
Epoch: [98][600/616]	Loss 2.5420e-01 (2.5098e-01)	Acc 0.730469 (0.735412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.739643)
Training Loss of Epoch 98: 0.2509096457463939
Training Acc of Epoch 98: 0.7355341717479674
Testing Acc of Epoch 98: 0.7396434782608695
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.4964e-01 (2.4964e-01)	Acc 0.741211 (0.741211)
Epoch: [99][300/616]	Loss 2.4482e-01 (2.4997e-01)	Acc 0.729492 (0.736361)
Epoch: [99][600/616]	Loss 2.4398e-01 (2.5059e-01)	Acc 0.747070 (0.736002)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740857)
Training Loss of Epoch 99: 0.2505349217633891
Training Acc of Epoch 99: 0.7360057799796748
Testing Acc of Epoch 99: 0.7408565217391304
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_6b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0167e-01 (5.0167e-01)	Acc 0.170898 (0.170898)
Epoch: [0][300/616]	Loss 2.5586e-01 (2.8524e-01)	Acc 0.737305 (0.695663)
Epoch: [0][600/616]	Loss 2.6414e-01 (2.7307e-01)	Acc 0.735352 (0.712101)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.717965)
Training Loss of Epoch 0: 0.2726948121941187
Training Acc of Epoch 0: 0.712549225101626
Testing Acc of Epoch 0: 0.7179652173913044
Model with the best training loss saved! The loss is 0.2726948121941187
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.7069e-01 (2.7069e-01)	Acc 0.700195 (0.700195)
Epoch: [1][300/616]	Loss 2.5678e-01 (2.5746e-01)	Acc 0.729492 (0.731604)
Epoch: [1][600/616]	Loss 3.2705e-01 (2.5847e-01)	Acc 0.676758 (0.730592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.724304)
Training Loss of Epoch 1: 0.259039886937878
Training Acc of Epoch 1: 0.7299939659552845
Testing Acc of Epoch 1: 0.724304347826087
Model with the best training loss saved! The loss is 0.259039886937878
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.7300e-01 (2.7300e-01)	Acc 0.710938 (0.710938)
Epoch: [2][300/616]	Loss 2.7286e-01 (2.6163e-01)	Acc 0.721680 (0.727461)
Epoch: [2][600/616]	Loss 2.4089e-01 (2.5849e-01)	Acc 0.750977 (0.730543)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.729630)
Training Loss of Epoch 2: 0.2584160768646535
Training Acc of Epoch 2: 0.730541793699187
Testing Acc of Epoch 2: 0.7296304347826087
Model with the best training loss saved! The loss is 0.2584160768646535
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4708e-01 (2.4708e-01)	Acc 0.726562 (0.726562)
Epoch: [3][300/616]	Loss 2.4959e-01 (2.5948e-01)	Acc 0.735352 (0.729878)
Epoch: [3][600/616]	Loss 2.3787e-01 (2.5836e-01)	Acc 0.756836 (0.730729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.737143)
Training Loss of Epoch 3: 0.2582857876773772
Training Acc of Epoch 3: 0.7307513973577235
Testing Acc of Epoch 3: 0.7371434782608696
Model with the best training loss saved! The loss is 0.2582857876773772
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4118e-01 (2.4118e-01)	Acc 0.746094 (0.746094)
Epoch: [4][300/616]	Loss 2.7202e-01 (2.5687e-01)	Acc 0.717773 (0.731221)
Epoch: [4][600/616]	Loss 2.7830e-01 (2.5707e-01)	Acc 0.716797 (0.731315)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.725261)
Training Loss of Epoch 4: 0.25722791838452097
Training Acc of Epoch 4: 0.7310912093495935
Testing Acc of Epoch 4: 0.7252608695652174
Model with the best training loss saved! The loss is 0.25722791838452097
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.5790e-01 (2.5790e-01)	Acc 0.723633 (0.723633)
Epoch: [5][300/616]	Loss 2.5463e-01 (2.5534e-01)	Acc 0.742188 (0.733061)
Epoch: [5][600/616]	Loss 2.5375e-01 (2.5667e-01)	Acc 0.745117 (0.731928)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.733548)
Training Loss of Epoch 5: 0.25674734578384617
Training Acc of Epoch 5: 0.7318470528455284
Testing Acc of Epoch 5: 0.7335478260869566
Model with the best training loss saved! The loss is 0.25674734578384617
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.6724e-01 (2.6724e-01)	Acc 0.713867 (0.713867)
Epoch: [6][300/616]	Loss 2.4557e-01 (2.5696e-01)	Acc 0.746094 (0.731679)
Epoch: [6][600/616]	Loss 2.6135e-01 (2.5714e-01)	Acc 0.716797 (0.731312)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.734757)
Training Loss of Epoch 6: 0.2573091293495845
Training Acc of Epoch 6: 0.7311118521341463
Testing Acc of Epoch 6: 0.7347565217391304
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4212e-01 (2.4212e-01)	Acc 0.754883 (0.754883)
Epoch: [7][300/616]	Loss 2.5840e-01 (2.5816e-01)	Acc 0.730469 (0.730540)
Epoch: [7][600/616]	Loss 2.6606e-01 (2.5805e-01)	Acc 0.718750 (0.730563)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.740961)
Training Loss of Epoch 7: 0.2579438458613264
Training Acc of Epoch 7: 0.7307244029471545
Testing Acc of Epoch 7: 0.7409608695652173
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4967e-01 (2.4967e-01)	Acc 0.748047 (0.748047)
Epoch: [8][300/616]	Loss 2.5967e-01 (2.5933e-01)	Acc 0.720703 (0.728159)
Epoch: [8][600/616]	Loss 2.5685e-01 (2.5878e-01)	Acc 0.737305 (0.728836)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741422)
Training Loss of Epoch 8: 0.25867638057324943
Training Acc of Epoch 8: 0.7289284806910569
Testing Acc of Epoch 8: 0.7414217391304347
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4712e-01 (2.4712e-01)	Acc 0.744141 (0.744141)
Epoch: [9][300/616]	Loss 2.5474e-01 (2.6023e-01)	Acc 0.734375 (0.728055)
Epoch: [9][600/616]	Loss 2.7021e-01 (2.5851e-01)	Acc 0.704102 (0.729632)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.731857)
Training Loss of Epoch 9: 0.2583919971454434
Training Acc of Epoch 9: 0.7297208460365854
Testing Acc of Epoch 9: 0.7318565217391304
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.6080e-01 (2.6080e-01)	Acc 0.726562 (0.726562)
Epoch: [10][300/616]	Loss 2.5803e-01 (2.5830e-01)	Acc 0.708984 (0.729359)
Epoch: [10][600/616]	Loss 2.5774e-01 (2.5849e-01)	Acc 0.756836 (0.729400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.729830)
Training Loss of Epoch 10: 0.2585518730365164
Training Acc of Epoch 10: 0.7293111661585366
Testing Acc of Epoch 10: 0.7298304347826087
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5506e-01 (2.5506e-01)	Acc 0.746094 (0.746094)
Epoch: [11][300/616]	Loss 2.5970e-01 (2.5721e-01)	Acc 0.737305 (0.731737)
Epoch: [11][600/616]	Loss 2.5529e-01 (2.5885e-01)	Acc 0.723633 (0.729577)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.734304)
Training Loss of Epoch 11: 0.2587694162275733
Training Acc of Epoch 11: 0.7296414507113821
Testing Acc of Epoch 11: 0.734304347826087
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6054e-01 (2.6054e-01)	Acc 0.724609 (0.724609)
Epoch: [12][300/616]	Loss 2.9086e-01 (2.6088e-01)	Acc 0.686523 (0.727468)
Epoch: [12][600/616]	Loss 2.5035e-01 (2.5963e-01)	Acc 0.743164 (0.728607)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.671474 (0.685652)
Training Loss of Epoch 12: 0.2597644865512848
Training Acc of Epoch 12: 0.7285029217479675
Testing Acc of Epoch 12: 0.6856521739130435
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.7740e-01 (2.7740e-01)	Acc 0.690430 (0.690430)
Epoch: [13][300/616]	Loss 2.5904e-01 (2.5964e-01)	Acc 0.730469 (0.727591)
Epoch: [13][600/616]	Loss 2.5297e-01 (2.6088e-01)	Acc 0.729492 (0.726840)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734243)
Training Loss of Epoch 13: 0.26086882132340256
Training Acc of Epoch 13: 0.7269563008130081
Testing Acc of Epoch 13: 0.7342434782608696
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6527e-01 (2.6527e-01)	Acc 0.713867 (0.713867)
Epoch: [14][300/616]	Loss 2.7443e-01 (2.5834e-01)	Acc 0.733398 (0.729463)
Epoch: [14][600/616]	Loss 2.3689e-01 (2.5864e-01)	Acc 0.755859 (0.729289)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.732509)
Training Loss of Epoch 14: 0.2584760021630341
Training Acc of Epoch 14: 0.7294874237804878
Testing Acc of Epoch 14: 0.7325086956521739
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4900e-01 (2.4900e-01)	Acc 0.736328 (0.736328)
Epoch: [15][300/616]	Loss 2.6114e-01 (2.5926e-01)	Acc 0.736328 (0.729216)
Epoch: [15][600/616]	Loss 2.5212e-01 (2.5823e-01)	Acc 0.725586 (0.730228)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.712713)
Training Loss of Epoch 15: 0.2582138436596568
Training Acc of Epoch 15: 0.7302162728658537
Testing Acc of Epoch 15: 0.7127130434782609
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.7769e-01 (2.7769e-01)	Acc 0.717773 (0.717773)
Epoch: [16][300/616]	Loss 2.5556e-01 (2.5926e-01)	Acc 0.736328 (0.728727)
Epoch: [16][600/616]	Loss 2.5779e-01 (2.5975e-01)	Acc 0.730469 (0.728152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.739826)
Training Loss of Epoch 16: 0.25973803808049456
Training Acc of Epoch 16: 0.7281631097560975
Testing Acc of Epoch 16: 0.7398260869565217
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4407e-01 (2.4407e-01)	Acc 0.751953 (0.751953)
Epoch: [17][300/616]	Loss 2.6353e-01 (2.5782e-01)	Acc 0.711914 (0.729700)
Epoch: [17][600/616]	Loss 2.8258e-01 (2.5916e-01)	Acc 0.693359 (0.728946)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.732504)
Training Loss of Epoch 17: 0.2590509945299567
Training Acc of Epoch 17: 0.7291428480691057
Testing Acc of Epoch 17: 0.732504347826087
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.5025e-01 (2.5025e-01)	Acc 0.746094 (0.746094)
Epoch: [18][300/616]	Loss 2.5956e-01 (2.6002e-01)	Acc 0.731445 (0.727705)
Epoch: [18][600/616]	Loss 2.5248e-01 (2.5923e-01)	Acc 0.733398 (0.728961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.729557)
Training Loss of Epoch 18: 0.25939924026407846
Training Acc of Epoch 18: 0.7287077616869919
Testing Acc of Epoch 18: 0.7295565217391304
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.6967e-01 (2.6967e-01)	Acc 0.721680 (0.721680)
Epoch: [19][300/616]	Loss 2.4766e-01 (2.5698e-01)	Acc 0.733398 (0.731312)
Epoch: [19][600/616]	Loss 2.6427e-01 (2.5799e-01)	Acc 0.713867 (0.729947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.725809)
Training Loss of Epoch 19: 0.2580445208200594
Training Acc of Epoch 19: 0.7298859883130081
Testing Acc of Epoch 19: 0.7258086956521739
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.7230e-01 (2.7230e-01)	Acc 0.708984 (0.708984)
Epoch: [20][300/616]	Loss 2.7554e-01 (2.5901e-01)	Acc 0.712891 (0.729508)
Epoch: [20][600/616]	Loss 2.7327e-01 (2.5983e-01)	Acc 0.706055 (0.728657)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.727922)
Training Loss of Epoch 20: 0.2598097232056827
Training Acc of Epoch 20: 0.7287077616869919
Testing Acc of Epoch 20: 0.7279217391304348
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.6646e-01 (2.6646e-01)	Acc 0.719727 (0.719727)
Epoch: [21][300/616]	Loss 2.4221e-01 (2.5816e-01)	Acc 0.751953 (0.730271)
Epoch: [21][600/616]	Loss 2.7643e-01 (2.5855e-01)	Acc 0.698242 (0.729944)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.732022)
Training Loss of Epoch 21: 0.2585183843849151
Training Acc of Epoch 21: 0.7299653836382114
Testing Acc of Epoch 21: 0.7320217391304348
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.7561e-01 (2.7561e-01)	Acc 0.704102 (0.704102)
Epoch: [22][300/616]	Loss 2.5645e-01 (2.5784e-01)	Acc 0.752930 (0.730384)
Epoch: [22][600/616]	Loss 2.6321e-01 (2.5929e-01)	Acc 0.701172 (0.728633)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735913)
Training Loss of Epoch 22: 0.2593178800451077
Training Acc of Epoch 22: 0.7286156631097561
Testing Acc of Epoch 22: 0.7359130434782609
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.5691e-01 (2.5691e-01)	Acc 0.737305 (0.737305)
Epoch: [23][300/616]	Loss 2.6435e-01 (2.5858e-01)	Acc 0.718750 (0.729901)
Epoch: [23][600/616]	Loss 2.5522e-01 (2.5934e-01)	Acc 0.745117 (0.729140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.742026)
Training Loss of Epoch 23: 0.25912677977627857
Training Acc of Epoch 23: 0.7293175177845529
Testing Acc of Epoch 23: 0.7420260869565217
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.5642e-01 (2.5642e-01)	Acc 0.725586 (0.725586)
Epoch: [24][300/616]	Loss 2.7331e-01 (2.5922e-01)	Acc 0.708984 (0.728691)
Epoch: [24][600/616]	Loss 2.6330e-01 (2.5977e-01)	Acc 0.722656 (0.728176)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.738470)
Training Loss of Epoch 24: 0.25957562911316634
Training Acc of Epoch 24: 0.728393356199187
Testing Acc of Epoch 24: 0.7384695652173913
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3732e-01 (2.3732e-01)	Acc 0.745117 (0.745117)
Epoch: [25][300/616]	Loss 2.4632e-01 (2.5981e-01)	Acc 0.732422 (0.727163)
Epoch: [25][600/616]	Loss 2.6520e-01 (2.5922e-01)	Acc 0.729492 (0.728222)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.733400)
Training Loss of Epoch 25: 0.2592935663655521
Training Acc of Epoch 25: 0.7282091590447154
Testing Acc of Epoch 25: 0.7334
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4552e-01 (2.4552e-01)	Acc 0.741211 (0.741211)
Epoch: [26][300/616]	Loss 2.5284e-01 (2.5782e-01)	Acc 0.733398 (0.730186)
Epoch: [26][600/616]	Loss 2.5380e-01 (2.5920e-01)	Acc 0.741211 (0.728982)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.728443)
Training Loss of Epoch 26: 0.25925511019985853
Training Acc of Epoch 26: 0.7288189151422764
Testing Acc of Epoch 26: 0.7284434782608695
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.7106e-01 (2.7106e-01)	Acc 0.717773 (0.717773)
Epoch: [27][300/616]	Loss 2.8540e-01 (2.5874e-01)	Acc 0.693359 (0.729265)
Epoch: [27][600/616]	Loss 2.5620e-01 (2.5863e-01)	Acc 0.731445 (0.729287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.737152)
Training Loss of Epoch 27: 0.2584975102325765
Training Acc of Epoch 27: 0.7294159679878048
Testing Acc of Epoch 27: 0.7371521739130434
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4667e-01 (2.4667e-01)	Acc 0.750977 (0.750977)
Epoch: [28][300/616]	Loss 2.6281e-01 (2.5717e-01)	Acc 0.730469 (0.730774)
Epoch: [28][600/616]	Loss 2.3877e-01 (2.5885e-01)	Acc 0.750977 (0.729034)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.732730)
Training Loss of Epoch 28: 0.25890660118765946
Training Acc of Epoch 28: 0.7289046620934959
Testing Acc of Epoch 28: 0.7327304347826087
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.6377e-01 (2.6377e-01)	Acc 0.720703 (0.720703)
Epoch: [29][300/616]	Loss 2.6926e-01 (2.5850e-01)	Acc 0.708984 (0.729583)
Epoch: [29][600/616]	Loss 2.4320e-01 (2.5943e-01)	Acc 0.750000 (0.728603)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.733657)
Training Loss of Epoch 29: 0.25932926031632153
Training Acc of Epoch 29: 0.728760162601626
Testing Acc of Epoch 29: 0.7336565217391304
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.5157e-01 (2.5157e-01)	Acc 0.737305 (0.737305)
Epoch: [30][300/616]	Loss 2.4468e-01 (2.5942e-01)	Acc 0.744141 (0.729421)
Epoch: [30][600/616]	Loss 2.4215e-01 (2.5933e-01)	Acc 0.752930 (0.729133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.731278)
Training Loss of Epoch 30: 0.25931341161088245
Training Acc of Epoch 30: 0.7290809197154472
Testing Acc of Epoch 30: 0.7312782608695653
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.6329e-01 (2.6329e-01)	Acc 0.716797 (0.716797)
Epoch: [31][300/616]	Loss 2.6516e-01 (2.5630e-01)	Acc 0.730469 (0.731838)
Epoch: [31][600/616]	Loss 2.6154e-01 (2.5839e-01)	Acc 0.734375 (0.729975)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.735230)
Training Loss of Epoch 31: 0.2585313118327924
Training Acc of Epoch 31: 0.7297684832317073
Testing Acc of Epoch 31: 0.7352304347826087
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4732e-01 (2.4732e-01)	Acc 0.735352 (0.735352)
Epoch: [32][300/616]	Loss 2.6856e-01 (2.5995e-01)	Acc 0.733398 (0.728130)
Epoch: [32][600/616]	Loss 2.4703e-01 (2.6096e-01)	Acc 0.733398 (0.727104)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.731022)
Training Loss of Epoch 32: 0.26094013711785885
Training Acc of Epoch 32: 0.7271817835365854
Testing Acc of Epoch 32: 0.7310217391304348
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.5937e-01 (2.5937e-01)	Acc 0.719727 (0.719727)
Epoch: [33][300/616]	Loss 2.5665e-01 (2.5690e-01)	Acc 0.732422 (0.731384)
Epoch: [33][600/616]	Loss 2.2716e-01 (2.5663e-01)	Acc 0.771484 (0.731216)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.712813)
Training Loss of Epoch 33: 0.256737010289983
Training Acc of Epoch 33: 0.7310896214430894
Testing Acc of Epoch 33: 0.7128130434782609
Model with the best training loss saved! The loss is 0.256737010289983
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.6710e-01 (2.6710e-01)	Acc 0.722656 (0.722656)
Epoch: [34][300/616]	Loss 2.4151e-01 (2.6064e-01)	Acc 0.753906 (0.727559)
Epoch: [34][600/616]	Loss 2.5954e-01 (2.5962e-01)	Acc 0.737305 (0.728711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.732157)
Training Loss of Epoch 34: 0.25954435557369293
Training Acc of Epoch 34: 0.7287395198170732
Testing Acc of Epoch 34: 0.7321565217391305
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.6867e-01 (2.6867e-01)	Acc 0.716797 (0.716797)
Epoch: [35][300/616]	Loss 2.7439e-01 (2.5919e-01)	Acc 0.708984 (0.728061)
Epoch: [35][600/616]	Loss 2.6041e-01 (2.5935e-01)	Acc 0.713867 (0.728605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.737722)
Training Loss of Epoch 35: 0.2592931298947916
Training Acc of Epoch 35: 0.7286347179878049
Testing Acc of Epoch 35: 0.7377217391304348
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.6353e-01 (2.6353e-01)	Acc 0.733398 (0.733398)
Epoch: [36][300/616]	Loss 2.6540e-01 (2.5845e-01)	Acc 0.721680 (0.729268)
Epoch: [36][600/616]	Loss 2.8532e-01 (2.6014e-01)	Acc 0.699219 (0.727607)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.715678)
Training Loss of Epoch 36: 0.2601300961602994
Training Acc of Epoch 36: 0.7277439024390244
Testing Acc of Epoch 36: 0.7156782608695652
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.6509e-01 (2.6509e-01)	Acc 0.729492 (0.729492)
Epoch: [37][300/616]	Loss 2.6610e-01 (2.6104e-01)	Acc 0.710938 (0.727182)
Epoch: [37][600/616]	Loss 2.4972e-01 (2.6053e-01)	Acc 0.731445 (0.727653)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.737530)
Training Loss of Epoch 37: 0.26042285408915544
Training Acc of Epoch 37: 0.7277994791666667
Testing Acc of Epoch 37: 0.7375304347826087
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.4358e-01 (2.4358e-01)	Acc 0.745117 (0.745117)
Epoch: [38][300/616]	Loss 2.5680e-01 (2.5785e-01)	Acc 0.717773 (0.730170)
Epoch: [38][600/616]	Loss 2.3622e-01 (2.5760e-01)	Acc 0.770508 (0.730815)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.728752)
Training Loss of Epoch 38: 0.2576589903938092
Training Acc of Epoch 38: 0.7308022103658537
Testing Acc of Epoch 38: 0.7287521739130435
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.6022e-01 (2.6022e-01)	Acc 0.731445 (0.731445)
Epoch: [39][300/616]	Loss 2.5100e-01 (2.5643e-01)	Acc 0.746094 (0.731828)
Epoch: [39][600/616]	Loss 2.6680e-01 (2.5653e-01)	Acc 0.719727 (0.732009)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.728617)
Training Loss of Epoch 39: 0.25668377432881334
Training Acc of Epoch 39: 0.7319058053861789
Testing Acc of Epoch 39: 0.7286173913043478
Model with the best training loss saved! The loss is 0.25668377432881334
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.5978e-01 (2.5978e-01)	Acc 0.734375 (0.734375)
Epoch: [40][300/616]	Loss 2.4950e-01 (2.5854e-01)	Acc 0.742188 (0.729427)
Epoch: [40][600/616]	Loss 2.4736e-01 (2.5979e-01)	Acc 0.756836 (0.728056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.715252)
Training Loss of Epoch 40: 0.2596695031334714
Training Acc of Epoch 40: 0.7281615218495935
Testing Acc of Epoch 40: 0.7152521739130435
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.6582e-01 (2.6582e-01)	Acc 0.714844 (0.714844)
Epoch: [41][300/616]	Loss 2.5649e-01 (2.5844e-01)	Acc 0.728516 (0.729456)
Epoch: [41][600/616]	Loss 2.7663e-01 (2.5907e-01)	Acc 0.705078 (0.728667)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.730543)
Training Loss of Epoch 41: 0.2590632874064329
Training Acc of Epoch 41: 0.728663300304878
Testing Acc of Epoch 41: 0.7305434782608695
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.6562e-01 (2.6562e-01)	Acc 0.718750 (0.718750)
Epoch: [42][300/616]	Loss 2.6341e-01 (2.5976e-01)	Acc 0.718750 (0.727951)
Epoch: [42][600/616]	Loss 2.3942e-01 (2.5943e-01)	Acc 0.735352 (0.728753)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.729083)
Training Loss of Epoch 42: 0.2591588088167392
Training Acc of Epoch 42: 0.729030106707317
Testing Acc of Epoch 42: 0.7290826086956522
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.4403e-01 (2.4403e-01)	Acc 0.743164 (0.743164)
Epoch: [43][300/616]	Loss 2.6567e-01 (2.5977e-01)	Acc 0.709961 (0.727146)
Epoch: [43][600/616]	Loss 2.5219e-01 (2.5936e-01)	Acc 0.745117 (0.728278)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.734917)
Training Loss of Epoch 43: 0.25916216383135415
Training Acc of Epoch 43: 0.7285283282520325
Testing Acc of Epoch 43: 0.7349173913043479
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4942e-01 (2.4942e-01)	Acc 0.745117 (0.745117)
Epoch: [44][300/616]	Loss 2.6380e-01 (2.5677e-01)	Acc 0.716797 (0.731354)
Epoch: [44][600/616]	Loss 2.6344e-01 (2.5859e-01)	Acc 0.719727 (0.729482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.727917)
Training Loss of Epoch 44: 0.25850939886356755
Training Acc of Epoch 44: 0.7295763465447155
Testing Acc of Epoch 44: 0.7279173913043478
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.6349e-01 (2.6349e-01)	Acc 0.717773 (0.717773)
Epoch: [45][300/616]	Loss 2.5648e-01 (2.5830e-01)	Acc 0.731445 (0.729051)
Epoch: [45][600/616]	Loss 2.7956e-01 (2.5948e-01)	Acc 0.697266 (0.728262)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.730478)
Training Loss of Epoch 45: 0.2596113906643255
Training Acc of Epoch 45: 0.728077362804878
Testing Acc of Epoch 45: 0.7304782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.8021e-01 (2.8021e-01)	Acc 0.720703 (0.720703)
Epoch: [46][300/616]	Loss 2.6263e-01 (2.5817e-01)	Acc 0.717773 (0.729615)
Epoch: [46][600/616]	Loss 2.5236e-01 (2.5856e-01)	Acc 0.730469 (0.729385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.737070)
Training Loss of Epoch 46: 0.2584529958362502
Training Acc of Epoch 46: 0.7295080665650406
Testing Acc of Epoch 46: 0.7370695652173913
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4775e-01 (2.4775e-01)	Acc 0.747070 (0.747070)
Epoch: [47][300/616]	Loss 2.4323e-01 (2.6030e-01)	Acc 0.759766 (0.727682)
Epoch: [47][600/616]	Loss 2.6504e-01 (2.6018e-01)	Acc 0.712891 (0.728171)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.735317)
Training Loss of Epoch 47: 0.2601542693812673
Training Acc of Epoch 47: 0.7281694613821138
Testing Acc of Epoch 47: 0.7353173913043478
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.5115e-01 (2.5115e-01)	Acc 0.720703 (0.720703)
Epoch: [48][300/616]	Loss 2.5395e-01 (2.5953e-01)	Acc 0.743164 (0.728486)
Epoch: [48][600/616]	Loss 2.3508e-01 (2.5957e-01)	Acc 0.761719 (0.728408)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.725922)
Training Loss of Epoch 48: 0.25967074979126936
Training Acc of Epoch 48: 0.7282298018292683
Testing Acc of Epoch 48: 0.7259217391304348
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.5389e-01 (2.5389e-01)	Acc 0.734375 (0.734375)
Epoch: [49][300/616]	Loss 2.5640e-01 (2.5881e-01)	Acc 0.734375 (0.729379)
Epoch: [49][600/616]	Loss 2.5652e-01 (2.5933e-01)	Acc 0.730469 (0.728628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.735617)
Training Loss of Epoch 49: 0.25926872857702454
Training Acc of Epoch 49: 0.7286823551829268
Testing Acc of Epoch 49: 0.7356173913043478
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4947e-01 (2.4947e-01)	Acc 0.740234 (0.740234)
Epoch: [50][300/616]	Loss 2.4461e-01 (2.5827e-01)	Acc 0.754883 (0.730047)
Epoch: [50][600/616]	Loss 2.4731e-01 (2.5833e-01)	Acc 0.749023 (0.730441)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.719783)
Training Loss of Epoch 50: 0.258454732197087
Training Acc of Epoch 50: 0.7301845147357724
Testing Acc of Epoch 50: 0.7197826086956521
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.6254e-01 (2.6254e-01)	Acc 0.730469 (0.730469)
Epoch: [51][300/616]	Loss 2.4669e-01 (2.5919e-01)	Acc 0.741211 (0.728327)
Epoch: [51][600/616]	Loss 2.5250e-01 (2.5924e-01)	Acc 0.740234 (0.728662)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.733217)
Training Loss of Epoch 51: 0.2592599516961633
Training Acc of Epoch 51: 0.7285854928861789
Testing Acc of Epoch 51: 0.7332173913043478
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.6200e-01 (2.6200e-01)	Acc 0.723633 (0.723633)
Epoch: [52][300/616]	Loss 2.4966e-01 (2.5828e-01)	Acc 0.723633 (0.729450)
Epoch: [52][600/616]	Loss 2.5782e-01 (2.5854e-01)	Acc 0.722656 (0.728998)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.707657)
Training Loss of Epoch 52: 0.25838812638104447
Training Acc of Epoch 52: 0.7291603150406504
Testing Acc of Epoch 52: 0.7076565217391304
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.7601e-01 (2.7601e-01)	Acc 0.703125 (0.703125)
Epoch: [53][300/616]	Loss 2.6225e-01 (2.6051e-01)	Acc 0.723633 (0.727646)
Epoch: [53][600/616]	Loss 2.7175e-01 (2.5996e-01)	Acc 0.723633 (0.728077)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.715491)
Training Loss of Epoch 53: 0.2600292661810309
Training Acc of Epoch 53: 0.7279963795731708
Testing Acc of Epoch 53: 0.715491304347826
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.6190e-01 (2.6190e-01)	Acc 0.742188 (0.742188)
Epoch: [54][300/616]	Loss 2.7332e-01 (2.6051e-01)	Acc 0.718750 (0.727494)
Epoch: [54][600/616]	Loss 2.6858e-01 (2.5924e-01)	Acc 0.708008 (0.729206)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.732752)
Training Loss of Epoch 54: 0.25929541924620064
Training Acc of Epoch 54: 0.7291444359756097
Testing Acc of Epoch 54: 0.7327521739130435
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.6281e-01 (2.6281e-01)	Acc 0.730469 (0.730469)
Epoch: [55][300/616]	Loss 2.4803e-01 (2.5812e-01)	Acc 0.741211 (0.730388)
Epoch: [55][600/616]	Loss 2.5698e-01 (2.5892e-01)	Acc 0.731445 (0.729469)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.739591)
Training Loss of Epoch 55: 0.2588023435778734
Training Acc of Epoch 55: 0.7295715828252033
Testing Acc of Epoch 55: 0.739591304347826
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5406e-01 (2.5406e-01)	Acc 0.732422 (0.732422)
Epoch: [56][300/616]	Loss 2.5373e-01 (2.5956e-01)	Acc 0.727539 (0.727954)
Epoch: [56][600/616]	Loss 2.6148e-01 (2.5905e-01)	Acc 0.739258 (0.728672)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.712148)
Training Loss of Epoch 56: 0.2589743625584657
Training Acc of Epoch 56: 0.7288252667682927
Testing Acc of Epoch 56: 0.7121478260869565
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.6705e-01 (2.6705e-01)	Acc 0.726562 (0.726562)
Epoch: [57][300/616]	Loss 2.5747e-01 (2.5945e-01)	Acc 0.735352 (0.728139)
Epoch: [57][600/616]	Loss 2.7184e-01 (2.5927e-01)	Acc 0.737305 (0.728754)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.732952)
Training Loss of Epoch 57: 0.25939647863066295
Training Acc of Epoch 57: 0.7286664761178862
Testing Acc of Epoch 57: 0.7329521739130435
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.7508e-01 (2.7508e-01)	Acc 0.724609 (0.724609)
Epoch: [58][300/616]	Loss 2.4842e-01 (2.6015e-01)	Acc 0.742188 (0.728363)
Epoch: [58][600/616]	Loss 2.5096e-01 (2.5968e-01)	Acc 0.746094 (0.728683)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.696839)
Training Loss of Epoch 58: 0.25961023113591886
Training Acc of Epoch 58: 0.7287728658536585
Testing Acc of Epoch 58: 0.6968391304347826
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.9534e-01 (2.9534e-01)	Acc 0.694336 (0.694336)
Epoch: [59][300/616]	Loss 2.5192e-01 (2.6001e-01)	Acc 0.725586 (0.727802)
Epoch: [59][600/616]	Loss 2.4556e-01 (2.5954e-01)	Acc 0.732422 (0.728618)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.737596)
Training Loss of Epoch 59: 0.2594169489736479
Training Acc of Epoch 59: 0.728734756097561
Testing Acc of Epoch 59: 0.7375956521739131
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5636e-01 (2.5636e-01)	Acc 0.719727 (0.719727)
Epoch: [60][300/616]	Loss 2.4615e-01 (2.5896e-01)	Acc 0.741211 (0.728405)
Epoch: [60][600/616]	Loss 2.5438e-01 (2.5884e-01)	Acc 0.726562 (0.729391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.734343)
Training Loss of Epoch 60: 0.25887363809880204
Training Acc of Epoch 60: 0.7293222815040651
Testing Acc of Epoch 60: 0.7343434782608695
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.5229e-01 (2.5229e-01)	Acc 0.725586 (0.725586)
Epoch: [61][300/616]	Loss 2.3996e-01 (2.5740e-01)	Acc 0.745117 (0.731861)
Epoch: [61][600/616]	Loss 2.8057e-01 (2.5828e-01)	Acc 0.693359 (0.730293)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.729800)
Training Loss of Epoch 61: 0.25854152891694043
Training Acc of Epoch 61: 0.729930449695122
Testing Acc of Epoch 61: 0.7298
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.6246e-01 (2.6246e-01)	Acc 0.729492 (0.729492)
Epoch: [62][300/616]	Loss 2.5527e-01 (2.5959e-01)	Acc 0.733398 (0.728230)
Epoch: [62][600/616]	Loss 2.7334e-01 (2.5857e-01)	Acc 0.709961 (0.729882)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.716483)
Training Loss of Epoch 62: 0.25856668372464375
Training Acc of Epoch 62: 0.7298669334349593
Testing Acc of Epoch 62: 0.7164826086956522
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.6576e-01 (2.6576e-01)	Acc 0.739258 (0.739258)
Epoch: [63][300/616]	Loss 2.7279e-01 (2.6044e-01)	Acc 0.710938 (0.727640)
Epoch: [63][600/616]	Loss 2.7347e-01 (2.5968e-01)	Acc 0.700195 (0.728524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.730778)
Training Loss of Epoch 63: 0.2596887880466818
Training Acc of Epoch 63: 0.7285045096544716
Testing Acc of Epoch 63: 0.7307782608695652
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5748e-01 (2.5748e-01)	Acc 0.734375 (0.734375)
Epoch: [64][300/616]	Loss 2.6007e-01 (2.5893e-01)	Acc 0.716797 (0.728626)
Epoch: [64][600/616]	Loss 2.4343e-01 (2.5959e-01)	Acc 0.756836 (0.728571)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.736652)
Training Loss of Epoch 64: 0.2597478413242635
Training Acc of Epoch 64: 0.728344131097561
Testing Acc of Epoch 64: 0.7366521739130435
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4962e-01 (2.4962e-01)	Acc 0.738281 (0.738281)
Epoch: [65][300/616]	Loss 2.5749e-01 (2.6027e-01)	Acc 0.723633 (0.726910)
Epoch: [65][600/616]	Loss 2.6986e-01 (2.5902e-01)	Acc 0.717773 (0.728740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.734174)
Training Loss of Epoch 65: 0.25897467955341186
Training Acc of Epoch 65: 0.7287966844512195
Testing Acc of Epoch 65: 0.7341739130434782
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.5778e-01 (2.5778e-01)	Acc 0.731445 (0.731445)
Epoch: [66][300/616]	Loss 2.7561e-01 (2.5774e-01)	Acc 0.723633 (0.730683)
Epoch: [66][600/616]	Loss 2.7967e-01 (2.5864e-01)	Acc 0.699219 (0.729819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.727883)
Training Loss of Epoch 66: 0.2589155113551675
Training Acc of Epoch 66: 0.7296239837398374
Testing Acc of Epoch 66: 0.7278826086956521
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.6461e-01 (2.6461e-01)	Acc 0.727539 (0.727539)
Epoch: [67][300/616]	Loss 3.0170e-01 (2.5813e-01)	Acc 0.680664 (0.729920)
Epoch: [67][600/616]	Loss 2.5261e-01 (2.5874e-01)	Acc 0.729492 (0.729429)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.734674)
Training Loss of Epoch 67: 0.2586600140827458
Training Acc of Epoch 67: 0.7295572916666667
Testing Acc of Epoch 67: 0.7346739130434783
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.5477e-01 (2.5477e-01)	Acc 0.748047 (0.748047)
Epoch: [68][300/616]	Loss 2.5852e-01 (2.6077e-01)	Acc 0.719727 (0.727328)
Epoch: [68][600/616]	Loss 2.6677e-01 (2.5882e-01)	Acc 0.724609 (0.729034)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.700278)
Training Loss of Epoch 68: 0.2587797732857185
Training Acc of Epoch 68: 0.729198424796748
Testing Acc of Epoch 68: 0.7002782608695652
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.7919e-01 (2.7919e-01)	Acc 0.695312 (0.695312)
Epoch: [69][300/616]	Loss 2.8017e-01 (2.5957e-01)	Acc 0.707031 (0.728048)
Epoch: [69][600/616]	Loss 2.4735e-01 (2.5889e-01)	Acc 0.748047 (0.729523)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734839)
Training Loss of Epoch 69: 0.258936259513948
Training Acc of Epoch 69: 0.7294763084349594
Testing Acc of Epoch 69: 0.7348391304347827
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.5925e-01 (2.5925e-01)	Acc 0.730469 (0.730469)
Epoch: [70][300/616]	Loss 2.4733e-01 (2.5797e-01)	Acc 0.750000 (0.729288)
Epoch: [70][600/616]	Loss 2.6074e-01 (2.5900e-01)	Acc 0.724609 (0.729170)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.725483)
Training Loss of Epoch 70: 0.2591189558670773
Training Acc of Epoch 70: 0.7289602388211383
Testing Acc of Epoch 70: 0.7254826086956522
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.7260e-01 (2.7260e-01)	Acc 0.709961 (0.709961)
Epoch: [71][300/616]	Loss 2.5276e-01 (2.5727e-01)	Acc 0.746094 (0.730758)
Epoch: [71][600/616]	Loss 2.9769e-01 (2.5863e-01)	Acc 0.674805 (0.729788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.731039)
Training Loss of Epoch 71: 0.258916528486624
Training Acc of Epoch 71: 0.7294493140243903
Testing Acc of Epoch 71: 0.7310391304347826
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.5264e-01 (2.5264e-01)	Acc 0.747070 (0.747070)
Epoch: [72][300/616]	Loss 2.9409e-01 (2.5888e-01)	Acc 0.708008 (0.729395)
Epoch: [72][600/616]	Loss 2.4712e-01 (2.5977e-01)	Acc 0.744141 (0.728480)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.721539)
Training Loss of Epoch 72: 0.25965478286995153
Training Acc of Epoch 72: 0.7285362677845528
Testing Acc of Epoch 72: 0.7215391304347826
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.6099e-01 (2.6099e-01)	Acc 0.733398 (0.733398)
Epoch: [73][300/616]	Loss 2.5954e-01 (2.5866e-01)	Acc 0.733398 (0.729411)
Epoch: [73][600/616]	Loss 2.5528e-01 (2.5800e-01)	Acc 0.732422 (0.730349)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.738909)
Training Loss of Epoch 73: 0.2580262440976089
Training Acc of Epoch 73: 0.7303020198170732
Testing Acc of Epoch 73: 0.7389086956521739
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.5561e-01 (2.5561e-01)	Acc 0.742188 (0.742188)
Epoch: [74][300/616]	Loss 2.6862e-01 (2.5769e-01)	Acc 0.707031 (0.730079)
Epoch: [74][600/616]	Loss 2.7496e-01 (2.5736e-01)	Acc 0.709961 (0.730592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.730143)
Training Loss of Epoch 74: 0.2573661618116425
Training Acc of Epoch 74: 0.7305497332317074
Testing Acc of Epoch 74: 0.7301434782608696
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5173e-01 (2.5173e-01)	Acc 0.734375 (0.734375)
Epoch: [75][300/616]	Loss 2.6217e-01 (2.4902e-01)	Acc 0.716797 (0.737902)
Epoch: [75][600/616]	Loss 2.3989e-01 (2.4931e-01)	Acc 0.754883 (0.737727)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.729278)
Training Loss of Epoch 75: 0.24917046631254802
Training Acc of Epoch 75: 0.7379239710365854
Testing Acc of Epoch 75: 0.7292782608695653
Model with the best training loss saved! The loss is 0.24917046631254802
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.6064e-01 (2.6064e-01)	Acc 0.719727 (0.719727)
Epoch: [76][300/616]	Loss 2.4967e-01 (2.5020e-01)	Acc 0.743164 (0.737785)
Epoch: [76][600/616]	Loss 2.5179e-01 (2.4966e-01)	Acc 0.740234 (0.738020)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.723217)
Training Loss of Epoch 76: 0.24972856110189018
Training Acc of Epoch 76: 0.7378731580284553
Testing Acc of Epoch 76: 0.7232173913043478
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.6458e-01 (2.6458e-01)	Acc 0.711914 (0.711914)
Epoch: [77][300/616]	Loss 2.6374e-01 (2.5022e-01)	Acc 0.726562 (0.736208)
Epoch: [77][600/616]	Loss 2.3993e-01 (2.5097e-01)	Acc 0.750977 (0.735948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738674)
Training Loss of Epoch 77: 0.2508863661105071
Training Acc of Epoch 77: 0.7360788236788618
Testing Acc of Epoch 77: 0.7386739130434783
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.5137e-01 (2.5137e-01)	Acc 0.732422 (0.732422)
Epoch: [78][300/616]	Loss 2.6108e-01 (2.5159e-01)	Acc 0.732422 (0.736101)
Epoch: [78][600/616]	Loss 2.4089e-01 (2.5052e-01)	Acc 0.743164 (0.737277)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.737474)
Training Loss of Epoch 78: 0.2504947234460009
Training Acc of Epoch 78: 0.7372649898373984
Testing Acc of Epoch 78: 0.7374739130434783
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.4895e-01 (2.4895e-01)	Acc 0.725586 (0.725586)
Epoch: [79][300/616]	Loss 2.6556e-01 (2.5306e-01)	Acc 0.718750 (0.733723)
Epoch: [79][600/616]	Loss 2.5690e-01 (2.5255e-01)	Acc 0.726562 (0.734256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.740035)
Training Loss of Epoch 79: 0.2525716878292037
Training Acc of Epoch 79: 0.7341892149390243
Testing Acc of Epoch 79: 0.7400347826086957
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4818e-01 (2.4818e-01)	Acc 0.742188 (0.742188)
Epoch: [80][300/616]	Loss 2.4928e-01 (2.5260e-01)	Acc 0.722656 (0.734602)
Epoch: [80][600/616]	Loss 2.5636e-01 (2.5216e-01)	Acc 0.734375 (0.735171)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.742313)
Training Loss of Epoch 80: 0.2520862371940923
Training Acc of Epoch 80: 0.7352515243902439
Testing Acc of Epoch 80: 0.7423130434782609
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.5343e-01 (2.5343e-01)	Acc 0.747070 (0.747070)
Epoch: [81][300/616]	Loss 2.4864e-01 (2.5177e-01)	Acc 0.741211 (0.735384)
Epoch: [81][600/616]	Loss 2.5844e-01 (2.5235e-01)	Acc 0.726562 (0.735018)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739639)
Training Loss of Epoch 81: 0.25206867887721796
Training Acc of Epoch 81: 0.7352896341463414
Testing Acc of Epoch 81: 0.7396391304347826
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.5493e-01 (2.5493e-01)	Acc 0.732422 (0.732422)
Epoch: [82][300/616]	Loss 2.4341e-01 (2.5090e-01)	Acc 0.748047 (0.736065)
Epoch: [82][600/616]	Loss 2.3808e-01 (2.5078e-01)	Acc 0.754883 (0.736187)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740230)
Training Loss of Epoch 82: 0.25075473632754347
Training Acc of Epoch 82: 0.7362534933943089
Testing Acc of Epoch 82: 0.7402304347826087
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3734e-01 (2.3734e-01)	Acc 0.759766 (0.759766)
Epoch: [83][300/616]	Loss 2.5665e-01 (2.4979e-01)	Acc 0.732422 (0.738268)
Epoch: [83][600/616]	Loss 2.3159e-01 (2.4936e-01)	Acc 0.761719 (0.737948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.729148)
Training Loss of Epoch 83: 0.2494060626601785
Training Acc of Epoch 83: 0.7378938008130081
Testing Acc of Epoch 83: 0.7291478260869565
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.4440e-01 (2.4440e-01)	Acc 0.743164 (0.743164)
Epoch: [84][300/616]	Loss 2.4859e-01 (2.5112e-01)	Acc 0.745117 (0.735803)
Epoch: [84][600/616]	Loss 2.3985e-01 (2.5084e-01)	Acc 0.747070 (0.736466)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.738417)
Training Loss of Epoch 84: 0.25068823518307226
Training Acc of Epoch 84: 0.7366171239837398
Testing Acc of Epoch 84: 0.7384173913043478
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2149e-01 (2.2149e-01)	Acc 0.777344 (0.777344)
Epoch: [85][300/616]	Loss 2.5369e-01 (2.4816e-01)	Acc 0.720703 (0.738602)
Epoch: [85][600/616]	Loss 2.4396e-01 (2.4886e-01)	Acc 0.738281 (0.737734)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.732270)
Training Loss of Epoch 85: 0.24892710689606706
Training Acc of Epoch 85: 0.7376730818089431
Testing Acc of Epoch 85: 0.7322695652173913
Model with the best training loss saved! The loss is 0.24892710689606706
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3901e-01 (2.3901e-01)	Acc 0.754883 (0.754883)
Epoch: [86][300/616]	Loss 2.6066e-01 (2.4958e-01)	Acc 0.716797 (0.736636)
Epoch: [86][600/616]	Loss 2.5850e-01 (2.5007e-01)	Acc 0.735352 (0.736067)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.740926)
Training Loss of Epoch 86: 0.24999141935410538
Training Acc of Epoch 86: 0.7361756859756098
Testing Acc of Epoch 86: 0.7409260869565217
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.5104e-01 (2.5104e-01)	Acc 0.717773 (0.717773)
Epoch: [87][300/616]	Loss 2.3867e-01 (2.4868e-01)	Acc 0.750000 (0.737746)
Epoch: [87][600/616]	Loss 2.5914e-01 (2.4911e-01)	Acc 0.733398 (0.737423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734126)
Training Loss of Epoch 87: 0.24896396594803508
Training Acc of Epoch 87: 0.7375873348577235
Testing Acc of Epoch 87: 0.7341260869565217
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.5092e-01 (2.5092e-01)	Acc 0.730469 (0.730469)
Epoch: [88][300/616]	Loss 2.3882e-01 (2.4982e-01)	Acc 0.753906 (0.736231)
Epoch: [88][600/616]	Loss 2.4802e-01 (2.4992e-01)	Acc 0.746094 (0.736825)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742570)
Training Loss of Epoch 88: 0.249991920832696
Training Acc of Epoch 88: 0.7367806783536586
Testing Acc of Epoch 88: 0.7425695652173913
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.5545e-01 (2.5545e-01)	Acc 0.730469 (0.730469)
Epoch: [89][300/616]	Loss 2.2360e-01 (2.4973e-01)	Acc 0.770508 (0.737327)
Epoch: [89][600/616]	Loss 2.4138e-01 (2.5016e-01)	Acc 0.747070 (0.736322)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739813)
Training Loss of Epoch 89: 0.2502189632353744
Training Acc of Epoch 89: 0.7362360264227642
Testing Acc of Epoch 89: 0.7398130434782608
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3206e-01 (2.3206e-01)	Acc 0.763672 (0.763672)
Epoch: [90][300/616]	Loss 2.2635e-01 (2.4975e-01)	Acc 0.752930 (0.736630)
Epoch: [90][600/616]	Loss 2.4915e-01 (2.5005e-01)	Acc 0.739258 (0.736099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.741409)
Training Loss of Epoch 90: 0.2500316371278065
Training Acc of Epoch 90: 0.7361534552845529
Testing Acc of Epoch 90: 0.7414086956521739
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.5428e-01 (2.5428e-01)	Acc 0.736328 (0.736328)
Epoch: [91][300/616]	Loss 2.4980e-01 (2.5103e-01)	Acc 0.743164 (0.735533)
Epoch: [91][600/616]	Loss 2.4469e-01 (2.5060e-01)	Acc 0.736328 (0.736071)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.743261)
Training Loss of Epoch 91: 0.2504690029514514
Training Acc of Epoch 91: 0.7362090320121951
Testing Acc of Epoch 91: 0.7432608695652174
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.4158e-01 (2.4158e-01)	Acc 0.757812 (0.757812)
Epoch: [92][300/616]	Loss 2.3830e-01 (2.4967e-01)	Acc 0.755859 (0.736607)
Epoch: [92][600/616]	Loss 2.4000e-01 (2.5066e-01)	Acc 0.749023 (0.735638)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739274)
Training Loss of Epoch 92: 0.25060790428301183
Training Acc of Epoch 92: 0.7357390116869919
Testing Acc of Epoch 92: 0.7392739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.5211e-01 (2.5211e-01)	Acc 0.730469 (0.730469)
Epoch: [93][300/616]	Loss 2.5745e-01 (2.5004e-01)	Acc 0.728516 (0.736863)
Epoch: [93][600/616]	Loss 2.5513e-01 (2.4996e-01)	Acc 0.737305 (0.736759)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.739470)
Training Loss of Epoch 93: 0.2498288660999236
Training Acc of Epoch 93: 0.7368823043699188
Testing Acc of Epoch 93: 0.7394695652173913
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3731e-01 (2.3731e-01)	Acc 0.744141 (0.744141)
Epoch: [94][300/616]	Loss 2.4054e-01 (2.4960e-01)	Acc 0.754883 (0.736977)
Epoch: [94][600/616]	Loss 2.6563e-01 (2.4895e-01)	Acc 0.715820 (0.737373)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.735639)
Training Loss of Epoch 94: 0.24893387025449334
Training Acc of Epoch 94: 0.7373856707317074
Testing Acc of Epoch 94: 0.7356391304347826
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.4465e-01 (2.4465e-01)	Acc 0.763672 (0.763672)
Epoch: [95][300/616]	Loss 2.3940e-01 (2.4803e-01)	Acc 0.752930 (0.738430)
Epoch: [95][600/616]	Loss 2.4156e-01 (2.4938e-01)	Acc 0.744141 (0.737022)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.734161)
Training Loss of Epoch 95: 0.2494099442067185
Training Acc of Epoch 95: 0.7370442708333333
Testing Acc of Epoch 95: 0.7341608695652174
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.4409e-01 (2.4409e-01)	Acc 0.753906 (0.753906)
Epoch: [96][300/616]	Loss 2.6227e-01 (2.5055e-01)	Acc 0.716797 (0.735942)
Epoch: [96][600/616]	Loss 2.4412e-01 (2.4974e-01)	Acc 0.741211 (0.736715)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742300)
Training Loss of Epoch 96: 0.2496877797735416
Training Acc of Epoch 96: 0.7368092606707317
Testing Acc of Epoch 96: 0.7423
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2988e-01 (2.2988e-01)	Acc 0.755859 (0.755859)
Epoch: [97][300/616]	Loss 2.6099e-01 (2.5078e-01)	Acc 0.712891 (0.735536)
Epoch: [97][600/616]	Loss 2.4376e-01 (2.5031e-01)	Acc 0.747070 (0.736119)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.732704)
Training Loss of Epoch 97: 0.250352693839771
Training Acc of Epoch 97: 0.7360502413617886
Testing Acc of Epoch 97: 0.732704347826087
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4047e-01 (2.4047e-01)	Acc 0.736328 (0.736328)
Epoch: [98][300/616]	Loss 2.3763e-01 (2.5065e-01)	Acc 0.745117 (0.736312)
Epoch: [98][600/616]	Loss 2.5121e-01 (2.5037e-01)	Acc 0.729492 (0.736190)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.728061)
Training Loss of Epoch 98: 0.2503237163632866
Training Acc of Epoch 98: 0.7362519054878048
Testing Acc of Epoch 98: 0.7280608695652174
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.6851e-01 (2.6851e-01)	Acc 0.705078 (0.705078)
Epoch: [99][300/616]	Loss 2.3711e-01 (2.5108e-01)	Acc 0.754883 (0.736030)
Epoch: [99][600/616]	Loss 2.4010e-01 (2.5086e-01)	Acc 0.764648 (0.735829)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739665)
Training Loss of Epoch 99: 0.2507746915991713
Training Acc of Epoch 99: 0.7359184451219513
Testing Acc of Epoch 99: 0.7396652173913043
Early stopping not satisfied.
