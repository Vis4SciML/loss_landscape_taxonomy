train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_7b
different_width False
resnet18_width 64
weight_precision 7
bias_precision 7
act_precision 10
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_7b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_7b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9897e-01 (4.9897e-01)	Acc 0.291992 (0.291992)
Epoch: [0][300/616]	Loss 2.5464e-01 (2.8530e-01)	Acc 0.728516 (0.695763)
Epoch: [0][600/616]	Loss 2.5681e-01 (2.6929e-01)	Acc 0.736328 (0.716032)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.738139)
Training Loss of Epoch 0: 0.2688887302226167
Training Acc of Epoch 0: 0.7165126397357724
Testing Acc of Epoch 0: 0.7381391304347826
Model with the best training loss saved! The loss is 0.2688887302226167
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4090e-01 (2.4090e-01)	Acc 0.754883 (0.754883)
Epoch: [1][300/616]	Loss 2.5047e-01 (2.5254e-01)	Acc 0.731445 (0.737120)
Epoch: [1][600/616]	Loss 2.6890e-01 (2.5128e-01)	Acc 0.716797 (0.738005)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.738661)
Training Loss of Epoch 1: 0.25133077623398326
Training Acc of Epoch 1: 0.7379668445121951
Testing Acc of Epoch 1: 0.7386608695652174
Model with the best training loss saved! The loss is 0.25133077623398326
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5627e-01 (2.5627e-01)	Acc 0.732422 (0.732422)
Epoch: [2][300/616]	Loss 2.4743e-01 (2.4951e-01)	Acc 0.734375 (0.739047)
Epoch: [2][600/616]	Loss 2.4297e-01 (2.4910e-01)	Acc 0.734375 (0.739454)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745983)
Training Loss of Epoch 2: 0.24894008316644808
Training Acc of Epoch 2: 0.7396944867886179
Testing Acc of Epoch 2: 0.7459826086956521
Model with the best training loss saved! The loss is 0.24894008316644808
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3913e-01 (2.3913e-01)	Acc 0.748047 (0.748047)
Epoch: [3][300/616]	Loss 2.4692e-01 (2.5104e-01)	Acc 0.744141 (0.738297)
Epoch: [3][600/616]	Loss 2.6111e-01 (2.4995e-01)	Acc 0.731445 (0.738931)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.722352)
Training Loss of Epoch 3: 0.2502830189175722
Training Acc of Epoch 3: 0.7385559578252032
Testing Acc of Epoch 3: 0.7223521739130435
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.7389e-01 (2.7389e-01)	Acc 0.708984 (0.708984)
Epoch: [4][300/616]	Loss 2.6639e-01 (2.4984e-01)	Acc 0.716797 (0.738625)
Epoch: [4][600/616]	Loss 2.5614e-01 (2.5044e-01)	Acc 0.722656 (0.738629)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.729239)
Training Loss of Epoch 4: 0.25049719056947445
Training Acc of Epoch 4: 0.7386321773373984
Testing Acc of Epoch 4: 0.7292391304347826
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.6438e-01 (2.6438e-01)	Acc 0.733398 (0.733398)
Epoch: [5][300/616]	Loss 2.5118e-01 (2.4838e-01)	Acc 0.730469 (0.740491)
Epoch: [5][600/616]	Loss 2.6499e-01 (2.4866e-01)	Acc 0.719727 (0.740239)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.743330)
Training Loss of Epoch 5: 0.24856979485934344
Training Acc of Epoch 5: 0.7404185721544716
Testing Acc of Epoch 5: 0.7433304347826087
Model with the best training loss saved! The loss is 0.24856979485934344
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4565e-01 (2.4565e-01)	Acc 0.743164 (0.743164)
Epoch: [6][300/616]	Loss 2.3505e-01 (2.4845e-01)	Acc 0.753906 (0.740400)
Epoch: [6][600/616]	Loss 2.6244e-01 (2.4997e-01)	Acc 0.727539 (0.739250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742048)
Training Loss of Epoch 6: 0.24992212158877675
Training Acc of Epoch 6: 0.7392641641260163
Testing Acc of Epoch 6: 0.7420478260869565
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.3303e-01 (2.3303e-01)	Acc 0.771484 (0.771484)
Epoch: [7][300/616]	Loss 2.6149e-01 (2.4990e-01)	Acc 0.732422 (0.738833)
Epoch: [7][600/616]	Loss 2.5965e-01 (2.4927e-01)	Acc 0.726562 (0.739875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.742861)
Training Loss of Epoch 7: 0.24915025595242415
Training Acc of Epoch 7: 0.7400454141260162
Testing Acc of Epoch 7: 0.7428608695652174
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4105e-01 (2.4105e-01)	Acc 0.739258 (0.739258)
Epoch: [8][300/616]	Loss 2.5822e-01 (2.5118e-01)	Acc 0.740234 (0.737512)
Epoch: [8][600/616]	Loss 2.5897e-01 (2.4994e-01)	Acc 0.722656 (0.739110)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.741848)
Training Loss of Epoch 8: 0.2499377985068453
Training Acc of Epoch 8: 0.7390561483739837
Testing Acc of Epoch 8: 0.7418478260869565
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4451e-01 (2.4451e-01)	Acc 0.746094 (0.746094)
Epoch: [9][300/616]	Loss 2.4486e-01 (2.5091e-01)	Acc 0.750000 (0.738437)
Epoch: [9][600/616]	Loss 2.5584e-01 (2.4969e-01)	Acc 0.728516 (0.739597)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.747370)
Training Loss of Epoch 9: 0.24962741647793996
Training Acc of Epoch 9: 0.7396928988821139
Testing Acc of Epoch 9: 0.7473695652173913
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3529e-01 (2.3529e-01)	Acc 0.756836 (0.756836)
Epoch: [10][300/616]	Loss 2.5911e-01 (2.5001e-01)	Acc 0.722656 (0.739112)
Epoch: [10][600/616]	Loss 2.4564e-01 (2.4965e-01)	Acc 0.746094 (0.739443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.744061)
Training Loss of Epoch 10: 0.24966486132726437
Training Acc of Epoch 10: 0.7394356580284552
Testing Acc of Epoch 10: 0.7440608695652174
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3721e-01 (2.3721e-01)	Acc 0.750000 (0.750000)
Epoch: [11][300/616]	Loss 2.4859e-01 (2.5057e-01)	Acc 0.734375 (0.738625)
Epoch: [11][600/616]	Loss 2.3903e-01 (2.5034e-01)	Acc 0.752930 (0.738739)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.735422)
Training Loss of Epoch 11: 0.2504517593034884
Training Acc of Epoch 11: 0.7386893419715447
Testing Acc of Epoch 11: 0.7354217391304347
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.5294e-01 (2.5294e-01)	Acc 0.736328 (0.736328)
Epoch: [12][300/616]	Loss 2.5259e-01 (2.4780e-01)	Acc 0.733398 (0.742090)
Epoch: [12][600/616]	Loss 2.4915e-01 (2.4925e-01)	Acc 0.735352 (0.740408)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.741309)
Training Loss of Epoch 12: 0.24939481107200065
Training Acc of Epoch 12: 0.7402566056910569
Testing Acc of Epoch 12: 0.7413086956521739
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.5331e-01 (2.5331e-01)	Acc 0.731445 (0.731445)
Epoch: [13][300/616]	Loss 2.5326e-01 (2.4874e-01)	Acc 0.729492 (0.740072)
Epoch: [13][600/616]	Loss 2.6138e-01 (2.4898e-01)	Acc 0.730469 (0.739976)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.731622)
Training Loss of Epoch 13: 0.24918152719009212
Training Acc of Epoch 13: 0.7398040523373983
Testing Acc of Epoch 13: 0.7316217391304348
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.5862e-01 (2.5862e-01)	Acc 0.729492 (0.729492)
Epoch: [14][300/616]	Loss 2.4538e-01 (2.5129e-01)	Acc 0.742188 (0.737535)
Epoch: [14][600/616]	Loss 2.5440e-01 (2.5032e-01)	Acc 0.737305 (0.738775)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.743804)
Training Loss of Epoch 14: 0.25031507606428816
Training Acc of Epoch 14: 0.7388370172764228
Testing Acc of Epoch 14: 0.743804347826087
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.5090e-01 (2.5090e-01)	Acc 0.745117 (0.745117)
Epoch: [15][300/616]	Loss 2.3459e-01 (2.5095e-01)	Acc 0.750977 (0.739355)
Epoch: [15][600/616]	Loss 2.4071e-01 (2.5011e-01)	Acc 0.746094 (0.739264)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.741087)
Training Loss of Epoch 15: 0.25012674886521286
Training Acc of Epoch 15: 0.7392197027439025
Testing Acc of Epoch 15: 0.7410869565217392
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3979e-01 (2.3979e-01)	Acc 0.756836 (0.756836)
Epoch: [16][300/616]	Loss 2.3832e-01 (2.5014e-01)	Acc 0.752930 (0.738606)
Epoch: [16][600/616]	Loss 2.3105e-01 (2.4986e-01)	Acc 0.766602 (0.739547)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.743104)
Training Loss of Epoch 16: 0.25000211935702377
Training Acc of Epoch 16: 0.7395039380081301
Testing Acc of Epoch 16: 0.7431043478260869
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.6007e-01 (2.6007e-01)	Acc 0.736328 (0.736328)
Epoch: [17][300/616]	Loss 2.4478e-01 (2.5057e-01)	Acc 0.756836 (0.739096)
Epoch: [17][600/616]	Loss 2.7608e-01 (2.5082e-01)	Acc 0.711914 (0.738343)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.734826)
Training Loss of Epoch 17: 0.2507672159410105
Training Acc of Epoch 17: 0.7384082825203252
Testing Acc of Epoch 17: 0.7348260869565217
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4876e-01 (2.4876e-01)	Acc 0.733398 (0.733398)
Epoch: [18][300/616]	Loss 2.2578e-01 (2.4968e-01)	Acc 0.755859 (0.739582)
Epoch: [18][600/616]	Loss 2.4205e-01 (2.5038e-01)	Acc 0.745117 (0.739084)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.744243)
Training Loss of Epoch 18: 0.2503084589310778
Training Acc of Epoch 18: 0.7391720655487805
Testing Acc of Epoch 18: 0.7442434782608696
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4054e-01 (2.4054e-01)	Acc 0.763672 (0.763672)
Epoch: [19][300/616]	Loss 2.6053e-01 (2.5178e-01)	Acc 0.724609 (0.737827)
Epoch: [19][600/616]	Loss 2.6080e-01 (2.5040e-01)	Acc 0.737305 (0.739125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.746074)
Training Loss of Epoch 19: 0.250459742933754
Training Acc of Epoch 19: 0.738989456300813
Testing Acc of Epoch 19: 0.7460739130434783
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4313e-01 (2.4313e-01)	Acc 0.743164 (0.743164)
Epoch: [20][300/616]	Loss 2.4302e-01 (2.5111e-01)	Acc 0.731445 (0.738609)
Epoch: [20][600/616]	Loss 2.5467e-01 (2.5017e-01)	Acc 0.734375 (0.739295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744452)
Training Loss of Epoch 20: 0.25012705764634824
Training Acc of Epoch 20: 0.7393229166666667
Testing Acc of Epoch 20: 0.7444521739130435
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4339e-01 (2.4339e-01)	Acc 0.750000 (0.750000)
Epoch: [21][300/616]	Loss 2.3794e-01 (2.5006e-01)	Acc 0.767578 (0.739508)
Epoch: [21][600/616]	Loss 2.6869e-01 (2.5054e-01)	Acc 0.728516 (0.738423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.738887)
Training Loss of Epoch 21: 0.2504586990771255
Training Acc of Epoch 21: 0.7384432164634146
Testing Acc of Epoch 21: 0.7388869565217391
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.5647e-01 (2.5647e-01)	Acc 0.729492 (0.729492)
Epoch: [22][300/616]	Loss 2.6410e-01 (2.5385e-01)	Acc 0.726562 (0.735524)
Epoch: [22][600/616]	Loss 2.4082e-01 (2.5182e-01)	Acc 0.755859 (0.737209)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.737865)
Training Loss of Epoch 22: 0.2518002606988922
Training Acc of Epoch 22: 0.7372475228658537
Testing Acc of Epoch 22: 0.7378652173913044
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3610e-01 (2.3610e-01)	Acc 0.748047 (0.748047)
Epoch: [23][300/616]	Loss 2.5268e-01 (2.4986e-01)	Acc 0.734375 (0.739329)
Epoch: [23][600/616]	Loss 2.4895e-01 (2.5034e-01)	Acc 0.745117 (0.738596)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.743626)
Training Loss of Epoch 23: 0.250309307255396
Training Acc of Epoch 23: 0.7386417047764228
Testing Acc of Epoch 23: 0.7436260869565218
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.4715e-01 (2.4715e-01)	Acc 0.728516 (0.728516)
Epoch: [24][300/616]	Loss 2.8051e-01 (2.5235e-01)	Acc 0.700195 (0.736915)
Epoch: [24][600/616]	Loss 2.6126e-01 (2.5170e-01)	Acc 0.726562 (0.737313)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.740726)
Training Loss of Epoch 24: 0.25167990818740876
Training Acc of Epoch 24: 0.7372903963414634
Testing Acc of Epoch 24: 0.7407260869565218
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.6017e-01 (2.6017e-01)	Acc 0.716797 (0.716797)
Epoch: [25][300/616]	Loss 2.4975e-01 (2.5207e-01)	Acc 0.741211 (0.736737)
Epoch: [25][600/616]	Loss 2.6000e-01 (2.5115e-01)	Acc 0.713867 (0.738051)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.735287)
Training Loss of Epoch 25: 0.2509508215314974
Training Acc of Epoch 25: 0.738158981199187
Testing Acc of Epoch 25: 0.7352869565217391
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4747e-01 (2.4747e-01)	Acc 0.742188 (0.742188)
Epoch: [26][300/616]	Loss 2.6396e-01 (2.5324e-01)	Acc 0.733398 (0.736133)
Epoch: [26][600/616]	Loss 2.4694e-01 (2.5313e-01)	Acc 0.741211 (0.735966)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.735148)
Training Loss of Epoch 26: 0.25300058787915763
Training Acc of Epoch 26: 0.7360534171747968
Testing Acc of Epoch 26: 0.7351478260869565
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.5425e-01 (2.5425e-01)	Acc 0.746094 (0.746094)
Epoch: [27][300/616]	Loss 2.3925e-01 (2.5361e-01)	Acc 0.745117 (0.735767)
Epoch: [27][600/616]	Loss 2.6109e-01 (2.5333e-01)	Acc 0.718750 (0.735946)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.726152)
Training Loss of Epoch 27: 0.25325591183774837
Training Acc of Epoch 27: 0.7358866869918699
Testing Acc of Epoch 27: 0.7261521739130434
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.6425e-01 (2.6425e-01)	Acc 0.742188 (0.742188)
Epoch: [28][300/616]	Loss 2.3474e-01 (2.4888e-01)	Acc 0.759766 (0.740309)
Epoch: [28][600/616]	Loss 2.5036e-01 (2.5109e-01)	Acc 0.737305 (0.737973)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.734639)
Training Loss of Epoch 28: 0.25125861121871607
Training Acc of Epoch 28: 0.7377747078252033
Testing Acc of Epoch 28: 0.7346391304347826
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4897e-01 (2.4897e-01)	Acc 0.748047 (0.748047)
Epoch: [29][300/616]	Loss 2.3800e-01 (2.4922e-01)	Acc 0.760742 (0.740234)
Epoch: [29][600/616]	Loss 2.5160e-01 (2.5063e-01)	Acc 0.732422 (0.738689)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.726530)
Training Loss of Epoch 29: 0.25073794218582834
Training Acc of Epoch 29: 0.7385464303861788
Testing Acc of Epoch 29: 0.7265304347826087
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4613e-01 (2.4613e-01)	Acc 0.737305 (0.737305)
Epoch: [30][300/616]	Loss 2.8204e-01 (2.5203e-01)	Acc 0.711914 (0.736581)
Epoch: [30][600/616]	Loss 2.4099e-01 (2.5075e-01)	Acc 0.734375 (0.737810)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739517)
Training Loss of Epoch 30: 0.2506784543273895
Training Acc of Epoch 30: 0.7378874491869919
Testing Acc of Epoch 30: 0.7395173913043478
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.5622e-01 (2.5622e-01)	Acc 0.728516 (0.728516)
Epoch: [31][300/616]	Loss 2.5648e-01 (2.5078e-01)	Acc 0.734375 (0.738482)
Epoch: [31][600/616]	Loss 2.4430e-01 (2.5046e-01)	Acc 0.750977 (0.738645)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.736157)
Training Loss of Epoch 31: 0.25055135246214827
Training Acc of Epoch 31: 0.738500381097561
Testing Acc of Epoch 31: 0.7361565217391305
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4866e-01 (2.4866e-01)	Acc 0.733398 (0.733398)
Epoch: [32][300/616]	Loss 2.5401e-01 (2.5095e-01)	Acc 0.738281 (0.738538)
Epoch: [32][600/616]	Loss 2.5551e-01 (2.5105e-01)	Acc 0.729492 (0.738010)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.735796)
Training Loss of Epoch 32: 0.2509765331095796
Training Acc of Epoch 32: 0.7379541412601626
Testing Acc of Epoch 32: 0.7357956521739131
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4139e-01 (2.4139e-01)	Acc 0.742188 (0.742188)
Epoch: [33][300/616]	Loss 2.5881e-01 (2.5043e-01)	Acc 0.720703 (0.738317)
Epoch: [33][600/616]	Loss 2.3510e-01 (2.5123e-01)	Acc 0.758789 (0.737704)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.721422)
Training Loss of Epoch 33: 0.2513155039975314
Training Acc of Epoch 33: 0.737620680894309
Testing Acc of Epoch 33: 0.7214217391304348
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.5965e-01 (2.5965e-01)	Acc 0.734375 (0.734375)
Epoch: [34][300/616]	Loss 2.4309e-01 (2.5031e-01)	Acc 0.744141 (0.738437)
Epoch: [34][600/616]	Loss 2.4934e-01 (2.5080e-01)	Acc 0.744141 (0.738015)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743243)
Training Loss of Epoch 34: 0.25080319526718886
Training Acc of Epoch 34: 0.7380494156504065
Testing Acc of Epoch 34: 0.7432434782608696
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2967e-01 (2.2967e-01)	Acc 0.773438 (0.773438)
Epoch: [35][300/616]	Loss 2.6649e-01 (2.4862e-01)	Acc 0.718750 (0.740526)
Epoch: [35][600/616]	Loss 2.5460e-01 (2.4934e-01)	Acc 0.733398 (0.739492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740448)
Training Loss of Epoch 35: 0.24929327443847812
Training Acc of Epoch 35: 0.7395499872967479
Testing Acc of Epoch 35: 0.7404478260869565
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.4005e-01 (2.4005e-01)	Acc 0.751953 (0.751953)
Epoch: [36][300/616]	Loss 2.5738e-01 (2.4948e-01)	Acc 0.710938 (0.739725)
Epoch: [36][600/616]	Loss 2.2938e-01 (2.4992e-01)	Acc 0.773438 (0.738777)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.743678)
Training Loss of Epoch 36: 0.24968013043810683
Training Acc of Epoch 36: 0.7390339176829268
Testing Acc of Epoch 36: 0.7436782608695652
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4097e-01 (2.4097e-01)	Acc 0.759766 (0.759766)
Epoch: [37][300/616]	Loss 2.5108e-01 (2.4922e-01)	Acc 0.737305 (0.739349)
Epoch: [37][600/616]	Loss 2.5552e-01 (2.5018e-01)	Acc 0.726562 (0.738806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.741691)
Training Loss of Epoch 37: 0.250238463137208
Training Acc of Epoch 37: 0.7387480945121951
Testing Acc of Epoch 37: 0.741691304347826
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.4774e-01 (2.4774e-01)	Acc 0.732422 (0.732422)
Epoch: [38][300/616]	Loss 2.3671e-01 (2.4890e-01)	Acc 0.755859 (0.740218)
Epoch: [38][600/616]	Loss 2.4202e-01 (2.4994e-01)	Acc 0.738281 (0.739133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745970)
Training Loss of Epoch 38: 0.24989870248771295
Training Acc of Epoch 38: 0.7391657139227642
Testing Acc of Epoch 38: 0.7459695652173913
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.4120e-01 (2.4120e-01)	Acc 0.759766 (0.759766)
Epoch: [39][300/616]	Loss 2.5082e-01 (2.5278e-01)	Acc 0.748047 (0.735731)
Epoch: [39][600/616]	Loss 2.5142e-01 (2.5190e-01)	Acc 0.728516 (0.737080)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.738970)
Training Loss of Epoch 39: 0.2518723320427949
Training Acc of Epoch 39: 0.7371061991869918
Testing Acc of Epoch 39: 0.7389695652173913
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4284e-01 (2.4284e-01)	Acc 0.739258 (0.739258)
Epoch: [40][300/616]	Loss 2.5364e-01 (2.4954e-01)	Acc 0.734375 (0.739300)
Epoch: [40][600/616]	Loss 2.5652e-01 (2.5011e-01)	Acc 0.748047 (0.739029)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740817)
Training Loss of Epoch 40: 0.250161978311655
Training Acc of Epoch 40: 0.7389704014227643
Testing Acc of Epoch 40: 0.7408173913043479
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.5819e-01 (2.5819e-01)	Acc 0.750000 (0.750000)
Epoch: [41][300/616]	Loss 2.4278e-01 (2.5089e-01)	Acc 0.751953 (0.737960)
Epoch: [41][600/616]	Loss 2.4211e-01 (2.4975e-01)	Acc 0.737305 (0.739580)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746122)
Training Loss of Epoch 41: 0.2495920799854325
Training Acc of Epoch 41: 0.7398612169715447
Testing Acc of Epoch 41: 0.7461217391304348
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3644e-01 (2.3644e-01)	Acc 0.754883 (0.754883)
Epoch: [42][300/616]	Loss 2.5352e-01 (2.4904e-01)	Acc 0.740234 (0.739884)
Epoch: [42][600/616]	Loss 2.3835e-01 (2.4978e-01)	Acc 0.764648 (0.739744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.713178)
Training Loss of Epoch 42: 0.24981294000536447
Training Acc of Epoch 42: 0.7397294207317073
Testing Acc of Epoch 42: 0.7131782608695653
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.7264e-01 (2.7264e-01)	Acc 0.700195 (0.700195)
Epoch: [43][300/616]	Loss 2.4306e-01 (2.4993e-01)	Acc 0.753906 (0.738654)
Epoch: [43][600/616]	Loss 2.4357e-01 (2.5000e-01)	Acc 0.747070 (0.739024)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.740004)
Training Loss of Epoch 43: 0.24991428886002642
Training Acc of Epoch 43: 0.7391673018292683
Testing Acc of Epoch 43: 0.7400043478260869
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.5048e-01 (2.5048e-01)	Acc 0.739258 (0.739258)
Epoch: [44][300/616]	Loss 2.4295e-01 (2.5071e-01)	Acc 0.752930 (0.738664)
Epoch: [44][600/616]	Loss 2.5161e-01 (2.5034e-01)	Acc 0.726562 (0.738725)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.743000)
Training Loss of Epoch 44: 0.25030377982108576
Training Acc of Epoch 44: 0.7387226880081301
Testing Acc of Epoch 44: 0.743
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3888e-01 (2.3888e-01)	Acc 0.761719 (0.761719)
Epoch: [45][300/616]	Loss 2.4171e-01 (2.5039e-01)	Acc 0.753906 (0.738800)
Epoch: [45][600/616]	Loss 2.4889e-01 (2.5045e-01)	Acc 0.736328 (0.738904)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742626)
Training Loss of Epoch 45: 0.2504621571399332
Training Acc of Epoch 45: 0.7388433689024391
Testing Acc of Epoch 45: 0.7426260869565218
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.6046e-01 (2.6046e-01)	Acc 0.716797 (0.716797)
Epoch: [46][300/616]	Loss 2.6904e-01 (2.5157e-01)	Acc 0.726562 (0.737113)
Epoch: [46][600/616]	Loss 2.4374e-01 (2.5029e-01)	Acc 0.750000 (0.738967)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740022)
Training Loss of Epoch 46: 0.25040293730855956
Training Acc of Epoch 46: 0.7388703633130081
Testing Acc of Epoch 46: 0.7400217391304348
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.6395e-01 (2.6395e-01)	Acc 0.726562 (0.726562)
Epoch: [47][300/616]	Loss 2.3519e-01 (2.5155e-01)	Acc 0.765625 (0.737055)
Epoch: [47][600/616]	Loss 2.5755e-01 (2.5110e-01)	Acc 0.733398 (0.737761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744870)
Training Loss of Epoch 47: 0.25114232883220766
Training Acc of Epoch 47: 0.7377000762195122
Testing Acc of Epoch 47: 0.7448695652173913
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.3969e-01 (2.3969e-01)	Acc 0.753906 (0.753906)
Epoch: [48][300/616]	Loss 2.5387e-01 (2.4878e-01)	Acc 0.737305 (0.740760)
Epoch: [48][600/616]	Loss 2.3135e-01 (2.4986e-01)	Acc 0.760742 (0.739245)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.743317)
Training Loss of Epoch 48: 0.24994669711686732
Training Acc of Epoch 48: 0.7391879446138211
Testing Acc of Epoch 48: 0.7433173913043478
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4416e-01 (2.4416e-01)	Acc 0.745117 (0.745117)
Epoch: [49][300/616]	Loss 2.6345e-01 (2.5231e-01)	Acc 0.723633 (0.736708)
Epoch: [49][600/616]	Loss 2.5011e-01 (2.5437e-01)	Acc 0.734375 (0.734747)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.742900)
Training Loss of Epoch 49: 0.25435361452703553
Training Acc of Epoch 49: 0.7346989329268293
Testing Acc of Epoch 49: 0.7429
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.5903e-01 (2.5903e-01)	Acc 0.723633 (0.723633)
Epoch: [50][300/616]	Loss 2.4816e-01 (2.5226e-01)	Acc 0.746094 (0.736704)
Epoch: [50][600/616]	Loss 2.5560e-01 (2.5173e-01)	Acc 0.734375 (0.737115)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.739278)
Training Loss of Epoch 50: 0.25176622169773755
Training Acc of Epoch 50: 0.7370220401422765
Testing Acc of Epoch 50: 0.7392782608695652
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.5988e-01 (2.5988e-01)	Acc 0.726562 (0.726562)
Epoch: [51][300/616]	Loss 2.1912e-01 (2.5091e-01)	Acc 0.778320 (0.737843)
Epoch: [51][600/616]	Loss 2.6301e-01 (2.5105e-01)	Acc 0.712891 (0.737524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.742148)
Training Loss of Epoch 51: 0.2511000133384534
Training Acc of Epoch 51: 0.7375714557926829
Testing Acc of Epoch 51: 0.7421478260869565
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4394e-01 (2.4394e-01)	Acc 0.753906 (0.753906)
Epoch: [52][300/616]	Loss 2.4433e-01 (2.5328e-01)	Acc 0.738281 (0.736497)
Epoch: [52][600/616]	Loss 2.5813e-01 (2.5570e-01)	Acc 0.728516 (0.732718)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.743183)
Training Loss of Epoch 52: 0.2555589724120086
Training Acc of Epoch 52: 0.7328871316056911
Testing Acc of Epoch 52: 0.7431826086956522
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.4537e-01 (2.4537e-01)	Acc 0.736328 (0.736328)
Epoch: [53][300/616]	Loss 2.5606e-01 (2.4932e-01)	Acc 0.721680 (0.739274)
Epoch: [53][600/616]	Loss 2.3843e-01 (2.5005e-01)	Acc 0.748047 (0.738687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.728257)
Training Loss of Epoch 53: 0.2500538883655052
Training Acc of Epoch 53: 0.7386782266260162
Testing Acc of Epoch 53: 0.7282565217391305
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.6169e-01 (2.6169e-01)	Acc 0.727539 (0.727539)
Epoch: [54][300/616]	Loss 2.7188e-01 (2.5304e-01)	Acc 0.714844 (0.735689)
Epoch: [54][600/616]	Loss 2.5389e-01 (2.5247e-01)	Acc 0.736328 (0.736426)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.738213)
Training Loss of Epoch 54: 0.25255675790755727
Training Acc of Epoch 54: 0.7363821138211382
Testing Acc of Epoch 54: 0.7382130434782609
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.5593e-01 (2.5593e-01)	Acc 0.732422 (0.732422)
Epoch: [55][300/616]	Loss 2.4614e-01 (2.5066e-01)	Acc 0.747070 (0.738609)
Epoch: [55][600/616]	Loss 2.4287e-01 (2.5008e-01)	Acc 0.751953 (0.738952)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745370)
Training Loss of Epoch 55: 0.2499982676128062
Training Acc of Epoch 55: 0.7390323297764227
Testing Acc of Epoch 55: 0.7453695652173913
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.3951e-01 (2.3951e-01)	Acc 0.761719 (0.761719)
Epoch: [56][300/616]	Loss 2.5348e-01 (2.5205e-01)	Acc 0.734375 (0.737434)
Epoch: [56][600/616]	Loss 2.6220e-01 (2.5099e-01)	Acc 0.728516 (0.737911)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739817)
Training Loss of Epoch 56: 0.2509185549689502
Training Acc of Epoch 56: 0.7379811356707318
Testing Acc of Epoch 56: 0.7398173913043479
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3971e-01 (2.3971e-01)	Acc 0.743164 (0.743164)
Epoch: [57][300/616]	Loss 2.5526e-01 (2.5037e-01)	Acc 0.736328 (0.738576)
Epoch: [57][600/616]	Loss 2.5790e-01 (2.5150e-01)	Acc 0.735352 (0.737409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.743635)
Training Loss of Epoch 57: 0.25136463957104255
Training Acc of Epoch 57: 0.7376159171747968
Testing Acc of Epoch 57: 0.7436347826086956
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3108e-01 (2.3108e-01)	Acc 0.745117 (0.745117)
Epoch: [58][300/616]	Loss 2.5160e-01 (2.5162e-01)	Acc 0.725586 (0.737470)
Epoch: [58][600/616]	Loss 2.4318e-01 (2.5135e-01)	Acc 0.744141 (0.738231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.743035)
Training Loss of Epoch 58: 0.25120944090005826
Training Acc of Epoch 58: 0.7383622332317074
Testing Acc of Epoch 58: 0.7430347826086956
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.5147e-01 (2.5147e-01)	Acc 0.732422 (0.732422)
Epoch: [59][300/616]	Loss 2.5937e-01 (2.4991e-01)	Acc 0.734375 (0.739430)
Epoch: [59][600/616]	Loss 2.6035e-01 (2.5115e-01)	Acc 0.744141 (0.737665)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.731135)
Training Loss of Epoch 59: 0.2511755263902308
Training Acc of Epoch 59: 0.7377318343495934
Testing Acc of Epoch 59: 0.7311347826086957
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.6480e-01 (2.6480e-01)	Acc 0.713867 (0.713867)
Epoch: [60][300/616]	Loss 2.5069e-01 (2.5144e-01)	Acc 0.731445 (0.737087)
Epoch: [60][600/616]	Loss 2.5282e-01 (2.5150e-01)	Acc 0.734375 (0.737435)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.734548)
Training Loss of Epoch 60: 0.25152197228214607
Training Acc of Epoch 60: 0.7373666158536586
Testing Acc of Epoch 60: 0.7345478260869566
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.5322e-01 (2.5322e-01)	Acc 0.752930 (0.752930)
Epoch: [61][300/616]	Loss 2.4893e-01 (2.4974e-01)	Acc 0.734375 (0.739800)
Epoch: [61][600/616]	Loss 2.6650e-01 (2.5048e-01)	Acc 0.721680 (0.738699)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736896)
Training Loss of Epoch 61: 0.2507146271505976
Training Acc of Epoch 61: 0.7383908155487805
Testing Acc of Epoch 61: 0.7368956521739131
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.6020e-01 (2.6020e-01)	Acc 0.736328 (0.736328)
Epoch: [62][300/616]	Loss 2.4700e-01 (2.5100e-01)	Acc 0.741211 (0.738236)
Epoch: [62][600/616]	Loss 2.4731e-01 (2.5229e-01)	Acc 0.746094 (0.737321)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743235)
Training Loss of Epoch 62: 0.2522722942315466
Training Acc of Epoch 62: 0.7372951600609756
Testing Acc of Epoch 62: 0.7432347826086957
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4582e-01 (2.4582e-01)	Acc 0.770508 (0.770508)
Epoch: [63][300/616]	Loss 2.5618e-01 (2.5077e-01)	Acc 0.732422 (0.738142)
Epoch: [63][600/616]	Loss 2.4344e-01 (2.5024e-01)	Acc 0.751953 (0.738882)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.743509)
Training Loss of Epoch 63: 0.25028665669080685
Training Acc of Epoch 63: 0.7387814405487805
Testing Acc of Epoch 63: 0.7435086956521739
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3988e-01 (2.3988e-01)	Acc 0.749023 (0.749023)
Epoch: [64][300/616]	Loss 2.4538e-01 (2.5024e-01)	Acc 0.744141 (0.739281)
Epoch: [64][600/616]	Loss 2.4135e-01 (2.5038e-01)	Acc 0.753906 (0.738852)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.736909)
Training Loss of Epoch 64: 0.25036845173293015
Training Acc of Epoch 64: 0.7388735391260163
Testing Acc of Epoch 64: 0.7369086956521739
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.6410e-01 (2.6410e-01)	Acc 0.744141 (0.744141)
Epoch: [65][300/616]	Loss 2.6030e-01 (2.5097e-01)	Acc 0.722656 (0.738424)
Epoch: [65][600/616]	Loss 2.4962e-01 (2.5048e-01)	Acc 0.731445 (0.738811)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.732135)
Training Loss of Epoch 65: 0.25050695921347393
Training Acc of Epoch 65: 0.7387560340447155
Testing Acc of Epoch 65: 0.7321347826086957
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.6086e-01 (2.6086e-01)	Acc 0.731445 (0.731445)
Epoch: [66][300/616]	Loss 2.4031e-01 (2.5107e-01)	Acc 0.758789 (0.738106)
Epoch: [66][600/616]	Loss 2.4684e-01 (2.5063e-01)	Acc 0.736328 (0.738150)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740926)
Training Loss of Epoch 66: 0.25062169713702626
Training Acc of Epoch 66: 0.738232024898374
Testing Acc of Epoch 66: 0.7409260869565217
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.4561e-01 (2.4561e-01)	Acc 0.747070 (0.747070)
Epoch: [67][300/616]	Loss 2.5271e-01 (2.5134e-01)	Acc 0.735352 (0.738038)
Epoch: [67][600/616]	Loss 2.3531e-01 (2.5073e-01)	Acc 0.761719 (0.738252)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.728661)
Training Loss of Epoch 67: 0.25071425420966575
Training Acc of Epoch 67: 0.7382431402439025
Testing Acc of Epoch 67: 0.7286608695652174
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.5778e-01 (2.5778e-01)	Acc 0.736328 (0.736328)
Epoch: [68][300/616]	Loss 2.6376e-01 (2.5015e-01)	Acc 0.726562 (0.738401)
Epoch: [68][600/616]	Loss 2.5113e-01 (2.5001e-01)	Acc 0.737305 (0.738718)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.744478)
Training Loss of Epoch 68: 0.24986209639204227
Training Acc of Epoch 68: 0.738891006097561
Testing Acc of Epoch 68: 0.7444782608695653
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5796e-01 (2.5796e-01)	Acc 0.726562 (0.726562)
Epoch: [69][300/616]	Loss 2.4006e-01 (2.5102e-01)	Acc 0.770508 (0.738492)
Epoch: [69][600/616]	Loss 2.4791e-01 (2.5086e-01)	Acc 0.745117 (0.738249)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739048)
Training Loss of Epoch 69: 0.2509483659170507
Training Acc of Epoch 69: 0.7380811737804878
Testing Acc of Epoch 69: 0.7390478260869565
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.5171e-01 (2.5171e-01)	Acc 0.730469 (0.730469)
Epoch: [70][300/616]	Loss 2.5013e-01 (2.5158e-01)	Acc 0.753906 (0.737373)
Epoch: [70][600/616]	Loss 2.7423e-01 (2.5077e-01)	Acc 0.723633 (0.738252)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.741813)
Training Loss of Epoch 70: 0.25096084271989216
Training Acc of Epoch 70: 0.7381224593495935
Testing Acc of Epoch 70: 0.7418130434782608
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4535e-01 (2.4535e-01)	Acc 0.750977 (0.750977)
Epoch: [71][300/616]	Loss 2.5911e-01 (2.5011e-01)	Acc 0.728516 (0.739443)
Epoch: [71][600/616]	Loss 2.7010e-01 (2.5198e-01)	Acc 0.698242 (0.737170)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.731878)
Training Loss of Epoch 71: 0.2521599332976147
Training Acc of Epoch 71: 0.7369616996951219
Testing Acc of Epoch 71: 0.7318782608695652
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.5307e-01 (2.5307e-01)	Acc 0.729492 (0.729492)
Epoch: [72][300/616]	Loss 2.6353e-01 (2.5255e-01)	Acc 0.729492 (0.736464)
Epoch: [72][600/616]	Loss 2.6027e-01 (2.5211e-01)	Acc 0.724609 (0.737056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.743026)
Training Loss of Epoch 72: 0.25206266803954674
Training Acc of Epoch 72: 0.7371347815040651
Testing Acc of Epoch 72: 0.7430260869565217
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4144e-01 (2.4144e-01)	Acc 0.745117 (0.745117)
Epoch: [73][300/616]	Loss 2.7236e-01 (2.5237e-01)	Acc 0.712891 (0.736078)
Epoch: [73][600/616]	Loss 2.4940e-01 (2.5221e-01)	Acc 0.738281 (0.736713)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.734943)
Training Loss of Epoch 73: 0.25231067390461276
Training Acc of Epoch 73: 0.7365345528455285
Testing Acc of Epoch 73: 0.7349434782608696
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4631e-01 (2.4631e-01)	Acc 0.754883 (0.754883)
Epoch: [74][300/616]	Loss 2.4527e-01 (2.5297e-01)	Acc 0.739258 (0.736607)
Epoch: [74][600/616]	Loss 2.4390e-01 (2.5379e-01)	Acc 0.743164 (0.735317)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.733496)
Training Loss of Epoch 74: 0.2538477170758131
Training Acc of Epoch 74: 0.7352499364837398
Testing Acc of Epoch 74: 0.733495652173913
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5779e-01 (2.5779e-01)	Acc 0.729492 (0.729492)
Epoch: [75][300/616]	Loss 2.3169e-01 (2.4301e-01)	Acc 0.755859 (0.745240)
Epoch: [75][600/616]	Loss 2.4651e-01 (2.4162e-01)	Acc 0.741211 (0.746328)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748848)
Training Loss of Epoch 75: 0.24166025594482576
Training Acc of Epoch 75: 0.7462430132113821
Testing Acc of Epoch 75: 0.7488478260869565
Model with the best training loss saved! The loss is 0.24166025594482576
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.5009e-01 (2.5009e-01)	Acc 0.728516 (0.728516)
Epoch: [76][300/616]	Loss 2.3537e-01 (2.4023e-01)	Acc 0.752930 (0.746979)
Epoch: [76][600/616]	Loss 2.1928e-01 (2.3970e-01)	Acc 0.764648 (0.747819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749465)
Training Loss of Epoch 76: 0.2397486561682166
Training Acc of Epoch 76: 0.7477674034552846
Testing Acc of Epoch 76: 0.7494652173913043
Model with the best training loss saved! The loss is 0.2397486561682166
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4393e-01 (2.4393e-01)	Acc 0.738281 (0.738281)
Epoch: [77][300/616]	Loss 2.5306e-01 (2.3981e-01)	Acc 0.731445 (0.747515)
Epoch: [77][600/616]	Loss 2.2948e-01 (2.3933e-01)	Acc 0.772461 (0.748165)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.751165)
Training Loss of Epoch 77: 0.2392443761592958
Training Acc of Epoch 77: 0.7482533028455285
Testing Acc of Epoch 77: 0.7511652173913044
Model with the best training loss saved! The loss is 0.2392443761592958
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4062e-01 (2.4062e-01)	Acc 0.749023 (0.749023)
Epoch: [78][300/616]	Loss 2.3133e-01 (2.3777e-01)	Acc 0.760742 (0.749932)
Epoch: [78][600/616]	Loss 2.3164e-01 (2.3822e-01)	Acc 0.741211 (0.749350)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752104)
Training Loss of Epoch 78: 0.23816499303026897
Training Acc of Epoch 78: 0.749415650406504
Testing Acc of Epoch 78: 0.7521043478260869
Model with the best training loss saved! The loss is 0.23816499303026897
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3458e-01 (2.3458e-01)	Acc 0.750977 (0.750977)
Epoch: [79][300/616]	Loss 2.2977e-01 (2.3739e-01)	Acc 0.744141 (0.749757)
Epoch: [79][600/616]	Loss 2.3901e-01 (2.3715e-01)	Acc 0.738281 (0.750086)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.751961)
Training Loss of Epoch 79: 0.2372387883382115
Training Acc of Epoch 79: 0.7499301321138211
Testing Acc of Epoch 79: 0.7519608695652173
Model with the best training loss saved! The loss is 0.2372387883382115
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4009e-01 (2.4009e-01)	Acc 0.748047 (0.748047)
Epoch: [80][300/616]	Loss 2.1895e-01 (2.3760e-01)	Acc 0.769531 (0.749244)
Epoch: [80][600/616]	Loss 2.1931e-01 (2.3750e-01)	Acc 0.769531 (0.749755)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750491)
Training Loss of Epoch 80: 0.23747976819674174
Training Acc of Epoch 80: 0.7497618140243902
Testing Acc of Epoch 80: 0.7504913043478261
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2841e-01 (2.2841e-01)	Acc 0.762695 (0.762695)
Epoch: [81][300/616]	Loss 2.4242e-01 (2.3656e-01)	Acc 0.739258 (0.750490)
Epoch: [81][600/616]	Loss 2.3679e-01 (2.3732e-01)	Acc 0.754883 (0.749600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.752452)
Training Loss of Epoch 81: 0.2372902109855559
Training Acc of Epoch 81: 0.7495998475609756
Testing Acc of Epoch 81: 0.7524521739130435
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.4153e-01 (2.4153e-01)	Acc 0.744141 (0.744141)
Epoch: [82][300/616]	Loss 2.2835e-01 (2.3614e-01)	Acc 0.774414 (0.751116)
Epoch: [82][600/616]	Loss 2.4763e-01 (2.3695e-01)	Acc 0.743164 (0.750054)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752091)
Training Loss of Epoch 82: 0.2369277187721516
Training Acc of Epoch 82: 0.7501079776422764
Testing Acc of Epoch 82: 0.7520913043478261
Model with the best training loss saved! The loss is 0.2369277187721516
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3159e-01 (2.3159e-01)	Acc 0.753906 (0.753906)
Epoch: [83][300/616]	Loss 2.4488e-01 (2.3726e-01)	Acc 0.747070 (0.749594)
Epoch: [83][600/616]	Loss 2.3058e-01 (2.3721e-01)	Acc 0.756836 (0.750086)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753022)
Training Loss of Epoch 83: 0.23725246009303302
Training Acc of Epoch 83: 0.7500158790650406
Testing Acc of Epoch 83: 0.7530217391304348
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3452e-01 (2.3452e-01)	Acc 0.756836 (0.756836)
Epoch: [84][300/616]	Loss 2.3121e-01 (2.3673e-01)	Acc 0.761719 (0.750775)
Epoch: [84][600/616]	Loss 2.3984e-01 (2.3672e-01)	Acc 0.745117 (0.750466)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753670)
Training Loss of Epoch 84: 0.23680704972608302
Training Acc of Epoch 84: 0.7502905868902439
Testing Acc of Epoch 84: 0.7536695652173913
Model with the best training loss saved! The loss is 0.23680704972608302
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3820e-01 (2.3820e-01)	Acc 0.743164 (0.743164)
Epoch: [85][300/616]	Loss 2.3115e-01 (2.3793e-01)	Acc 0.756836 (0.749053)
Epoch: [85][600/616]	Loss 2.4766e-01 (2.3709e-01)	Acc 0.743164 (0.749942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751426)
Training Loss of Epoch 85: 0.2370315483672832
Training Acc of Epoch 85: 0.7500793953252033
Testing Acc of Epoch 85: 0.7514260869565217
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.2633e-01 (2.2633e-01)	Acc 0.769531 (0.769531)
Epoch: [86][300/616]	Loss 2.4189e-01 (2.3673e-01)	Acc 0.741211 (0.750256)
Epoch: [86][600/616]	Loss 2.5051e-01 (2.3661e-01)	Acc 0.726562 (0.750305)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751235)
Training Loss of Epoch 86: 0.23663994878288208
Training Acc of Epoch 86: 0.7502302464430894
Testing Acc of Epoch 86: 0.7512347826086957
Model with the best training loss saved! The loss is 0.23663994878288208
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.4971e-01 (2.4971e-01)	Acc 0.737305 (0.737305)
Epoch: [87][300/616]	Loss 2.2948e-01 (2.3635e-01)	Acc 0.773438 (0.750529)
Epoch: [87][600/616]	Loss 2.2359e-01 (2.3697e-01)	Acc 0.768555 (0.749969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750148)
Training Loss of Epoch 87: 0.23705534406793796
Training Acc of Epoch 87: 0.7498809070121951
Testing Acc of Epoch 87: 0.7501478260869565
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3534e-01 (2.3534e-01)	Acc 0.757812 (0.757812)
Epoch: [88][300/616]	Loss 2.4041e-01 (2.3764e-01)	Acc 0.750977 (0.749358)
Epoch: [88][600/616]	Loss 2.5060e-01 (2.3738e-01)	Acc 0.731445 (0.749776)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.751665)
Training Loss of Epoch 88: 0.23737297205905605
Training Acc of Epoch 88: 0.7497189405487805
Testing Acc of Epoch 88: 0.7516652173913043
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.4816e-01 (2.4816e-01)	Acc 0.737305 (0.737305)
Epoch: [89][300/616]	Loss 2.4280e-01 (2.3738e-01)	Acc 0.755859 (0.750182)
Epoch: [89][600/616]	Loss 2.5116e-01 (2.3703e-01)	Acc 0.729492 (0.750291)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751878)
Training Loss of Epoch 89: 0.23699636139520786
Training Acc of Epoch 89: 0.7502969385162601
Testing Acc of Epoch 89: 0.7518782608695652
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3412e-01 (2.3412e-01)	Acc 0.741211 (0.741211)
Epoch: [90][300/616]	Loss 2.4092e-01 (2.3692e-01)	Acc 0.745117 (0.750857)
Epoch: [90][600/616]	Loss 2.3513e-01 (2.3666e-01)	Acc 0.744141 (0.750455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751413)
Training Loss of Epoch 90: 0.23667741822517985
Training Acc of Epoch 90: 0.7503271087398374
Testing Acc of Epoch 90: 0.7514130434782609
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2864e-01 (2.2864e-01)	Acc 0.765625 (0.765625)
Epoch: [91][300/616]	Loss 2.4816e-01 (2.3639e-01)	Acc 0.741211 (0.750996)
Epoch: [91][600/616]	Loss 2.2574e-01 (2.3623e-01)	Acc 0.758789 (0.750933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748870)
Training Loss of Epoch 91: 0.23618568249834263
Training Acc of Epoch 91: 0.7508654090447154
Testing Acc of Epoch 91: 0.7488695652173913
Model with the best training loss saved! The loss is 0.23618568249834263
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.4203e-01 (2.4203e-01)	Acc 0.739258 (0.739258)
Epoch: [92][300/616]	Loss 2.4422e-01 (2.3661e-01)	Acc 0.734375 (0.750058)
Epoch: [92][600/616]	Loss 2.3856e-01 (2.3657e-01)	Acc 0.749023 (0.750382)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752304)
Training Loss of Epoch 92: 0.2366237691989759
Training Acc of Epoch 92: 0.7503477515243903
Testing Acc of Epoch 92: 0.7523043478260869
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.4051e-01 (2.4051e-01)	Acc 0.736328 (0.736328)
Epoch: [93][300/616]	Loss 2.3700e-01 (2.3677e-01)	Acc 0.750000 (0.749702)
Epoch: [93][600/616]	Loss 2.3016e-01 (2.3693e-01)	Acc 0.747070 (0.749933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.746304)
Training Loss of Epoch 93: 0.23694822933615708
Training Acc of Epoch 93: 0.7499904725609756
Testing Acc of Epoch 93: 0.746304347826087
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2574e-01 (2.2574e-01)	Acc 0.770508 (0.770508)
Epoch: [94][300/616]	Loss 2.4734e-01 (2.3660e-01)	Acc 0.733398 (0.750688)
Epoch: [94][600/616]	Loss 2.3691e-01 (2.3706e-01)	Acc 0.758789 (0.749795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.751648)
Training Loss of Epoch 94: 0.23698262678898446
Training Acc of Epoch 94: 0.7498904344512195
Testing Acc of Epoch 94: 0.7516478260869566
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.4746e-01 (2.4746e-01)	Acc 0.730469 (0.730469)
Epoch: [95][300/616]	Loss 2.3078e-01 (2.3727e-01)	Acc 0.752930 (0.749270)
Epoch: [95][600/616]	Loss 2.3455e-01 (2.3725e-01)	Acc 0.764648 (0.749750)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.751617)
Training Loss of Epoch 95: 0.2371356685714024
Training Acc of Epoch 95: 0.7498618521341464
Testing Acc of Epoch 95: 0.7516173913043478
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3584e-01 (2.3584e-01)	Acc 0.755859 (0.755859)
Epoch: [96][300/616]	Loss 2.3119e-01 (2.3603e-01)	Acc 0.761719 (0.751155)
Epoch: [96][600/616]	Loss 2.3243e-01 (2.3623e-01)	Acc 0.752930 (0.750702)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751609)
Training Loss of Epoch 96: 0.23622534231441777
Training Acc of Epoch 96: 0.750682799796748
Testing Acc of Epoch 96: 0.7516086956521739
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.3418e-01 (2.3418e-01)	Acc 0.764648 (0.764648)
Epoch: [97][300/616]	Loss 2.4435e-01 (2.3648e-01)	Acc 0.730469 (0.750477)
Epoch: [97][600/616]	Loss 2.4321e-01 (2.3672e-01)	Acc 0.744141 (0.750159)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752774)
Training Loss of Epoch 97: 0.23674681785145427
Training Acc of Epoch 97: 0.7501254446138211
Testing Acc of Epoch 97: 0.7527739130434783
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3178e-01 (2.3178e-01)	Acc 0.749023 (0.749023)
Epoch: [98][300/616]	Loss 2.3631e-01 (2.3607e-01)	Acc 0.751953 (0.751116)
Epoch: [98][600/616]	Loss 2.3450e-01 (2.3691e-01)	Acc 0.745117 (0.750226)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.752074)
Training Loss of Epoch 98: 0.23688667557103846
Training Acc of Epoch 98: 0.7502794715447154
Testing Acc of Epoch 98: 0.7520739130434783
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2543e-01 (2.2543e-01)	Acc 0.762695 (0.762695)
Epoch: [99][300/616]	Loss 2.4712e-01 (2.3714e-01)	Acc 0.725586 (0.749406)
Epoch: [99][600/616]	Loss 2.4736e-01 (2.3673e-01)	Acc 0.729492 (0.750375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753287)
Training Loss of Epoch 99: 0.23683065011249324
Training Acc of Epoch 99: 0.7503001143292682
Testing Acc of Epoch 99: 0.7532869565217392
Early stopping not satisfied.
