train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_7b
different_width False
resnet18_width 64
weight_precision 7
bias_precision 7
act_precision 10
batch_norm False
dropout False
exp_num 5
lr 0.05
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.05/lr_decay/JT_7b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_7b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.05
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9947e-01 (4.9947e-01)	Acc 0.225586 (0.225586)
Epoch: [0][300/616]	Loss 2.5474e-01 (2.7958e-01)	Acc 0.729492 (0.701561)
Epoch: [0][600/616]	Loss 2.6741e-01 (2.6713e-01)	Acc 0.718750 (0.717523)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.737548)
Training Loss of Epoch 0: 0.26673890461282035
Training Acc of Epoch 0: 0.7179766895325204
Testing Acc of Epoch 0: 0.7375478260869566
Model with the best training loss saved! The loss is 0.26673890461282035
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.5990e-01 (2.5990e-01)	Acc 0.727539 (0.727539)
Epoch: [1][300/616]	Loss 2.4881e-01 (2.5131e-01)	Acc 0.745117 (0.737123)
Epoch: [1][600/616]	Loss 2.6722e-01 (2.5185e-01)	Acc 0.718750 (0.736408)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.744687)
Training Loss of Epoch 1: 0.2518451043503071
Training Acc of Epoch 1: 0.7363741742886178
Testing Acc of Epoch 1: 0.7446869565217391
Model with the best training loss saved! The loss is 0.2518451043503071
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3243e-01 (2.3243e-01)	Acc 0.762695 (0.762695)
Epoch: [2][300/616]	Loss 2.4478e-01 (2.5008e-01)	Acc 0.743164 (0.736987)
Epoch: [2][600/616]	Loss 2.5562e-01 (2.4960e-01)	Acc 0.729492 (0.738233)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.740604)
Training Loss of Epoch 2: 0.2495146602634492
Training Acc of Epoch 2: 0.7383272992886178
Testing Acc of Epoch 2: 0.740604347826087
Model with the best training loss saved! The loss is 0.2495146602634492
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.5560e-01 (2.5560e-01)	Acc 0.727539 (0.727539)
Epoch: [3][300/616]	Loss 2.3127e-01 (2.4859e-01)	Acc 0.755859 (0.739193)
Epoch: [3][600/616]	Loss 2.5168e-01 (2.4894e-01)	Acc 0.751953 (0.739103)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737870)
Training Loss of Epoch 3: 0.24886476053455012
Training Acc of Epoch 3: 0.7391720655487805
Testing Acc of Epoch 3: 0.7378695652173913
Model with the best training loss saved! The loss is 0.24886476053455012
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5363e-01 (2.5363e-01)	Acc 0.718750 (0.718750)
Epoch: [4][300/616]	Loss 2.4534e-01 (2.4836e-01)	Acc 0.738281 (0.740121)
Epoch: [4][600/616]	Loss 2.3190e-01 (2.4814e-01)	Acc 0.763672 (0.740152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742635)
Training Loss of Epoch 4: 0.24816661452859398
Training Acc of Epoch 4: 0.7400581173780488
Testing Acc of Epoch 4: 0.7426347826086956
Model with the best training loss saved! The loss is 0.24816661452859398
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3843e-01 (2.3843e-01)	Acc 0.755859 (0.755859)
Epoch: [5][300/616]	Loss 2.4978e-01 (2.5016e-01)	Acc 0.750000 (0.738379)
Epoch: [5][600/616]	Loss 2.6295e-01 (2.4973e-01)	Acc 0.728516 (0.738257)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740174)
Training Loss of Epoch 5: 0.2497547585547455
Training Acc of Epoch 5: 0.7381558053861789
Testing Acc of Epoch 5: 0.7401739130434782
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.2320e-01 (2.2320e-01)	Acc 0.767578 (0.767578)
Epoch: [6][300/616]	Loss 2.5429e-01 (2.5034e-01)	Acc 0.725586 (0.737859)
Epoch: [6][600/616]	Loss 2.5154e-01 (2.5001e-01)	Acc 0.720703 (0.738306)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.742578)
Training Loss of Epoch 6: 0.2500097699281646
Training Acc of Epoch 6: 0.7383415904471544
Testing Acc of Epoch 6: 0.7425782608695652
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5547e-01 (2.5547e-01)	Acc 0.727539 (0.727539)
Epoch: [7][300/616]	Loss 2.5065e-01 (2.4795e-01)	Acc 0.724609 (0.740822)
Epoch: [7][600/616]	Loss 2.4668e-01 (2.4869e-01)	Acc 0.737305 (0.739557)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.741991)
Training Loss of Epoch 7: 0.24877170319479655
Training Acc of Epoch 7: 0.7394991742886179
Testing Acc of Epoch 7: 0.7419913043478261
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4192e-01 (2.4192e-01)	Acc 0.771484 (0.771484)
Epoch: [8][300/616]	Loss 2.6892e-01 (2.4874e-01)	Acc 0.721680 (0.739829)
Epoch: [8][600/616]	Loss 2.3918e-01 (2.4955e-01)	Acc 0.745117 (0.738536)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.739752)
Training Loss of Epoch 8: 0.24955810286649843
Training Acc of Epoch 8: 0.738597243394309
Testing Acc of Epoch 8: 0.7397521739130435
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5562e-01 (2.5562e-01)	Acc 0.732422 (0.732422)
Epoch: [9][300/616]	Loss 2.4096e-01 (2.5032e-01)	Acc 0.747070 (0.738278)
Epoch: [9][600/616]	Loss 2.4535e-01 (2.5031e-01)	Acc 0.751953 (0.738025)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740096)
Training Loss of Epoch 9: 0.2502174283430828
Training Acc of Epoch 9: 0.7381669207317073
Testing Acc of Epoch 9: 0.740095652173913
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5021e-01 (2.5021e-01)	Acc 0.745117 (0.745117)
Epoch: [10][300/616]	Loss 2.5393e-01 (2.5045e-01)	Acc 0.740234 (0.738122)
Epoch: [10][600/616]	Loss 2.6708e-01 (2.5049e-01)	Acc 0.713867 (0.738125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737652)
Training Loss of Epoch 10: 0.2504292911872631
Training Acc of Epoch 10: 0.7382050304878048
Testing Acc of Epoch 10: 0.7376521739130435
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5688e-01 (2.5688e-01)	Acc 0.726562 (0.726562)
Epoch: [11][300/616]	Loss 2.6279e-01 (2.5027e-01)	Acc 0.718750 (0.737263)
Epoch: [11][600/616]	Loss 2.6138e-01 (2.5009e-01)	Acc 0.717773 (0.738218)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744943)
Training Loss of Epoch 11: 0.2500968454814539
Training Acc of Epoch 11: 0.7382479039634147
Testing Acc of Epoch 11: 0.7449434782608696
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.1968e-01 (2.1968e-01)	Acc 0.789062 (0.789062)
Epoch: [12][300/616]	Loss 2.4637e-01 (2.5060e-01)	Acc 0.744141 (0.738012)
Epoch: [12][600/616]	Loss 2.5558e-01 (2.5102e-01)	Acc 0.721680 (0.737641)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.734070)
Training Loss of Epoch 12: 0.251038715994455
Training Acc of Epoch 12: 0.7375920985772357
Testing Acc of Epoch 12: 0.7340695652173913
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.6352e-01 (2.6352e-01)	Acc 0.729492 (0.729492)
Epoch: [13][300/616]	Loss 2.4412e-01 (2.4866e-01)	Acc 0.742188 (0.739569)
Epoch: [13][600/616]	Loss 2.4448e-01 (2.4970e-01)	Acc 0.748047 (0.738471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.735696)
Training Loss of Epoch 13: 0.24971012630598333
Training Acc of Epoch 13: 0.7384845020325204
Testing Acc of Epoch 13: 0.7356956521739131
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.4701e-01 (2.4701e-01)	Acc 0.740234 (0.740234)
Epoch: [14][300/616]	Loss 2.3091e-01 (2.5084e-01)	Acc 0.772461 (0.737577)
Epoch: [14][600/616]	Loss 2.5143e-01 (2.5078e-01)	Acc 0.736328 (0.737860)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742248)
Training Loss of Epoch 14: 0.2508371115457721
Training Acc of Epoch 14: 0.7377985264227642
Testing Acc of Epoch 14: 0.7422478260869565
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.6585e-01 (2.6585e-01)	Acc 0.712891 (0.712891)
Epoch: [15][300/616]	Loss 2.3780e-01 (2.5033e-01)	Acc 0.742188 (0.737963)
Epoch: [15][600/616]	Loss 2.5814e-01 (2.5006e-01)	Acc 0.722656 (0.738361)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739874)
Training Loss of Epoch 15: 0.2501672785698883
Training Acc of Epoch 15: 0.7381907393292683
Testing Acc of Epoch 15: 0.7398739130434783
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.5484e-01 (2.5484e-01)	Acc 0.740234 (0.740234)
Epoch: [16][300/616]	Loss 2.5204e-01 (2.4967e-01)	Acc 0.731445 (0.738833)
Epoch: [16][600/616]	Loss 2.5022e-01 (2.4915e-01)	Acc 0.743164 (0.739193)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.744926)
Training Loss of Epoch 16: 0.24910415292270785
Training Acc of Epoch 16: 0.7392625762195122
Testing Acc of Epoch 16: 0.7449260869565217
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3764e-01 (2.3764e-01)	Acc 0.753906 (0.753906)
Epoch: [17][300/616]	Loss 2.5352e-01 (2.4919e-01)	Acc 0.726562 (0.739196)
Epoch: [17][600/616]	Loss 2.4124e-01 (2.4959e-01)	Acc 0.748047 (0.739053)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.742361)
Training Loss of Epoch 17: 0.24962153429907513
Training Acc of Epoch 17: 0.7390116869918699
Testing Acc of Epoch 17: 0.7423608695652174
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3994e-01 (2.3994e-01)	Acc 0.752930 (0.752930)
Epoch: [18][300/616]	Loss 2.5615e-01 (2.4934e-01)	Acc 0.732422 (0.738956)
Epoch: [18][600/616]	Loss 2.5638e-01 (2.4954e-01)	Acc 0.735352 (0.738549)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740691)
Training Loss of Epoch 18: 0.24950864613056184
Training Acc of Epoch 18: 0.7385797764227642
Testing Acc of Epoch 18: 0.740691304347826
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.2879e-01 (2.2879e-01)	Acc 0.769531 (0.769531)
Epoch: [19][300/616]	Loss 2.5645e-01 (2.4991e-01)	Acc 0.723633 (0.738638)
Epoch: [19][600/616]	Loss 2.6683e-01 (2.4972e-01)	Acc 0.715820 (0.738757)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.742491)
Training Loss of Epoch 19: 0.24965612665424503
Training Acc of Epoch 19: 0.7388322535569106
Testing Acc of Epoch 19: 0.7424913043478261
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.5751e-01 (2.5751e-01)	Acc 0.733398 (0.733398)
Epoch: [20][300/616]	Loss 2.6808e-01 (2.5016e-01)	Acc 0.717773 (0.738330)
Epoch: [20][600/616]	Loss 2.5173e-01 (2.4996e-01)	Acc 0.728516 (0.738570)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734061)
Training Loss of Epoch 20: 0.2500185315686513
Training Acc of Epoch 20: 0.7384225736788618
Testing Acc of Epoch 20: 0.7340608695652174
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.6797e-01 (2.6797e-01)	Acc 0.730469 (0.730469)
Epoch: [21][300/616]	Loss 2.4928e-01 (2.5197e-01)	Acc 0.740234 (0.736747)
Epoch: [21][600/616]	Loss 2.5107e-01 (2.5295e-01)	Acc 0.736328 (0.735451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.741204)
Training Loss of Epoch 21: 0.25288974645176554
Training Acc of Epoch 21: 0.7355135289634146
Testing Acc of Epoch 21: 0.7412043478260869
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4183e-01 (2.4183e-01)	Acc 0.742188 (0.742188)
Epoch: [22][300/616]	Loss 2.7090e-01 (2.5607e-01)	Acc 0.717773 (0.731900)
Epoch: [22][600/616]	Loss 2.3417e-01 (2.5358e-01)	Acc 0.763672 (0.734741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745426)
Training Loss of Epoch 22: 0.2533997369006397
Training Acc of Epoch 22: 0.7349260035569106
Testing Acc of Epoch 22: 0.7454260869565218
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2658e-01 (2.2658e-01)	Acc 0.765625 (0.765625)
Epoch: [23][300/616]	Loss 2.5153e-01 (2.5161e-01)	Acc 0.747070 (0.736052)
Epoch: [23][600/616]	Loss 2.5853e-01 (2.5822e-01)	Acc 0.721680 (0.728831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.723674)
Training Loss of Epoch 23: 0.2585251891516088
Training Acc of Epoch 23: 0.7285473831300813
Testing Acc of Epoch 23: 0.7236739130434783
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.7312e-01 (2.7312e-01)	Acc 0.719727 (0.719727)
Epoch: [24][300/616]	Loss 2.5527e-01 (2.6179e-01)	Acc 0.733398 (0.726329)
Epoch: [24][600/616]	Loss 2.6329e-01 (2.5586e-01)	Acc 0.719727 (0.732263)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742430)
Training Loss of Epoch 24: 0.2556643943961074
Training Acc of Epoch 24: 0.7326092479674797
Testing Acc of Epoch 24: 0.7424304347826087
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4232e-01 (2.4232e-01)	Acc 0.746094 (0.746094)
Epoch: [25][300/616]	Loss 2.9659e-01 (2.6958e-01)	Acc 0.697266 (0.712900)
Epoch: [25][600/616]	Loss 2.4496e-01 (2.6750e-01)	Acc 0.739258 (0.718911)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.736643)
Training Loss of Epoch 25: 0.2672702964001555
Training Acc of Epoch 25: 0.7191215701219512
Testing Acc of Epoch 25: 0.7366434782608695
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4746e-01 (2.4746e-01)	Acc 0.738281 (0.738281)
Epoch: [26][300/616]	Loss 2.5085e-01 (2.5762e-01)	Acc 0.742188 (0.731351)
Epoch: [26][600/616]	Loss 2.5886e-01 (2.5554e-01)	Acc 0.719727 (0.733676)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740522)
Training Loss of Epoch 26: 0.2554827786073452
Training Acc of Epoch 26: 0.7336699695121951
Testing Acc of Epoch 26: 0.7405217391304347
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4661e-01 (2.4661e-01)	Acc 0.734375 (0.734375)
Epoch: [27][300/616]	Loss 2.4597e-01 (2.5217e-01)	Acc 0.750977 (0.737078)
Epoch: [27][600/616]	Loss 2.5984e-01 (2.5125e-01)	Acc 0.725586 (0.737851)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742852)
Training Loss of Epoch 27: 0.25126812974127327
Training Acc of Epoch 27: 0.7377223069105691
Testing Acc of Epoch 27: 0.7428521739130435
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4291e-01 (2.4291e-01)	Acc 0.753906 (0.753906)
Epoch: [28][300/616]	Loss 2.4039e-01 (2.5040e-01)	Acc 0.723633 (0.738213)
Epoch: [28][600/616]	Loss 2.4100e-01 (2.5139e-01)	Acc 0.743164 (0.737230)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.739343)
Training Loss of Epoch 28: 0.2513745634294138
Training Acc of Epoch 28: 0.737158600101626
Testing Acc of Epoch 28: 0.7393434782608695
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4954e-01 (2.4954e-01)	Acc 0.746094 (0.746094)
Epoch: [29][300/616]	Loss 2.3808e-01 (2.5059e-01)	Acc 0.744141 (0.738064)
Epoch: [29][600/616]	Loss 2.6663e-01 (2.5189e-01)	Acc 0.714844 (0.736996)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.736957)
Training Loss of Epoch 29: 0.2518533302274177
Training Acc of Epoch 29: 0.7371189024390243
Testing Acc of Epoch 29: 0.7369565217391304
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4136e-01 (2.4136e-01)	Acc 0.745117 (0.745117)
Epoch: [30][300/616]	Loss 2.5788e-01 (2.5094e-01)	Acc 0.726562 (0.737821)
Epoch: [30][600/616]	Loss 2.6034e-01 (2.5113e-01)	Acc 0.726562 (0.737643)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.735343)
Training Loss of Epoch 30: 0.25138606930651314
Training Acc of Epoch 30: 0.7373697916666667
Testing Acc of Epoch 30: 0.7353434782608695
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.5693e-01 (2.5693e-01)	Acc 0.731445 (0.731445)
Epoch: [31][300/616]	Loss 2.5647e-01 (2.5002e-01)	Acc 0.732422 (0.738453)
Epoch: [31][600/616]	Loss 2.5785e-01 (2.4983e-01)	Acc 0.722656 (0.738605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.743874)
Training Loss of Epoch 31: 0.24983714067839025
Training Acc of Epoch 31: 0.7386909298780487
Testing Acc of Epoch 31: 0.7438739130434783
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4183e-01 (2.4183e-01)	Acc 0.754883 (0.754883)
Epoch: [32][300/616]	Loss 2.5792e-01 (2.5218e-01)	Acc 0.718750 (0.736906)
Epoch: [32][600/616]	Loss 2.5572e-01 (2.5077e-01)	Acc 0.725586 (0.737935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.739300)
Training Loss of Epoch 32: 0.25079656226363606
Training Acc of Epoch 32: 0.7379080919715447
Testing Acc of Epoch 32: 0.7393
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.5321e-01 (2.5321e-01)	Acc 0.730469 (0.730469)
Epoch: [33][300/616]	Loss 2.4982e-01 (2.5000e-01)	Acc 0.737305 (0.738187)
Epoch: [33][600/616]	Loss 2.5012e-01 (2.5002e-01)	Acc 0.728516 (0.738106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740196)
Training Loss of Epoch 33: 0.2501288652420044
Training Acc of Epoch 33: 0.7379509654471544
Testing Acc of Epoch 33: 0.740195652173913
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.5416e-01 (2.5416e-01)	Acc 0.731445 (0.731445)
Epoch: [34][300/616]	Loss 2.3524e-01 (2.5067e-01)	Acc 0.762695 (0.737337)
Epoch: [34][600/616]	Loss 2.6451e-01 (2.4977e-01)	Acc 0.715820 (0.738471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.737274)
Training Loss of Epoch 34: 0.24981770425792632
Training Acc of Epoch 34: 0.7383431783536586
Testing Acc of Epoch 34: 0.7372739130434782
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3976e-01 (2.3976e-01)	Acc 0.758789 (0.758789)
Epoch: [35][300/616]	Loss 2.4648e-01 (2.4922e-01)	Acc 0.737305 (0.739160)
Epoch: [35][600/616]	Loss 2.4593e-01 (2.4940e-01)	Acc 0.733398 (0.738423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.740413)
Training Loss of Epoch 35: 0.24940146563983545
Training Acc of Epoch 35: 0.7384463922764227
Testing Acc of Epoch 35: 0.7404130434782609
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.4430e-01 (2.4430e-01)	Acc 0.751953 (0.751953)
Epoch: [36][300/616]	Loss 2.4973e-01 (2.5105e-01)	Acc 0.741211 (0.737178)
Epoch: [36][600/616]	Loss 2.3080e-01 (2.5036e-01)	Acc 0.771484 (0.737752)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.741013)
Training Loss of Epoch 36: 0.2503587402221633
Training Acc of Epoch 36: 0.7377508892276423
Testing Acc of Epoch 36: 0.7410130434782609
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4231e-01 (2.4231e-01)	Acc 0.740234 (0.740234)
Epoch: [37][300/616]	Loss 2.6369e-01 (2.4958e-01)	Acc 0.716797 (0.737976)
Epoch: [37][600/616]	Loss 2.3573e-01 (2.5000e-01)	Acc 0.748047 (0.737496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.687500 (0.711583)
Training Loss of Epoch 37: 0.25013161766819836
Training Acc of Epoch 37: 0.7373443851626016
Testing Acc of Epoch 37: 0.7115826086956522
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.5701e-01 (2.5701e-01)	Acc 0.725586 (0.725586)
Epoch: [38][300/616]	Loss 2.3903e-01 (2.4999e-01)	Acc 0.752930 (0.737532)
Epoch: [38][600/616]	Loss 2.4482e-01 (2.5013e-01)	Acc 0.750000 (0.737522)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.726296)
Training Loss of Epoch 38: 0.25004003772406075
Training Acc of Epoch 38: 0.7376175050813009
Testing Acc of Epoch 38: 0.726295652173913
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.6230e-01 (2.6230e-01)	Acc 0.722656 (0.722656)
Epoch: [39][300/616]	Loss 2.5490e-01 (2.5013e-01)	Acc 0.726562 (0.737542)
Epoch: [39][600/616]	Loss 2.4069e-01 (2.5080e-01)	Acc 0.747070 (0.737030)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.730378)
Training Loss of Epoch 39: 0.25081089565424414
Training Acc of Epoch 39: 0.7370426829268293
Testing Acc of Epoch 39: 0.7303782608695653
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.5058e-01 (2.5058e-01)	Acc 0.735352 (0.735352)
Epoch: [40][300/616]	Loss 2.7117e-01 (2.5184e-01)	Acc 0.715820 (0.735592)
Epoch: [40][600/616]	Loss 2.6205e-01 (2.5128e-01)	Acc 0.715820 (0.736541)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740165)
Training Loss of Epoch 40: 0.25135760944548663
Training Acc of Epoch 40: 0.7364789761178862
Testing Acc of Epoch 40: 0.7401652173913044
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.3593e-01 (2.3593e-01)	Acc 0.754883 (0.754883)
Epoch: [41][300/616]	Loss 2.6045e-01 (2.4955e-01)	Acc 0.723633 (0.738534)
Epoch: [41][600/616]	Loss 2.6505e-01 (2.5047e-01)	Acc 0.700195 (0.737994)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.731617)
Training Loss of Epoch 41: 0.25046610655338786
Training Acc of Epoch 41: 0.7379557291666666
Testing Acc of Epoch 41: 0.7316173913043478
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.5611e-01 (2.5611e-01)	Acc 0.735352 (0.735352)
Epoch: [42][300/616]	Loss 2.4840e-01 (2.4947e-01)	Acc 0.743164 (0.738405)
Epoch: [42][600/616]	Loss 2.5038e-01 (2.5016e-01)	Acc 0.741211 (0.737337)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.742643)
Training Loss of Epoch 42: 0.25018773469013894
Training Acc of Epoch 42: 0.7373158028455284
Testing Acc of Epoch 42: 0.7426434782608695
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.5062e-01 (2.5062e-01)	Acc 0.718750 (0.718750)
Epoch: [43][300/616]	Loss 2.4687e-01 (2.5124e-01)	Acc 0.750977 (0.736760)
Epoch: [43][600/616]	Loss 2.5569e-01 (2.5142e-01)	Acc 0.721680 (0.736609)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.735396)
Training Loss of Epoch 43: 0.2514570552401426
Training Acc of Epoch 43: 0.7365202616869919
Testing Acc of Epoch 43: 0.735395652173913
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.5782e-01 (2.5782e-01)	Acc 0.726562 (0.726562)
Epoch: [44][300/616]	Loss 2.5056e-01 (2.5160e-01)	Acc 0.735352 (0.737681)
Epoch: [44][600/616]	Loss 2.4339e-01 (2.5162e-01)	Acc 0.732422 (0.737219)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741587)
Training Loss of Epoch 44: 0.2514763580832055
Training Acc of Epoch 44: 0.7373412093495935
Testing Acc of Epoch 44: 0.7415869565217391
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.5544e-01 (2.5544e-01)	Acc 0.734375 (0.734375)
Epoch: [45][300/616]	Loss 2.4019e-01 (2.5064e-01)	Acc 0.755859 (0.737954)
Epoch: [45][600/616]	Loss 2.6763e-01 (2.5201e-01)	Acc 0.704102 (0.735743)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.719378)
Training Loss of Epoch 45: 0.25199842273704404
Training Acc of Epoch 45: 0.7357707698170731
Testing Acc of Epoch 45: 0.7193782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.7279e-01 (2.7279e-01)	Acc 0.711914 (0.711914)
Epoch: [46][300/616]	Loss 2.4532e-01 (2.5369e-01)	Acc 0.744141 (0.733843)
Epoch: [46][600/616]	Loss 2.4647e-01 (2.5350e-01)	Acc 0.739258 (0.734656)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.736339)
Training Loss of Epoch 46: 0.2535017219743108
Training Acc of Epoch 46: 0.7345560213414634
Testing Acc of Epoch 46: 0.7363391304347826
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4811e-01 (2.4811e-01)	Acc 0.735352 (0.735352)
Epoch: [47][300/616]	Loss 2.4678e-01 (2.5193e-01)	Acc 0.747070 (0.736669)
Epoch: [47][600/616]	Loss 2.3349e-01 (2.5138e-01)	Acc 0.752930 (0.736801)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.721570)
Training Loss of Epoch 47: 0.25140898908541454
Training Acc of Epoch 47: 0.7366584095528456
Testing Acc of Epoch 47: 0.7215695652173914
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.7451e-01 (2.7451e-01)	Acc 0.725586 (0.725586)
Epoch: [48][300/616]	Loss 2.6986e-01 (2.6459e-01)	Acc 0.710938 (0.718838)
Epoch: [48][600/616]	Loss 2.6046e-01 (2.5837e-01)	Acc 0.727539 (0.727235)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.732157)
Training Loss of Epoch 48: 0.2583602805205477
Training Acc of Epoch 48: 0.7271960746951219
Testing Acc of Epoch 48: 0.7321565217391305
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.5836e-01 (2.5836e-01)	Acc 0.742188 (0.742188)
Epoch: [49][300/616]	Loss 2.6464e-01 (2.5390e-01)	Acc 0.708984 (0.733918)
Epoch: [49][600/616]	Loss 2.4403e-01 (2.5267e-01)	Acc 0.732422 (0.735119)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.733891)
Training Loss of Epoch 49: 0.2527836197760047
Training Acc of Epoch 49: 0.7350069867886179
Testing Acc of Epoch 49: 0.7338913043478261
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4291e-01 (2.4291e-01)	Acc 0.737305 (0.737305)
Epoch: [50][300/616]	Loss 2.5754e-01 (2.5330e-01)	Acc 0.744141 (0.733960)
Epoch: [50][600/616]	Loss 2.6263e-01 (2.5217e-01)	Acc 0.718750 (0.735599)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.740339)
Training Loss of Epoch 50: 0.25222127728345917
Training Acc of Epoch 50: 0.7355151168699187
Testing Acc of Epoch 50: 0.7403391304347826
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.6578e-01 (2.6578e-01)	Acc 0.724609 (0.724609)
Epoch: [51][300/616]	Loss 2.6637e-01 (2.5232e-01)	Acc 0.710938 (0.735381)
Epoch: [51][600/616]	Loss 2.4299e-01 (2.5228e-01)	Acc 0.739258 (0.735693)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.742922)
Training Loss of Epoch 51: 0.2521416427158728
Training Acc of Epoch 51: 0.7358612804878049
Testing Acc of Epoch 51: 0.7429217391304348
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4403e-01 (2.4403e-01)	Acc 0.749023 (0.749023)
Epoch: [52][300/616]	Loss 2.4379e-01 (2.5076e-01)	Acc 0.754883 (0.737743)
Epoch: [52][600/616]	Loss 2.2935e-01 (2.5028e-01)	Acc 0.754883 (0.737680)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.741665)
Training Loss of Epoch 52: 0.25017170237331854
Training Acc of Epoch 52: 0.7378286966463414
Testing Acc of Epoch 52: 0.7416652173913043
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5639e-01 (2.5639e-01)	Acc 0.728516 (0.728516)
Epoch: [53][300/616]	Loss 2.6976e-01 (2.5489e-01)	Acc 0.710938 (0.733437)
Epoch: [53][600/616]	Loss 2.4455e-01 (2.5285e-01)	Acc 0.743164 (0.735057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.740957)
Training Loss of Epoch 53: 0.2527179279705373
Training Acc of Epoch 53: 0.7351880081300813
Testing Acc of Epoch 53: 0.7409565217391304
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.4540e-01 (2.4540e-01)	Acc 0.736328 (0.736328)
Epoch: [54][300/616]	Loss 2.5697e-01 (2.5137e-01)	Acc 0.733398 (0.735738)
Epoch: [54][600/616]	Loss 2.5685e-01 (2.5058e-01)	Acc 0.738281 (0.736819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.734417)
Training Loss of Epoch 54: 0.2506751149408216
Training Acc of Epoch 54: 0.73671875
Testing Acc of Epoch 54: 0.7344173913043478
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.6994e-01 (2.6994e-01)	Acc 0.723633 (0.723633)
Epoch: [55][300/616]	Loss 2.5590e-01 (2.5091e-01)	Acc 0.730469 (0.737272)
Epoch: [55][600/616]	Loss 2.6088e-01 (2.5111e-01)	Acc 0.721680 (0.736996)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.737935)
Training Loss of Epoch 55: 0.25109558590059355
Training Acc of Epoch 55: 0.7370458587398374
Testing Acc of Epoch 55: 0.7379347826086956
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5753e-01 (2.5753e-01)	Acc 0.729492 (0.729492)
Epoch: [56][300/616]	Loss 2.5379e-01 (2.5129e-01)	Acc 0.741211 (0.736857)
Epoch: [56][600/616]	Loss 2.7937e-01 (2.5104e-01)	Acc 0.704102 (0.737438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.739683)
Training Loss of Epoch 56: 0.2510693509161957
Training Acc of Epoch 56: 0.7373951981707317
Testing Acc of Epoch 56: 0.7396826086956522
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5478e-01 (2.5478e-01)	Acc 0.735352 (0.735352)
Epoch: [57][300/616]	Loss 2.6076e-01 (2.5112e-01)	Acc 0.731445 (0.737165)
Epoch: [57][600/616]	Loss 2.2661e-01 (2.5015e-01)	Acc 0.766602 (0.737877)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745622)
Training Loss of Epoch 57: 0.2501009162121672
Training Acc of Epoch 57: 0.7379430259146341
Testing Acc of Epoch 57: 0.7456217391304348
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.4247e-01 (2.4247e-01)	Acc 0.753906 (0.753906)
Epoch: [58][300/616]	Loss 2.5407e-01 (2.4894e-01)	Acc 0.721680 (0.739229)
Epoch: [58][600/616]	Loss 2.5568e-01 (2.4991e-01)	Acc 0.725586 (0.738137)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738322)
Training Loss of Epoch 58: 0.2500453357531772
Training Acc of Epoch 58: 0.7380224212398374
Testing Acc of Epoch 58: 0.7383217391304348
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.4806e-01 (2.4806e-01)	Acc 0.735352 (0.735352)
Epoch: [59][300/616]	Loss 2.5520e-01 (2.4975e-01)	Acc 0.733398 (0.737908)
Epoch: [59][600/616]	Loss 2.4327e-01 (2.5030e-01)	Acc 0.734375 (0.737667)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744504)
Training Loss of Epoch 59: 0.25023265049709537
Training Acc of Epoch 59: 0.7377588287601626
Testing Acc of Epoch 59: 0.744504347826087
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.2913e-01 (2.2913e-01)	Acc 0.749023 (0.749023)
Epoch: [60][300/616]	Loss 2.3334e-01 (2.4945e-01)	Acc 0.754883 (0.738538)
Epoch: [60][600/616]	Loss 2.3981e-01 (2.4981e-01)	Acc 0.750000 (0.738330)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.743978)
Training Loss of Epoch 60: 0.2497642616188623
Training Acc of Epoch 60: 0.7384416285569105
Testing Acc of Epoch 60: 0.7439782608695652
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.4266e-01 (2.4266e-01)	Acc 0.747070 (0.747070)
Epoch: [61][300/616]	Loss 2.5037e-01 (2.4934e-01)	Acc 0.743164 (0.738437)
Epoch: [61][600/616]	Loss 2.6118e-01 (2.5028e-01)	Acc 0.732422 (0.737888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.730748)
Training Loss of Epoch 61: 0.25037119454000056
Training Acc of Epoch 61: 0.7377238948170731
Testing Acc of Epoch 61: 0.7307478260869565
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3647e-01 (2.3647e-01)	Acc 0.755859 (0.755859)
Epoch: [62][300/616]	Loss 2.2676e-01 (2.5165e-01)	Acc 0.761719 (0.736357)
Epoch: [62][600/616]	Loss 2.7070e-01 (2.5015e-01)	Acc 0.708008 (0.738091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.740804)
Training Loss of Epoch 62: 0.25015224339516184
Training Acc of Epoch 62: 0.7381192835365854
Testing Acc of Epoch 62: 0.740804347826087
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3885e-01 (2.3885e-01)	Acc 0.749023 (0.749023)
Epoch: [63][300/616]	Loss 2.6258e-01 (2.4934e-01)	Acc 0.720703 (0.738706)
Epoch: [63][600/616]	Loss 2.5199e-01 (2.4950e-01)	Acc 0.736328 (0.738492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.733039)
Training Loss of Epoch 63: 0.24944377066643258
Training Acc of Epoch 63: 0.7386512322154472
Testing Acc of Epoch 63: 0.7330391304347826
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5026e-01 (2.5026e-01)	Acc 0.745117 (0.745117)
Epoch: [64][300/616]	Loss 2.5192e-01 (2.4982e-01)	Acc 0.738281 (0.738346)
Epoch: [64][600/616]	Loss 2.5247e-01 (2.5067e-01)	Acc 0.746094 (0.737643)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.734052)
Training Loss of Epoch 64: 0.2506003778882143
Training Acc of Epoch 64: 0.7377620045731708
Testing Acc of Epoch 64: 0.7340521739130434
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.6232e-01 (2.6232e-01)	Acc 0.717773 (0.717773)
Epoch: [65][300/616]	Loss 2.4979e-01 (2.5019e-01)	Acc 0.735352 (0.738310)
Epoch: [65][600/616]	Loss 2.5212e-01 (2.5002e-01)	Acc 0.730469 (0.738224)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.740930)
Training Loss of Epoch 65: 0.2500121315320333
Training Acc of Epoch 65: 0.7381923272357723
Testing Acc of Epoch 65: 0.7409304347826087
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.5081e-01 (2.5081e-01)	Acc 0.752930 (0.752930)
Epoch: [66][300/616]	Loss 2.5500e-01 (2.5029e-01)	Acc 0.732422 (0.738249)
Epoch: [66][600/616]	Loss 2.5232e-01 (2.5018e-01)	Acc 0.733398 (0.738272)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.739574)
Training Loss of Epoch 66: 0.25011220109171983
Training Acc of Epoch 66: 0.7382701346544716
Testing Acc of Epoch 66: 0.7395739130434783
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.5459e-01 (2.5459e-01)	Acc 0.737305 (0.737305)
Epoch: [67][300/616]	Loss 2.5100e-01 (2.5072e-01)	Acc 0.723633 (0.737785)
Epoch: [67][600/616]	Loss 2.5933e-01 (2.5127e-01)	Acc 0.728516 (0.737248)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.740317)
Training Loss of Epoch 67: 0.251158995211609
Training Acc of Epoch 67: 0.7373158028455284
Testing Acc of Epoch 67: 0.7403173913043478
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.5459e-01 (2.5459e-01)	Acc 0.741211 (0.741211)
Epoch: [68][300/616]	Loss 2.4470e-01 (2.5409e-01)	Acc 0.741211 (0.734819)
Epoch: [68][600/616]	Loss 2.5424e-01 (2.5337e-01)	Acc 0.722656 (0.735270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734539)
Training Loss of Epoch 68: 0.25323369091119224
Training Acc of Epoch 68: 0.7354325457317074
Testing Acc of Epoch 68: 0.7345391304347826
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5873e-01 (2.5873e-01)	Acc 0.739258 (0.739258)
Epoch: [69][300/616]	Loss 2.5847e-01 (2.5122e-01)	Acc 0.720703 (0.737214)
Epoch: [69][600/616]	Loss 2.7754e-01 (2.5248e-01)	Acc 0.699219 (0.735901)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.732217)
Training Loss of Epoch 69: 0.25242667590699547
Training Acc of Epoch 69: 0.7360010162601626
Testing Acc of Epoch 69: 0.7322173913043478
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.5327e-01 (2.5327e-01)	Acc 0.729492 (0.729492)
Epoch: [70][300/616]	Loss 2.4077e-01 (2.5038e-01)	Acc 0.756836 (0.738606)
Epoch: [70][600/616]	Loss 2.4949e-01 (2.5134e-01)	Acc 0.743164 (0.736975)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.742426)
Training Loss of Epoch 70: 0.25132433546752464
Training Acc of Epoch 70: 0.7369616996951219
Testing Acc of Epoch 70: 0.7424260869565218
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4184e-01 (2.4184e-01)	Acc 0.737305 (0.737305)
Epoch: [71][300/616]	Loss 2.4011e-01 (2.5236e-01)	Acc 0.739258 (0.735186)
Epoch: [71][600/616]	Loss 2.6005e-01 (2.5209e-01)	Acc 0.714844 (0.735605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.744483)
Training Loss of Epoch 71: 0.25203245398474905
Training Acc of Epoch 71: 0.7355691056910569
Testing Acc of Epoch 71: 0.7444826086956522
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4634e-01 (2.4634e-01)	Acc 0.731445 (0.731445)
Epoch: [72][300/616]	Loss 2.7359e-01 (2.5092e-01)	Acc 0.700195 (0.736743)
Epoch: [72][600/616]	Loss 2.3365e-01 (2.5215e-01)	Acc 0.763672 (0.735958)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.731826)
Training Loss of Epoch 72: 0.252091641033568
Training Acc of Epoch 72: 0.7360947027439024
Testing Acc of Epoch 72: 0.7318260869565217
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.6721e-01 (2.6721e-01)	Acc 0.713867 (0.713867)
Epoch: [73][300/616]	Loss 2.5281e-01 (2.5121e-01)	Acc 0.729492 (0.736773)
Epoch: [73][600/616]	Loss 2.3573e-01 (2.5192e-01)	Acc 0.762695 (0.736206)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.731865)
Training Loss of Epoch 73: 0.25180217754550094
Training Acc of Epoch 73: 0.7363138338414634
Testing Acc of Epoch 73: 0.7318652173913044
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.6182e-01 (2.6182e-01)	Acc 0.717773 (0.717773)
Epoch: [74][300/616]	Loss 2.4167e-01 (2.5105e-01)	Acc 0.757812 (0.736808)
Epoch: [74][600/616]	Loss 2.6680e-01 (2.5053e-01)	Acc 0.723633 (0.737579)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.740430)
Training Loss of Epoch 74: 0.2504581996822745
Training Acc of Epoch 74: 0.7376270325203252
Testing Acc of Epoch 74: 0.7404304347826087
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5175e-01 (2.5175e-01)	Acc 0.739258 (0.739258)
Epoch: [75][300/616]	Loss 2.3956e-01 (2.4189e-01)	Acc 0.759766 (0.744923)
Epoch: [75][600/616]	Loss 2.5250e-01 (2.4108e-01)	Acc 0.731445 (0.745900)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.747922)
Training Loss of Epoch 75: 0.2410926553049708
Training Acc of Epoch 75: 0.7459270198170732
Testing Acc of Epoch 75: 0.7479217391304348
Model with the best training loss saved! The loss is 0.2410926553049708
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.4380e-01 (2.4380e-01)	Acc 0.746094 (0.746094)
Epoch: [76][300/616]	Loss 2.3617e-01 (2.4038e-01)	Acc 0.748047 (0.746220)
Epoch: [76][600/616]	Loss 2.3776e-01 (2.3967e-01)	Acc 0.747070 (0.747314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.744687)
Training Loss of Epoch 76: 0.2396989251539959
Training Acc of Epoch 76: 0.747289443597561
Testing Acc of Epoch 76: 0.7446869565217391
Model with the best training loss saved! The loss is 0.2396989251539959
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4905e-01 (2.4905e-01)	Acc 0.729492 (0.729492)
Epoch: [77][300/616]	Loss 2.2861e-01 (2.4031e-01)	Acc 0.751953 (0.746535)
Epoch: [77][600/616]	Loss 2.4984e-01 (2.3971e-01)	Acc 0.736328 (0.747226)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748474)
Training Loss of Epoch 77: 0.23974282765776161
Training Acc of Epoch 77: 0.7471544715447155
Testing Acc of Epoch 77: 0.7484739130434782
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3846e-01 (2.3846e-01)	Acc 0.754883 (0.754883)
Epoch: [78][300/616]	Loss 2.5162e-01 (2.3914e-01)	Acc 0.718750 (0.747654)
Epoch: [78][600/616]	Loss 2.3383e-01 (2.3902e-01)	Acc 0.750000 (0.747841)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.749848)
Training Loss of Epoch 78: 0.23894990028404609
Training Acc of Epoch 78: 0.7479309578252032
Testing Acc of Epoch 78: 0.7498478260869565
Model with the best training loss saved! The loss is 0.23894990028404609
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.4083e-01 (2.4083e-01)	Acc 0.733398 (0.733398)
Epoch: [79][300/616]	Loss 2.4515e-01 (2.3899e-01)	Acc 0.733398 (0.747294)
Epoch: [79][600/616]	Loss 2.4873e-01 (2.3872e-01)	Acc 0.739258 (0.747849)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.748909)
Training Loss of Epoch 79: 0.23873744888034293
Training Acc of Epoch 79: 0.7478452108739837
Testing Acc of Epoch 79: 0.7489086956521739
Model with the best training loss saved! The loss is 0.23873744888034293
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4235e-01 (2.4235e-01)	Acc 0.750977 (0.750977)
Epoch: [80][300/616]	Loss 2.3526e-01 (2.3671e-01)	Acc 0.748047 (0.750016)
Epoch: [80][600/616]	Loss 2.4468e-01 (2.3782e-01)	Acc 0.743164 (0.748806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.749578)
Training Loss of Epoch 80: 0.23786266669025266
Training Acc of Epoch 80: 0.7487661966463415
Testing Acc of Epoch 80: 0.7495782608695652
Model with the best training loss saved! The loss is 0.23786266669025266
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3754e-01 (2.3754e-01)	Acc 0.749023 (0.749023)
Epoch: [81][300/616]	Loss 2.3857e-01 (2.3660e-01)	Acc 0.749023 (0.750503)
Epoch: [81][600/616]	Loss 2.2658e-01 (2.3695e-01)	Acc 0.769531 (0.749711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749400)
Training Loss of Epoch 81: 0.23695693207465537
Training Acc of Epoch 81: 0.7497395833333333
Testing Acc of Epoch 81: 0.7494
Model with the best training loss saved! The loss is 0.23695693207465537
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2571e-01 (2.2571e-01)	Acc 0.761719 (0.761719)
Epoch: [82][300/616]	Loss 2.4652e-01 (2.3702e-01)	Acc 0.742188 (0.749562)
Epoch: [82][600/616]	Loss 2.4453e-01 (2.3714e-01)	Acc 0.738281 (0.749277)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750752)
Training Loss of Epoch 82: 0.23708751746794074
Training Acc of Epoch 82: 0.7494331173780487
Testing Acc of Epoch 82: 0.7507521739130435
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3323e-01 (2.3323e-01)	Acc 0.749023 (0.749023)
Epoch: [83][300/616]	Loss 2.2721e-01 (2.3716e-01)	Acc 0.750977 (0.749504)
Epoch: [83][600/616]	Loss 2.3996e-01 (2.3728e-01)	Acc 0.746094 (0.749152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749165)
Training Loss of Epoch 83: 0.23730946675064118
Training Acc of Epoch 83: 0.7491536458333333
Testing Acc of Epoch 83: 0.7491652173913044
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3026e-01 (2.3026e-01)	Acc 0.764648 (0.764648)
Epoch: [84][300/616]	Loss 2.4516e-01 (2.3755e-01)	Acc 0.746094 (0.749199)
Epoch: [84][600/616]	Loss 2.2345e-01 (2.3718e-01)	Acc 0.777344 (0.749514)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746565)
Training Loss of Epoch 84: 0.23728188000074246
Training Acc of Epoch 84: 0.7493965955284553
Testing Acc of Epoch 84: 0.7465652173913043
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3465e-01 (2.3465e-01)	Acc 0.745117 (0.745117)
Epoch: [85][300/616]	Loss 2.3941e-01 (2.3627e-01)	Acc 0.744141 (0.751058)
Epoch: [85][600/616]	Loss 2.3520e-01 (2.3769e-01)	Acc 0.752930 (0.749116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746883)
Training Loss of Epoch 85: 0.2377600955284708
Training Acc of Epoch 85: 0.7490663109756097
Testing Acc of Epoch 85: 0.7468826086956522
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.4939e-01 (2.4939e-01)	Acc 0.737305 (0.737305)
Epoch: [86][300/616]	Loss 2.3744e-01 (2.3761e-01)	Acc 0.740234 (0.748628)
Epoch: [86][600/616]	Loss 2.3672e-01 (2.3797e-01)	Acc 0.752930 (0.748525)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.748670)
Training Loss of Epoch 86: 0.23801375033894207
Training Acc of Epoch 86: 0.7485407139227642
Testing Acc of Epoch 86: 0.7486695652173913
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3252e-01 (2.3252e-01)	Acc 0.747070 (0.747070)
Epoch: [87][300/616]	Loss 2.2295e-01 (2.3774e-01)	Acc 0.769531 (0.748368)
Epoch: [87][600/616]	Loss 2.2363e-01 (2.3793e-01)	Acc 0.775391 (0.748648)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.749152)
Training Loss of Epoch 87: 0.23791645588428995
Training Acc of Epoch 87: 0.748656631097561
Testing Acc of Epoch 87: 0.7491521739130435
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.2843e-01 (2.2843e-01)	Acc 0.745117 (0.745117)
Epoch: [88][300/616]	Loss 2.2227e-01 (2.3790e-01)	Acc 0.762695 (0.748582)
Epoch: [88][600/616]	Loss 2.4517e-01 (2.3807e-01)	Acc 0.733398 (0.748677)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748743)
Training Loss of Epoch 88: 0.23802101684779656
Training Acc of Epoch 88: 0.7487995426829268
Testing Acc of Epoch 88: 0.7487434782608695
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.4078e-01 (2.4078e-01)	Acc 0.741211 (0.741211)
Epoch: [89][300/616]	Loss 2.4442e-01 (2.3862e-01)	Acc 0.748047 (0.747975)
Epoch: [89][600/616]	Loss 2.2755e-01 (2.3826e-01)	Acc 0.757812 (0.748278)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750404)
Training Loss of Epoch 89: 0.23819111951482974
Training Acc of Epoch 89: 0.7483723958333334
Testing Acc of Epoch 89: 0.750404347826087
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3748e-01 (2.3748e-01)	Acc 0.748047 (0.748047)
Epoch: [90][300/616]	Loss 2.3560e-01 (2.3810e-01)	Acc 0.746094 (0.748939)
Epoch: [90][600/616]	Loss 2.3896e-01 (2.3833e-01)	Acc 0.747070 (0.748603)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750330)
Training Loss of Epoch 90: 0.2382620768091543
Training Acc of Epoch 90: 0.7486026422764228
Testing Acc of Epoch 90: 0.7503304347826087
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.3085e-01 (2.3085e-01)	Acc 0.740234 (0.740234)
Epoch: [91][300/616]	Loss 2.3360e-01 (2.3770e-01)	Acc 0.753906 (0.749108)
Epoch: [91][600/616]	Loss 2.3164e-01 (2.3792e-01)	Acc 0.751953 (0.748684)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.749591)
Training Loss of Epoch 91: 0.23783901799015883
Training Acc of Epoch 91: 0.7488138338414634
Testing Acc of Epoch 91: 0.7495913043478261
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2967e-01 (2.2967e-01)	Acc 0.758789 (0.758789)
Epoch: [92][300/616]	Loss 2.3309e-01 (2.3731e-01)	Acc 0.747070 (0.749494)
Epoch: [92][600/616]	Loss 2.2384e-01 (2.3729e-01)	Acc 0.756836 (0.749459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750609)
Training Loss of Epoch 92: 0.237354906181979
Training Acc of Epoch 92: 0.7493997713414634
Testing Acc of Epoch 92: 0.7506086956521739
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2715e-01 (2.2715e-01)	Acc 0.774414 (0.774414)
Epoch: [93][300/616]	Loss 2.3864e-01 (2.3852e-01)	Acc 0.738281 (0.747748)
Epoch: [93][600/616]	Loss 2.3108e-01 (2.3775e-01)	Acc 0.751953 (0.749048)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751870)
Training Loss of Epoch 93: 0.23776414782051147
Training Acc of Epoch 93: 0.7490901295731708
Testing Acc of Epoch 93: 0.7518695652173913
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3196e-01 (2.3196e-01)	Acc 0.746094 (0.746094)
Epoch: [94][300/616]	Loss 2.4267e-01 (2.3810e-01)	Acc 0.745117 (0.748650)
Epoch: [94][600/616]	Loss 2.4370e-01 (2.3765e-01)	Acc 0.745117 (0.749046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.751826)
Training Loss of Epoch 94: 0.23758373083622475
Training Acc of Epoch 94: 0.7490710746951219
Testing Acc of Epoch 94: 0.7518260869565218
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.5134e-01 (2.5134e-01)	Acc 0.729492 (0.729492)
Epoch: [95][300/616]	Loss 2.2749e-01 (2.3697e-01)	Acc 0.761719 (0.749948)
Epoch: [95][600/616]	Loss 2.4393e-01 (2.3711e-01)	Acc 0.727539 (0.749753)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.747765)
Training Loss of Epoch 95: 0.237173757635481
Training Acc of Epoch 95: 0.7497252921747968
Testing Acc of Epoch 95: 0.7477652173913043
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.4176e-01 (2.4176e-01)	Acc 0.750000 (0.750000)
Epoch: [96][300/616]	Loss 2.4005e-01 (2.3741e-01)	Acc 0.756836 (0.749406)
Epoch: [96][600/616]	Loss 2.3762e-01 (2.3731e-01)	Acc 0.741211 (0.749573)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751213)
Training Loss of Epoch 96: 0.237315437415751
Training Acc of Epoch 96: 0.7494966336382114
Testing Acc of Epoch 96: 0.7512130434782609
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.4244e-01 (2.4244e-01)	Acc 0.754883 (0.754883)
Epoch: [97][300/616]	Loss 2.3818e-01 (2.3710e-01)	Acc 0.752930 (0.749825)
Epoch: [97][600/616]	Loss 2.3425e-01 (2.3737e-01)	Acc 0.751953 (0.749539)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.741578)
Training Loss of Epoch 97: 0.23740434414002953
Training Acc of Epoch 97: 0.7495013973577236
Testing Acc of Epoch 97: 0.7415782608695652
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.5486e-01 (2.5486e-01)	Acc 0.741211 (0.741211)
Epoch: [98][300/616]	Loss 2.4602e-01 (2.3779e-01)	Acc 0.739258 (0.749017)
Epoch: [98][600/616]	Loss 2.3658e-01 (2.3741e-01)	Acc 0.766602 (0.749246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750383)
Training Loss of Epoch 98: 0.23739097406709098
Training Acc of Epoch 98: 0.7492203379065041
Testing Acc of Epoch 98: 0.7503826086956522
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2263e-01 (2.2263e-01)	Acc 0.762695 (0.762695)
Epoch: [99][300/616]	Loss 2.3822e-01 (2.3617e-01)	Acc 0.742188 (0.750616)
Epoch: [99][600/616]	Loss 2.3079e-01 (2.3702e-01)	Acc 0.752930 (0.749829)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749852)
Training Loss of Epoch 99: 0.2370211196139576
Training Acc of Epoch 99: 0.7498856707317073
Testing Acc of Epoch 99: 0.7498521739130435
Early stopping not satisfied.
