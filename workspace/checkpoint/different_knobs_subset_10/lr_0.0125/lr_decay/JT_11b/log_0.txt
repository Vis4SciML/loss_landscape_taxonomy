train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_11b
different_width False
resnet18_width 64
weight_precision 11
bias_precision 11
act_precision 14
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_11b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_11b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9896e-01 (4.9896e-01)	Acc 0.299805 (0.299805)
Epoch: [0][300/616]	Loss 2.5474e-01 (2.7232e-01)	Acc 0.728516 (0.711677)
Epoch: [0][600/616]	Loss 2.5448e-01 (2.5797e-01)	Acc 0.716797 (0.728314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748030)
Training Loss of Epoch 0: 0.25765699598847364
Training Acc of Epoch 0: 0.7286712398373983
Testing Acc of Epoch 0: 0.7480304347826087
Model with the best training loss saved! The loss is 0.25765699598847364
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.3762e-01 (2.3762e-01)	Acc 0.743164 (0.743164)
Epoch: [1][300/616]	Loss 2.2931e-01 (2.3951e-01)	Acc 0.760742 (0.748813)
Epoch: [1][600/616]	Loss 2.3853e-01 (2.3891e-01)	Acc 0.758789 (0.748880)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751457)
Training Loss of Epoch 1: 0.23905845466183454
Training Acc of Epoch 1: 0.7487312627032521
Testing Acc of Epoch 1: 0.7514565217391305
Model with the best training loss saved! The loss is 0.23905845466183454
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3527e-01 (2.3527e-01)	Acc 0.763672 (0.763672)
Epoch: [2][300/616]	Loss 2.4211e-01 (2.3756e-01)	Acc 0.746094 (0.749994)
Epoch: [2][600/616]	Loss 2.4108e-01 (2.3725e-01)	Acc 0.745117 (0.750266)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750404)
Training Loss of Epoch 2: 0.23728575931816567
Training Acc of Epoch 2: 0.7502381859756098
Testing Acc of Epoch 2: 0.750404347826087
Model with the best training loss saved! The loss is 0.23728575931816567
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.2777e-01 (2.2777e-01)	Acc 0.761719 (0.761719)
Epoch: [3][300/616]	Loss 2.3661e-01 (2.3533e-01)	Acc 0.758789 (0.751518)
Epoch: [3][600/616]	Loss 2.4599e-01 (2.3538e-01)	Acc 0.743164 (0.751895)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754000)
Training Loss of Epoch 3: 0.2353474414930111
Training Acc of Epoch 3: 0.7519181910569106
Testing Acc of Epoch 3: 0.754
Model with the best training loss saved! The loss is 0.2353474414930111
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4071e-01 (2.4071e-01)	Acc 0.746094 (0.746094)
Epoch: [4][300/616]	Loss 2.3557e-01 (2.3465e-01)	Acc 0.748047 (0.752920)
Epoch: [4][600/616]	Loss 2.4322e-01 (2.3479e-01)	Acc 0.739258 (0.751945)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.754239)
Training Loss of Epoch 4: 0.23476481866545792
Training Acc of Epoch 4: 0.7519959984756097
Testing Acc of Epoch 4: 0.7542391304347826
Model with the best training loss saved! The loss is 0.23476481866545792
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3837e-01 (2.3837e-01)	Acc 0.742188 (0.742188)
Epoch: [5][300/616]	Loss 2.4244e-01 (2.3541e-01)	Acc 0.753906 (0.751152)
Epoch: [5][600/616]	Loss 2.2925e-01 (2.3469e-01)	Acc 0.756836 (0.752185)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752883)
Training Loss of Epoch 5: 0.23471985985108507
Training Acc of Epoch 5: 0.7521373221544716
Testing Acc of Epoch 5: 0.7528826086956522
Model with the best training loss saved! The loss is 0.23471985985108507
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3483e-01 (2.3483e-01)	Acc 0.748047 (0.748047)
Epoch: [6][300/616]	Loss 2.1991e-01 (2.3435e-01)	Acc 0.757812 (0.752060)
Epoch: [6][600/616]	Loss 2.2177e-01 (2.3399e-01)	Acc 0.764648 (0.753203)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753435)
Training Loss of Epoch 6: 0.23399196647531617
Training Acc of Epoch 6: 0.7532298018292682
Testing Acc of Epoch 6: 0.7534347826086957
Model with the best training loss saved! The loss is 0.23399196647531617
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2956e-01 (2.2956e-01)	Acc 0.767578 (0.767578)
Epoch: [7][300/616]	Loss 2.3727e-01 (2.3332e-01)	Acc 0.745117 (0.754150)
Epoch: [7][600/616]	Loss 2.5637e-01 (2.3399e-01)	Acc 0.720703 (0.753442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754343)
Training Loss of Epoch 7: 0.23394305349850072
Training Acc of Epoch 7: 0.7535283282520325
Testing Acc of Epoch 7: 0.7543434782608696
Model with the best training loss saved! The loss is 0.23394305349850072
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5028e-01 (2.5028e-01)	Acc 0.723633 (0.723633)
Epoch: [8][300/616]	Loss 2.4188e-01 (2.3408e-01)	Acc 0.736328 (0.753274)
Epoch: [8][600/616]	Loss 2.3229e-01 (2.3390e-01)	Acc 0.757812 (0.753313)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754478)
Training Loss of Epoch 8: 0.23391727887518038
Training Acc of Epoch 8: 0.7532694994918699
Testing Acc of Epoch 8: 0.7544782608695653
Model with the best training loss saved! The loss is 0.23391727887518038
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3394e-01 (2.3394e-01)	Acc 0.745117 (0.745117)
Epoch: [9][300/616]	Loss 2.3323e-01 (2.3438e-01)	Acc 0.747070 (0.752145)
Epoch: [9][600/616]	Loss 2.2410e-01 (2.3403e-01)	Acc 0.766602 (0.752964)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.755087)
Training Loss of Epoch 9: 0.23386028866942335
Training Acc of Epoch 9: 0.753174225101626
Testing Acc of Epoch 9: 0.7550869565217392
Model with the best training loss saved! The loss is 0.23386028866942335
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2390e-01 (2.2390e-01)	Acc 0.763672 (0.763672)
Epoch: [10][300/616]	Loss 2.3086e-01 (2.3387e-01)	Acc 0.751953 (0.752200)
Epoch: [10][600/616]	Loss 2.3585e-01 (2.3402e-01)	Acc 0.751953 (0.752918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752009)
Training Loss of Epoch 10: 0.23390006752518133
Training Acc of Epoch 10: 0.7530614837398374
Testing Acc of Epoch 10: 0.7520086956521739
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4057e-01 (2.4057e-01)	Acc 0.737305 (0.737305)
Epoch: [11][300/616]	Loss 2.4561e-01 (2.3411e-01)	Acc 0.750977 (0.752660)
Epoch: [11][600/616]	Loss 2.1643e-01 (2.3362e-01)	Acc 0.769531 (0.753287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755974)
Training Loss of Epoch 11: 0.23365154101596616
Training Acc of Epoch 11: 0.7532647357723578
Testing Acc of Epoch 11: 0.7559739130434783
Model with the best training loss saved! The loss is 0.23365154101596616
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3245e-01 (2.3245e-01)	Acc 0.748047 (0.748047)
Epoch: [12][300/616]	Loss 2.3853e-01 (2.3343e-01)	Acc 0.752930 (0.754338)
Epoch: [12][600/616]	Loss 2.3306e-01 (2.3373e-01)	Acc 0.750000 (0.753645)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755974)
Training Loss of Epoch 12: 0.23360547641428506
Training Acc of Epoch 12: 0.753687118902439
Testing Acc of Epoch 12: 0.7559739130434783
Model with the best training loss saved! The loss is 0.23360547641428506
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.1766e-01 (2.1766e-01)	Acc 0.767578 (0.767578)
Epoch: [13][300/616]	Loss 2.2586e-01 (2.3339e-01)	Acc 0.761719 (0.753497)
Epoch: [13][600/616]	Loss 2.5866e-01 (2.3353e-01)	Acc 0.723633 (0.753219)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751065)
Training Loss of Epoch 13: 0.2335866430668327
Training Acc of Epoch 13: 0.7532012195121951
Testing Acc of Epoch 13: 0.7510652173913044
Model with the best training loss saved! The loss is 0.2335866430668327
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2897e-01 (2.2897e-01)	Acc 0.759766 (0.759766)
Epoch: [14][300/616]	Loss 2.1943e-01 (2.3325e-01)	Acc 0.773438 (0.754036)
Epoch: [14][600/616]	Loss 2.3567e-01 (2.3343e-01)	Acc 0.756836 (0.753520)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755430)
Training Loss of Epoch 14: 0.23347487711324924
Training Acc of Epoch 14: 0.753442581300813
Testing Acc of Epoch 14: 0.7554304347826087
Model with the best training loss saved! The loss is 0.23347487711324924
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.2741e-01 (2.2741e-01)	Acc 0.759766 (0.759766)
Epoch: [15][300/616]	Loss 2.5277e-01 (2.3325e-01)	Acc 0.722656 (0.754468)
Epoch: [15][600/616]	Loss 2.2832e-01 (2.3336e-01)	Acc 0.760742 (0.753773)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752670)
Training Loss of Epoch 15: 0.23337680438185127
Training Acc of Epoch 15: 0.7537458714430895
Testing Acc of Epoch 15: 0.7526695652173913
Model with the best training loss saved! The loss is 0.23337680438185127
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3554e-01 (2.3554e-01)	Acc 0.735352 (0.735352)
Epoch: [16][300/616]	Loss 2.3773e-01 (2.3381e-01)	Acc 0.731445 (0.753533)
Epoch: [16][600/616]	Loss 2.2710e-01 (2.3346e-01)	Acc 0.756836 (0.753890)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754970)
Training Loss of Epoch 16: 0.23336460224496638
Training Acc of Epoch 16: 0.7540015243902439
Testing Acc of Epoch 16: 0.7549695652173913
Model with the best training loss saved! The loss is 0.23336460224496638
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3981e-01 (2.3981e-01)	Acc 0.738281 (0.738281)
Epoch: [17][300/616]	Loss 2.3479e-01 (2.3402e-01)	Acc 0.755859 (0.753128)
Epoch: [17][600/616]	Loss 2.2248e-01 (2.3337e-01)	Acc 0.760742 (0.753888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.756400)
Training Loss of Epoch 17: 0.23335864669908352
Training Acc of Epoch 17: 0.7539126016260163
Testing Acc of Epoch 17: 0.7564
Model with the best training loss saved! The loss is 0.23335864669908352
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3417e-01 (2.3417e-01)	Acc 0.752930 (0.752930)
Epoch: [18][300/616]	Loss 2.2998e-01 (2.3389e-01)	Acc 0.754883 (0.752998)
Epoch: [18][600/616]	Loss 2.4195e-01 (2.3325e-01)	Acc 0.740234 (0.753922)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754691)
Training Loss of Epoch 18: 0.23323241074879963
Training Acc of Epoch 18: 0.7539602388211382
Testing Acc of Epoch 18: 0.754691304347826
Model with the best training loss saved! The loss is 0.23323241074879963
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4636e-01 (2.4636e-01)	Acc 0.728516 (0.728516)
Epoch: [19][300/616]	Loss 2.2996e-01 (2.3373e-01)	Acc 0.762695 (0.752888)
Epoch: [19][600/616]	Loss 2.3511e-01 (2.3344e-01)	Acc 0.751953 (0.753624)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754774)
Training Loss of Epoch 19: 0.2334647940426338
Training Acc of Epoch 19: 0.753610899390244
Testing Acc of Epoch 19: 0.7547739130434783
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4620e-01 (2.4620e-01)	Acc 0.738281 (0.738281)
Epoch: [20][300/616]	Loss 2.3090e-01 (2.3328e-01)	Acc 0.739258 (0.754386)
Epoch: [20][600/616]	Loss 2.3992e-01 (2.3335e-01)	Acc 0.746094 (0.753758)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754374)
Training Loss of Epoch 20: 0.23331256033928413
Training Acc of Epoch 20: 0.7538157393292683
Testing Acc of Epoch 20: 0.7543739130434782
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3219e-01 (2.3219e-01)	Acc 0.768555 (0.768555)
Epoch: [21][300/616]	Loss 2.3488e-01 (2.3283e-01)	Acc 0.755859 (0.754954)
Epoch: [21][600/616]	Loss 2.4048e-01 (2.3296e-01)	Acc 0.754883 (0.754285)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753570)
Training Loss of Epoch 21: 0.23309133295121232
Training Acc of Epoch 21: 0.7541904852642276
Testing Acc of Epoch 21: 0.7535695652173913
Model with the best training loss saved! The loss is 0.23309133295121232
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3958e-01 (2.3958e-01)	Acc 0.745117 (0.745117)
Epoch: [22][300/616]	Loss 2.4255e-01 (2.3228e-01)	Acc 0.739258 (0.754863)
Epoch: [22][600/616]	Loss 2.3363e-01 (2.3329e-01)	Acc 0.753906 (0.753942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755548)
Training Loss of Epoch 22: 0.23328792028795414
Training Acc of Epoch 22: 0.7539745299796748
Testing Acc of Epoch 22: 0.7555478260869565
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2682e-01 (2.2682e-01)	Acc 0.761719 (0.761719)
Epoch: [23][300/616]	Loss 2.3395e-01 (2.3219e-01)	Acc 0.752930 (0.755697)
Epoch: [23][600/616]	Loss 2.3023e-01 (2.3297e-01)	Acc 0.753906 (0.754673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757378)
Training Loss of Epoch 23: 0.23296969787376684
Training Acc of Epoch 23: 0.7546239837398374
Testing Acc of Epoch 23: 0.7573782608695652
Model with the best training loss saved! The loss is 0.23296969787376684
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2425e-01 (2.2425e-01)	Acc 0.773438 (0.773438)
Epoch: [24][300/616]	Loss 2.3654e-01 (2.3287e-01)	Acc 0.728516 (0.754721)
Epoch: [24][600/616]	Loss 2.2264e-01 (2.3335e-01)	Acc 0.773438 (0.754043)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754665)
Training Loss of Epoch 24: 0.23335126192589117
Training Acc of Epoch 24: 0.7540840955284552
Testing Acc of Epoch 24: 0.7546652173913043
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4159e-01 (2.4159e-01)	Acc 0.744141 (0.744141)
Epoch: [25][300/616]	Loss 2.2670e-01 (2.3226e-01)	Acc 0.763672 (0.754607)
Epoch: [25][600/616]	Loss 2.2449e-01 (2.3336e-01)	Acc 0.763672 (0.753677)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751883)
Training Loss of Epoch 25: 0.23331356184269356
Training Acc of Epoch 25: 0.7537442835365854
Testing Acc of Epoch 25: 0.7518826086956522
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4734e-01 (2.4734e-01)	Acc 0.730469 (0.730469)
Epoch: [26][300/616]	Loss 2.3223e-01 (2.3281e-01)	Acc 0.757812 (0.754263)
Epoch: [26][600/616]	Loss 2.3647e-01 (2.3319e-01)	Acc 0.765625 (0.754017)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755474)
Training Loss of Epoch 26: 0.23317072493274038
Training Acc of Epoch 26: 0.7540412220528455
Testing Acc of Epoch 26: 0.7554739130434782
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.3880e-01 (2.3880e-01)	Acc 0.752930 (0.752930)
Epoch: [27][300/616]	Loss 2.5093e-01 (2.3299e-01)	Acc 0.718750 (0.753961)
Epoch: [27][600/616]	Loss 2.1889e-01 (2.3336e-01)	Acc 0.777344 (0.753521)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.754687)
Training Loss of Epoch 27: 0.23319604782069603
Training Acc of Epoch 27: 0.7537474593495935
Testing Acc of Epoch 27: 0.7546869565217391
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.2890e-01 (2.2890e-01)	Acc 0.759766 (0.759766)
Epoch: [28][300/616]	Loss 2.3383e-01 (2.3342e-01)	Acc 0.766602 (0.753679)
Epoch: [28][600/616]	Loss 2.4095e-01 (2.3307e-01)	Acc 0.738281 (0.754033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.756313)
Training Loss of Epoch 28: 0.23302496218584418
Training Acc of Epoch 28: 0.7540729801829268
Testing Acc of Epoch 28: 0.7563130434782609
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4509e-01 (2.4509e-01)	Acc 0.729492 (0.729492)
Epoch: [29][300/616]	Loss 2.3151e-01 (2.3352e-01)	Acc 0.766602 (0.753858)
Epoch: [29][600/616]	Loss 2.2139e-01 (2.3330e-01)	Acc 0.777344 (0.754139)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756274)
Training Loss of Epoch 29: 0.23327591070314732
Training Acc of Epoch 29: 0.754201600609756
Testing Acc of Epoch 29: 0.7562739130434782
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2982e-01 (2.2982e-01)	Acc 0.763672 (0.763672)
Epoch: [30][300/616]	Loss 2.2898e-01 (2.3303e-01)	Acc 0.750000 (0.753702)
Epoch: [30][600/616]	Loss 2.3518e-01 (2.3306e-01)	Acc 0.753906 (0.754194)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753800)
Training Loss of Epoch 30: 0.23309040050196453
Training Acc of Epoch 30: 0.7541444359756098
Testing Acc of Epoch 30: 0.7538
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3656e-01 (2.3656e-01)	Acc 0.745117 (0.745117)
Epoch: [31][300/616]	Loss 2.1969e-01 (2.3319e-01)	Acc 0.771484 (0.754153)
Epoch: [31][600/616]	Loss 2.4155e-01 (2.3328e-01)	Acc 0.741211 (0.754439)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756017)
Training Loss of Epoch 31: 0.233305748473338
Training Acc of Epoch 31: 0.7544223196138211
Testing Acc of Epoch 31: 0.7560173913043479
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4824e-01 (2.4824e-01)	Acc 0.740234 (0.740234)
Epoch: [32][300/616]	Loss 2.3486e-01 (2.3299e-01)	Acc 0.756836 (0.754309)
Epoch: [32][600/616]	Loss 2.3494e-01 (2.3310e-01)	Acc 0.753906 (0.754254)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.755974)
Training Loss of Epoch 32: 0.23321847140304441
Training Acc of Epoch 32: 0.7541460238821138
Testing Acc of Epoch 32: 0.7559739130434783
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2745e-01 (2.2745e-01)	Acc 0.775391 (0.775391)
Epoch: [33][300/616]	Loss 2.3980e-01 (2.3251e-01)	Acc 0.740234 (0.755110)
Epoch: [33][600/616]	Loss 2.3871e-01 (2.3294e-01)	Acc 0.755859 (0.754506)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756783)
Training Loss of Epoch 33: 0.2329940570563805
Training Acc of Epoch 33: 0.7544763084349594
Testing Acc of Epoch 33: 0.7567826086956522
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3045e-01 (2.3045e-01)	Acc 0.751953 (0.751953)
Epoch: [34][300/616]	Loss 2.2079e-01 (2.3280e-01)	Acc 0.772461 (0.754046)
Epoch: [34][600/616]	Loss 2.3897e-01 (2.3299e-01)	Acc 0.758789 (0.753897)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.754883)
Training Loss of Epoch 34: 0.23298708631740353
Training Acc of Epoch 34: 0.7539030741869919
Testing Acc of Epoch 34: 0.7548826086956522
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4082e-01 (2.4082e-01)	Acc 0.752930 (0.752930)
Epoch: [35][300/616]	Loss 2.3308e-01 (2.3301e-01)	Acc 0.745117 (0.754704)
Epoch: [35][600/616]	Loss 2.5007e-01 (2.3339e-01)	Acc 0.737305 (0.754005)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754439)
Training Loss of Epoch 35: 0.233327212808578
Training Acc of Epoch 35: 0.7540872713414634
Testing Acc of Epoch 35: 0.7544391304347826
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.1814e-01 (2.1814e-01)	Acc 0.762695 (0.762695)
Epoch: [36][300/616]	Loss 2.4625e-01 (2.3288e-01)	Acc 0.743164 (0.754237)
Epoch: [36][600/616]	Loss 2.3862e-01 (2.3297e-01)	Acc 0.749023 (0.754335)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754204)
Training Loss of Epoch 36: 0.23293680975107642
Training Acc of Epoch 36: 0.7543842098577236
Testing Acc of Epoch 36: 0.7542043478260869
Model with the best training loss saved! The loss is 0.23293680975107642
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.2363e-01 (2.2363e-01)	Acc 0.760742 (0.760742)
Epoch: [37][300/616]	Loss 2.3619e-01 (2.3363e-01)	Acc 0.754883 (0.753296)
Epoch: [37][600/616]	Loss 2.1100e-01 (2.3313e-01)	Acc 0.782227 (0.753966)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753861)
Training Loss of Epoch 37: 0.23313505400002488
Training Acc of Epoch 37: 0.7539443597560975
Testing Acc of Epoch 37: 0.7538608695652174
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.2161e-01 (2.2161e-01)	Acc 0.768555 (0.768555)
Epoch: [38][300/616]	Loss 2.2739e-01 (2.3317e-01)	Acc 0.757812 (0.754140)
Epoch: [38][600/616]	Loss 2.3718e-01 (2.3309e-01)	Acc 0.750977 (0.754134)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755204)
Training Loss of Epoch 38: 0.23307611275979173
Training Acc of Epoch 38: 0.7541888973577235
Testing Acc of Epoch 38: 0.7552043478260869
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.3353e-01 (2.3353e-01)	Acc 0.753906 (0.753906)
Epoch: [39][300/616]	Loss 2.2169e-01 (2.3368e-01)	Acc 0.788086 (0.753890)
Epoch: [39][600/616]	Loss 2.3722e-01 (2.3324e-01)	Acc 0.757812 (0.754009)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754691)
Training Loss of Epoch 39: 0.23318720854879396
Training Acc of Epoch 39: 0.7540825076219512
Testing Acc of Epoch 39: 0.754691304347826
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.3471e-01 (2.3471e-01)	Acc 0.750977 (0.750977)
Epoch: [40][300/616]	Loss 2.4294e-01 (2.3350e-01)	Acc 0.742188 (0.753799)
Epoch: [40][600/616]	Loss 2.3602e-01 (2.3291e-01)	Acc 0.751953 (0.754592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754896)
Training Loss of Epoch 40: 0.23285156391500458
Training Acc of Epoch 40: 0.7545985772357724
Testing Acc of Epoch 40: 0.7548956521739131
Model with the best training loss saved! The loss is 0.23285156391500458
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.3359e-01 (2.3359e-01)	Acc 0.757812 (0.757812)
Epoch: [41][300/616]	Loss 2.2567e-01 (2.3326e-01)	Acc 0.760742 (0.753926)
Epoch: [41][600/616]	Loss 2.2853e-01 (2.3292e-01)	Acc 0.761719 (0.754329)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755039)
Training Loss of Epoch 41: 0.23284558418805038
Training Acc of Epoch 41: 0.754419143800813
Testing Acc of Epoch 41: 0.7550391304347827
Model with the best training loss saved! The loss is 0.23284558418805038
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.2363e-01 (2.2363e-01)	Acc 0.758789 (0.758789)
Epoch: [42][300/616]	Loss 2.4041e-01 (2.3313e-01)	Acc 0.763672 (0.754455)
Epoch: [42][600/616]	Loss 2.3854e-01 (2.3333e-01)	Acc 0.741211 (0.754126)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753770)
Training Loss of Epoch 42: 0.2333095963892898
Training Acc of Epoch 42: 0.7541237931910569
Testing Acc of Epoch 42: 0.7537695652173914
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3423e-01 (2.3423e-01)	Acc 0.758789 (0.758789)
Epoch: [43][300/616]	Loss 2.2947e-01 (2.3257e-01)	Acc 0.766602 (0.754854)
Epoch: [43][600/616]	Loss 2.2933e-01 (2.3264e-01)	Acc 0.755859 (0.754925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754487)
Training Loss of Epoch 43: 0.23268547860102926
Training Acc of Epoch 43: 0.7548510543699187
Testing Acc of Epoch 43: 0.7544869565217391
Model with the best training loss saved! The loss is 0.23268547860102926
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.1230e-01 (2.1230e-01)	Acc 0.774414 (0.774414)
Epoch: [44][300/616]	Loss 2.3626e-01 (2.3326e-01)	Acc 0.751953 (0.754257)
Epoch: [44][600/616]	Loss 2.3505e-01 (2.3310e-01)	Acc 0.753906 (0.754257)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754635)
Training Loss of Epoch 44: 0.23314265725089284
Training Acc of Epoch 44: 0.7542206554878049
Testing Acc of Epoch 44: 0.7546347826086957
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2325e-01 (2.2325e-01)	Acc 0.773438 (0.773438)
Epoch: [45][300/616]	Loss 2.2381e-01 (2.3323e-01)	Acc 0.764648 (0.753686)
Epoch: [45][600/616]	Loss 2.2275e-01 (2.3304e-01)	Acc 0.767578 (0.754260)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754522)
Training Loss of Epoch 45: 0.23299746792006298
Training Acc of Epoch 45: 0.7543032266260162
Testing Acc of Epoch 45: 0.7545217391304347
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.5235e-01 (2.5235e-01)	Acc 0.729492 (0.729492)
Epoch: [46][300/616]	Loss 2.4118e-01 (2.3250e-01)	Acc 0.743164 (0.754867)
Epoch: [46][600/616]	Loss 2.2462e-01 (2.3265e-01)	Acc 0.758789 (0.754673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756404)
Training Loss of Epoch 46: 0.2327510836647778
Training Acc of Epoch 46: 0.7545382367886179
Testing Acc of Epoch 46: 0.756404347826087
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.3077e-01 (2.3077e-01)	Acc 0.767578 (0.767578)
Epoch: [47][300/616]	Loss 2.3083e-01 (2.3259e-01)	Acc 0.760742 (0.754708)
Epoch: [47][600/616]	Loss 2.2845e-01 (2.3311e-01)	Acc 0.764648 (0.754046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756913)
Training Loss of Epoch 47: 0.23303477507296616
Training Acc of Epoch 47: 0.7541126778455285
Testing Acc of Epoch 47: 0.7569130434782608
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.3661e-01 (2.3661e-01)	Acc 0.728516 (0.728516)
Epoch: [48][300/616]	Loss 2.3026e-01 (2.3258e-01)	Acc 0.752930 (0.755022)
Epoch: [48][600/616]	Loss 2.2758e-01 (2.3302e-01)	Acc 0.753906 (0.754065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755448)
Training Loss of Epoch 48: 0.2328654336008599
Training Acc of Epoch 48: 0.7542635289634146
Testing Acc of Epoch 48: 0.7554478260869565
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3820e-01 (2.3820e-01)	Acc 0.747070 (0.747070)
Epoch: [49][300/616]	Loss 2.4102e-01 (2.3317e-01)	Acc 0.757812 (0.754263)
Epoch: [49][600/616]	Loss 2.3408e-01 (2.3289e-01)	Acc 0.748047 (0.754447)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754543)
Training Loss of Epoch 49: 0.2328750095958632
Training Acc of Epoch 49: 0.7544794842479675
Testing Acc of Epoch 49: 0.7545434782608695
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4037e-01 (2.4037e-01)	Acc 0.729492 (0.729492)
Epoch: [50][300/616]	Loss 2.2999e-01 (2.3332e-01)	Acc 0.749023 (0.753660)
Epoch: [50][600/616]	Loss 2.3674e-01 (2.3304e-01)	Acc 0.745117 (0.754426)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754826)
Training Loss of Epoch 50: 0.23307769674111187
Training Acc of Epoch 50: 0.754322281504065
Testing Acc of Epoch 50: 0.7548260869565218
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3948e-01 (2.3948e-01)	Acc 0.733398 (0.733398)
Epoch: [51][300/616]	Loss 2.5252e-01 (2.3267e-01)	Acc 0.731445 (0.755064)
Epoch: [51][600/616]	Loss 2.2919e-01 (2.3299e-01)	Acc 0.767578 (0.754400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755396)
Training Loss of Epoch 51: 0.23295771387534414
Training Acc of Epoch 51: 0.7543985010162602
Testing Acc of Epoch 51: 0.755395652173913
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.2081e-01 (2.2081e-01)	Acc 0.761719 (0.761719)
Epoch: [52][300/616]	Loss 2.1969e-01 (2.3310e-01)	Acc 0.770508 (0.753994)
Epoch: [52][600/616]	Loss 2.2981e-01 (2.3285e-01)	Acc 0.750977 (0.754286)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.755309)
Training Loss of Epoch 52: 0.23290190653103154
Training Acc of Epoch 52: 0.754176194105691
Testing Acc of Epoch 52: 0.755308695652174
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3120e-01 (2.3120e-01)	Acc 0.762695 (0.762695)
Epoch: [53][300/616]	Loss 2.4193e-01 (2.3254e-01)	Acc 0.746094 (0.754351)
Epoch: [53][600/616]	Loss 2.4450e-01 (2.3294e-01)	Acc 0.746094 (0.753984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.756270)
Training Loss of Epoch 53: 0.2329322553989364
Training Acc of Epoch 53: 0.7539919969512195
Testing Acc of Epoch 53: 0.7562695652173913
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.2889e-01 (2.2889e-01)	Acc 0.757812 (0.757812)
Epoch: [54][300/616]	Loss 2.2208e-01 (2.3316e-01)	Acc 0.760742 (0.753283)
Epoch: [54][600/616]	Loss 2.1699e-01 (2.3293e-01)	Acc 0.774414 (0.754386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755452)
Training Loss of Epoch 54: 0.2328667168694783
Training Acc of Epoch 54: 0.7544366107723577
Testing Acc of Epoch 54: 0.7554521739130434
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3391e-01 (2.3391e-01)	Acc 0.758789 (0.758789)
Epoch: [55][300/616]	Loss 2.4431e-01 (2.3291e-01)	Acc 0.741211 (0.753981)
Epoch: [55][600/616]	Loss 2.4017e-01 (2.3285e-01)	Acc 0.737305 (0.754197)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755943)
Training Loss of Epoch 55: 0.23285999874758526
Training Acc of Epoch 55: 0.754201600609756
Testing Acc of Epoch 55: 0.7559434782608696
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4790e-01 (2.4790e-01)	Acc 0.745117 (0.745117)
Epoch: [56][300/616]	Loss 2.3981e-01 (2.3313e-01)	Acc 0.746094 (0.754010)
Epoch: [56][600/616]	Loss 2.4269e-01 (2.3329e-01)	Acc 0.746094 (0.753832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754635)
Training Loss of Epoch 56: 0.23327711898136913
Training Acc of Epoch 56: 0.7539141895325203
Testing Acc of Epoch 56: 0.7546347826086957
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.2952e-01 (2.2952e-01)	Acc 0.762695 (0.762695)
Epoch: [57][300/616]	Loss 2.3268e-01 (2.3316e-01)	Acc 0.748047 (0.754192)
Epoch: [57][600/616]	Loss 2.2784e-01 (2.3287e-01)	Acc 0.762695 (0.754087)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753430)
Training Loss of Epoch 57: 0.2328448738993668
Training Acc of Epoch 57: 0.7541603150406504
Testing Acc of Epoch 57: 0.7534304347826087
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3180e-01 (2.3180e-01)	Acc 0.757812 (0.757812)
Epoch: [58][300/616]	Loss 2.2587e-01 (2.3270e-01)	Acc 0.756836 (0.754915)
Epoch: [58][600/616]	Loss 2.2469e-01 (2.3317e-01)	Acc 0.766602 (0.754189)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754304)
Training Loss of Epoch 58: 0.23311631543849543
Training Acc of Epoch 58: 0.7542127159552846
Testing Acc of Epoch 58: 0.7543043478260869
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3857e-01 (2.3857e-01)	Acc 0.747070 (0.747070)
Epoch: [59][300/616]	Loss 2.2122e-01 (2.3233e-01)	Acc 0.769531 (0.755324)
Epoch: [59][600/616]	Loss 2.3930e-01 (2.3339e-01)	Acc 0.753906 (0.754238)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754748)
Training Loss of Epoch 59: 0.23334203570838866
Training Acc of Epoch 59: 0.7542063643292682
Testing Acc of Epoch 59: 0.7547478260869566
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.2449e-01 (2.2449e-01)	Acc 0.771484 (0.771484)
Epoch: [60][300/616]	Loss 2.4202e-01 (2.3305e-01)	Acc 0.745117 (0.753815)
Epoch: [60][600/616]	Loss 2.2275e-01 (2.3286e-01)	Acc 0.769531 (0.754410)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754730)
Training Loss of Epoch 60: 0.23288907607396445
Training Acc of Epoch 60: 0.7543191056910569
Testing Acc of Epoch 60: 0.7547304347826087
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.2666e-01 (2.2666e-01)	Acc 0.761719 (0.761719)
Epoch: [61][300/616]	Loss 2.4323e-01 (2.3354e-01)	Acc 0.737305 (0.754172)
Epoch: [61][600/616]	Loss 2.2535e-01 (2.3311e-01)	Acc 0.761719 (0.754264)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756100)
Training Loss of Epoch 61: 0.23314113096008457
Training Acc of Epoch 61: 0.754222243394309
Testing Acc of Epoch 61: 0.7561
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.2582e-01 (2.2582e-01)	Acc 0.766602 (0.766602)
Epoch: [62][300/616]	Loss 2.1912e-01 (2.3287e-01)	Acc 0.759766 (0.754208)
Epoch: [62][600/616]	Loss 2.3131e-01 (2.3281e-01)	Acc 0.754883 (0.754724)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755365)
Training Loss of Epoch 62: 0.23283773478938313
Training Acc of Epoch 62: 0.7546525660569106
Testing Acc of Epoch 62: 0.7553652173913044
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3018e-01 (2.3018e-01)	Acc 0.762695 (0.762695)
Epoch: [63][300/616]	Loss 2.2083e-01 (2.3225e-01)	Acc 0.767578 (0.754841)
Epoch: [63][600/616]	Loss 2.2937e-01 (2.3298e-01)	Acc 0.749023 (0.754189)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754600)
Training Loss of Epoch 63: 0.23296970412498566
Training Acc of Epoch 63: 0.7541936610772357
Testing Acc of Epoch 63: 0.7546
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3733e-01 (2.3733e-01)	Acc 0.745117 (0.745117)
Epoch: [64][300/616]	Loss 2.2310e-01 (2.3266e-01)	Acc 0.752930 (0.754685)
Epoch: [64][600/616]	Loss 2.3747e-01 (2.3264e-01)	Acc 0.750977 (0.754598)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.749391)
Training Loss of Epoch 64: 0.232701955190519
Training Acc of Epoch 64: 0.7545191819105691
Testing Acc of Epoch 64: 0.7493913043478261
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.2818e-01 (2.2818e-01)	Acc 0.755859 (0.755859)
Epoch: [65][300/616]	Loss 2.1948e-01 (2.3228e-01)	Acc 0.779297 (0.755123)
Epoch: [65][600/616]	Loss 2.2898e-01 (2.3304e-01)	Acc 0.754883 (0.754191)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757204)
Training Loss of Epoch 65: 0.23303328764632467
Training Acc of Epoch 65: 0.7542190675813009
Testing Acc of Epoch 65: 0.7572043478260869
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3386e-01 (2.3386e-01)	Acc 0.749023 (0.749023)
Epoch: [66][300/616]	Loss 2.2639e-01 (2.3292e-01)	Acc 0.769531 (0.753900)
Epoch: [66][600/616]	Loss 2.4016e-01 (2.3304e-01)	Acc 0.744141 (0.753900)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.755417)
Training Loss of Epoch 66: 0.23303663519824425
Training Acc of Epoch 66: 0.7539126016260163
Testing Acc of Epoch 66: 0.7554173913043478
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3737e-01 (2.3737e-01)	Acc 0.746094 (0.746094)
Epoch: [67][300/616]	Loss 2.3876e-01 (2.3242e-01)	Acc 0.750000 (0.755064)
Epoch: [67][600/616]	Loss 2.4593e-01 (2.3306e-01)	Acc 0.725586 (0.754156)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755857)
Training Loss of Epoch 67: 0.2330887825023837
Training Acc of Epoch 67: 0.7541222052845529
Testing Acc of Epoch 67: 0.7558565217391304
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3622e-01 (2.3622e-01)	Acc 0.761719 (0.761719)
Epoch: [68][300/616]	Loss 2.2766e-01 (2.3335e-01)	Acc 0.763672 (0.753848)
Epoch: [68][600/616]	Loss 2.2921e-01 (2.3309e-01)	Acc 0.747070 (0.754264)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754965)
Training Loss of Epoch 68: 0.23296662859800385
Training Acc of Epoch 68: 0.7544064405487805
Testing Acc of Epoch 68: 0.7549652173913044
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.3128e-01 (2.3128e-01)	Acc 0.756836 (0.756836)
Epoch: [69][300/616]	Loss 2.3597e-01 (2.3358e-01)	Acc 0.756836 (0.753660)
Epoch: [69][600/616]	Loss 2.4134e-01 (2.3319e-01)	Acc 0.738281 (0.754114)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754574)
Training Loss of Epoch 69: 0.2331549279573487
Training Acc of Epoch 69: 0.7541539634146341
Testing Acc of Epoch 69: 0.7545739130434782
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3972e-01 (2.3972e-01)	Acc 0.752930 (0.752930)
Epoch: [70][300/616]	Loss 2.4002e-01 (2.3279e-01)	Acc 0.740234 (0.754133)
Epoch: [70][600/616]	Loss 2.1928e-01 (2.3344e-01)	Acc 0.774414 (0.753438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755270)
Training Loss of Epoch 70: 0.23341195934671696
Training Acc of Epoch 70: 0.7534933943089431
Testing Acc of Epoch 70: 0.7552695652173913
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4776e-01 (2.4776e-01)	Acc 0.736328 (0.736328)
Epoch: [71][300/616]	Loss 2.4032e-01 (2.3251e-01)	Acc 0.750000 (0.754743)
Epoch: [71][600/616]	Loss 2.2509e-01 (2.3299e-01)	Acc 0.767578 (0.754334)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752413)
Training Loss of Epoch 71: 0.23299280700644825
Training Acc of Epoch 71: 0.7543635670731708
Testing Acc of Epoch 71: 0.7524130434782609
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4532e-01 (2.4532e-01)	Acc 0.750000 (0.750000)
Epoch: [72][300/616]	Loss 2.2516e-01 (2.3333e-01)	Acc 0.763672 (0.753757)
Epoch: [72][600/616]	Loss 2.2845e-01 (2.3323e-01)	Acc 0.760742 (0.753892)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757561)
Training Loss of Epoch 72: 0.23332102402923552
Training Acc of Epoch 72: 0.75385543699187
Testing Acc of Epoch 72: 0.7575608695652174
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4267e-01 (2.4267e-01)	Acc 0.749023 (0.749023)
Epoch: [73][300/616]	Loss 2.1814e-01 (2.3256e-01)	Acc 0.778320 (0.754513)
Epoch: [73][600/616]	Loss 2.4894e-01 (2.3262e-01)	Acc 0.741211 (0.754444)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754070)
Training Loss of Epoch 73: 0.23267626045196038
Training Acc of Epoch 73: 0.7544127921747967
Testing Acc of Epoch 73: 0.7540695652173913
Model with the best training loss saved! The loss is 0.23267626045196038
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3232e-01 (2.3232e-01)	Acc 0.745117 (0.745117)
Epoch: [74][300/616]	Loss 2.3455e-01 (2.3311e-01)	Acc 0.751953 (0.754120)
Epoch: [74][600/616]	Loss 2.3411e-01 (2.3295e-01)	Acc 0.751953 (0.754511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755065)
Training Loss of Epoch 74: 0.23293126053441832
Training Acc of Epoch 74: 0.7545779344512196
Testing Acc of Epoch 74: 0.7550652173913044
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.2389e-01 (2.2389e-01)	Acc 0.755859 (0.755859)
Epoch: [75][300/616]	Loss 2.2174e-01 (2.2857e-01)	Acc 0.760742 (0.758030)
Epoch: [75][600/616]	Loss 2.2377e-01 (2.2872e-01)	Acc 0.765625 (0.757741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758661)
Training Loss of Epoch 75: 0.22868536572630813
Training Acc of Epoch 75: 0.7577458079268292
Testing Acc of Epoch 75: 0.7586608695652174
Model with the best training loss saved! The loss is 0.22868536572630813
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2306e-01 (2.2306e-01)	Acc 0.753906 (0.753906)
Epoch: [76][300/616]	Loss 2.2232e-01 (2.2868e-01)	Acc 0.769531 (0.758396)
Epoch: [76][600/616]	Loss 2.2731e-01 (2.2836e-01)	Acc 0.759766 (0.758042)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758635)
Training Loss of Epoch 76: 0.2283556324437382
Training Acc of Epoch 76: 0.7580173399390244
Testing Acc of Epoch 76: 0.7586347826086957
Model with the best training loss saved! The loss is 0.2283556324437382
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.2012e-01 (2.2012e-01)	Acc 0.763672 (0.763672)
Epoch: [77][300/616]	Loss 2.2482e-01 (2.2818e-01)	Acc 0.754883 (0.758225)
Epoch: [77][600/616]	Loss 2.2912e-01 (2.2811e-01)	Acc 0.766602 (0.758302)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.758600)
Training Loss of Epoch 77: 0.22817191908030007
Training Acc of Epoch 77: 0.7582063008130081
Testing Acc of Epoch 77: 0.7586
Model with the best training loss saved! The loss is 0.22817191908030007
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3676e-01 (2.3676e-01)	Acc 0.751953 (0.751953)
Epoch: [78][300/616]	Loss 2.2555e-01 (2.2839e-01)	Acc 0.760742 (0.758069)
Epoch: [78][600/616]	Loss 2.2025e-01 (2.2806e-01)	Acc 0.757812 (0.758456)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.758570)
Training Loss of Epoch 78: 0.22805357262855622
Training Acc of Epoch 78: 0.7584889481707318
Testing Acc of Epoch 78: 0.7585695652173913
Model with the best training loss saved! The loss is 0.22805357262855622
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.1642e-01 (2.1642e-01)	Acc 0.765625 (0.765625)
Epoch: [79][300/616]	Loss 2.2416e-01 (2.2754e-01)	Acc 0.762695 (0.759165)
Epoch: [79][600/616]	Loss 2.3744e-01 (2.2789e-01)	Acc 0.751953 (0.758562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758361)
Training Loss of Epoch 79: 0.2279151639802669
Training Acc of Epoch 79: 0.7585651676829268
Testing Acc of Epoch 79: 0.7583608695652174
Model with the best training loss saved! The loss is 0.2279151639802669
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2463e-01 (2.2463e-01)	Acc 0.753906 (0.753906)
Epoch: [80][300/616]	Loss 2.2468e-01 (2.2794e-01)	Acc 0.759766 (0.758565)
Epoch: [80][600/616]	Loss 2.2851e-01 (2.2784e-01)	Acc 0.766602 (0.758654)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759470)
Training Loss of Epoch 80: 0.22783833601610448
Training Acc of Epoch 80: 0.7586270960365854
Testing Acc of Epoch 80: 0.7594695652173913
Model with the best training loss saved! The loss is 0.22783833601610448
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3398e-01 (2.3398e-01)	Acc 0.734375 (0.734375)
Epoch: [81][300/616]	Loss 2.2041e-01 (2.2805e-01)	Acc 0.769531 (0.758377)
Epoch: [81][600/616]	Loss 2.3209e-01 (2.2792e-01)	Acc 0.747070 (0.758705)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759065)
Training Loss of Epoch 81: 0.22791419334527924
Training Acc of Epoch 81: 0.758690612296748
Testing Acc of Epoch 81: 0.7590652173913044
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2496e-01 (2.2496e-01)	Acc 0.779297 (0.779297)
Epoch: [82][300/616]	Loss 2.2278e-01 (2.2813e-01)	Acc 0.778320 (0.758659)
Epoch: [82][600/616]	Loss 2.3467e-01 (2.2778e-01)	Acc 0.753906 (0.758714)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760000)
Training Loss of Epoch 82: 0.22778852266993949
Training Acc of Epoch 82: 0.758666793699187
Testing Acc of Epoch 82: 0.76
Model with the best training loss saved! The loss is 0.22778852266993949
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3853e-01 (2.3853e-01)	Acc 0.757812 (0.757812)
Epoch: [83][300/616]	Loss 2.3038e-01 (2.2829e-01)	Acc 0.765625 (0.758085)
Epoch: [83][600/616]	Loss 2.3415e-01 (2.2786e-01)	Acc 0.755859 (0.758649)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759243)
Training Loss of Epoch 83: 0.22776506010109815
Training Acc of Epoch 83: 0.7587906504065041
Testing Acc of Epoch 83: 0.7592434782608696
Model with the best training loss saved! The loss is 0.22776506010109815
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.2458e-01 (2.2458e-01)	Acc 0.766602 (0.766602)
Epoch: [84][300/616]	Loss 2.3016e-01 (2.2746e-01)	Acc 0.759766 (0.759610)
Epoch: [84][600/616]	Loss 2.3463e-01 (2.2769e-01)	Acc 0.754883 (0.758901)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759491)
Training Loss of Epoch 84: 0.2277341680555809
Training Acc of Epoch 84: 0.7588255843495935
Testing Acc of Epoch 84: 0.7594913043478261
Model with the best training loss saved! The loss is 0.2277341680555809
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3066e-01 (2.3066e-01)	Acc 0.750977 (0.750977)
Epoch: [85][300/616]	Loss 2.1692e-01 (2.2770e-01)	Acc 0.766602 (0.758406)
Epoch: [85][600/616]	Loss 2.3174e-01 (2.2772e-01)	Acc 0.748047 (0.758908)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759757)
Training Loss of Epoch 85: 0.2277341321958759
Training Acc of Epoch 85: 0.7588589303861789
Testing Acc of Epoch 85: 0.7597565217391304
Model with the best training loss saved! The loss is 0.2277341321958759
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.2037e-01 (2.2037e-01)	Acc 0.770508 (0.770508)
Epoch: [86][300/616]	Loss 2.2346e-01 (2.2745e-01)	Acc 0.753906 (0.759337)
Epoch: [86][600/616]	Loss 2.2531e-01 (2.2750e-01)	Acc 0.768555 (0.759134)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759874)
Training Loss of Epoch 86: 0.22763312185198312
Training Acc of Epoch 86: 0.7589542047764227
Testing Acc of Epoch 86: 0.7598739130434783
Model with the best training loss saved! The loss is 0.22763312185198312
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3148e-01 (2.3148e-01)	Acc 0.759766 (0.759766)
Epoch: [87][300/616]	Loss 2.3175e-01 (2.2773e-01)	Acc 0.751953 (0.758601)
Epoch: [87][600/616]	Loss 2.2989e-01 (2.2779e-01)	Acc 0.753906 (0.758800)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758291)
Training Loss of Epoch 87: 0.22766375362388486
Training Acc of Epoch 87: 0.7589049796747968
Testing Acc of Epoch 87: 0.7582913043478261
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.1151e-01 (2.1151e-01)	Acc 0.782227 (0.782227)
Epoch: [88][300/616]	Loss 2.2615e-01 (2.2755e-01)	Acc 0.757812 (0.759516)
Epoch: [88][600/616]	Loss 2.1561e-01 (2.2766e-01)	Acc 0.786133 (0.759197)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759196)
Training Loss of Epoch 88: 0.227620811118343
Training Acc of Epoch 88: 0.759130462398374
Testing Acc of Epoch 88: 0.759195652173913
Model with the best training loss saved! The loss is 0.227620811118343
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2521e-01 (2.2521e-01)	Acc 0.761719 (0.761719)
Epoch: [89][300/616]	Loss 2.2897e-01 (2.2766e-01)	Acc 0.756836 (0.758747)
Epoch: [89][600/616]	Loss 2.3564e-01 (2.2755e-01)	Acc 0.743164 (0.758901)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759609)
Training Loss of Epoch 89: 0.22751408076383234
Training Acc of Epoch 89: 0.7589637322154471
Testing Acc of Epoch 89: 0.7596086956521739
Model with the best training loss saved! The loss is 0.22751408076383234
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2339e-01 (2.2339e-01)	Acc 0.755859 (0.755859)
Epoch: [90][300/616]	Loss 2.2549e-01 (2.2722e-01)	Acc 0.764648 (0.759798)
Epoch: [90][600/616]	Loss 2.1821e-01 (2.2749e-01)	Acc 0.762695 (0.759117)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759135)
Training Loss of Epoch 90: 0.2274802977718958
Training Acc of Epoch 90: 0.7590923526422764
Testing Acc of Epoch 90: 0.7591347826086956
Model with the best training loss saved! The loss is 0.2274802977718958
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.3191e-01 (2.3191e-01)	Acc 0.749023 (0.749023)
Epoch: [91][300/616]	Loss 2.5307e-01 (2.2772e-01)	Acc 0.727539 (0.759029)
Epoch: [91][600/616]	Loss 2.3276e-01 (2.2762e-01)	Acc 0.750000 (0.758869)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759609)
Training Loss of Epoch 91: 0.22755444951173737
Training Acc of Epoch 91: 0.7589335619918699
Testing Acc of Epoch 91: 0.7596086956521739
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.1127e-01 (2.1127e-01)	Acc 0.770508 (0.770508)
Epoch: [92][300/616]	Loss 2.1845e-01 (2.2792e-01)	Acc 0.758789 (0.758559)
Epoch: [92][600/616]	Loss 2.4025e-01 (2.2762e-01)	Acc 0.749023 (0.758864)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759326)
Training Loss of Epoch 92: 0.22758397585977383
Training Acc of Epoch 92: 0.7588954522357724
Testing Acc of Epoch 92: 0.7593260869565217
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2526e-01 (2.2526e-01)	Acc 0.759766 (0.759766)
Epoch: [93][300/616]	Loss 2.2174e-01 (2.2736e-01)	Acc 0.761719 (0.758951)
Epoch: [93][600/616]	Loss 2.3603e-01 (2.2747e-01)	Acc 0.759766 (0.759031)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759870)
Training Loss of Epoch 93: 0.2274960647995879
Training Acc of Epoch 93: 0.7590097815040651
Testing Acc of Epoch 93: 0.7598695652173914
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.1695e-01 (2.1695e-01)	Acc 0.778320 (0.778320)
Epoch: [94][300/616]	Loss 2.2147e-01 (2.2810e-01)	Acc 0.752930 (0.758361)
Epoch: [94][600/616]	Loss 2.1716e-01 (2.2759e-01)	Acc 0.776367 (0.759124)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758983)
Training Loss of Epoch 94: 0.22748222070011667
Training Acc of Epoch 94: 0.759251143292683
Testing Acc of Epoch 94: 0.7589826086956522
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2907e-01 (2.2907e-01)	Acc 0.748047 (0.748047)
Epoch: [95][300/616]	Loss 2.1472e-01 (2.2718e-01)	Acc 0.785156 (0.759516)
Epoch: [95][600/616]	Loss 2.2218e-01 (2.2749e-01)	Acc 0.758789 (0.758948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759004)
Training Loss of Epoch 95: 0.2274204873699483
Training Acc of Epoch 95: 0.7589811991869919
Testing Acc of Epoch 95: 0.759004347826087
Model with the best training loss saved! The loss is 0.2274204873699483
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.1590e-01 (2.1590e-01)	Acc 0.781250 (0.781250)
Epoch: [96][300/616]	Loss 2.2797e-01 (2.2748e-01)	Acc 0.762695 (0.758714)
Epoch: [96][600/616]	Loss 2.1900e-01 (2.2759e-01)	Acc 0.770508 (0.758862)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759435)
Training Loss of Epoch 96: 0.22747881957670538
Training Acc of Epoch 96: 0.7590669461382114
Testing Acc of Epoch 96: 0.7594347826086957
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2214e-01 (2.2214e-01)	Acc 0.763672 (0.763672)
Epoch: [97][300/616]	Loss 2.2986e-01 (2.2794e-01)	Acc 0.750977 (0.759036)
Epoch: [97][600/616]	Loss 2.2914e-01 (2.2738e-01)	Acc 0.772461 (0.759286)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759287)
Training Loss of Epoch 97: 0.22744232549415372
Training Acc of Epoch 97: 0.7591828633130081
Testing Acc of Epoch 97: 0.7592869565217392
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4260e-01 (2.4260e-01)	Acc 0.730469 (0.730469)
Epoch: [98][300/616]	Loss 2.1314e-01 (2.2724e-01)	Acc 0.763672 (0.758968)
Epoch: [98][600/616]	Loss 2.3849e-01 (2.2733e-01)	Acc 0.750977 (0.759135)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759278)
Training Loss of Epoch 98: 0.22739997610328644
Training Acc of Epoch 98: 0.7590431275406504
Testing Acc of Epoch 98: 0.7592782608695652
Model with the best training loss saved! The loss is 0.22739997610328644
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2855e-01 (2.2855e-01)	Acc 0.750000 (0.750000)
Epoch: [99][300/616]	Loss 2.0422e-01 (2.2765e-01)	Acc 0.793945 (0.758783)
Epoch: [99][600/616]	Loss 2.1648e-01 (2.2738e-01)	Acc 0.766602 (0.759095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759952)
Training Loss of Epoch 99: 0.22742281219338983
Training Acc of Epoch 99: 0.7590494791666667
Testing Acc of Epoch 99: 0.7599521739130435
Early stopping not satisfied.
