train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_11b
different_width False
resnet18_width 64
weight_precision 11
bias_precision 11
act_precision 14
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_11b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_11b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9919e-01 (4.9919e-01)	Acc 0.179688 (0.179688)
Epoch: [0][300/616]	Loss 2.3893e-01 (2.7662e-01)	Acc 0.748047 (0.708339)
Epoch: [0][600/616]	Loss 2.4177e-01 (2.6070e-01)	Acc 0.742188 (0.725773)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.750535)
Training Loss of Epoch 0: 0.2600816171828324
Training Acc of Epoch 0: 0.7264624618902439
Testing Acc of Epoch 0: 0.7505347826086957
Model with the best training loss saved! The loss is 0.2600816171828324
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.1502e-01 (2.1502e-01)	Acc 0.784180 (0.784180)
Epoch: [1][300/616]	Loss 2.5327e-01 (2.4066e-01)	Acc 0.723633 (0.747002)
Epoch: [1][600/616]	Loss 2.3029e-01 (2.3971e-01)	Acc 0.766602 (0.747961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753174)
Training Loss of Epoch 1: 0.23968275073097972
Training Acc of Epoch 1: 0.7480119410569106
Testing Acc of Epoch 1: 0.7531739130434782
Model with the best training loss saved! The loss is 0.23968275073097972
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3994e-01 (2.3994e-01)	Acc 0.741211 (0.741211)
Epoch: [2][300/616]	Loss 2.5717e-01 (2.3838e-01)	Acc 0.720703 (0.749033)
Epoch: [2][600/616]	Loss 2.2708e-01 (2.3737e-01)	Acc 0.764648 (0.750213)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752517)
Training Loss of Epoch 2: 0.2372973327471958
Training Acc of Epoch 2: 0.7503064659552846
Testing Acc of Epoch 2: 0.7525173913043478
Model with the best training loss saved! The loss is 0.2372973327471958
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4750e-01 (2.4750e-01)	Acc 0.736328 (0.736328)
Epoch: [3][300/616]	Loss 2.3215e-01 (2.3670e-01)	Acc 0.744141 (0.750146)
Epoch: [3][600/616]	Loss 2.3773e-01 (2.3660e-01)	Acc 0.750000 (0.750744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753552)
Training Loss of Epoch 3: 0.23658772123053792
Training Acc of Epoch 3: 0.750854293699187
Testing Acc of Epoch 3: 0.7535521739130435
Model with the best training loss saved! The loss is 0.23658772123053792
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3209e-01 (2.3209e-01)	Acc 0.739258 (0.739258)
Epoch: [4][300/616]	Loss 2.4972e-01 (2.3675e-01)	Acc 0.720703 (0.750188)
Epoch: [4][600/616]	Loss 2.3193e-01 (2.3603e-01)	Acc 0.758789 (0.751168)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749317)
Training Loss of Epoch 4: 0.23605518588205662
Training Acc of Epoch 4: 0.7510781885162602
Testing Acc of Epoch 4: 0.7493173913043478
Model with the best training loss saved! The loss is 0.23605518588205662
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.4378e-01 (2.4378e-01)	Acc 0.743164 (0.743164)
Epoch: [5][300/616]	Loss 2.4389e-01 (2.3516e-01)	Acc 0.737305 (0.752417)
Epoch: [5][600/616]	Loss 2.3659e-01 (2.3545e-01)	Acc 0.733398 (0.751605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754257)
Training Loss of Epoch 5: 0.23544069484966557
Training Acc of Epoch 5: 0.7516149009146341
Testing Acc of Epoch 5: 0.7542565217391304
Model with the best training loss saved! The loss is 0.23544069484966557
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3196e-01 (2.3196e-01)	Acc 0.763672 (0.763672)
Epoch: [6][300/616]	Loss 2.2621e-01 (2.3502e-01)	Acc 0.758789 (0.751859)
Epoch: [6][600/616]	Loss 2.2978e-01 (2.3507e-01)	Acc 0.764648 (0.751878)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.749522)
Training Loss of Epoch 6: 0.23504640443053673
Training Acc of Epoch 6: 0.75190231199187
Testing Acc of Epoch 6: 0.7495217391304347
Model with the best training loss saved! The loss is 0.23504640443053673
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2463e-01 (2.2463e-01)	Acc 0.757812 (0.757812)
Epoch: [7][300/616]	Loss 2.3317e-01 (2.3543e-01)	Acc 0.755859 (0.751505)
Epoch: [7][600/616]	Loss 2.4528e-01 (2.3535e-01)	Acc 0.732422 (0.751628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.750226)
Training Loss of Epoch 7: 0.23537327967038968
Training Acc of Epoch 7: 0.751683180894309
Testing Acc of Epoch 7: 0.7502260869565217
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.2405e-01 (2.2405e-01)	Acc 0.766602 (0.766602)
Epoch: [8][300/616]	Loss 2.3586e-01 (2.3571e-01)	Acc 0.741211 (0.750247)
Epoch: [8][600/616]	Loss 2.3674e-01 (2.3493e-01)	Acc 0.756836 (0.751644)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752804)
Training Loss of Epoch 8: 0.23482898597310228
Training Acc of Epoch 8: 0.7518118013211382
Testing Acc of Epoch 8: 0.752804347826087
Model with the best training loss saved! The loss is 0.23482898597310228
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3065e-01 (2.3065e-01)	Acc 0.736328 (0.736328)
Epoch: [9][300/616]	Loss 2.2695e-01 (2.3411e-01)	Acc 0.762695 (0.752621)
Epoch: [9][600/616]	Loss 2.4809e-01 (2.3418e-01)	Acc 0.737305 (0.752424)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752626)
Training Loss of Epoch 9: 0.23422656931528232
Training Acc of Epoch 9: 0.7524247332317073
Testing Acc of Epoch 9: 0.7526260869565218
Model with the best training loss saved! The loss is 0.23422656931528232
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3335e-01 (2.3335e-01)	Acc 0.748047 (0.748047)
Epoch: [10][300/616]	Loss 2.3582e-01 (2.3386e-01)	Acc 0.751953 (0.752427)
Epoch: [10][600/616]	Loss 2.3687e-01 (2.3414e-01)	Acc 0.737305 (0.752247)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753239)
Training Loss of Epoch 10: 0.2340432545276192
Training Acc of Epoch 10: 0.7523755081300812
Testing Acc of Epoch 10: 0.7532391304347826
Model with the best training loss saved! The loss is 0.2340432545276192
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4852e-01 (2.4852e-01)	Acc 0.737305 (0.737305)
Epoch: [11][300/616]	Loss 2.2624e-01 (2.3404e-01)	Acc 0.764648 (0.752151)
Epoch: [11][600/616]	Loss 2.4172e-01 (2.3414e-01)	Acc 0.732422 (0.752397)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754857)
Training Loss of Epoch 11: 0.23410591102712522
Training Acc of Epoch 11: 0.7524501397357723
Testing Acc of Epoch 11: 0.7548565217391304
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.2087e-01 (2.2087e-01)	Acc 0.771484 (0.771484)
Epoch: [12][300/616]	Loss 2.2783e-01 (2.3450e-01)	Acc 0.761719 (0.752443)
Epoch: [12][600/616]	Loss 2.3811e-01 (2.3425e-01)	Acc 0.751953 (0.752580)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754009)
Training Loss of Epoch 12: 0.2342439292165322
Training Acc of Epoch 12: 0.7525803480691057
Testing Acc of Epoch 12: 0.7540086956521739
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.2818e-01 (2.2818e-01)	Acc 0.762695 (0.762695)
Epoch: [13][300/616]	Loss 2.2081e-01 (2.3411e-01)	Acc 0.764648 (0.752888)
Epoch: [13][600/616]	Loss 2.3572e-01 (2.3426e-01)	Acc 0.758789 (0.752367)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747943)
Training Loss of Epoch 13: 0.2342157719823403
Training Acc of Epoch 13: 0.7524580792682927
Testing Acc of Epoch 13: 0.7479434782608696
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2676e-01 (2.2676e-01)	Acc 0.761719 (0.761719)
Epoch: [14][300/616]	Loss 2.1925e-01 (2.3303e-01)	Acc 0.776367 (0.753517)
Epoch: [14][600/616]	Loss 2.3655e-01 (2.3385e-01)	Acc 0.740234 (0.752673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750978)
Training Loss of Epoch 14: 0.23391141581341504
Training Acc of Epoch 14: 0.7526740345528455
Testing Acc of Epoch 14: 0.7509782608695652
Model with the best training loss saved! The loss is 0.23391141581341504
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.5321e-01 (2.5321e-01)	Acc 0.727539 (0.727539)
Epoch: [15][300/616]	Loss 2.5591e-01 (2.3466e-01)	Acc 0.721680 (0.751930)
Epoch: [15][600/616]	Loss 2.3104e-01 (2.3391e-01)	Acc 0.761719 (0.752562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755322)
Training Loss of Epoch 15: 0.23392260195763132
Training Acc of Epoch 15: 0.752491425304878
Testing Acc of Epoch 15: 0.7553217391304348
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3471e-01 (2.3471e-01)	Acc 0.750977 (0.750977)
Epoch: [16][300/616]	Loss 2.2848e-01 (2.3372e-01)	Acc 0.751953 (0.752842)
Epoch: [16][600/616]	Loss 2.3646e-01 (2.3371e-01)	Acc 0.757812 (0.752938)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752278)
Training Loss of Epoch 16: 0.2338350208551903
Training Acc of Epoch 16: 0.752758193597561
Testing Acc of Epoch 16: 0.7522782608695652
Model with the best training loss saved! The loss is 0.2338350208551903
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3424e-01 (2.3424e-01)	Acc 0.747070 (0.747070)
Epoch: [17][300/616]	Loss 2.3957e-01 (2.3468e-01)	Acc 0.746094 (0.751742)
Epoch: [17][600/616]	Loss 2.2877e-01 (2.3383e-01)	Acc 0.760742 (0.752496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754639)
Training Loss of Epoch 17: 0.23387839927421353
Training Acc of Epoch 17: 0.7524437881097561
Testing Acc of Epoch 17: 0.7546391304347826
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4918e-01 (2.4918e-01)	Acc 0.732422 (0.732422)
Epoch: [18][300/616]	Loss 2.3498e-01 (2.3328e-01)	Acc 0.756836 (0.753429)
Epoch: [18][600/616]	Loss 2.3249e-01 (2.3366e-01)	Acc 0.758789 (0.752896)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.755361)
Training Loss of Epoch 18: 0.2336588969802469
Training Acc of Epoch 18: 0.752905868902439
Testing Acc of Epoch 18: 0.7553608695652174
Model with the best training loss saved! The loss is 0.2336588969802469
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.1890e-01 (2.1890e-01)	Acc 0.774414 (0.774414)
Epoch: [19][300/616]	Loss 2.4396e-01 (2.3306e-01)	Acc 0.744141 (0.753459)
Epoch: [19][600/616]	Loss 2.3708e-01 (2.3339e-01)	Acc 0.746094 (0.753013)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.754830)
Training Loss of Epoch 19: 0.23335631293494527
Training Acc of Epoch 19: 0.7530170223577236
Testing Acc of Epoch 19: 0.7548304347826087
Model with the best training loss saved! The loss is 0.23335631293494527
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.1998e-01 (2.1998e-01)	Acc 0.766602 (0.766602)
Epoch: [20][300/616]	Loss 2.3187e-01 (2.3349e-01)	Acc 0.758789 (0.753504)
Epoch: [20][600/616]	Loss 2.2871e-01 (2.3340e-01)	Acc 0.773438 (0.753624)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755200)
Training Loss of Epoch 20: 0.2334489146868388
Training Acc of Epoch 20: 0.7535092733739838
Testing Acc of Epoch 20: 0.7552
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.5191e-01 (2.5191e-01)	Acc 0.722656 (0.722656)
Epoch: [21][300/616]	Loss 2.1803e-01 (2.3329e-01)	Acc 0.765625 (0.753218)
Epoch: [21][600/616]	Loss 2.4033e-01 (2.3366e-01)	Acc 0.729492 (0.752756)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.753170)
Training Loss of Epoch 21: 0.23366821110248565
Training Acc of Epoch 21: 0.7528471163617886
Testing Acc of Epoch 21: 0.7531695652173913
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.2706e-01 (2.2706e-01)	Acc 0.762695 (0.762695)
Epoch: [22][300/616]	Loss 2.3348e-01 (2.3418e-01)	Acc 0.747070 (0.752070)
Epoch: [22][600/616]	Loss 2.4924e-01 (2.3389e-01)	Acc 0.734375 (0.752532)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.752813)
Training Loss of Epoch 22: 0.23390394291742062
Training Acc of Epoch 22: 0.7525755843495935
Testing Acc of Epoch 22: 0.7528130434782608
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.1819e-01 (2.1819e-01)	Acc 0.769531 (0.769531)
Epoch: [23][300/616]	Loss 2.3970e-01 (2.3325e-01)	Acc 0.740234 (0.754004)
Epoch: [23][600/616]	Loss 2.4919e-01 (2.3316e-01)	Acc 0.729492 (0.753646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752578)
Training Loss of Epoch 23: 0.23332576664482674
Training Acc of Epoch 23: 0.7534870426829269
Testing Acc of Epoch 23: 0.7525782608695653
Model with the best training loss saved! The loss is 0.23332576664482674
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2865e-01 (2.2865e-01)	Acc 0.764648 (0.764648)
Epoch: [24][300/616]	Loss 2.3301e-01 (2.3376e-01)	Acc 0.742188 (0.752628)
Epoch: [24][600/616]	Loss 2.2135e-01 (2.3362e-01)	Acc 0.763672 (0.752886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754526)
Training Loss of Epoch 24: 0.23365218307429214
Training Acc of Epoch 24: 0.7528296493902439
Testing Acc of Epoch 24: 0.7545260869565218
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3520e-01 (2.3520e-01)	Acc 0.741211 (0.741211)
Epoch: [25][300/616]	Loss 2.2098e-01 (2.3277e-01)	Acc 0.771484 (0.754221)
Epoch: [25][600/616]	Loss 2.3486e-01 (2.3343e-01)	Acc 0.753906 (0.753432)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751226)
Training Loss of Epoch 25: 0.2334689396183665
Training Acc of Epoch 25: 0.7533568343495934
Testing Acc of Epoch 25: 0.7512260869565217
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3643e-01 (2.3643e-01)	Acc 0.748047 (0.748047)
Epoch: [26][300/616]	Loss 2.3499e-01 (2.3326e-01)	Acc 0.738281 (0.753403)
Epoch: [26][600/616]	Loss 2.4172e-01 (2.3350e-01)	Acc 0.741211 (0.752959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751522)
Training Loss of Epoch 26: 0.23350798715420854
Training Acc of Epoch 26: 0.7529344512195122
Testing Acc of Epoch 26: 0.7515217391304347
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.3366e-01 (2.3366e-01)	Acc 0.746094 (0.746094)
Epoch: [27][300/616]	Loss 2.2878e-01 (2.3365e-01)	Acc 0.746094 (0.753455)
Epoch: [27][600/616]	Loss 2.2768e-01 (2.3357e-01)	Acc 0.759766 (0.752930)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752752)
Training Loss of Epoch 27: 0.23366184837934448
Training Acc of Epoch 27: 0.7527661331300813
Testing Acc of Epoch 27: 0.7527521739130435
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.2071e-01 (2.2071e-01)	Acc 0.766602 (0.766602)
Epoch: [28][300/616]	Loss 2.2631e-01 (2.3366e-01)	Acc 0.752930 (0.752596)
Epoch: [28][600/616]	Loss 2.3246e-01 (2.3338e-01)	Acc 0.745117 (0.753063)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754204)
Training Loss of Epoch 28: 0.23342900055695356
Training Acc of Epoch 28: 0.753028137703252
Testing Acc of Epoch 28: 0.7542043478260869
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4808e-01 (2.4808e-01)	Acc 0.723633 (0.723633)
Epoch: [29][300/616]	Loss 2.3113e-01 (2.3442e-01)	Acc 0.751953 (0.751561)
Epoch: [29][600/616]	Loss 2.4945e-01 (2.3404e-01)	Acc 0.733398 (0.752561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755391)
Training Loss of Epoch 29: 0.2338637921868301
Training Acc of Epoch 29: 0.752735962906504
Testing Acc of Epoch 29: 0.7553913043478261
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4810e-01 (2.4810e-01)	Acc 0.725586 (0.725586)
Epoch: [30][300/616]	Loss 2.3023e-01 (2.3372e-01)	Acc 0.758789 (0.753254)
Epoch: [30][600/616]	Loss 2.3840e-01 (2.3326e-01)	Acc 0.763672 (0.753404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754913)
Training Loss of Epoch 30: 0.2332688397750622
Training Acc of Epoch 30: 0.7534028836382114
Testing Acc of Epoch 30: 0.7549130434782608
Model with the best training loss saved! The loss is 0.2332688397750622
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3396e-01 (2.3396e-01)	Acc 0.749023 (0.749023)
Epoch: [31][300/616]	Loss 2.4778e-01 (2.3337e-01)	Acc 0.732422 (0.752933)
Epoch: [31][600/616]	Loss 2.3823e-01 (2.3340e-01)	Acc 0.748047 (0.753185)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750361)
Training Loss of Epoch 31: 0.2334415229597712
Training Acc of Epoch 31: 0.7531440548780488
Testing Acc of Epoch 31: 0.7503608695652174
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3750e-01 (2.3750e-01)	Acc 0.748047 (0.748047)
Epoch: [32][300/616]	Loss 2.3000e-01 (2.3501e-01)	Acc 0.753906 (0.751246)
Epoch: [32][600/616]	Loss 2.1715e-01 (2.3392e-01)	Acc 0.766602 (0.752631)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752678)
Training Loss of Epoch 32: 0.23384940946489816
Training Acc of Epoch 32: 0.7527724847560976
Testing Acc of Epoch 32: 0.7526782608695652
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2151e-01 (2.2151e-01)	Acc 0.770508 (0.770508)
Epoch: [33][300/616]	Loss 2.3802e-01 (2.3382e-01)	Acc 0.760742 (0.752469)
Epoch: [33][600/616]	Loss 2.3802e-01 (2.3350e-01)	Acc 0.752930 (0.752918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750887)
Training Loss of Epoch 33: 0.2334281083771853
Training Acc of Epoch 33: 0.753028137703252
Testing Acc of Epoch 33: 0.7508869565217391
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3273e-01 (2.3273e-01)	Acc 0.758789 (0.758789)
Epoch: [34][300/616]	Loss 2.3900e-01 (2.3294e-01)	Acc 0.750977 (0.754117)
Epoch: [34][600/616]	Loss 2.4038e-01 (2.3345e-01)	Acc 0.741211 (0.753318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753535)
Training Loss of Epoch 34: 0.23346416763658445
Training Acc of Epoch 34: 0.7532853785569106
Testing Acc of Epoch 34: 0.7535347826086957
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3987e-01 (2.3987e-01)	Acc 0.739258 (0.739258)
Epoch: [35][300/616]	Loss 2.3678e-01 (2.3361e-01)	Acc 0.748047 (0.752466)
Epoch: [35][600/616]	Loss 2.3363e-01 (2.3352e-01)	Acc 0.755859 (0.752941)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754978)
Training Loss of Epoch 35: 0.2334981673616704
Training Acc of Epoch 35: 0.752905868902439
Testing Acc of Epoch 35: 0.7549782608695652
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3984e-01 (2.3984e-01)	Acc 0.746094 (0.746094)
Epoch: [36][300/616]	Loss 2.3667e-01 (2.3336e-01)	Acc 0.759766 (0.753034)
Epoch: [36][600/616]	Loss 2.3466e-01 (2.3360e-01)	Acc 0.761719 (0.753156)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753374)
Training Loss of Epoch 36: 0.2335739692536796
Training Acc of Epoch 36: 0.7531758130081301
Testing Acc of Epoch 36: 0.7533739130434782
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4305e-01 (2.4305e-01)	Acc 0.733398 (0.733398)
Epoch: [37][300/616]	Loss 2.3030e-01 (2.3437e-01)	Acc 0.764648 (0.751755)
Epoch: [37][600/616]	Loss 2.4603e-01 (2.3339e-01)	Acc 0.737305 (0.753305)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754061)
Training Loss of Epoch 37: 0.23338958613756228
Training Acc of Epoch 37: 0.7532806148373984
Testing Acc of Epoch 37: 0.7540608695652173
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3675e-01 (2.3675e-01)	Acc 0.743164 (0.743164)
Epoch: [38][300/616]	Loss 2.2130e-01 (2.3315e-01)	Acc 0.769531 (0.753799)
Epoch: [38][600/616]	Loss 2.2366e-01 (2.3356e-01)	Acc 0.765625 (0.753186)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.756709)
Training Loss of Epoch 38: 0.23356817689368395
Training Acc of Epoch 38: 0.753198043699187
Testing Acc of Epoch 38: 0.7567086956521739
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2744e-01 (2.2744e-01)	Acc 0.755859 (0.755859)
Epoch: [39][300/616]	Loss 2.3547e-01 (2.3394e-01)	Acc 0.766602 (0.753082)
Epoch: [39][600/616]	Loss 2.2947e-01 (2.3358e-01)	Acc 0.753906 (0.753238)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754026)
Training Loss of Epoch 39: 0.23362327622688883
Training Acc of Epoch 39: 0.7531154725609757
Testing Acc of Epoch 39: 0.7540260869565217
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.2647e-01 (2.2647e-01)	Acc 0.771484 (0.771484)
Epoch: [40][300/616]	Loss 2.2199e-01 (2.3353e-01)	Acc 0.764648 (0.753141)
Epoch: [40][600/616]	Loss 2.3504e-01 (2.3330e-01)	Acc 0.749023 (0.753497)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754865)
Training Loss of Epoch 40: 0.23342670287058606
Training Acc of Epoch 40: 0.7534235264227642
Testing Acc of Epoch 40: 0.7548652173913043
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.2737e-01 (2.2737e-01)	Acc 0.770508 (0.770508)
Epoch: [41][300/616]	Loss 2.2573e-01 (2.3224e-01)	Acc 0.757812 (0.754792)
Epoch: [41][600/616]	Loss 2.2636e-01 (2.3333e-01)	Acc 0.766602 (0.753443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751783)
Training Loss of Epoch 41: 0.2334477222305003
Training Acc of Epoch 41: 0.7534012957317073
Testing Acc of Epoch 41: 0.7517826086956522
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3123e-01 (2.3123e-01)	Acc 0.759766 (0.759766)
Epoch: [42][300/616]	Loss 2.3443e-01 (2.3314e-01)	Acc 0.753906 (0.753958)
Epoch: [42][600/616]	Loss 2.3746e-01 (2.3347e-01)	Acc 0.752930 (0.753154)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754387)
Training Loss of Epoch 42: 0.23341664495022316
Training Acc of Epoch 42: 0.7531821646341463
Testing Acc of Epoch 42: 0.7543869565217391
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.2662e-01 (2.2662e-01)	Acc 0.766602 (0.766602)
Epoch: [43][300/616]	Loss 2.3310e-01 (2.3327e-01)	Acc 0.766602 (0.753510)
Epoch: [43][600/616]	Loss 2.2796e-01 (2.3374e-01)	Acc 0.756836 (0.753024)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756035)
Training Loss of Epoch 43: 0.2336813408911713
Training Acc of Epoch 43: 0.7530662474593496
Testing Acc of Epoch 43: 0.7560347826086956
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4064e-01 (2.4064e-01)	Acc 0.749023 (0.749023)
Epoch: [44][300/616]	Loss 2.4478e-01 (2.3341e-01)	Acc 0.738281 (0.753579)
Epoch: [44][600/616]	Loss 2.2693e-01 (2.3336e-01)	Acc 0.762695 (0.753303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755935)
Training Loss of Epoch 44: 0.23339994935000816
Training Acc of Epoch 44: 0.7532567962398374
Testing Acc of Epoch 44: 0.7559347826086956
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3010e-01 (2.3010e-01)	Acc 0.756836 (0.756836)
Epoch: [45][300/616]	Loss 2.2367e-01 (2.3287e-01)	Acc 0.766602 (0.753812)
Epoch: [45][600/616]	Loss 2.4863e-01 (2.3306e-01)	Acc 0.717773 (0.753732)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752391)
Training Loss of Epoch 45: 0.2330978850523631
Training Acc of Epoch 45: 0.7535854928861788
Testing Acc of Epoch 45: 0.7523913043478261
Model with the best training loss saved! The loss is 0.2330978850523631
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.3404e-01 (2.3404e-01)	Acc 0.752930 (0.752930)
Epoch: [46][300/616]	Loss 2.4229e-01 (2.3411e-01)	Acc 0.745117 (0.753053)
Epoch: [46][600/616]	Loss 2.3219e-01 (2.3342e-01)	Acc 0.759766 (0.753368)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756326)
Training Loss of Epoch 46: 0.23347864558057088
Training Acc of Epoch 46: 0.7533123729674797
Testing Acc of Epoch 46: 0.7563260869565217
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2636e-01 (2.2636e-01)	Acc 0.766602 (0.766602)
Epoch: [47][300/616]	Loss 2.2895e-01 (2.3393e-01)	Acc 0.764648 (0.752732)
Epoch: [47][600/616]	Loss 2.4035e-01 (2.3333e-01)	Acc 0.745117 (0.753578)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752526)
Training Loss of Epoch 47: 0.23338017088126362
Training Acc of Epoch 47: 0.7534965701219513
Testing Acc of Epoch 47: 0.7525260869565218
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.2886e-01 (2.2886e-01)	Acc 0.759766 (0.759766)
Epoch: [48][300/616]	Loss 2.3946e-01 (2.3345e-01)	Acc 0.750977 (0.753196)
Epoch: [48][600/616]	Loss 2.3666e-01 (2.3346e-01)	Acc 0.747070 (0.753052)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751252)
Training Loss of Epoch 48: 0.2335159467245505
Training Acc of Epoch 48: 0.752905868902439
Testing Acc of Epoch 48: 0.7512521739130434
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.2539e-01 (2.2539e-01)	Acc 0.761719 (0.761719)
Epoch: [49][300/616]	Loss 2.1718e-01 (2.3346e-01)	Acc 0.779297 (0.753439)
Epoch: [49][600/616]	Loss 2.4312e-01 (2.3338e-01)	Acc 0.741211 (0.753565)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752017)
Training Loss of Epoch 49: 0.2333858125578097
Training Acc of Epoch 49: 0.7535712017276422
Testing Acc of Epoch 49: 0.7520173913043479
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4405e-01 (2.4405e-01)	Acc 0.751953 (0.751953)
Epoch: [50][300/616]	Loss 2.0642e-01 (2.3376e-01)	Acc 0.793945 (0.753118)
Epoch: [50][600/616]	Loss 2.2264e-01 (2.3316e-01)	Acc 0.765625 (0.753651)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755500)
Training Loss of Epoch 50: 0.2331609049705955
Training Acc of Epoch 50: 0.7536331300813008
Testing Acc of Epoch 50: 0.7555
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2250e-01 (2.2250e-01)	Acc 0.768555 (0.768555)
Epoch: [51][300/616]	Loss 2.4813e-01 (2.3341e-01)	Acc 0.723633 (0.753231)
Epoch: [51][600/616]	Loss 2.1785e-01 (2.3389e-01)	Acc 0.781250 (0.753000)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753013)
Training Loss of Epoch 51: 0.23384680798867855
Training Acc of Epoch 51: 0.7531043572154471
Testing Acc of Epoch 51: 0.7530130434782609
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.2261e-01 (2.2261e-01)	Acc 0.769531 (0.769531)
Epoch: [52][300/616]	Loss 2.2701e-01 (2.3391e-01)	Acc 0.757812 (0.752761)
Epoch: [52][600/616]	Loss 2.3784e-01 (2.3313e-01)	Acc 0.738281 (0.753708)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753757)
Training Loss of Epoch 52: 0.2331529671341423
Training Acc of Epoch 52: 0.7536061356707318
Testing Acc of Epoch 52: 0.7537565217391304
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.2631e-01 (2.2631e-01)	Acc 0.770508 (0.770508)
Epoch: [53][300/616]	Loss 2.3341e-01 (2.3363e-01)	Acc 0.751953 (0.753209)
Epoch: [53][600/616]	Loss 2.2038e-01 (2.3341e-01)	Acc 0.776367 (0.753320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753409)
Training Loss of Epoch 53: 0.23338497385261506
Training Acc of Epoch 53: 0.7533631859756098
Testing Acc of Epoch 53: 0.7534086956521739
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3878e-01 (2.3878e-01)	Acc 0.749023 (0.749023)
Epoch: [54][300/616]	Loss 2.2335e-01 (2.3274e-01)	Acc 0.760742 (0.754247)
Epoch: [54][600/616]	Loss 2.3711e-01 (2.3288e-01)	Acc 0.740234 (0.753752)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754796)
Training Loss of Epoch 54: 0.23293307808841146
Training Acc of Epoch 54: 0.7537299923780488
Testing Acc of Epoch 54: 0.7547956521739131
Model with the best training loss saved! The loss is 0.23293307808841146
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3891e-01 (2.3891e-01)	Acc 0.750000 (0.750000)
Epoch: [55][300/616]	Loss 2.3106e-01 (2.3378e-01)	Acc 0.757812 (0.753098)
Epoch: [55][600/616]	Loss 2.4493e-01 (2.3333e-01)	Acc 0.741211 (0.753422)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.751339)
Training Loss of Epoch 55: 0.2333102777721436
Training Acc of Epoch 55: 0.7534711636178861
Testing Acc of Epoch 55: 0.7513391304347826
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.3687e-01 (2.3687e-01)	Acc 0.746094 (0.746094)
Epoch: [56][300/616]	Loss 2.2533e-01 (2.3331e-01)	Acc 0.771484 (0.753682)
Epoch: [56][600/616]	Loss 2.6058e-01 (2.3357e-01)	Acc 0.724609 (0.753256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754509)
Training Loss of Epoch 56: 0.23357116403133887
Training Acc of Epoch 56: 0.7532647357723578
Testing Acc of Epoch 56: 0.7545086956521739
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.4168e-01 (2.4168e-01)	Acc 0.735352 (0.735352)
Epoch: [57][300/616]	Loss 2.4799e-01 (2.3321e-01)	Acc 0.727539 (0.752943)
Epoch: [57][600/616]	Loss 2.3989e-01 (2.3334e-01)	Acc 0.741211 (0.753195)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754517)
Training Loss of Epoch 57: 0.23338187826358206
Training Acc of Epoch 57: 0.753125
Testing Acc of Epoch 57: 0.7545173913043478
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.2941e-01 (2.2941e-01)	Acc 0.754883 (0.754883)
Epoch: [58][300/616]	Loss 2.3709e-01 (2.3340e-01)	Acc 0.758789 (0.753851)
Epoch: [58][600/616]	Loss 2.3948e-01 (2.3356e-01)	Acc 0.748047 (0.753412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751600)
Training Loss of Epoch 58: 0.23356255021521716
Training Acc of Epoch 58: 0.7534044715447155
Testing Acc of Epoch 58: 0.7516
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.5318e-01 (2.5318e-01)	Acc 0.729492 (0.729492)
Epoch: [59][300/616]	Loss 2.2380e-01 (2.3262e-01)	Acc 0.759766 (0.754169)
Epoch: [59][600/616]	Loss 2.4607e-01 (2.3355e-01)	Acc 0.726562 (0.753225)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753791)
Training Loss of Epoch 59: 0.2335036973643109
Training Acc of Epoch 59: 0.753296493902439
Testing Acc of Epoch 59: 0.753791304347826
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3901e-01 (2.3901e-01)	Acc 0.745117 (0.745117)
Epoch: [60][300/616]	Loss 2.3287e-01 (2.3338e-01)	Acc 0.743164 (0.753267)
Epoch: [60][600/616]	Loss 2.2951e-01 (2.3311e-01)	Acc 0.746094 (0.753793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755530)
Training Loss of Epoch 60: 0.23313015113031962
Training Acc of Epoch 60: 0.7537268165650407
Testing Acc of Epoch 60: 0.7555304347826087
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3883e-01 (2.3883e-01)	Acc 0.745117 (0.745117)
Epoch: [61][300/616]	Loss 2.3084e-01 (2.3251e-01)	Acc 0.760742 (0.754549)
Epoch: [61][600/616]	Loss 2.1606e-01 (2.3318e-01)	Acc 0.774414 (0.753840)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752730)
Training Loss of Epoch 61: 0.23325666741627019
Training Acc of Epoch 61: 0.7537426956300813
Testing Acc of Epoch 61: 0.7527304347826087
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3159e-01 (2.3159e-01)	Acc 0.760742 (0.760742)
Epoch: [62][300/616]	Loss 2.4005e-01 (2.3362e-01)	Acc 0.750977 (0.753293)
Epoch: [62][600/616]	Loss 2.3495e-01 (2.3325e-01)	Acc 0.744141 (0.753589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755948)
Training Loss of Epoch 62: 0.23331070595640477
Training Acc of Epoch 62: 0.753541031504065
Testing Acc of Epoch 62: 0.7559478260869565
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3749e-01 (2.3749e-01)	Acc 0.743164 (0.743164)
Epoch: [63][300/616]	Loss 2.2860e-01 (2.3333e-01)	Acc 0.769531 (0.754380)
Epoch: [63][600/616]	Loss 2.4181e-01 (2.3368e-01)	Acc 0.738281 (0.753346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756161)
Training Loss of Epoch 63: 0.23365514479516966
Training Acc of Epoch 63: 0.7533044334349593
Testing Acc of Epoch 63: 0.7561608695652174
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.1741e-01 (2.1741e-01)	Acc 0.774414 (0.774414)
Epoch: [64][300/616]	Loss 2.2451e-01 (2.3292e-01)	Acc 0.770508 (0.754052)
Epoch: [64][600/616]	Loss 2.3731e-01 (2.3337e-01)	Acc 0.763672 (0.753417)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755943)
Training Loss of Epoch 64: 0.23337433800949314
Training Acc of Epoch 64: 0.7533393673780487
Testing Acc of Epoch 64: 0.7559434782608696
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3120e-01 (2.3120e-01)	Acc 0.748047 (0.748047)
Epoch: [65][300/616]	Loss 2.5056e-01 (2.3321e-01)	Acc 0.743164 (0.753922)
Epoch: [65][600/616]	Loss 2.2755e-01 (2.3372e-01)	Acc 0.755859 (0.752988)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753083)
Training Loss of Epoch 65: 0.2336560825264551
Training Acc of Epoch 65: 0.7530583079268293
Testing Acc of Epoch 65: 0.7530826086956521
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3329e-01 (2.3329e-01)	Acc 0.753906 (0.753906)
Epoch: [66][300/616]	Loss 2.4905e-01 (2.3313e-01)	Acc 0.738281 (0.754088)
Epoch: [66][600/616]	Loss 2.2689e-01 (2.3330e-01)	Acc 0.764648 (0.753487)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754517)
Training Loss of Epoch 66: 0.23329316428521785
Training Acc of Epoch 66: 0.7534822789634147
Testing Acc of Epoch 66: 0.7545173913043478
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3431e-01 (2.3431e-01)	Acc 0.754883 (0.754883)
Epoch: [67][300/616]	Loss 2.4181e-01 (2.3380e-01)	Acc 0.744141 (0.753309)
Epoch: [67][600/616]	Loss 2.3924e-01 (2.3390e-01)	Acc 0.739258 (0.753147)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755861)
Training Loss of Epoch 67: 0.23384413014097913
Training Acc of Epoch 67: 0.7532059832317073
Testing Acc of Epoch 67: 0.7558608695652174
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3798e-01 (2.3798e-01)	Acc 0.745117 (0.745117)
Epoch: [68][300/616]	Loss 2.4992e-01 (2.3277e-01)	Acc 0.720703 (0.753809)
Epoch: [68][600/616]	Loss 2.3511e-01 (2.3340e-01)	Acc 0.757812 (0.753196)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.752513)
Training Loss of Epoch 68: 0.23336478038047387
Training Acc of Epoch 68: 0.7532615599593496
Testing Acc of Epoch 68: 0.7525130434782609
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4010e-01 (2.4010e-01)	Acc 0.747070 (0.747070)
Epoch: [69][300/616]	Loss 2.3396e-01 (2.3311e-01)	Acc 0.757812 (0.753085)
Epoch: [69][600/616]	Loss 2.2885e-01 (2.3338e-01)	Acc 0.769531 (0.753279)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754535)
Training Loss of Epoch 69: 0.2333904841081883
Training Acc of Epoch 69: 0.7532567962398374
Testing Acc of Epoch 69: 0.7545347826086957
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3699e-01 (2.3699e-01)	Acc 0.746094 (0.746094)
Epoch: [70][300/616]	Loss 2.2398e-01 (2.3388e-01)	Acc 0.756836 (0.752638)
Epoch: [70][600/616]	Loss 2.5033e-01 (2.3350e-01)	Acc 0.736328 (0.753329)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751830)
Training Loss of Epoch 70: 0.23350150071508516
Training Acc of Epoch 70: 0.7532409171747968
Testing Acc of Epoch 70: 0.7518304347826087
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.3982e-01 (2.3982e-01)	Acc 0.747070 (0.747070)
Epoch: [71][300/616]	Loss 2.4713e-01 (2.3269e-01)	Acc 0.734375 (0.753663)
Epoch: [71][600/616]	Loss 2.4078e-01 (2.3313e-01)	Acc 0.751953 (0.753836)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753735)
Training Loss of Epoch 71: 0.23316074815707477
Training Acc of Epoch 71: 0.7537966844512195
Testing Acc of Epoch 71: 0.7537347826086956
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.3139e-01 (2.3139e-01)	Acc 0.743164 (0.743164)
Epoch: [72][300/616]	Loss 2.3291e-01 (2.3311e-01)	Acc 0.755859 (0.753520)
Epoch: [72][600/616]	Loss 2.2121e-01 (2.3355e-01)	Acc 0.774414 (0.753347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754830)
Training Loss of Epoch 72: 0.23357071854719302
Training Acc of Epoch 72: 0.7532504446138212
Testing Acc of Epoch 72: 0.7548304347826087
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.2773e-01 (2.2773e-01)	Acc 0.760742 (0.760742)
Epoch: [73][300/616]	Loss 2.2432e-01 (2.3266e-01)	Acc 0.771484 (0.754552)
Epoch: [73][600/616]	Loss 2.3525e-01 (2.3319e-01)	Acc 0.760742 (0.753325)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754017)
Training Loss of Epoch 73: 0.2331676530401881
Training Acc of Epoch 73: 0.7534521087398374
Testing Acc of Epoch 73: 0.7540173913043479
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4717e-01 (2.4717e-01)	Acc 0.730469 (0.730469)
Epoch: [74][300/616]	Loss 2.2781e-01 (2.3311e-01)	Acc 0.768555 (0.753686)
Epoch: [74][600/616]	Loss 2.3257e-01 (2.3374e-01)	Acc 0.754883 (0.752998)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753078)
Training Loss of Epoch 74: 0.2336314770264354
Training Acc of Epoch 74: 0.7530932418699187
Testing Acc of Epoch 74: 0.7530782608695652
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.3118e-01 (2.3118e-01)	Acc 0.750977 (0.750977)
Epoch: [75][300/616]	Loss 2.2144e-01 (2.2903e-01)	Acc 0.771484 (0.756914)
Epoch: [75][600/616]	Loss 2.2693e-01 (2.2917e-01)	Acc 0.756836 (0.756984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757600)
Training Loss of Epoch 75: 0.22921371350928052
Training Acc of Epoch 75: 0.7569455030487805
Testing Acc of Epoch 75: 0.7576
Model with the best training loss saved! The loss is 0.22921371350928052
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2856e-01 (2.2856e-01)	Acc 0.764648 (0.764648)
Epoch: [76][300/616]	Loss 2.3038e-01 (2.2937e-01)	Acc 0.759766 (0.756885)
Epoch: [76][600/616]	Loss 2.2567e-01 (2.2888e-01)	Acc 0.764648 (0.757219)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758252)
Training Loss of Epoch 76: 0.22885852986719551
Training Acc of Epoch 76: 0.7572154471544715
Testing Acc of Epoch 76: 0.7582521739130434
Model with the best training loss saved! The loss is 0.22885852986719551
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4118e-01 (2.4118e-01)	Acc 0.730469 (0.730469)
Epoch: [77][300/616]	Loss 2.2135e-01 (2.2872e-01)	Acc 0.760742 (0.757800)
Epoch: [77][600/616]	Loss 2.2035e-01 (2.2874e-01)	Acc 0.760742 (0.757423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757426)
Training Loss of Epoch 77: 0.22877204890658215
Training Acc of Epoch 77: 0.7573821773373983
Testing Acc of Epoch 77: 0.7574260869565217
Model with the best training loss saved! The loss is 0.22877204890658215
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.1666e-01 (2.1666e-01)	Acc 0.775391 (0.775391)
Epoch: [78][300/616]	Loss 2.3310e-01 (2.2876e-01)	Acc 0.739258 (0.757102)
Epoch: [78][600/616]	Loss 2.2961e-01 (2.2868e-01)	Acc 0.749023 (0.757463)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757835)
Training Loss of Epoch 78: 0.22872007198450042
Training Acc of Epoch 78: 0.7574409298780488
Testing Acc of Epoch 78: 0.7578347826086956
Model with the best training loss saved! The loss is 0.22872007198450042
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3333e-01 (2.3333e-01)	Acc 0.755859 (0.755859)
Epoch: [79][300/616]	Loss 2.1680e-01 (2.2835e-01)	Acc 0.767578 (0.757774)
Epoch: [79][600/616]	Loss 2.2149e-01 (2.2864e-01)	Acc 0.770508 (0.757574)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758139)
Training Loss of Epoch 79: 0.22868913068034785
Training Acc of Epoch 79: 0.7575838414634146
Testing Acc of Epoch 79: 0.7581391304347826
Model with the best training loss saved! The loss is 0.22868913068034785
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2700e-01 (2.2700e-01)	Acc 0.767578 (0.767578)
Epoch: [80][300/616]	Loss 2.0940e-01 (2.2820e-01)	Acc 0.778320 (0.758276)
Epoch: [80][600/616]	Loss 2.2226e-01 (2.2868e-01)	Acc 0.759766 (0.757338)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757809)
Training Loss of Epoch 80: 0.22867999079266216
Training Acc of Epoch 80: 0.7573678861788617
Testing Acc of Epoch 80: 0.7578086956521739
Model with the best training loss saved! The loss is 0.22867999079266216
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4078e-01 (2.4078e-01)	Acc 0.743164 (0.743164)
Epoch: [81][300/616]	Loss 2.3580e-01 (2.2930e-01)	Acc 0.753906 (0.756933)
Epoch: [81][600/616]	Loss 2.2209e-01 (2.2873e-01)	Acc 0.773438 (0.757561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756878)
Training Loss of Epoch 81: 0.22866001451403145
Training Acc of Epoch 81: 0.7576425940040651
Testing Acc of Epoch 81: 0.7568782608695652
Model with the best training loss saved! The loss is 0.22866001451403145
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2991e-01 (2.2991e-01)	Acc 0.750977 (0.750977)
Epoch: [82][300/616]	Loss 2.2321e-01 (2.2857e-01)	Acc 0.767578 (0.757342)
Epoch: [82][600/616]	Loss 2.4347e-01 (2.2865e-01)	Acc 0.743164 (0.757634)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757957)
Training Loss of Epoch 82: 0.22861249037389833
Training Acc of Epoch 82: 0.7576632367886179
Testing Acc of Epoch 82: 0.7579565217391304
Model with the best training loss saved! The loss is 0.22861249037389833
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.2553e-01 (2.2553e-01)	Acc 0.766602 (0.766602)
Epoch: [83][300/616]	Loss 2.2940e-01 (2.2886e-01)	Acc 0.754883 (0.757452)
Epoch: [83][600/616]	Loss 2.1869e-01 (2.2871e-01)	Acc 0.763672 (0.757580)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757883)
Training Loss of Epoch 83: 0.22863331480724056
Training Acc of Epoch 83: 0.7576822916666667
Testing Acc of Epoch 83: 0.7578826086956522
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.1760e-01 (2.1760e-01)	Acc 0.766602 (0.766602)
Epoch: [84][300/616]	Loss 2.3142e-01 (2.2903e-01)	Acc 0.743164 (0.757439)
Epoch: [84][600/616]	Loss 2.1305e-01 (2.2864e-01)	Acc 0.791992 (0.757600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758317)
Training Loss of Epoch 84: 0.22862600563018304
Training Acc of Epoch 84: 0.7576473577235773
Testing Acc of Epoch 84: 0.7583173913043478
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1925e-01 (2.1925e-01)	Acc 0.764648 (0.764648)
Epoch: [85][300/616]	Loss 2.2421e-01 (2.2834e-01)	Acc 0.765625 (0.758192)
Epoch: [85][600/616]	Loss 2.3646e-01 (2.2835e-01)	Acc 0.755859 (0.758016)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.758839)
Training Loss of Epoch 85: 0.2284406709234889
Training Acc of Epoch 85: 0.7579284171747968
Testing Acc of Epoch 85: 0.7588391304347826
Model with the best training loss saved! The loss is 0.2284406709234889
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3222e-01 (2.3222e-01)	Acc 0.756836 (0.756836)
Epoch: [86][300/616]	Loss 2.2720e-01 (2.2817e-01)	Acc 0.751953 (0.757809)
Epoch: [86][600/616]	Loss 2.3371e-01 (2.2862e-01)	Acc 0.757812 (0.757725)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758109)
Training Loss of Epoch 86: 0.22852957585478217
Training Acc of Epoch 86: 0.7578696646341463
Testing Acc of Epoch 86: 0.7581086956521739
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.1709e-01 (2.1709e-01)	Acc 0.766602 (0.766602)
Epoch: [87][300/616]	Loss 2.2122e-01 (2.2845e-01)	Acc 0.770508 (0.758147)
Epoch: [87][600/616]	Loss 2.1221e-01 (2.2861e-01)	Acc 0.795898 (0.757510)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758174)
Training Loss of Epoch 87: 0.22853318886543678
Training Acc of Epoch 87: 0.7576124237804878
Testing Acc of Epoch 87: 0.7581739130434783
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.2879e-01 (2.2879e-01)	Acc 0.750000 (0.750000)
Epoch: [88][300/616]	Loss 2.1659e-01 (2.2810e-01)	Acc 0.771484 (0.758179)
Epoch: [88][600/616]	Loss 2.3474e-01 (2.2846e-01)	Acc 0.748047 (0.757592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757978)
Training Loss of Epoch 88: 0.22844877148546824
Training Acc of Epoch 88: 0.7576378302845529
Testing Acc of Epoch 88: 0.7579782608695652
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.3994e-01 (2.3994e-01)	Acc 0.733398 (0.733398)
Epoch: [89][300/616]	Loss 2.1250e-01 (2.2936e-01)	Acc 0.764648 (0.756748)
Epoch: [89][600/616]	Loss 2.3445e-01 (2.2851e-01)	Acc 0.747070 (0.757678)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757365)
Training Loss of Epoch 89: 0.2284124381173917
Training Acc of Epoch 89: 0.7577378683943089
Testing Acc of Epoch 89: 0.7573652173913044
Model with the best training loss saved! The loss is 0.2284124381173917
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4286e-01 (2.4286e-01)	Acc 0.739258 (0.739258)
Epoch: [90][300/616]	Loss 2.2626e-01 (2.2846e-01)	Acc 0.761719 (0.757851)
Epoch: [90][600/616]	Loss 2.3769e-01 (2.2838e-01)	Acc 0.749023 (0.757819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758500)
Training Loss of Epoch 90: 0.22842079282291536
Training Acc of Epoch 90: 0.7578442581300813
Testing Acc of Epoch 90: 0.7585
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.1885e-01 (2.1885e-01)	Acc 0.771484 (0.771484)
Epoch: [91][300/616]	Loss 2.2992e-01 (2.2858e-01)	Acc 0.756836 (0.757485)
Epoch: [91][600/616]	Loss 2.1731e-01 (2.2835e-01)	Acc 0.769531 (0.757895)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758683)
Training Loss of Epoch 91: 0.22838011764413943
Training Acc of Epoch 91: 0.7578109120934959
Testing Acc of Epoch 91: 0.7586826086956522
Model with the best training loss saved! The loss is 0.22838011764413943
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2663e-01 (2.2663e-01)	Acc 0.758789 (0.758789)
Epoch: [92][300/616]	Loss 2.3941e-01 (2.2814e-01)	Acc 0.749023 (0.758273)
Epoch: [92][600/616]	Loss 2.1881e-01 (2.2832e-01)	Acc 0.762695 (0.757817)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758783)
Training Loss of Epoch 92: 0.22837078232106153
Training Acc of Epoch 92: 0.7577600990853659
Testing Acc of Epoch 92: 0.7587826086956522
Model with the best training loss saved! The loss is 0.22837078232106153
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3553e-01 (2.3553e-01)	Acc 0.750977 (0.750977)
Epoch: [93][300/616]	Loss 2.3382e-01 (2.2803e-01)	Acc 0.746094 (0.758280)
Epoch: [93][600/616]	Loss 2.1370e-01 (2.2824e-01)	Acc 0.770508 (0.757905)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758674)
Training Loss of Epoch 93: 0.22836363107208313
Training Acc of Epoch 93: 0.757763274898374
Testing Acc of Epoch 93: 0.7586739130434783
Model with the best training loss saved! The loss is 0.22836363107208313
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2634e-01 (2.2634e-01)	Acc 0.761719 (0.761719)
Epoch: [94][300/616]	Loss 2.2664e-01 (2.2785e-01)	Acc 0.757812 (0.758912)
Epoch: [94][600/616]	Loss 2.1222e-01 (2.2839e-01)	Acc 0.781250 (0.757825)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758100)
Training Loss of Epoch 94: 0.22834992643778887
Training Acc of Epoch 94: 0.7578490218495935
Testing Acc of Epoch 94: 0.7581
Model with the best training loss saved! The loss is 0.22834992643778887
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.1394e-01 (2.1394e-01)	Acc 0.764648 (0.764648)
Epoch: [95][300/616]	Loss 2.2223e-01 (2.2822e-01)	Acc 0.757812 (0.757751)
Epoch: [95][600/616]	Loss 2.2955e-01 (2.2835e-01)	Acc 0.753906 (0.757827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758257)
Training Loss of Epoch 95: 0.22838488652454159
Training Acc of Epoch 95: 0.7578013846544716
Testing Acc of Epoch 95: 0.7582565217391304
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3220e-01 (2.3220e-01)	Acc 0.757812 (0.757812)
Epoch: [96][300/616]	Loss 2.2252e-01 (2.2869e-01)	Acc 0.762695 (0.757319)
Epoch: [96][600/616]	Loss 2.2925e-01 (2.2832e-01)	Acc 0.764648 (0.757921)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757861)
Training Loss of Epoch 96: 0.22831088045263678
Training Acc of Epoch 96: 0.7579538236788618
Testing Acc of Epoch 96: 0.7578608695652174
Model with the best training loss saved! The loss is 0.22831088045263678
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.1596e-01 (2.1596e-01)	Acc 0.773438 (0.773438)
Epoch: [97][300/616]	Loss 2.3708e-01 (2.2907e-01)	Acc 0.745117 (0.757229)
Epoch: [97][600/616]	Loss 2.3112e-01 (2.2838e-01)	Acc 0.752930 (0.757912)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758061)
Training Loss of Epoch 97: 0.22834026554251105
Training Acc of Epoch 97: 0.7579569994918699
Testing Acc of Epoch 97: 0.7580608695652173
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3095e-01 (2.3095e-01)	Acc 0.742188 (0.742188)
Epoch: [98][300/616]	Loss 2.2739e-01 (2.2826e-01)	Acc 0.760742 (0.758448)
Epoch: [98][600/616]	Loss 2.2678e-01 (2.2840e-01)	Acc 0.747070 (0.757848)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757630)
Training Loss of Epoch 98: 0.22835175882025463
Training Acc of Epoch 98: 0.7579188897357724
Testing Acc of Epoch 98: 0.7576304347826087
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.1665e-01 (2.1665e-01)	Acc 0.777344 (0.777344)
Epoch: [99][300/616]	Loss 2.1370e-01 (2.2814e-01)	Acc 0.777344 (0.758280)
Epoch: [99][600/616]	Loss 2.2841e-01 (2.2828e-01)	Acc 0.759766 (0.757920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758057)
Training Loss of Epoch 99: 0.22831201478233182
Training Acc of Epoch 99: 0.7579284171747968
Testing Acc of Epoch 99: 0.7580565217391304
Early stopping not satisfied.
