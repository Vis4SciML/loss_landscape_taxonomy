train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_11b
different_width False
resnet18_width 64
weight_precision 11
bias_precision 11
act_precision 14
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_11b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_11b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=14, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=11, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0008e-01 (5.0008e-01)	Acc 0.203125 (0.203125)
Epoch: [0][300/616]	Loss 2.2764e-01 (2.8227e-01)	Acc 0.762695 (0.697415)
Epoch: [0][600/616]	Loss 2.5770e-01 (2.6433e-01)	Acc 0.720703 (0.720102)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747922)
Training Loss of Epoch 0: 0.2638285171210281
Training Acc of Epoch 0: 0.7206951854674797
Testing Acc of Epoch 0: 0.7479217391304348
Model with the best training loss saved! The loss is 0.2638285171210281
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.3374e-01 (2.3374e-01)	Acc 0.772461 (0.772461)
Epoch: [1][300/616]	Loss 2.3795e-01 (2.3961e-01)	Acc 0.749023 (0.748845)
Epoch: [1][600/616]	Loss 2.3837e-01 (2.3941e-01)	Acc 0.752930 (0.748671)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.746778)
Training Loss of Epoch 1: 0.2394455878715205
Training Acc of Epoch 1: 0.7485835873983739
Testing Acc of Epoch 1: 0.7467782608695652
Model with the best training loss saved! The loss is 0.2394455878715205
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3453e-01 (2.3453e-01)	Acc 0.755859 (0.755859)
Epoch: [2][300/616]	Loss 2.3864e-01 (2.3720e-01)	Acc 0.743164 (0.751032)
Epoch: [2][600/616]	Loss 2.4742e-01 (2.3743e-01)	Acc 0.738281 (0.750034)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747587)
Training Loss of Epoch 2: 0.237535179921282
Training Acc of Epoch 2: 0.749828506097561
Testing Acc of Epoch 2: 0.7475869565217391
Model with the best training loss saved! The loss is 0.237535179921282
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3841e-01 (2.3841e-01)	Acc 0.739258 (0.739258)
Epoch: [3][300/616]	Loss 2.4139e-01 (2.3656e-01)	Acc 0.750977 (0.750662)
Epoch: [3][600/616]	Loss 2.3859e-01 (2.3634e-01)	Acc 0.746094 (0.750892)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750335)
Training Loss of Epoch 3: 0.2364602970640834
Training Acc of Epoch 3: 0.7507590193089431
Testing Acc of Epoch 3: 0.7503347826086957
Model with the best training loss saved! The loss is 0.2364602970640834
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.2961e-01 (2.2961e-01)	Acc 0.759766 (0.759766)
Epoch: [4][300/616]	Loss 2.4528e-01 (2.3668e-01)	Acc 0.756836 (0.750003)
Epoch: [4][600/616]	Loss 2.2292e-01 (2.3553e-01)	Acc 0.756836 (0.751199)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752478)
Training Loss of Epoch 4: 0.23551598346330285
Training Acc of Epoch 4: 0.751270325203252
Testing Acc of Epoch 4: 0.7524782608695653
Model with the best training loss saved! The loss is 0.23551598346330285
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.4256e-01 (2.4256e-01)	Acc 0.759766 (0.759766)
Epoch: [5][300/616]	Loss 2.1733e-01 (2.3503e-01)	Acc 0.768555 (0.752722)
Epoch: [5][600/616]	Loss 2.3966e-01 (2.3510e-01)	Acc 0.752930 (0.752133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751670)
Training Loss of Epoch 5: 0.235222022417115
Training Acc of Epoch 5: 0.7519817073170731
Testing Acc of Epoch 5: 0.7516695652173913
Model with the best training loss saved! The loss is 0.235222022417115
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3502e-01 (2.3502e-01)	Acc 0.739258 (0.739258)
Epoch: [6][300/616]	Loss 2.5508e-01 (2.3423e-01)	Acc 0.726562 (0.753371)
Epoch: [6][600/616]	Loss 2.2908e-01 (2.3452e-01)	Acc 0.759766 (0.752824)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753730)
Training Loss of Epoch 6: 0.23453116867600418
Training Acc of Epoch 6: 0.7528439405487805
Testing Acc of Epoch 6: 0.7537304347826087
Model with the best training loss saved! The loss is 0.23453116867600418
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.3283e-01 (2.3283e-01)	Acc 0.754883 (0.754883)
Epoch: [7][300/616]	Loss 2.3840e-01 (2.3416e-01)	Acc 0.740234 (0.753494)
Epoch: [7][600/616]	Loss 2.2334e-01 (2.3425e-01)	Acc 0.773438 (0.752943)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754752)
Training Loss of Epoch 7: 0.2343530197695988
Training Acc of Epoch 7: 0.752856643800813
Testing Acc of Epoch 7: 0.7547521739130435
Model with the best training loss saved! The loss is 0.2343530197695988
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.2663e-01 (2.2663e-01)	Acc 0.753906 (0.753906)
Epoch: [8][300/616]	Loss 2.3680e-01 (2.3413e-01)	Acc 0.752930 (0.752735)
Epoch: [8][600/616]	Loss 2.2810e-01 (2.3356e-01)	Acc 0.763672 (0.753232)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753809)
Training Loss of Epoch 8: 0.23367540739416107
Training Acc of Epoch 8: 0.7531138846544716
Testing Acc of Epoch 8: 0.7538086956521739
Model with the best training loss saved! The loss is 0.23367540739416107
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3235e-01 (2.3235e-01)	Acc 0.755859 (0.755859)
Epoch: [9][300/616]	Loss 2.4609e-01 (2.3337e-01)	Acc 0.751953 (0.753997)
Epoch: [9][600/616]	Loss 2.4001e-01 (2.3384e-01)	Acc 0.750000 (0.753316)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753800)
Training Loss of Epoch 9: 0.23377703420999574
Training Acc of Epoch 9: 0.7533885924796748
Testing Acc of Epoch 9: 0.7538
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3035e-01 (2.3035e-01)	Acc 0.763672 (0.763672)
Epoch: [10][300/616]	Loss 2.3217e-01 (2.3290e-01)	Acc 0.749023 (0.754341)
Epoch: [10][600/616]	Loss 2.4555e-01 (2.3364e-01)	Acc 0.733398 (0.753235)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752126)
Training Loss of Epoch 10: 0.23365222683282402
Training Acc of Epoch 10: 0.753174225101626
Testing Acc of Epoch 10: 0.7521260869565217
Model with the best training loss saved! The loss is 0.23365222683282402
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3371e-01 (2.3371e-01)	Acc 0.759766 (0.759766)
Epoch: [11][300/616]	Loss 2.3812e-01 (2.3339e-01)	Acc 0.745117 (0.753452)
Epoch: [11][600/616]	Loss 2.2498e-01 (2.3349e-01)	Acc 0.758789 (0.753026)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753300)
Training Loss of Epoch 11: 0.23353507286164818
Training Acc of Epoch 11: 0.7530694232723577
Testing Acc of Epoch 11: 0.7533
Model with the best training loss saved! The loss is 0.23353507286164818
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3921e-01 (2.3921e-01)	Acc 0.750977 (0.750977)
Epoch: [12][300/616]	Loss 2.2637e-01 (2.3339e-01)	Acc 0.766602 (0.752686)
Epoch: [12][600/616]	Loss 2.3661e-01 (2.3349e-01)	Acc 0.747070 (0.753230)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751974)
Training Loss of Epoch 12: 0.23356704968747086
Training Acc of Epoch 12: 0.7531789888211382
Testing Acc of Epoch 12: 0.7519739130434783
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3500e-01 (2.3500e-01)	Acc 0.765625 (0.765625)
Epoch: [13][300/616]	Loss 2.3409e-01 (2.3466e-01)	Acc 0.753906 (0.752518)
Epoch: [13][600/616]	Loss 2.3197e-01 (2.3369e-01)	Acc 0.756836 (0.753568)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755183)
Training Loss of Epoch 13: 0.2336457798151466
Training Acc of Epoch 13: 0.7535632621951219
Testing Acc of Epoch 13: 0.7551826086956521
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3467e-01 (2.3467e-01)	Acc 0.748047 (0.748047)
Epoch: [14][300/616]	Loss 2.3878e-01 (2.3356e-01)	Acc 0.756836 (0.753244)
Epoch: [14][600/616]	Loss 2.2602e-01 (2.3340e-01)	Acc 0.763672 (0.753368)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755313)
Training Loss of Epoch 14: 0.23330831840270902
Training Acc of Epoch 14: 0.7534775152439024
Testing Acc of Epoch 14: 0.7553130434782609
Model with the best training loss saved! The loss is 0.23330831840270902
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.2541e-01 (2.2541e-01)	Acc 0.768555 (0.768555)
Epoch: [15][300/616]	Loss 2.3830e-01 (2.3335e-01)	Acc 0.758789 (0.753864)
Epoch: [15][600/616]	Loss 2.3990e-01 (2.3300e-01)	Acc 0.745117 (0.753968)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.752748)
Training Loss of Epoch 15: 0.23309889319466381
Training Acc of Epoch 15: 0.7538586128048781
Testing Acc of Epoch 15: 0.7527478260869566
Model with the best training loss saved! The loss is 0.23309889319466381
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2860e-01 (2.2860e-01)	Acc 0.764648 (0.764648)
Epoch: [16][300/616]	Loss 2.2476e-01 (2.3406e-01)	Acc 0.754883 (0.753108)
Epoch: [16][600/616]	Loss 2.4029e-01 (2.3335e-01)	Acc 0.745117 (0.753581)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754278)
Training Loss of Epoch 16: 0.23332379511216791
Training Acc of Epoch 16: 0.7536394817073171
Testing Acc of Epoch 16: 0.7542782608695652
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.2070e-01 (2.2070e-01)	Acc 0.764648 (0.764648)
Epoch: [17][300/616]	Loss 2.4544e-01 (2.3309e-01)	Acc 0.740234 (0.752660)
Epoch: [17][600/616]	Loss 2.2694e-01 (2.3315e-01)	Acc 0.767578 (0.753500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755613)
Training Loss of Epoch 17: 0.23315633106522443
Training Acc of Epoch 17: 0.7534521087398374
Testing Acc of Epoch 17: 0.7556130434782609
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3527e-01 (2.3527e-01)	Acc 0.744141 (0.744141)
Epoch: [18][300/616]	Loss 2.3812e-01 (2.3233e-01)	Acc 0.742188 (0.754257)
Epoch: [18][600/616]	Loss 2.2475e-01 (2.3297e-01)	Acc 0.765625 (0.753710)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755261)
Training Loss of Epoch 18: 0.23301000478791029
Training Acc of Epoch 18: 0.7536093114837399
Testing Acc of Epoch 18: 0.7552608695652174
Model with the best training loss saved! The loss is 0.23301000478791029
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3084e-01 (2.3084e-01)	Acc 0.764648 (0.764648)
Epoch: [19][300/616]	Loss 2.2399e-01 (2.3293e-01)	Acc 0.776367 (0.753767)
Epoch: [19][600/616]	Loss 2.3664e-01 (2.3309e-01)	Acc 0.743164 (0.753859)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754413)
Training Loss of Epoch 19: 0.23313850222564325
Training Acc of Epoch 19: 0.7538522611788618
Testing Acc of Epoch 19: 0.7544130434782609
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.1571e-01 (2.1571e-01)	Acc 0.767578 (0.767578)
Epoch: [20][300/616]	Loss 2.3597e-01 (2.3303e-01)	Acc 0.748047 (0.754373)
Epoch: [20][600/616]	Loss 2.4110e-01 (2.3298e-01)	Acc 0.748047 (0.754373)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753983)
Training Loss of Epoch 20: 0.23301295575087633
Training Acc of Epoch 20: 0.7543206935975609
Testing Acc of Epoch 20: 0.7539826086956521
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.6057e-01 (2.6057e-01)	Acc 0.704102 (0.704102)
Epoch: [21][300/616]	Loss 2.2633e-01 (2.3333e-01)	Acc 0.768555 (0.753368)
Epoch: [21][600/616]	Loss 2.1656e-01 (2.3318e-01)	Acc 0.778320 (0.753814)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755787)
Training Loss of Epoch 21: 0.23320119204075357
Training Acc of Epoch 21: 0.7537061737804878
Testing Acc of Epoch 21: 0.7557869565217391
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.5975e-01 (2.5975e-01)	Acc 0.721680 (0.721680)
Epoch: [22][300/616]	Loss 2.4037e-01 (2.3296e-01)	Acc 0.738281 (0.754276)
Epoch: [22][600/616]	Loss 2.5197e-01 (2.3299e-01)	Acc 0.727539 (0.753858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754004)
Training Loss of Epoch 22: 0.23310253843059384
Training Acc of Epoch 22: 0.7537236407520326
Testing Acc of Epoch 22: 0.7540043478260869
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2792e-01 (2.2792e-01)	Acc 0.762695 (0.762695)
Epoch: [23][300/616]	Loss 2.2551e-01 (2.3254e-01)	Acc 0.756836 (0.754124)
Epoch: [23][600/616]	Loss 2.3875e-01 (2.3294e-01)	Acc 0.734375 (0.753567)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754465)
Training Loss of Epoch 23: 0.23303599471483774
Training Acc of Epoch 23: 0.7534457571138211
Testing Acc of Epoch 23: 0.7544652173913043
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.1536e-01 (2.1536e-01)	Acc 0.778320 (0.778320)
Epoch: [24][300/616]	Loss 2.4539e-01 (2.3341e-01)	Acc 0.739258 (0.753942)
Epoch: [24][600/616]	Loss 2.4110e-01 (2.3322e-01)	Acc 0.749023 (0.753783)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754143)
Training Loss of Epoch 24: 0.23322622865680756
Training Acc of Epoch 24: 0.7538522611788618
Testing Acc of Epoch 24: 0.7541434782608696
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3610e-01 (2.3610e-01)	Acc 0.755859 (0.755859)
Epoch: [25][300/616]	Loss 2.1034e-01 (2.3271e-01)	Acc 0.784180 (0.754665)
Epoch: [25][600/616]	Loss 2.3769e-01 (2.3314e-01)	Acc 0.751953 (0.753965)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754030)
Training Loss of Epoch 25: 0.2330030263681722
Training Acc of Epoch 25: 0.7541349085365854
Testing Acc of Epoch 25: 0.7540304347826087
Model with the best training loss saved! The loss is 0.2330030263681722
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.0961e-01 (2.0961e-01)	Acc 0.776367 (0.776367)
Epoch: [26][300/616]	Loss 2.3429e-01 (2.3275e-01)	Acc 0.753906 (0.754283)
Epoch: [26][600/616]	Loss 2.3479e-01 (2.3307e-01)	Acc 0.755859 (0.753884)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754409)
Training Loss of Epoch 26: 0.2330639700094859
Training Acc of Epoch 26: 0.7538998983739837
Testing Acc of Epoch 26: 0.7544086956521739
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4683e-01 (2.4683e-01)	Acc 0.738281 (0.738281)
Epoch: [27][300/616]	Loss 2.3223e-01 (2.3306e-01)	Acc 0.761719 (0.753932)
Epoch: [27][600/616]	Loss 2.2360e-01 (2.3305e-01)	Acc 0.763672 (0.754246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752143)
Training Loss of Epoch 27: 0.23313249204216935
Training Acc of Epoch 27: 0.7540920350609757
Testing Acc of Epoch 27: 0.7521434782608696
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3136e-01 (2.3136e-01)	Acc 0.751953 (0.751953)
Epoch: [28][300/616]	Loss 2.2188e-01 (2.3272e-01)	Acc 0.768555 (0.754237)
Epoch: [28][600/616]	Loss 2.5464e-01 (2.3311e-01)	Acc 0.730469 (0.753780)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753274)
Training Loss of Epoch 28: 0.23313046555693556
Training Acc of Epoch 28: 0.7537823932926829
Testing Acc of Epoch 28: 0.7532739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.3725e-01 (2.3725e-01)	Acc 0.753906 (0.753906)
Epoch: [29][300/616]	Loss 2.2204e-01 (2.3312e-01)	Acc 0.775391 (0.753760)
Epoch: [29][600/616]	Loss 2.2574e-01 (2.3324e-01)	Acc 0.764648 (0.753630)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756078)
Training Loss of Epoch 29: 0.23314792874867354
Training Acc of Epoch 29: 0.7537792174796748
Testing Acc of Epoch 29: 0.7560782608695652
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.3583e-01 (2.3583e-01)	Acc 0.739258 (0.739258)
Epoch: [30][300/616]	Loss 2.3294e-01 (2.3328e-01)	Acc 0.756836 (0.753536)
Epoch: [30][600/616]	Loss 2.5349e-01 (2.3302e-01)	Acc 0.725586 (0.753986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753426)
Training Loss of Epoch 30: 0.23311134322387417
Training Acc of Epoch 30: 0.7539014862804878
Testing Acc of Epoch 30: 0.7534260869565217
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.2660e-01 (2.2660e-01)	Acc 0.760742 (0.760742)
Epoch: [31][300/616]	Loss 2.3064e-01 (2.3290e-01)	Acc 0.749023 (0.754458)
Epoch: [31][600/616]	Loss 2.4070e-01 (2.3299e-01)	Acc 0.748047 (0.753773)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754552)
Training Loss of Epoch 31: 0.23298551658304725
Training Acc of Epoch 31: 0.7537760416666667
Testing Acc of Epoch 31: 0.7545521739130435
Model with the best training loss saved! The loss is 0.23298551658304725
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3622e-01 (2.3622e-01)	Acc 0.747070 (0.747070)
Epoch: [32][300/616]	Loss 2.2656e-01 (2.3363e-01)	Acc 0.772461 (0.753605)
Epoch: [32][600/616]	Loss 2.3700e-01 (2.3299e-01)	Acc 0.750977 (0.754106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.754435)
Training Loss of Epoch 32: 0.23294755184068913
Training Acc of Epoch 32: 0.7540983866869919
Testing Acc of Epoch 32: 0.7544347826086957
Model with the best training loss saved! The loss is 0.23294755184068913
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.3153e-01 (2.3153e-01)	Acc 0.752930 (0.752930)
Epoch: [33][300/616]	Loss 2.4506e-01 (2.3262e-01)	Acc 0.744141 (0.754052)
Epoch: [33][600/616]	Loss 2.4379e-01 (2.3291e-01)	Acc 0.742188 (0.753973)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751991)
Training Loss of Epoch 33: 0.2329428147978899
Training Acc of Epoch 33: 0.7539681783536586
Testing Acc of Epoch 33: 0.7519913043478261
Model with the best training loss saved! The loss is 0.2329428147978899
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3790e-01 (2.3790e-01)	Acc 0.747070 (0.747070)
Epoch: [34][300/616]	Loss 2.4702e-01 (2.3363e-01)	Acc 0.722656 (0.753413)
Epoch: [34][600/616]	Loss 2.3028e-01 (2.3357e-01)	Acc 0.757812 (0.753645)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754930)
Training Loss of Epoch 34: 0.23353531309744208
Training Acc of Epoch 34: 0.7537363440040651
Testing Acc of Epoch 34: 0.7549304347826087
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4201e-01 (2.4201e-01)	Acc 0.743164 (0.743164)
Epoch: [35][300/616]	Loss 2.3364e-01 (2.3249e-01)	Acc 0.759766 (0.754970)
Epoch: [35][600/616]	Loss 2.2458e-01 (2.3284e-01)	Acc 0.756836 (0.754002)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754548)
Training Loss of Epoch 35: 0.2329104130103336
Training Acc of Epoch 35: 0.7539665904471544
Testing Acc of Epoch 35: 0.7545478260869565
Model with the best training loss saved! The loss is 0.2329104130103336
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.2970e-01 (2.2970e-01)	Acc 0.747070 (0.747070)
Epoch: [36][300/616]	Loss 2.5047e-01 (2.3291e-01)	Acc 0.732422 (0.754322)
Epoch: [36][600/616]	Loss 2.5301e-01 (2.3282e-01)	Acc 0.725586 (0.753773)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754422)
Training Loss of Epoch 36: 0.23275781891694883
Training Acc of Epoch 36: 0.7539205411585366
Testing Acc of Epoch 36: 0.7544217391304348
Model with the best training loss saved! The loss is 0.23275781891694883
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4431e-01 (2.4431e-01)	Acc 0.744141 (0.744141)
Epoch: [37][300/616]	Loss 2.2837e-01 (2.3405e-01)	Acc 0.750977 (0.752644)
Epoch: [37][600/616]	Loss 2.3602e-01 (2.3307e-01)	Acc 0.741211 (0.753448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754678)
Training Loss of Epoch 37: 0.2331405554845081
Training Acc of Epoch 37: 0.7532933180894309
Testing Acc of Epoch 37: 0.7546782608695652
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3118e-01 (2.3118e-01)	Acc 0.749023 (0.749023)
Epoch: [38][300/616]	Loss 2.2123e-01 (2.3344e-01)	Acc 0.773438 (0.753562)
Epoch: [38][600/616]	Loss 2.2928e-01 (2.3335e-01)	Acc 0.758789 (0.753518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.754113)
Training Loss of Epoch 38: 0.23337190776336483
Training Acc of Epoch 38: 0.7535045096544716
Testing Acc of Epoch 38: 0.7541130434782609
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2792e-01 (2.2792e-01)	Acc 0.766602 (0.766602)
Epoch: [39][300/616]	Loss 2.3413e-01 (2.3279e-01)	Acc 0.746094 (0.753997)
Epoch: [39][600/616]	Loss 2.3377e-01 (2.3308e-01)	Acc 0.749023 (0.753596)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754200)
Training Loss of Epoch 39: 0.2331021195020133
Training Acc of Epoch 39: 0.7535664380081301
Testing Acc of Epoch 39: 0.7542
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.1746e-01 (2.1746e-01)	Acc 0.770508 (0.770508)
Epoch: [40][300/616]	Loss 2.2854e-01 (2.3324e-01)	Acc 0.757812 (0.753134)
Epoch: [40][600/616]	Loss 2.3490e-01 (2.3294e-01)	Acc 0.754883 (0.753801)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756113)
Training Loss of Epoch 40: 0.2329523395474364
Training Acc of Epoch 40: 0.753758574695122
Testing Acc of Epoch 40: 0.7561130434782609
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.2979e-01 (2.2979e-01)	Acc 0.750977 (0.750977)
Epoch: [41][300/616]	Loss 2.2517e-01 (2.3237e-01)	Acc 0.776367 (0.753731)
Epoch: [41][600/616]	Loss 2.2095e-01 (2.3284e-01)	Acc 0.753906 (0.753884)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752691)
Training Loss of Epoch 41: 0.2328295105114216
Training Acc of Epoch 41: 0.7538792555894309
Testing Acc of Epoch 41: 0.752691304347826
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3584e-01 (2.3584e-01)	Acc 0.749023 (0.749023)
Epoch: [42][300/616]	Loss 2.3425e-01 (2.3371e-01)	Acc 0.758789 (0.753021)
Epoch: [42][600/616]	Loss 2.2512e-01 (2.3314e-01)	Acc 0.774414 (0.753799)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753391)
Training Loss of Epoch 42: 0.2330313932604906
Training Acc of Epoch 42: 0.7540015243902439
Testing Acc of Epoch 42: 0.7533913043478261
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3803e-01 (2.3803e-01)	Acc 0.752930 (0.752930)
Epoch: [43][300/616]	Loss 2.3493e-01 (2.3341e-01)	Acc 0.763672 (0.753348)
Epoch: [43][600/616]	Loss 2.2996e-01 (2.3295e-01)	Acc 0.767578 (0.753757)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755096)
Training Loss of Epoch 43: 0.23280834936029543
Training Acc of Epoch 43: 0.7539745299796748
Testing Acc of Epoch 43: 0.7550956521739131
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3509e-01 (2.3509e-01)	Acc 0.748047 (0.748047)
Epoch: [44][300/616]	Loss 2.3188e-01 (2.3322e-01)	Acc 0.766602 (0.753643)
Epoch: [44][600/616]	Loss 2.4527e-01 (2.3297e-01)	Acc 0.745117 (0.753961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755348)
Training Loss of Epoch 44: 0.23291951417922974
Training Acc of Epoch 44: 0.7540396341463415
Testing Acc of Epoch 44: 0.7553478260869565
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2367e-01 (2.2367e-01)	Acc 0.752930 (0.752930)
Epoch: [45][300/616]	Loss 2.5486e-01 (2.3287e-01)	Acc 0.741211 (0.754266)
Epoch: [45][600/616]	Loss 2.2121e-01 (2.3320e-01)	Acc 0.766602 (0.753646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754730)
Training Loss of Epoch 45: 0.23330142129727496
Training Acc of Epoch 45: 0.7534616361788617
Testing Acc of Epoch 45: 0.7547304347826087
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.3106e-01 (2.3106e-01)	Acc 0.750977 (0.750977)
Epoch: [46][300/616]	Loss 2.3841e-01 (2.3264e-01)	Acc 0.754883 (0.754305)
Epoch: [46][600/616]	Loss 2.3292e-01 (2.3288e-01)	Acc 0.742188 (0.754150)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.754983)
Training Loss of Epoch 46: 0.23298343403068014
Training Acc of Epoch 46: 0.7540110518292683
Testing Acc of Epoch 46: 0.7549826086956521
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2755e-01 (2.2755e-01)	Acc 0.769531 (0.769531)
Epoch: [47][300/616]	Loss 2.3720e-01 (2.3240e-01)	Acc 0.753906 (0.754549)
Epoch: [47][600/616]	Loss 2.4496e-01 (2.3281e-01)	Acc 0.739258 (0.754244)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751661)
Training Loss of Epoch 47: 0.23283217428176384
Training Acc of Epoch 47: 0.7542682926829268
Testing Acc of Epoch 47: 0.7516608695652174
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.2987e-01 (2.2987e-01)	Acc 0.759766 (0.759766)
Epoch: [48][300/616]	Loss 2.4764e-01 (2.3229e-01)	Acc 0.728516 (0.755039)
Epoch: [48][600/616]	Loss 2.4370e-01 (2.3294e-01)	Acc 0.733398 (0.754127)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751178)
Training Loss of Epoch 48: 0.23291924406842487
Training Acc of Epoch 48: 0.7540936229674797
Testing Acc of Epoch 48: 0.7511782608695652
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3549e-01 (2.3549e-01)	Acc 0.757812 (0.757812)
Epoch: [49][300/616]	Loss 2.3547e-01 (2.3315e-01)	Acc 0.758789 (0.753708)
Epoch: [49][600/616]	Loss 2.3759e-01 (2.3296e-01)	Acc 0.747070 (0.754109)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750278)
Training Loss of Epoch 49: 0.23297752270369027
Training Acc of Epoch 49: 0.7540491615853658
Testing Acc of Epoch 49: 0.7502782608695652
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.5167e-01 (2.5167e-01)	Acc 0.723633 (0.723633)
Epoch: [50][300/616]	Loss 2.3218e-01 (2.3275e-01)	Acc 0.745117 (0.754146)
Epoch: [50][600/616]	Loss 2.4159e-01 (2.3311e-01)	Acc 0.734375 (0.753807)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755839)
Training Loss of Epoch 50: 0.2332112168393484
Training Acc of Epoch 50: 0.7537728658536585
Testing Acc of Epoch 50: 0.7558391304347826
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2976e-01 (2.2976e-01)	Acc 0.768555 (0.768555)
Epoch: [51][300/616]	Loss 2.3509e-01 (2.3365e-01)	Acc 0.731445 (0.752576)
Epoch: [51][600/616]	Loss 2.4696e-01 (2.3310e-01)	Acc 0.734375 (0.753679)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755391)
Training Loss of Epoch 51: 0.23308454986994828
Training Acc of Epoch 51: 0.7537252286585366
Testing Acc of Epoch 51: 0.7553913043478261
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.2536e-01 (2.2536e-01)	Acc 0.759766 (0.759766)
Epoch: [52][300/616]	Loss 2.3444e-01 (2.3303e-01)	Acc 0.762695 (0.754565)
Epoch: [52][600/616]	Loss 2.3693e-01 (2.3314e-01)	Acc 0.750977 (0.753978)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755670)
Training Loss of Epoch 52: 0.23313035407686622
Training Acc of Epoch 52: 0.7539284806910569
Testing Acc of Epoch 52: 0.7556695652173913
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3084e-01 (2.3084e-01)	Acc 0.770508 (0.770508)
Epoch: [53][300/616]	Loss 2.1209e-01 (2.3270e-01)	Acc 0.784180 (0.754104)
Epoch: [53][600/616]	Loss 2.2882e-01 (2.3285e-01)	Acc 0.758789 (0.754106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754091)
Training Loss of Epoch 53: 0.23289446995510318
Training Acc of Epoch 53: 0.7540872713414634
Testing Acc of Epoch 53: 0.7540913043478261
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.4588e-01 (2.4588e-01)	Acc 0.741211 (0.741211)
Epoch: [54][300/616]	Loss 2.3824e-01 (2.3376e-01)	Acc 0.750000 (0.753163)
Epoch: [54][600/616]	Loss 2.3391e-01 (2.3308e-01)	Acc 0.765625 (0.754077)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.748461)
Training Loss of Epoch 54: 0.23319932980750635
Training Acc of Epoch 54: 0.7539459476626016
Testing Acc of Epoch 54: 0.7484608695652174
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3836e-01 (2.3836e-01)	Acc 0.740234 (0.740234)
Epoch: [55][300/616]	Loss 2.2360e-01 (2.3273e-01)	Acc 0.773438 (0.754078)
Epoch: [55][600/616]	Loss 2.3318e-01 (2.3286e-01)	Acc 0.754883 (0.754156)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753722)
Training Loss of Epoch 55: 0.23290369093902713
Training Acc of Epoch 55: 0.7541126778455285
Testing Acc of Epoch 55: 0.7537217391304348
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4246e-01 (2.4246e-01)	Acc 0.741211 (0.741211)
Epoch: [56][300/616]	Loss 2.3380e-01 (2.3364e-01)	Acc 0.751953 (0.753167)
Epoch: [56][600/616]	Loss 2.4114e-01 (2.3306e-01)	Acc 0.744141 (0.753640)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751583)
Training Loss of Epoch 56: 0.23304628044124542
Training Acc of Epoch 56: 0.7536744156504065
Testing Acc of Epoch 56: 0.7515826086956522
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.2909e-01 (2.2909e-01)	Acc 0.749023 (0.749023)
Epoch: [57][300/616]	Loss 2.2963e-01 (2.3272e-01)	Acc 0.754883 (0.754250)
Epoch: [57][600/616]	Loss 2.3487e-01 (2.3314e-01)	Acc 0.753906 (0.753572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755470)
Training Loss of Epoch 57: 0.23306377413796214
Training Acc of Epoch 57: 0.7536855309959349
Testing Acc of Epoch 57: 0.7554695652173913
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3002e-01 (2.3002e-01)	Acc 0.757812 (0.757812)
Epoch: [58][300/616]	Loss 2.3976e-01 (2.3292e-01)	Acc 0.740234 (0.753835)
Epoch: [58][600/616]	Loss 2.3678e-01 (2.3275e-01)	Acc 0.749023 (0.754182)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754696)
Training Loss of Epoch 58: 0.23277616609887378
Training Acc of Epoch 58: 0.7541317327235773
Testing Acc of Epoch 58: 0.754695652173913
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.4426e-01 (2.4426e-01)	Acc 0.733398 (0.733398)
Epoch: [59][300/616]	Loss 2.3409e-01 (2.3275e-01)	Acc 0.751953 (0.754000)
Epoch: [59][600/616]	Loss 2.2008e-01 (2.3288e-01)	Acc 0.769531 (0.753922)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749109)
Training Loss of Epoch 59: 0.23300405214957107
Training Acc of Epoch 59: 0.753663300304878
Testing Acc of Epoch 59: 0.7491086956521739
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3898e-01 (2.3898e-01)	Acc 0.747070 (0.747070)
Epoch: [60][300/616]	Loss 2.2750e-01 (2.3293e-01)	Acc 0.764648 (0.753861)
Epoch: [60][600/616]	Loss 2.2969e-01 (2.3302e-01)	Acc 0.757812 (0.753879)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753309)
Training Loss of Epoch 60: 0.23307704099310123
Training Acc of Epoch 60: 0.7539173653455284
Testing Acc of Epoch 60: 0.753308695652174
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3249e-01 (2.3249e-01)	Acc 0.752930 (0.752930)
Epoch: [61][300/616]	Loss 2.2979e-01 (2.3259e-01)	Acc 0.763672 (0.754815)
Epoch: [61][600/616]	Loss 2.3527e-01 (2.3255e-01)	Acc 0.754883 (0.754496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756530)
Training Loss of Epoch 61: 0.2326175399912082
Training Acc of Epoch 61: 0.7543365726626017
Testing Acc of Epoch 61: 0.7565304347826087
Model with the best training loss saved! The loss is 0.2326175399912082
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3345e-01 (2.3345e-01)	Acc 0.754883 (0.754883)
Epoch: [62][300/616]	Loss 2.3182e-01 (2.3264e-01)	Acc 0.757812 (0.754049)
Epoch: [62][600/616]	Loss 2.3589e-01 (2.3299e-01)	Acc 0.753906 (0.753700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755448)
Training Loss of Epoch 62: 0.23295680047050724
Training Acc of Epoch 62: 0.753687118902439
Testing Acc of Epoch 62: 0.7554478260869565
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3167e-01 (2.3167e-01)	Acc 0.748047 (0.748047)
Epoch: [63][300/616]	Loss 2.4097e-01 (2.3231e-01)	Acc 0.736328 (0.754883)
Epoch: [63][600/616]	Loss 2.3030e-01 (2.3300e-01)	Acc 0.758789 (0.754061)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753226)
Training Loss of Epoch 63: 0.2330655818063069
Training Acc of Epoch 63: 0.754003112296748
Testing Acc of Epoch 63: 0.7532260869565217
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3046e-01 (2.3046e-01)	Acc 0.748047 (0.748047)
Epoch: [64][300/616]	Loss 2.2639e-01 (2.3325e-01)	Acc 0.759766 (0.753215)
Epoch: [64][600/616]	Loss 2.3125e-01 (2.3279e-01)	Acc 0.758789 (0.754269)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752013)
Training Loss of Epoch 64: 0.23276586690084722
Training Acc of Epoch 64: 0.7543492759146342
Testing Acc of Epoch 64: 0.7520130434782609
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4669e-01 (2.4669e-01)	Acc 0.734375 (0.734375)
Epoch: [65][300/616]	Loss 2.2922e-01 (2.3281e-01)	Acc 0.756836 (0.754004)
Epoch: [65][600/616]	Loss 2.4177e-01 (2.3304e-01)	Acc 0.753906 (0.753984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.753383)
Training Loss of Epoch 65: 0.23299727926893932
Training Acc of Epoch 65: 0.7540682164634146
Testing Acc of Epoch 65: 0.7533826086956522
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.2187e-01 (2.2187e-01)	Acc 0.770508 (0.770508)
Epoch: [66][300/616]	Loss 2.3464e-01 (2.3286e-01)	Acc 0.743164 (0.754367)
Epoch: [66][600/616]	Loss 2.4679e-01 (2.3293e-01)	Acc 0.742188 (0.753929)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753374)
Training Loss of Epoch 66: 0.2329462429614571
Training Acc of Epoch 66: 0.7538538490853659
Testing Acc of Epoch 66: 0.7533739130434782
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.2750e-01 (2.2750e-01)	Acc 0.750977 (0.750977)
Epoch: [67][300/616]	Loss 2.3759e-01 (2.3217e-01)	Acc 0.743164 (0.755026)
Epoch: [67][600/616]	Loss 2.2258e-01 (2.3296e-01)	Acc 0.758789 (0.754041)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755543)
Training Loss of Epoch 67: 0.23293278558952052
Training Acc of Epoch 67: 0.7541222052845529
Testing Acc of Epoch 67: 0.7555434782608695
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3304e-01 (2.3304e-01)	Acc 0.754883 (0.754883)
Epoch: [68][300/616]	Loss 2.2811e-01 (2.3261e-01)	Acc 0.759766 (0.754114)
Epoch: [68][600/616]	Loss 2.3659e-01 (2.3293e-01)	Acc 0.747070 (0.753711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753000)
Training Loss of Epoch 68: 0.23290146371213402
Training Acc of Epoch 68: 0.7537029979674796
Testing Acc of Epoch 68: 0.753
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.2290e-01 (2.2290e-01)	Acc 0.766602 (0.766602)
Epoch: [69][300/616]	Loss 2.2079e-01 (2.3356e-01)	Acc 0.768555 (0.753332)
Epoch: [69][600/616]	Loss 2.3841e-01 (2.3310e-01)	Acc 0.758789 (0.753922)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755843)
Training Loss of Epoch 69: 0.23303305274102745
Training Acc of Epoch 69: 0.7539919969512195
Testing Acc of Epoch 69: 0.7558434782608696
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.2791e-01 (2.2791e-01)	Acc 0.754883 (0.754883)
Epoch: [70][300/616]	Loss 2.2115e-01 (2.3243e-01)	Acc 0.759766 (0.754597)
Epoch: [70][600/616]	Loss 2.4500e-01 (2.3283e-01)	Acc 0.744141 (0.754382)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.754687)
Training Loss of Epoch 70: 0.2328053539361411
Training Acc of Epoch 70: 0.7543619791666667
Testing Acc of Epoch 70: 0.7546869565217391
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.2968e-01 (2.2968e-01)	Acc 0.767578 (0.767578)
Epoch: [71][300/616]	Loss 2.3677e-01 (2.3350e-01)	Acc 0.746094 (0.753017)
Epoch: [71][600/616]	Loss 2.4100e-01 (2.3315e-01)	Acc 0.747070 (0.753697)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754170)
Training Loss of Epoch 71: 0.2331204191455996
Training Acc of Epoch 71: 0.7537172891260162
Testing Acc of Epoch 71: 0.7541695652173913
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4488e-01 (2.4488e-01)	Acc 0.732422 (0.732422)
Epoch: [72][300/616]	Loss 2.3691e-01 (2.3281e-01)	Acc 0.755859 (0.754091)
Epoch: [72][600/616]	Loss 2.4005e-01 (2.3281e-01)	Acc 0.748047 (0.753880)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755974)
Training Loss of Epoch 72: 0.23287921543043805
Training Acc of Epoch 72: 0.7538109756097561
Testing Acc of Epoch 72: 0.7559739130434783
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.2130e-01 (2.2130e-01)	Acc 0.765625 (0.765625)
Epoch: [73][300/616]	Loss 2.2974e-01 (2.3303e-01)	Acc 0.753906 (0.754013)
Epoch: [73][600/616]	Loss 2.4051e-01 (2.3293e-01)	Acc 0.747070 (0.754100)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.752665)
Training Loss of Epoch 73: 0.23297552056913454
Training Acc of Epoch 73: 0.7540364583333333
Testing Acc of Epoch 73: 0.7526652173913043
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4739e-01 (2.4739e-01)	Acc 0.727539 (0.727539)
Epoch: [74][300/616]	Loss 2.3059e-01 (2.3298e-01)	Acc 0.769531 (0.754065)
Epoch: [74][600/616]	Loss 2.3480e-01 (2.3297e-01)	Acc 0.757812 (0.754059)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756404)
Training Loss of Epoch 74: 0.2329321159095299
Training Acc of Epoch 74: 0.7541031504065041
Testing Acc of Epoch 74: 0.756404347826087
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4298e-01 (2.4298e-01)	Acc 0.738281 (0.738281)
Epoch: [75][300/616]	Loss 2.2061e-01 (2.2882e-01)	Acc 0.772461 (0.757576)
Epoch: [75][600/616]	Loss 2.2479e-01 (2.2871e-01)	Acc 0.752930 (0.757494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758974)
Training Loss of Epoch 75: 0.22871012043177597
Training Acc of Epoch 75: 0.7574806275406504
Testing Acc of Epoch 75: 0.7589739130434783
Model with the best training loss saved! The loss is 0.22871012043177597
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2780e-01 (2.2780e-01)	Acc 0.764648 (0.764648)
Epoch: [76][300/616]	Loss 2.2786e-01 (2.2818e-01)	Acc 0.772461 (0.757975)
Epoch: [76][600/616]	Loss 2.2410e-01 (2.2828e-01)	Acc 0.761719 (0.758014)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758635)
Training Loss of Epoch 76: 0.22825378993662393
Training Acc of Epoch 76: 0.7580760924796748
Testing Acc of Epoch 76: 0.7586347826086957
Model with the best training loss saved! The loss is 0.22825378993662393
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.2801e-01 (2.2801e-01)	Acc 0.763672 (0.763672)
Epoch: [77][300/616]	Loss 2.2749e-01 (2.2812e-01)	Acc 0.757812 (0.758494)
Epoch: [77][600/616]	Loss 2.2264e-01 (2.2834e-01)	Acc 0.772461 (0.758048)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758422)
Training Loss of Epoch 77: 0.22815482129411
Training Acc of Epoch 77: 0.758301575203252
Testing Acc of Epoch 77: 0.7584217391304348
Model with the best training loss saved! The loss is 0.22815482129411
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.1425e-01 (2.1425e-01)	Acc 0.782227 (0.782227)
Epoch: [78][300/616]	Loss 2.4252e-01 (2.2792e-01)	Acc 0.748047 (0.758351)
Epoch: [78][600/616]	Loss 2.3058e-01 (2.2810e-01)	Acc 0.748047 (0.758058)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759143)
Training Loss of Epoch 78: 0.2280261267006882
Training Acc of Epoch 78: 0.758177718495935
Testing Acc of Epoch 78: 0.7591434782608696
Model with the best training loss saved! The loss is 0.2280261267006882
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2712e-01 (2.2712e-01)	Acc 0.756836 (0.756836)
Epoch: [79][300/616]	Loss 2.2848e-01 (2.2768e-01)	Acc 0.753906 (0.758387)
Epoch: [79][600/616]	Loss 2.3735e-01 (2.2805e-01)	Acc 0.740234 (0.758305)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758652)
Training Loss of Epoch 79: 0.22792422112410632
Training Acc of Epoch 79: 0.7585143546747968
Testing Acc of Epoch 79: 0.7586521739130435
Model with the best training loss saved! The loss is 0.22792422112410632
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2543e-01 (2.2543e-01)	Acc 0.764648 (0.764648)
Epoch: [80][300/616]	Loss 2.3622e-01 (2.2798e-01)	Acc 0.744141 (0.759114)
Epoch: [80][600/616]	Loss 2.3814e-01 (2.2799e-01)	Acc 0.750000 (0.758500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758939)
Training Loss of Epoch 80: 0.22788880435916467
Training Acc of Epoch 80: 0.7586445630081301
Testing Acc of Epoch 80: 0.7589391304347826
Model with the best training loss saved! The loss is 0.22788880435916467
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3998e-01 (2.3998e-01)	Acc 0.751953 (0.751953)
Epoch: [81][300/616]	Loss 2.1791e-01 (2.2784e-01)	Acc 0.773438 (0.758796)
Epoch: [81][600/616]	Loss 2.1342e-01 (2.2789e-01)	Acc 0.776367 (0.758615)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759000)
Training Loss of Epoch 81: 0.22782626011507298
Training Acc of Epoch 81: 0.7586588541666667
Testing Acc of Epoch 81: 0.759
Model with the best training loss saved! The loss is 0.22782626011507298
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.1684e-01 (2.1684e-01)	Acc 0.773438 (0.773438)
Epoch: [82][300/616]	Loss 2.2408e-01 (2.2752e-01)	Acc 0.761719 (0.759146)
Epoch: [82][600/616]	Loss 2.2349e-01 (2.2775e-01)	Acc 0.767578 (0.758703)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758809)
Training Loss of Epoch 82: 0.22775052570230592
Training Acc of Epoch 82: 0.7587509527439025
Testing Acc of Epoch 82: 0.7588086956521739
Model with the best training loss saved! The loss is 0.22775052570230592
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4630e-01 (2.4630e-01)	Acc 0.742188 (0.742188)
Epoch: [83][300/616]	Loss 2.2337e-01 (2.2704e-01)	Acc 0.762695 (0.759902)
Epoch: [83][600/616]	Loss 2.0582e-01 (2.2766e-01)	Acc 0.787109 (0.758739)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759370)
Training Loss of Epoch 83: 0.22771192442595475
Training Acc of Epoch 83: 0.7587461890243903
Testing Acc of Epoch 83: 0.7593695652173913
Model with the best training loss saved! The loss is 0.22771192442595475
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.1284e-01 (2.1284e-01)	Acc 0.774414 (0.774414)
Epoch: [84][300/616]	Loss 2.2293e-01 (2.2855e-01)	Acc 0.753906 (0.758114)
Epoch: [84][600/616]	Loss 2.1675e-01 (2.2774e-01)	Acc 0.752930 (0.758700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.759778)
Training Loss of Epoch 84: 0.22771885201213807
Training Acc of Epoch 84: 0.7587271341463414
Testing Acc of Epoch 84: 0.7597782608695652
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1595e-01 (2.1595e-01)	Acc 0.776367 (0.776367)
Epoch: [85][300/616]	Loss 2.2455e-01 (2.2760e-01)	Acc 0.757812 (0.759110)
Epoch: [85][600/616]	Loss 2.2286e-01 (2.2770e-01)	Acc 0.757812 (0.758729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758865)
Training Loss of Epoch 85: 0.22767437703725768
Training Acc of Epoch 85: 0.7588335238821138
Testing Acc of Epoch 85: 0.7588652173913043
Model with the best training loss saved! The loss is 0.22767437703725768
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3442e-01 (2.3442e-01)	Acc 0.740234 (0.740234)
Epoch: [86][300/616]	Loss 2.2241e-01 (2.2781e-01)	Acc 0.762695 (0.759509)
Epoch: [86][600/616]	Loss 2.3060e-01 (2.2770e-01)	Acc 0.760742 (0.758822)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760013)
Training Loss of Epoch 86: 0.22769148478178475
Training Acc of Epoch 86: 0.7588589303861789
Testing Acc of Epoch 86: 0.7600130434782608
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.4027e-01 (2.4027e-01)	Acc 0.753906 (0.753906)
Epoch: [87][300/616]	Loss 2.3371e-01 (2.2822e-01)	Acc 0.733398 (0.758127)
Epoch: [87][600/616]	Loss 2.2215e-01 (2.2752e-01)	Acc 0.760742 (0.759028)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759961)
Training Loss of Epoch 87: 0.2276129685281738
Training Acc of Epoch 87: 0.7589589684959349
Testing Acc of Epoch 87: 0.7599608695652174
Model with the best training loss saved! The loss is 0.2276129685281738
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.1512e-01 (2.1512e-01)	Acc 0.778320 (0.778320)
Epoch: [88][300/616]	Loss 2.3342e-01 (2.2721e-01)	Acc 0.752930 (0.759474)
Epoch: [88][600/616]	Loss 2.1233e-01 (2.2757e-01)	Acc 0.779297 (0.759112)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759383)
Training Loss of Epoch 88: 0.22758136444944677
Training Acc of Epoch 88: 0.7590891768292682
Testing Acc of Epoch 88: 0.7593826086956522
Model with the best training loss saved! The loss is 0.22758136444944677
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.1977e-01 (2.1977e-01)	Acc 0.771484 (0.771484)
Epoch: [89][300/616]	Loss 2.3614e-01 (2.2725e-01)	Acc 0.760742 (0.759529)
Epoch: [89][600/616]	Loss 2.4684e-01 (2.2752e-01)	Acc 0.738281 (0.759015)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759678)
Training Loss of Epoch 89: 0.2275831383902852
Training Acc of Epoch 89: 0.758935149898374
Testing Acc of Epoch 89: 0.7596782608695652
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.1532e-01 (2.1532e-01)	Acc 0.767578 (0.767578)
Epoch: [90][300/616]	Loss 2.1894e-01 (2.2705e-01)	Acc 0.772461 (0.760145)
Epoch: [90][600/616]	Loss 2.0882e-01 (2.2742e-01)	Acc 0.782227 (0.759161)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759717)
Training Loss of Epoch 90: 0.227529386703561
Training Acc of Epoch 90: 0.7589653201219512
Testing Acc of Epoch 90: 0.7597173913043478
Model with the best training loss saved! The loss is 0.227529386703561
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2516e-01 (2.2516e-01)	Acc 0.768555 (0.768555)
Epoch: [91][300/616]	Loss 2.4006e-01 (2.2788e-01)	Acc 0.750000 (0.759415)
Epoch: [91][600/616]	Loss 2.2661e-01 (2.2759e-01)	Acc 0.766602 (0.759096)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760087)
Training Loss of Epoch 91: 0.22762170496994888
Training Acc of Epoch 91: 0.7590891768292682
Testing Acc of Epoch 91: 0.7600869565217391
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.4947e-01 (2.4947e-01)	Acc 0.739258 (0.739258)
Epoch: [92][300/616]	Loss 2.3905e-01 (2.2762e-01)	Acc 0.738281 (0.759003)
Epoch: [92][600/616]	Loss 2.2112e-01 (2.2754e-01)	Acc 0.760742 (0.759083)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759578)
Training Loss of Epoch 92: 0.22759214879536047
Training Acc of Epoch 92: 0.7590034298780488
Testing Acc of Epoch 92: 0.7595782608695653
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2710e-01 (2.2710e-01)	Acc 0.768555 (0.768555)
Epoch: [93][300/616]	Loss 2.2960e-01 (2.2750e-01)	Acc 0.749023 (0.759211)
Epoch: [93][600/616]	Loss 2.2345e-01 (2.2761e-01)	Acc 0.761719 (0.758960)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760065)
Training Loss of Epoch 93: 0.22754155305342946
Training Acc of Epoch 93: 0.7590764735772357
Testing Acc of Epoch 93: 0.7600652173913044
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.4011e-01 (2.4011e-01)	Acc 0.757812 (0.757812)
Epoch: [94][300/616]	Loss 2.2534e-01 (2.2768e-01)	Acc 0.763672 (0.758776)
Epoch: [94][600/616]	Loss 2.3182e-01 (2.2750e-01)	Acc 0.752930 (0.759080)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759217)
Training Loss of Epoch 94: 0.22756874534657331
Training Acc of Epoch 94: 0.7590431275406504
Testing Acc of Epoch 94: 0.7592173913043478
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2051e-01 (2.2051e-01)	Acc 0.768555 (0.768555)
Epoch: [95][300/616]	Loss 2.3688e-01 (2.2788e-01)	Acc 0.744141 (0.758659)
Epoch: [95][600/616]	Loss 2.3674e-01 (2.2751e-01)	Acc 0.759766 (0.759017)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758917)
Training Loss of Epoch 95: 0.22752006024849125
Training Acc of Epoch 95: 0.7589875508130082
Testing Acc of Epoch 95: 0.7589173913043479
Model with the best training loss saved! The loss is 0.22752006024849125
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2555e-01 (2.2555e-01)	Acc 0.760742 (0.760742)
Epoch: [96][300/616]	Loss 2.3591e-01 (2.2754e-01)	Acc 0.741211 (0.759107)
Epoch: [96][600/616]	Loss 2.2885e-01 (2.2737e-01)	Acc 0.763672 (0.759106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759639)
Training Loss of Epoch 96: 0.22746255475331129
Training Acc of Epoch 96: 0.7590129573170732
Testing Acc of Epoch 96: 0.7596391304347826
Model with the best training loss saved! The loss is 0.22746255475331129
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2333e-01 (2.2333e-01)	Acc 0.762695 (0.762695)
Epoch: [97][300/616]	Loss 2.2697e-01 (2.2781e-01)	Acc 0.748047 (0.758676)
Epoch: [97][600/616]	Loss 2.1823e-01 (2.2742e-01)	Acc 0.785156 (0.759210)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759478)
Training Loss of Epoch 97: 0.22749245174047422
Training Acc of Epoch 97: 0.7591495172764228
Testing Acc of Epoch 97: 0.7594782608695653
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.2591e-01 (2.2591e-01)	Acc 0.758789 (0.758789)
Epoch: [98][300/616]	Loss 2.3999e-01 (2.2747e-01)	Acc 0.746094 (0.758977)
Epoch: [98][600/616]	Loss 2.3859e-01 (2.2759e-01)	Acc 0.745117 (0.759082)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759861)
Training Loss of Epoch 98: 0.22753569471642254
Training Acc of Epoch 98: 0.7591415777439025
Testing Acc of Epoch 98: 0.7598608695652174
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2281e-01 (2.2281e-01)	Acc 0.766602 (0.766602)
Epoch: [99][300/616]	Loss 2.2195e-01 (2.2764e-01)	Acc 0.759766 (0.759250)
Epoch: [99][600/616]	Loss 2.2798e-01 (2.2739e-01)	Acc 0.763672 (0.759163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759509)
Training Loss of Epoch 99: 0.2274489543786863
Training Acc of Epoch 99: 0.7590955284552846
Testing Acc of Epoch 99: 0.7595086956521739
Model with the best training loss saved! The loss is 0.2274489543786863
Early stopping not satisfied.
