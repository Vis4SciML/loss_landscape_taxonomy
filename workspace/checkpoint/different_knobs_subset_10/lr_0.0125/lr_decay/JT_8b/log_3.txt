train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_8b
different_width False
resnet18_width 64
weight_precision 8
bias_precision 8
act_precision 11
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_8b/
file_prefix exp_3
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_8b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0031e-01 (5.0031e-01)	Acc 0.295898 (0.295898)
Epoch: [0][300/616]	Loss 2.3360e-01 (2.7262e-01)	Acc 0.766602 (0.708640)
Epoch: [0][600/616]	Loss 2.4923e-01 (2.5868e-01)	Acc 0.732422 (0.726116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.748830)
Training Loss of Epoch 0: 0.25826223029838347
Training Acc of Epoch 0: 0.7266800050813008
Testing Acc of Epoch 0: 0.7488304347826087
Model with the best training loss saved! The loss is 0.25826223029838347
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.2969e-01 (2.2969e-01)	Acc 0.747070 (0.747070)
Epoch: [1][300/616]	Loss 2.3243e-01 (2.4026e-01)	Acc 0.754883 (0.748521)
Epoch: [1][600/616]	Loss 2.4416e-01 (2.4012e-01)	Acc 0.738281 (0.747959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745170)
Training Loss of Epoch 1: 0.24030393773947306
Training Acc of Epoch 1: 0.7477705792682927
Testing Acc of Epoch 1: 0.7451695652173913
Model with the best training loss saved! The loss is 0.24030393773947306
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3702e-01 (2.3702e-01)	Acc 0.755859 (0.755859)
Epoch: [2][300/616]	Loss 2.3503e-01 (2.3868e-01)	Acc 0.741211 (0.749312)
Epoch: [2][600/616]	Loss 2.3826e-01 (2.3808e-01)	Acc 0.744141 (0.749402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748757)
Training Loss of Epoch 2: 0.23809325930064287
Training Acc of Epoch 2: 0.7493727769308943
Testing Acc of Epoch 2: 0.7487565217391304
Model with the best training loss saved! The loss is 0.23809325930064287
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3511e-01 (2.3511e-01)	Acc 0.752930 (0.752930)
Epoch: [3][300/616]	Loss 2.3703e-01 (2.3673e-01)	Acc 0.737305 (0.750860)
Epoch: [3][600/616]	Loss 2.3121e-01 (2.3713e-01)	Acc 0.757812 (0.750125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753635)
Training Loss of Epoch 3: 0.23719867601142666
Training Acc of Epoch 3: 0.7500809832317074
Testing Acc of Epoch 3: 0.7536347826086957
Model with the best training loss saved! The loss is 0.23719867601142666
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3326e-01 (2.3326e-01)	Acc 0.749023 (0.749023)
Epoch: [4][300/616]	Loss 2.3234e-01 (2.3640e-01)	Acc 0.765625 (0.750944)
Epoch: [4][600/616]	Loss 2.4366e-01 (2.3649e-01)	Acc 0.753906 (0.750585)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752526)
Training Loss of Epoch 4: 0.23644127782767382
Training Acc of Epoch 4: 0.7506891514227643
Testing Acc of Epoch 4: 0.7525260869565218
Model with the best training loss saved! The loss is 0.23644127782767382
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.4190e-01 (2.4190e-01)	Acc 0.741211 (0.741211)
Epoch: [5][300/616]	Loss 2.3592e-01 (2.3612e-01)	Acc 0.740234 (0.750565)
Epoch: [5][600/616]	Loss 2.4544e-01 (2.3598e-01)	Acc 0.733398 (0.750986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751857)
Training Loss of Epoch 5: 0.236065623624538
Training Acc of Epoch 5: 0.7508018927845529
Testing Acc of Epoch 5: 0.7518565217391304
Model with the best training loss saved! The loss is 0.236065623624538
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3851e-01 (2.3851e-01)	Acc 0.733398 (0.733398)
Epoch: [6][300/616]	Loss 2.4744e-01 (2.3576e-01)	Acc 0.741211 (0.751295)
Epoch: [6][600/616]	Loss 2.4755e-01 (2.3570e-01)	Acc 0.733398 (0.751482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.751300)
Training Loss of Epoch 6: 0.23565974940614
Training Acc of Epoch 6: 0.7515466209349594
Testing Acc of Epoch 6: 0.7513
Model with the best training loss saved! The loss is 0.23565974940614
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4143e-01 (2.4143e-01)	Acc 0.735352 (0.735352)
Epoch: [7][300/616]	Loss 2.3521e-01 (2.3553e-01)	Acc 0.755859 (0.751895)
Epoch: [7][600/616]	Loss 2.3514e-01 (2.3557e-01)	Acc 0.755859 (0.751448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752613)
Training Loss of Epoch 7: 0.2355208315500399
Training Acc of Epoch 7: 0.7515085111788617
Testing Acc of Epoch 7: 0.7526130434782609
Model with the best training loss saved! The loss is 0.2355208315500399
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3637e-01 (2.3637e-01)	Acc 0.751953 (0.751953)
Epoch: [8][300/616]	Loss 2.3388e-01 (2.3502e-01)	Acc 0.754883 (0.751995)
Epoch: [8][600/616]	Loss 2.6984e-01 (2.3540e-01)	Acc 0.706055 (0.751604)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754035)
Training Loss of Epoch 8: 0.235471114322422
Training Acc of Epoch 8: 0.7514846925813008
Testing Acc of Epoch 8: 0.7540347826086956
Model with the best training loss saved! The loss is 0.235471114322422
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5456e-01 (2.5456e-01)	Acc 0.716797 (0.716797)
Epoch: [9][300/616]	Loss 2.3777e-01 (2.3432e-01)	Acc 0.741211 (0.752417)
Epoch: [9][600/616]	Loss 2.3020e-01 (2.3517e-01)	Acc 0.744141 (0.751618)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751039)
Training Loss of Epoch 9: 0.23515018253791622
Training Acc of Epoch 9: 0.7516482469512196
Testing Acc of Epoch 9: 0.7510391304347827
Model with the best training loss saved! The loss is 0.23515018253791622
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2974e-01 (2.2974e-01)	Acc 0.750977 (0.750977)
Epoch: [10][300/616]	Loss 2.4704e-01 (2.3500e-01)	Acc 0.736328 (0.752407)
Epoch: [10][600/616]	Loss 2.3343e-01 (2.3523e-01)	Acc 0.744141 (0.751831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754961)
Training Loss of Epoch 10: 0.23522942119497595
Training Acc of Epoch 10: 0.7518721417682926
Testing Acc of Epoch 10: 0.7549608695652174
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3736e-01 (2.3736e-01)	Acc 0.758789 (0.758789)
Epoch: [11][300/616]	Loss 2.2407e-01 (2.3474e-01)	Acc 0.776367 (0.752200)
Epoch: [11][600/616]	Loss 2.5861e-01 (2.3505e-01)	Acc 0.713867 (0.752073)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753170)
Training Loss of Epoch 11: 0.23502764154255876
Training Acc of Epoch 11: 0.7520118775406504
Testing Acc of Epoch 11: 0.7531695652173913
Model with the best training loss saved! The loss is 0.23502764154255876
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4462e-01 (2.4462e-01)	Acc 0.748047 (0.748047)
Epoch: [12][300/616]	Loss 2.2178e-01 (2.3545e-01)	Acc 0.764648 (0.751385)
Epoch: [12][600/616]	Loss 2.3476e-01 (2.3531e-01)	Acc 0.752930 (0.751687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751922)
Training Loss of Epoch 12: 0.23527480140934146
Training Acc of Epoch 12: 0.7517022357723577
Testing Acc of Epoch 12: 0.7519217391304348
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3484e-01 (2.3484e-01)	Acc 0.766602 (0.766602)
Epoch: [13][300/616]	Loss 2.4260e-01 (2.3530e-01)	Acc 0.739258 (0.751421)
Epoch: [13][600/616]	Loss 2.4143e-01 (2.3480e-01)	Acc 0.741211 (0.751999)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754630)
Training Loss of Epoch 13: 0.23474515250058678
Training Acc of Epoch 13: 0.7520341082317074
Testing Acc of Epoch 13: 0.7546304347826087
Model with the best training loss saved! The loss is 0.23474515250058678
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2646e-01 (2.2646e-01)	Acc 0.753906 (0.753906)
Epoch: [14][300/616]	Loss 2.3140e-01 (2.3446e-01)	Acc 0.750977 (0.752284)
Epoch: [14][600/616]	Loss 2.2779e-01 (2.3518e-01)	Acc 0.765625 (0.751869)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749183)
Training Loss of Epoch 14: 0.23508721640924127
Training Acc of Epoch 14: 0.7519372459349594
Testing Acc of Epoch 14: 0.7491826086956521
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4104e-01 (2.4104e-01)	Acc 0.763672 (0.763672)
Epoch: [15][300/616]	Loss 2.3286e-01 (2.3511e-01)	Acc 0.757812 (0.751437)
Epoch: [15][600/616]	Loss 2.2650e-01 (2.3480e-01)	Acc 0.764648 (0.752034)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751561)
Training Loss of Epoch 15: 0.2347285485849148
Training Acc of Epoch 15: 0.7521674923780488
Testing Acc of Epoch 15: 0.7515608695652174
Model with the best training loss saved! The loss is 0.2347285485849148
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4021e-01 (2.4021e-01)	Acc 0.743164 (0.743164)
Epoch: [16][300/616]	Loss 2.3808e-01 (2.3481e-01)	Acc 0.747070 (0.751554)
Epoch: [16][600/616]	Loss 2.4643e-01 (2.3497e-01)	Acc 0.729492 (0.751630)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748730)
Training Loss of Epoch 16: 0.2348694235086441
Training Acc of Epoch 16: 0.7518324441056911
Testing Acc of Epoch 16: 0.7487304347826087
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3809e-01 (2.3809e-01)	Acc 0.747070 (0.747070)
Epoch: [17][300/616]	Loss 2.4412e-01 (2.3421e-01)	Acc 0.741211 (0.753021)
Epoch: [17][600/616]	Loss 2.2265e-01 (2.3450e-01)	Acc 0.770508 (0.752618)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753696)
Training Loss of Epoch 17: 0.2345311972426205
Training Acc of Epoch 17: 0.7525787601626016
Testing Acc of Epoch 17: 0.753695652173913
Model with the best training loss saved! The loss is 0.2345311972426205
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3547e-01 (2.3547e-01)	Acc 0.747070 (0.747070)
Epoch: [18][300/616]	Loss 2.2274e-01 (2.3560e-01)	Acc 0.756836 (0.751129)
Epoch: [18][600/616]	Loss 2.2532e-01 (2.3498e-01)	Acc 0.756836 (0.751917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753783)
Training Loss of Epoch 18: 0.23500985197904634
Training Acc of Epoch 18: 0.7518959603658537
Testing Acc of Epoch 18: 0.7537826086956522
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4179e-01 (2.4179e-01)	Acc 0.745117 (0.745117)
Epoch: [19][300/616]	Loss 2.4742e-01 (2.3496e-01)	Acc 0.735352 (0.752164)
Epoch: [19][600/616]	Loss 2.2057e-01 (2.3459e-01)	Acc 0.760742 (0.752315)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753604)
Training Loss of Epoch 19: 0.2345286028656533
Training Acc of Epoch 19: 0.7523786839430894
Testing Acc of Epoch 19: 0.753604347826087
Model with the best training loss saved! The loss is 0.2345286028656533
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.2966e-01 (2.2966e-01)	Acc 0.751953 (0.751953)
Epoch: [20][300/616]	Loss 2.4280e-01 (2.3488e-01)	Acc 0.745117 (0.751658)
Epoch: [20][600/616]	Loss 2.2580e-01 (2.3467e-01)	Acc 0.757812 (0.751893)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751604)
Training Loss of Epoch 20: 0.2346319277112077
Training Acc of Epoch 20: 0.7520277566056911
Testing Acc of Epoch 20: 0.751604347826087
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3050e-01 (2.3050e-01)	Acc 0.758789 (0.758789)
Epoch: [21][300/616]	Loss 2.2962e-01 (2.3427e-01)	Acc 0.759766 (0.752281)
Epoch: [21][600/616]	Loss 2.2549e-01 (2.3469e-01)	Acc 0.767578 (0.752327)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753957)
Training Loss of Epoch 21: 0.2346058036011409
Training Acc of Epoch 21: 0.7524183816056911
Testing Acc of Epoch 21: 0.7539565217391304
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.2779e-01 (2.2779e-01)	Acc 0.763672 (0.763672)
Epoch: [22][300/616]	Loss 2.3611e-01 (2.3431e-01)	Acc 0.741211 (0.753141)
Epoch: [22][600/616]	Loss 2.4472e-01 (2.3469e-01)	Acc 0.753906 (0.752207)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753787)
Training Loss of Epoch 22: 0.23471391549924525
Training Acc of Epoch 22: 0.7522802337398374
Testing Acc of Epoch 22: 0.7537869565217391
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2874e-01 (2.2874e-01)	Acc 0.758789 (0.758789)
Epoch: [23][300/616]	Loss 2.4445e-01 (2.3503e-01)	Acc 0.732422 (0.751904)
Epoch: [23][600/616]	Loss 2.3669e-01 (2.3474e-01)	Acc 0.740234 (0.752112)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751717)
Training Loss of Epoch 23: 0.23468636351872266
Training Acc of Epoch 23: 0.7521754319105691
Testing Acc of Epoch 23: 0.7517173913043478
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.3511e-01 (2.3511e-01)	Acc 0.761719 (0.761719)
Epoch: [24][300/616]	Loss 2.3644e-01 (2.3467e-01)	Acc 0.752930 (0.751853)
Epoch: [24][600/616]	Loss 2.4541e-01 (2.3464e-01)	Acc 0.731445 (0.752494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752913)
Training Loss of Epoch 24: 0.23465133532275997
Training Acc of Epoch 24: 0.7524453760162602
Testing Acc of Epoch 24: 0.7529130434782608
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3838e-01 (2.3838e-01)	Acc 0.750000 (0.750000)
Epoch: [25][300/616]	Loss 2.3540e-01 (2.3436e-01)	Acc 0.744141 (0.752829)
Epoch: [25][600/616]	Loss 2.3241e-01 (2.3450e-01)	Acc 0.740234 (0.752273)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753817)
Training Loss of Epoch 25: 0.23451102715197616
Training Acc of Epoch 25: 0.752221481199187
Testing Acc of Epoch 25: 0.7538173913043478
Model with the best training loss saved! The loss is 0.23451102715197616
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4339e-01 (2.4339e-01)	Acc 0.744141 (0.744141)
Epoch: [26][300/616]	Loss 2.3476e-01 (2.3437e-01)	Acc 0.756836 (0.752433)
Epoch: [26][600/616]	Loss 2.3288e-01 (2.3447e-01)	Acc 0.749023 (0.752346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752343)
Training Loss of Epoch 26: 0.23437850097330606
Training Acc of Epoch 26: 0.7524088541666667
Testing Acc of Epoch 26: 0.7523434782608696
Model with the best training loss saved! The loss is 0.23437850097330606
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.2332e-01 (2.2332e-01)	Acc 0.758789 (0.758789)
Epoch: [27][300/616]	Loss 2.2934e-01 (2.3449e-01)	Acc 0.762695 (0.752949)
Epoch: [27][600/616]	Loss 2.3840e-01 (2.3461e-01)	Acc 0.755859 (0.752517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752657)
Training Loss of Epoch 27: 0.23456705195632407
Training Acc of Epoch 27: 0.7525311229674797
Testing Acc of Epoch 27: 0.7526565217391304
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.5404e-01 (2.5404e-01)	Acc 0.717773 (0.717773)
Epoch: [28][300/616]	Loss 2.3716e-01 (2.3489e-01)	Acc 0.752930 (0.752073)
Epoch: [28][600/616]	Loss 2.3697e-01 (2.3471e-01)	Acc 0.737305 (0.752117)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753791)
Training Loss of Epoch 28: 0.23460966610327
Training Acc of Epoch 28: 0.7521881351626016
Testing Acc of Epoch 28: 0.753791304347826
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4742e-01 (2.4742e-01)	Acc 0.731445 (0.731445)
Epoch: [29][300/616]	Loss 2.2882e-01 (2.3452e-01)	Acc 0.756836 (0.752119)
Epoch: [29][600/616]	Loss 2.3939e-01 (2.3498e-01)	Acc 0.746094 (0.751633)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752604)
Training Loss of Epoch 29: 0.2349534423855262
Training Acc of Epoch 29: 0.7516276041666666
Testing Acc of Epoch 29: 0.752604347826087
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2448e-01 (2.2448e-01)	Acc 0.762695 (0.762695)
Epoch: [30][300/616]	Loss 2.2833e-01 (2.3413e-01)	Acc 0.752930 (0.753186)
Epoch: [30][600/616]	Loss 2.3940e-01 (2.3449e-01)	Acc 0.749023 (0.752432)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753609)
Training Loss of Epoch 30: 0.23445891059511076
Training Acc of Epoch 30: 0.752415205792683
Testing Acc of Epoch 30: 0.7536086956521739
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3868e-01 (2.3868e-01)	Acc 0.752930 (0.752930)
Epoch: [31][300/616]	Loss 2.4215e-01 (2.3491e-01)	Acc 0.753906 (0.751499)
Epoch: [31][600/616]	Loss 2.4081e-01 (2.3505e-01)	Acc 0.752930 (0.751929)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753504)
Training Loss of Epoch 31: 0.23514749986854025
Training Acc of Epoch 31: 0.7517308180894309
Testing Acc of Epoch 31: 0.753504347826087
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3104e-01 (2.3104e-01)	Acc 0.761719 (0.761719)
Epoch: [32][300/616]	Loss 2.3563e-01 (2.3504e-01)	Acc 0.745117 (0.751914)
Epoch: [32][600/616]	Loss 2.2749e-01 (2.3480e-01)	Acc 0.749023 (0.752007)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754743)
Training Loss of Epoch 32: 0.23474937362399528
Training Acc of Epoch 32: 0.7520341082317074
Testing Acc of Epoch 32: 0.7547434782608695
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2853e-01 (2.2853e-01)	Acc 0.766602 (0.766602)
Epoch: [33][300/616]	Loss 2.4350e-01 (2.3465e-01)	Acc 0.748047 (0.752479)
Epoch: [33][600/616]	Loss 2.3148e-01 (2.3456e-01)	Acc 0.753906 (0.752530)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751665)
Training Loss of Epoch 33: 0.23456889715621143
Training Acc of Epoch 33: 0.7524818978658536
Testing Acc of Epoch 33: 0.7516652173913043
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3394e-01 (2.3394e-01)	Acc 0.756836 (0.756836)
Epoch: [34][300/616]	Loss 2.3228e-01 (2.3477e-01)	Acc 0.759766 (0.752913)
Epoch: [34][600/616]	Loss 2.2914e-01 (2.3483e-01)	Acc 0.750977 (0.752434)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.751926)
Training Loss of Epoch 34: 0.2347468090978095
Training Acc of Epoch 34: 0.752513655995935
Testing Acc of Epoch 34: 0.7519260869565217
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2339e-01 (2.2339e-01)	Acc 0.758789 (0.758789)
Epoch: [35][300/616]	Loss 2.2818e-01 (2.3486e-01)	Acc 0.760742 (0.751888)
Epoch: [35][600/616]	Loss 2.2189e-01 (2.3461e-01)	Acc 0.772461 (0.752210)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752165)
Training Loss of Epoch 35: 0.23458415914357192
Training Acc of Epoch 35: 0.7521786077235773
Testing Acc of Epoch 35: 0.7521652173913044
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.4450e-01 (2.4450e-01)	Acc 0.737305 (0.737305)
Epoch: [36][300/616]	Loss 2.3646e-01 (2.3452e-01)	Acc 0.755859 (0.752573)
Epoch: [36][600/616]	Loss 2.3509e-01 (2.3481e-01)	Acc 0.752930 (0.752289)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747574)
Training Loss of Epoch 36: 0.23475878672386571
Training Acc of Epoch 36: 0.752294524898374
Testing Acc of Epoch 36: 0.7475739130434783
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3545e-01 (2.3545e-01)	Acc 0.749023 (0.749023)
Epoch: [37][300/616]	Loss 2.2858e-01 (2.3520e-01)	Acc 0.756836 (0.751999)
Epoch: [37][600/616]	Loss 2.2989e-01 (2.3483e-01)	Acc 0.756836 (0.752151)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751196)
Training Loss of Epoch 37: 0.23483309816054213
Training Acc of Epoch 37: 0.752124618902439
Testing Acc of Epoch 37: 0.751195652173913
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.1777e-01 (2.1777e-01)	Acc 0.789062 (0.789062)
Epoch: [38][300/616]	Loss 2.2488e-01 (2.3553e-01)	Acc 0.765625 (0.752060)
Epoch: [38][600/616]	Loss 2.5220e-01 (2.3538e-01)	Acc 0.732422 (0.751740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754100)
Training Loss of Epoch 38: 0.23538673120300943
Training Acc of Epoch 38: 0.7516958841463415
Testing Acc of Epoch 38: 0.7541
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2684e-01 (2.2684e-01)	Acc 0.752930 (0.752930)
Epoch: [39][300/616]	Loss 2.2948e-01 (2.3499e-01)	Acc 0.758789 (0.752102)
Epoch: [39][600/616]	Loss 2.2627e-01 (2.3451e-01)	Acc 0.766602 (0.752418)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753578)
Training Loss of Epoch 39: 0.23456506091889326
Training Acc of Epoch 39: 0.7523104039634146
Testing Acc of Epoch 39: 0.7535782608695653
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4228e-01 (2.4228e-01)	Acc 0.737305 (0.737305)
Epoch: [40][300/616]	Loss 2.4730e-01 (2.3505e-01)	Acc 0.737305 (0.752323)
Epoch: [40][600/616]	Loss 2.1859e-01 (2.3470e-01)	Acc 0.775391 (0.752369)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753491)
Training Loss of Epoch 40: 0.23469677417258905
Training Acc of Epoch 40: 0.7524040904471545
Testing Acc of Epoch 40: 0.7534913043478261
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.3345e-01 (2.3345e-01)	Acc 0.752930 (0.752930)
Epoch: [41][300/616]	Loss 2.3834e-01 (2.3383e-01)	Acc 0.741211 (0.753809)
Epoch: [41][600/616]	Loss 2.5556e-01 (2.3494e-01)	Acc 0.729492 (0.752062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752130)
Training Loss of Epoch 41: 0.2349129520053786
Training Acc of Epoch 41: 0.7520420477642277
Testing Acc of Epoch 41: 0.7521304347826087
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.2928e-01 (2.2928e-01)	Acc 0.756836 (0.756836)
Epoch: [42][300/616]	Loss 2.2446e-01 (2.3483e-01)	Acc 0.769531 (0.752093)
Epoch: [42][600/616]	Loss 2.4656e-01 (2.3450e-01)	Acc 0.728516 (0.752367)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748587)
Training Loss of Epoch 42: 0.23459031506767117
Training Acc of Epoch 42: 0.752246887703252
Testing Acc of Epoch 42: 0.7485869565217391
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.5017e-01 (2.5017e-01)	Acc 0.734375 (0.734375)
Epoch: [43][300/616]	Loss 2.3613e-01 (2.3419e-01)	Acc 0.747070 (0.752703)
Epoch: [43][600/616]	Loss 2.3550e-01 (2.3457e-01)	Acc 0.760742 (0.752281)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754022)
Training Loss of Epoch 43: 0.2345589995868807
Training Acc of Epoch 43: 0.7522722942073171
Testing Acc of Epoch 43: 0.7540217391304348
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.2525e-01 (2.2525e-01)	Acc 0.760742 (0.760742)
Epoch: [44][300/616]	Loss 2.3304e-01 (2.3478e-01)	Acc 0.766602 (0.751771)
Epoch: [44][600/616]	Loss 2.4295e-01 (2.3454e-01)	Acc 0.727539 (0.752465)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753648)
Training Loss of Epoch 44: 0.23457237310041257
Training Acc of Epoch 44: 0.7524723704268292
Testing Acc of Epoch 44: 0.7536478260869566
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.5410e-01 (2.5410e-01)	Acc 0.724609 (0.724609)
Epoch: [45][300/616]	Loss 2.3408e-01 (2.3499e-01)	Acc 0.753906 (0.751486)
Epoch: [45][600/616]	Loss 2.4236e-01 (2.3482e-01)	Acc 0.735352 (0.751935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752487)
Training Loss of Epoch 45: 0.2347380073332205
Training Acc of Epoch 45: 0.7520087017276422
Testing Acc of Epoch 45: 0.7524869565217391
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.3790e-01 (2.3790e-01)	Acc 0.751953 (0.751953)
Epoch: [46][300/616]	Loss 2.3315e-01 (2.3497e-01)	Acc 0.748047 (0.752183)
Epoch: [46][600/616]	Loss 2.3453e-01 (2.3476e-01)	Acc 0.750977 (0.752496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753848)
Training Loss of Epoch 46: 0.2346754166168895
Training Acc of Epoch 46: 0.7525819359756097
Testing Acc of Epoch 46: 0.7538478260869566
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.3092e-01 (2.3092e-01)	Acc 0.747070 (0.747070)
Epoch: [47][300/616]	Loss 2.4306e-01 (2.3426e-01)	Acc 0.747070 (0.752930)
Epoch: [47][600/616]	Loss 2.3840e-01 (2.3475e-01)	Acc 0.757812 (0.752263)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754543)
Training Loss of Epoch 47: 0.23477119718625294
Training Acc of Epoch 47: 0.7521865472560976
Testing Acc of Epoch 47: 0.7545434782608695
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.3362e-01 (2.3362e-01)	Acc 0.753906 (0.753906)
Epoch: [48][300/616]	Loss 2.3679e-01 (2.3384e-01)	Acc 0.745117 (0.753222)
Epoch: [48][600/616]	Loss 2.3532e-01 (2.3439e-01)	Acc 0.750000 (0.752254)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754848)
Training Loss of Epoch 48: 0.2344573469423666
Training Acc of Epoch 48: 0.7521516133130082
Testing Acc of Epoch 48: 0.7548478260869566
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.2277e-01 (2.2277e-01)	Acc 0.755859 (0.755859)
Epoch: [49][300/616]	Loss 2.3307e-01 (2.3478e-01)	Acc 0.755859 (0.752583)
Epoch: [49][600/616]	Loss 2.2176e-01 (2.3484e-01)	Acc 0.751953 (0.752202)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754000)
Training Loss of Epoch 49: 0.23478676836665083
Training Acc of Epoch 49: 0.7522278328252032
Testing Acc of Epoch 49: 0.754
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2664e-01 (2.2664e-01)	Acc 0.755859 (0.755859)
Epoch: [50][300/616]	Loss 2.3276e-01 (2.3394e-01)	Acc 0.749023 (0.753063)
Epoch: [50][600/616]	Loss 2.3161e-01 (2.3445e-01)	Acc 0.757812 (0.752142)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751265)
Training Loss of Epoch 50: 0.23454650531454785
Training Acc of Epoch 50: 0.7520420477642277
Testing Acc of Epoch 50: 0.7512652173913044
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.4550e-01 (2.4550e-01)	Acc 0.746094 (0.746094)
Epoch: [51][300/616]	Loss 2.3434e-01 (2.3431e-01)	Acc 0.749023 (0.752355)
Epoch: [51][600/616]	Loss 2.3974e-01 (2.3432e-01)	Acc 0.734375 (0.752483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753587)
Training Loss of Epoch 51: 0.2343748232698053
Training Acc of Epoch 51: 0.7524072662601626
Testing Acc of Epoch 51: 0.7535869565217391
Model with the best training loss saved! The loss is 0.2343748232698053
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.3979e-01 (2.3979e-01)	Acc 0.748047 (0.748047)
Epoch: [52][300/616]	Loss 2.4120e-01 (2.3455e-01)	Acc 0.740234 (0.752161)
Epoch: [52][600/616]	Loss 2.4248e-01 (2.3467e-01)	Acc 0.750000 (0.752129)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754896)
Training Loss of Epoch 52: 0.23464196879204696
Training Acc of Epoch 52: 0.7521928988821138
Testing Acc of Epoch 52: 0.7548956521739131
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.2289e-01 (2.2289e-01)	Acc 0.766602 (0.766602)
Epoch: [53][300/616]	Loss 2.5707e-01 (2.3565e-01)	Acc 0.714844 (0.750808)
Epoch: [53][600/616]	Loss 2.1803e-01 (2.3460e-01)	Acc 0.764648 (0.752314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751000)
Training Loss of Epoch 53: 0.23467098770102834
Training Acc of Epoch 53: 0.7521627286585366
Testing Acc of Epoch 53: 0.751
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3914e-01 (2.3914e-01)	Acc 0.744141 (0.744141)
Epoch: [54][300/616]	Loss 2.3108e-01 (2.3482e-01)	Acc 0.762695 (0.752388)
Epoch: [54][600/616]	Loss 2.3451e-01 (2.3473e-01)	Acc 0.761719 (0.752345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752974)
Training Loss of Epoch 54: 0.23475157897162244
Training Acc of Epoch 54: 0.7523770960365853
Testing Acc of Epoch 54: 0.7529739130434783
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.2440e-01 (2.2440e-01)	Acc 0.761719 (0.761719)
Epoch: [55][300/616]	Loss 2.2166e-01 (2.3467e-01)	Acc 0.766602 (0.752115)
Epoch: [55][600/616]	Loss 2.2333e-01 (2.3420e-01)	Acc 0.761719 (0.752642)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751265)
Training Loss of Epoch 55: 0.23434842992119673
Training Acc of Epoch 55: 0.7524850736788617
Testing Acc of Epoch 55: 0.7512652173913044
Model with the best training loss saved! The loss is 0.23434842992119673
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4418e-01 (2.4418e-01)	Acc 0.742188 (0.742188)
Epoch: [56][300/616]	Loss 2.4417e-01 (2.3587e-01)	Acc 0.732422 (0.750276)
Epoch: [56][600/616]	Loss 2.2682e-01 (2.3497e-01)	Acc 0.765625 (0.751909)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753622)
Training Loss of Epoch 56: 0.23495127914882288
Training Acc of Epoch 56: 0.7518848450203252
Testing Acc of Epoch 56: 0.7536217391304347
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.4445e-01 (2.4445e-01)	Acc 0.742188 (0.742188)
Epoch: [57][300/616]	Loss 2.2481e-01 (2.3475e-01)	Acc 0.763672 (0.751911)
Epoch: [57][600/616]	Loss 2.3158e-01 (2.3473e-01)	Acc 0.752930 (0.751822)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754039)
Training Loss of Epoch 57: 0.23468473972828408
Training Acc of Epoch 57: 0.7519197789634147
Testing Acc of Epoch 57: 0.7540391304347827
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3192e-01 (2.3192e-01)	Acc 0.759766 (0.759766)
Epoch: [58][300/616]	Loss 2.4268e-01 (2.3448e-01)	Acc 0.750000 (0.752219)
Epoch: [58][600/616]	Loss 2.4275e-01 (2.3502e-01)	Acc 0.735352 (0.751721)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753743)
Training Loss of Epoch 58: 0.23502473831176757
Training Acc of Epoch 58: 0.7516815929878049
Testing Acc of Epoch 58: 0.7537434782608695
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3188e-01 (2.3188e-01)	Acc 0.759766 (0.759766)
Epoch: [59][300/616]	Loss 2.3164e-01 (2.3446e-01)	Acc 0.754883 (0.753024)
Epoch: [59][600/616]	Loss 2.3660e-01 (2.3457e-01)	Acc 0.744141 (0.752218)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750013)
Training Loss of Epoch 59: 0.23458072506799932
Training Acc of Epoch 59: 0.7522135416666667
Testing Acc of Epoch 59: 0.7500130434782609
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.2183e-01 (2.2183e-01)	Acc 0.761719 (0.761719)
Epoch: [60][300/616]	Loss 2.1659e-01 (2.3462e-01)	Acc 0.773438 (0.752849)
Epoch: [60][600/616]	Loss 2.4590e-01 (2.3461e-01)	Acc 0.732422 (0.752406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.753604)
Training Loss of Epoch 60: 0.23462803656977366
Training Acc of Epoch 60: 0.7523659806910569
Testing Acc of Epoch 60: 0.753604347826087
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3938e-01 (2.3938e-01)	Acc 0.740234 (0.740234)
Epoch: [61][300/616]	Loss 2.3106e-01 (2.3485e-01)	Acc 0.756836 (0.751771)
Epoch: [61][600/616]	Loss 2.2910e-01 (2.3477e-01)	Acc 0.758789 (0.752236)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.751583)
Training Loss of Epoch 61: 0.2348244047988721
Training Acc of Epoch 61: 0.7520611026422764
Testing Acc of Epoch 61: 0.7515826086956522
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3200e-01 (2.3200e-01)	Acc 0.741211 (0.741211)
Epoch: [62][300/616]	Loss 2.3239e-01 (2.3545e-01)	Acc 0.751953 (0.750912)
Epoch: [62][600/616]	Loss 2.2206e-01 (2.3463e-01)	Acc 0.773438 (0.752416)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753739)
Training Loss of Epoch 62: 0.23462051167720702
Training Acc of Epoch 62: 0.7524310848577236
Testing Acc of Epoch 62: 0.7537391304347826
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.2790e-01 (2.2790e-01)	Acc 0.761719 (0.761719)
Epoch: [63][300/616]	Loss 2.2161e-01 (2.3448e-01)	Acc 0.774414 (0.752278)
Epoch: [63][600/616]	Loss 2.3362e-01 (2.3472e-01)	Acc 0.742188 (0.752221)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.745643)
Training Loss of Epoch 63: 0.23464502466887963
Training Acc of Epoch 63: 0.7522548272357723
Testing Acc of Epoch 63: 0.7456434782608695
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3412e-01 (2.3412e-01)	Acc 0.756836 (0.756836)
Epoch: [64][300/616]	Loss 2.4025e-01 (2.3448e-01)	Acc 0.735352 (0.752829)
Epoch: [64][600/616]	Loss 2.3028e-01 (2.3491e-01)	Acc 0.754883 (0.751926)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.753057)
Training Loss of Epoch 64: 0.2349370968777959
Training Acc of Epoch 64: 0.7519150152439025
Testing Acc of Epoch 64: 0.7530565217391304
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.2526e-01 (2.2526e-01)	Acc 0.767578 (0.767578)
Epoch: [65][300/616]	Loss 2.2692e-01 (2.3449e-01)	Acc 0.753906 (0.752339)
Epoch: [65][600/616]	Loss 2.3028e-01 (2.3446e-01)	Acc 0.762695 (0.752380)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753461)
Training Loss of Epoch 65: 0.23452524433775646
Training Acc of Epoch 65: 0.7522643546747968
Testing Acc of Epoch 65: 0.7534608695652174
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.2701e-01 (2.2701e-01)	Acc 0.759766 (0.759766)
Epoch: [66][300/616]	Loss 2.3838e-01 (2.3476e-01)	Acc 0.751953 (0.751963)
Epoch: [66][600/616]	Loss 2.3623e-01 (2.3462e-01)	Acc 0.758789 (0.751977)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753230)
Training Loss of Epoch 66: 0.2345747517134116
Training Acc of Epoch 66: 0.7520087017276422
Testing Acc of Epoch 66: 0.7532304347826086
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3142e-01 (2.3142e-01)	Acc 0.750977 (0.750977)
Epoch: [67][300/616]	Loss 2.2182e-01 (2.3421e-01)	Acc 0.772461 (0.752307)
Epoch: [67][600/616]	Loss 2.3150e-01 (2.3455e-01)	Acc 0.751953 (0.751913)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752230)
Training Loss of Epoch 67: 0.23443389517989585
Training Acc of Epoch 67: 0.7520388719512195
Testing Acc of Epoch 67: 0.7522304347826086
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3548e-01 (2.3548e-01)	Acc 0.755859 (0.755859)
Epoch: [68][300/616]	Loss 2.4496e-01 (2.3490e-01)	Acc 0.739258 (0.751989)
Epoch: [68][600/616]	Loss 2.2605e-01 (2.3478e-01)	Acc 0.759766 (0.752218)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752843)
Training Loss of Epoch 68: 0.23480426225720383
Training Acc of Epoch 68: 0.7521627286585366
Testing Acc of Epoch 68: 0.7528434782608696
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5821e-01 (2.5821e-01)	Acc 0.719727 (0.719727)
Epoch: [69][300/616]	Loss 2.2830e-01 (2.3449e-01)	Acc 0.756836 (0.752226)
Epoch: [69][600/616]	Loss 2.5284e-01 (2.3448e-01)	Acc 0.741211 (0.752301)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751074)
Training Loss of Epoch 69: 0.23442608202860607
Training Acc of Epoch 69: 0.7523723323170731
Testing Acc of Epoch 69: 0.7510739130434783
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.2214e-01 (2.2214e-01)	Acc 0.768555 (0.768555)
Epoch: [70][300/616]	Loss 2.2441e-01 (2.3545e-01)	Acc 0.773438 (0.750964)
Epoch: [70][600/616]	Loss 2.3717e-01 (2.3475e-01)	Acc 0.747070 (0.752211)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752374)
Training Loss of Epoch 70: 0.2347011618497895
Training Acc of Epoch 70: 0.7522913490853659
Testing Acc of Epoch 70: 0.7523739130434782
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4292e-01 (2.4292e-01)	Acc 0.735352 (0.735352)
Epoch: [71][300/616]	Loss 2.3110e-01 (2.3507e-01)	Acc 0.763672 (0.751531)
Epoch: [71][600/616]	Loss 2.1981e-01 (2.3446e-01)	Acc 0.767578 (0.752210)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754513)
Training Loss of Epoch 71: 0.23445948783944293
Training Acc of Epoch 71: 0.7522151295731707
Testing Acc of Epoch 71: 0.7545130434782609
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.2250e-01 (2.2250e-01)	Acc 0.764648 (0.764648)
Epoch: [72][300/616]	Loss 2.2203e-01 (2.3425e-01)	Acc 0.774414 (0.752368)
Epoch: [72][600/616]	Loss 2.3431e-01 (2.3462e-01)	Acc 0.757812 (0.752371)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752735)
Training Loss of Epoch 72: 0.23463690249415917
Training Acc of Epoch 72: 0.7523326346544715
Testing Acc of Epoch 72: 0.7527347826086956
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.2055e-01 (2.2055e-01)	Acc 0.777344 (0.777344)
Epoch: [73][300/616]	Loss 2.3934e-01 (2.3501e-01)	Acc 0.746094 (0.751911)
Epoch: [73][600/616]	Loss 2.3271e-01 (2.3491e-01)	Acc 0.751953 (0.752064)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752135)
Training Loss of Epoch 73: 0.23488817234349443
Training Acc of Epoch 73: 0.7520595147357724
Testing Acc of Epoch 73: 0.7521347826086957
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.2961e-01 (2.2961e-01)	Acc 0.752930 (0.752930)
Epoch: [74][300/616]	Loss 2.3529e-01 (2.3478e-01)	Acc 0.760742 (0.751937)
Epoch: [74][600/616]	Loss 2.1993e-01 (2.3453e-01)	Acc 0.762695 (0.752328)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753704)
Training Loss of Epoch 74: 0.23450235394927543
Training Acc of Epoch 74: 0.7524294969512195
Testing Acc of Epoch 74: 0.753704347826087
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4784e-01 (2.4784e-01)	Acc 0.742188 (0.742188)
Epoch: [75][300/616]	Loss 2.3732e-01 (2.3129e-01)	Acc 0.749023 (0.754445)
Epoch: [75][600/616]	Loss 2.2452e-01 (2.3083e-01)	Acc 0.770508 (0.755240)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755435)
Training Loss of Epoch 75: 0.23075751260044128
Training Acc of Epoch 75: 0.755344893292683
Testing Acc of Epoch 75: 0.7554347826086957
Model with the best training loss saved! The loss is 0.23075751260044128
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3315e-01 (2.3315e-01)	Acc 0.750977 (0.750977)
Epoch: [76][300/616]	Loss 2.3220e-01 (2.3103e-01)	Acc 0.746094 (0.755006)
Epoch: [76][600/616]	Loss 2.1261e-01 (2.3040e-01)	Acc 0.783203 (0.755812)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756574)
Training Loss of Epoch 76: 0.23043289792731525
Training Acc of Epoch 76: 0.7558037982723578
Testing Acc of Epoch 76: 0.7565739130434782
Model with the best training loss saved! The loss is 0.23043289792731525
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4535e-01 (2.4535e-01)	Acc 0.746094 (0.746094)
Epoch: [77][300/616]	Loss 2.4456e-01 (2.3057e-01)	Acc 0.736328 (0.755610)
Epoch: [77][600/616]	Loss 2.4531e-01 (2.3027e-01)	Acc 0.730469 (0.756309)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756626)
Training Loss of Epoch 77: 0.23027067530930526
Training Acc of Epoch 77: 0.7562452362804878
Testing Acc of Epoch 77: 0.7566260869565218
Model with the best training loss saved! The loss is 0.23027067530930526
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4204e-01 (2.4204e-01)	Acc 0.747070 (0.747070)
Epoch: [78][300/616]	Loss 2.3243e-01 (2.2976e-01)	Acc 0.749023 (0.756761)
Epoch: [78][600/616]	Loss 2.2508e-01 (2.3008e-01)	Acc 0.759766 (0.756454)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755583)
Training Loss of Epoch 78: 0.23012175102059435
Training Acc of Epoch 78: 0.7564103785569106
Testing Acc of Epoch 78: 0.7555826086956522
Model with the best training loss saved! The loss is 0.23012175102059435
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3113e-01 (2.3113e-01)	Acc 0.752930 (0.752930)
Epoch: [79][300/616]	Loss 2.1838e-01 (2.3046e-01)	Acc 0.767578 (0.755282)
Epoch: [79][600/616]	Loss 2.3868e-01 (2.3015e-01)	Acc 0.739258 (0.756071)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756683)
Training Loss of Epoch 79: 0.23017093657962676
Training Acc of Epoch 79: 0.7559848196138211
Testing Acc of Epoch 79: 0.7566826086956522
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3115e-01 (2.3115e-01)	Acc 0.762695 (0.762695)
Epoch: [80][300/616]	Loss 2.1286e-01 (2.3031e-01)	Acc 0.780273 (0.756404)
Epoch: [80][600/616]	Loss 2.2913e-01 (2.3031e-01)	Acc 0.750000 (0.756204)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755952)
Training Loss of Epoch 80: 0.2302368047033868
Training Acc of Epoch 80: 0.7562166539634146
Testing Acc of Epoch 80: 0.7559521739130435
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3877e-01 (2.3877e-01)	Acc 0.755859 (0.755859)
Epoch: [81][300/616]	Loss 2.2417e-01 (2.2957e-01)	Acc 0.766602 (0.757086)
Epoch: [81][600/616]	Loss 2.2859e-01 (2.3014e-01)	Acc 0.755859 (0.756049)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756152)
Training Loss of Epoch 81: 0.23011094438351268
Training Acc of Epoch 81: 0.7561388465447154
Testing Acc of Epoch 81: 0.7561521739130435
Model with the best training loss saved! The loss is 0.23011094438351268
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3809e-01 (2.3809e-01)	Acc 0.733398 (0.733398)
Epoch: [82][300/616]	Loss 2.2331e-01 (2.3032e-01)	Acc 0.763672 (0.755772)
Epoch: [82][600/616]	Loss 2.3970e-01 (2.3016e-01)	Acc 0.747070 (0.756209)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756248)
Training Loss of Epoch 82: 0.23013815385539357
Training Acc of Epoch 82: 0.7562341209349593
Testing Acc of Epoch 82: 0.7562478260869565
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.2580e-01 (2.2580e-01)	Acc 0.756836 (0.756836)
Epoch: [83][300/616]	Loss 2.2021e-01 (2.3029e-01)	Acc 0.776367 (0.755986)
Epoch: [83][600/616]	Loss 2.4197e-01 (2.2992e-01)	Acc 0.748047 (0.756389)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756226)
Training Loss of Epoch 83: 0.22998598014436117
Training Acc of Epoch 83: 0.7562404725609756
Testing Acc of Epoch 83: 0.7562260869565217
Model with the best training loss saved! The loss is 0.22998598014436117
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.2118e-01 (2.2118e-01)	Acc 0.774414 (0.774414)
Epoch: [84][300/616]	Loss 2.3159e-01 (2.2978e-01)	Acc 0.748047 (0.756768)
Epoch: [84][600/616]	Loss 2.1911e-01 (2.3007e-01)	Acc 0.769531 (0.756140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755052)
Training Loss of Epoch 84: 0.23001138771452556
Training Acc of Epoch 84: 0.7562579395325203
Testing Acc of Epoch 84: 0.7550521739130435
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4112e-01 (2.4112e-01)	Acc 0.745117 (0.745117)
Epoch: [85][300/616]	Loss 2.3972e-01 (2.3076e-01)	Acc 0.742188 (0.755762)
Epoch: [85][600/616]	Loss 2.2280e-01 (2.3016e-01)	Acc 0.766602 (0.756415)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757157)
Training Loss of Epoch 85: 0.23010936171058716
Training Acc of Epoch 85: 0.7564596036585366
Testing Acc of Epoch 85: 0.7571565217391304
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.2698e-01 (2.2698e-01)	Acc 0.752930 (0.752930)
Epoch: [86][300/616]	Loss 2.2690e-01 (2.2952e-01)	Acc 0.760742 (0.756849)
Epoch: [86][600/616]	Loss 2.2597e-01 (2.3001e-01)	Acc 0.762695 (0.756066)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756652)
Training Loss of Epoch 86: 0.22999426802484
Training Acc of Epoch 86: 0.756103912601626
Testing Acc of Epoch 86: 0.7566521739130435
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.2918e-01 (2.2918e-01)	Acc 0.748047 (0.748047)
Epoch: [87][300/616]	Loss 2.3576e-01 (2.3006e-01)	Acc 0.744141 (0.756768)
Epoch: [87][600/616]	Loss 2.3795e-01 (2.3007e-01)	Acc 0.750000 (0.756339)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755087)
Training Loss of Epoch 87: 0.23005538444693496
Training Acc of Epoch 87: 0.7563262195121951
Testing Acc of Epoch 87: 0.7550869565217392
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3249e-01 (2.3249e-01)	Acc 0.750000 (0.750000)
Epoch: [88][300/616]	Loss 2.3221e-01 (2.3059e-01)	Acc 0.752930 (0.755866)
Epoch: [88][600/616]	Loss 2.2889e-01 (2.2988e-01)	Acc 0.758789 (0.756581)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756643)
Training Loss of Epoch 88: 0.2299670289687025
Training Acc of Epoch 88: 0.756470719004065
Testing Acc of Epoch 88: 0.7566434782608695
Model with the best training loss saved! The loss is 0.2299670289687025
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2933e-01 (2.2933e-01)	Acc 0.763672 (0.763672)
Epoch: [89][300/616]	Loss 2.2144e-01 (2.3033e-01)	Acc 0.759766 (0.755908)
Epoch: [89][600/616]	Loss 2.3091e-01 (2.3007e-01)	Acc 0.754883 (0.756103)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.756135)
Training Loss of Epoch 89: 0.23003041671543586
Training Acc of Epoch 89: 0.7561467860772357
Testing Acc of Epoch 89: 0.7561347826086956
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4667e-01 (2.4667e-01)	Acc 0.731445 (0.731445)
Epoch: [90][300/616]	Loss 2.1609e-01 (2.3011e-01)	Acc 0.778320 (0.756174)
Epoch: [90][600/616]	Loss 2.2942e-01 (2.2989e-01)	Acc 0.751953 (0.756332)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756561)
Training Loss of Epoch 90: 0.22994305382414562
Training Acc of Epoch 90: 0.756323043699187
Testing Acc of Epoch 90: 0.7565608695652174
Model with the best training loss saved! The loss is 0.22994305382414562
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.1832e-01 (2.1832e-01)	Acc 0.765625 (0.765625)
Epoch: [91][300/616]	Loss 2.3894e-01 (2.3021e-01)	Acc 0.748047 (0.756330)
Epoch: [91][600/616]	Loss 2.1739e-01 (2.2996e-01)	Acc 0.773438 (0.756409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755691)
Training Loss of Epoch 91: 0.22999989225612424
Training Acc of Epoch 91: 0.756373856707317
Testing Acc of Epoch 91: 0.755691304347826
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2676e-01 (2.2676e-01)	Acc 0.754883 (0.754883)
Epoch: [92][300/616]	Loss 2.2123e-01 (2.3045e-01)	Acc 0.768555 (0.756232)
Epoch: [92][600/616]	Loss 2.2062e-01 (2.2992e-01)	Acc 0.766602 (0.756464)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756665)
Training Loss of Epoch 92: 0.22986127142983723
Training Acc of Epoch 92: 0.7564913617886179
Testing Acc of Epoch 92: 0.7566652173913043
Model with the best training loss saved! The loss is 0.22986127142983723
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3357e-01 (2.3357e-01)	Acc 0.742188 (0.742188)
Epoch: [93][300/616]	Loss 2.2571e-01 (2.3006e-01)	Acc 0.768555 (0.755580)
Epoch: [93][600/616]	Loss 2.1476e-01 (2.2995e-01)	Acc 0.772461 (0.756259)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756822)
Training Loss of Epoch 93: 0.22988067436509016
Training Acc of Epoch 93: 0.756348450203252
Testing Acc of Epoch 93: 0.7568217391304348
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3323e-01 (2.3323e-01)	Acc 0.735352 (0.735352)
Epoch: [94][300/616]	Loss 2.1789e-01 (2.2953e-01)	Acc 0.774414 (0.756920)
Epoch: [94][600/616]	Loss 2.1154e-01 (2.2995e-01)	Acc 0.785156 (0.756430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757048)
Training Loss of Epoch 94: 0.23001582188819483
Training Acc of Epoch 94: 0.7563817962398374
Testing Acc of Epoch 94: 0.7570478260869565
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2831e-01 (2.2831e-01)	Acc 0.760742 (0.760742)
Epoch: [95][300/616]	Loss 2.3079e-01 (2.2957e-01)	Acc 0.769531 (0.756829)
Epoch: [95][600/616]	Loss 2.2334e-01 (2.2987e-01)	Acc 0.770508 (0.756376)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756913)
Training Loss of Epoch 95: 0.22991945808980524
Training Acc of Epoch 95: 0.756373856707317
Testing Acc of Epoch 95: 0.7569130434782608
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2884e-01 (2.2884e-01)	Acc 0.762695 (0.762695)
Epoch: [96][300/616]	Loss 2.3001e-01 (2.3046e-01)	Acc 0.760742 (0.756080)
Epoch: [96][600/616]	Loss 2.2361e-01 (2.3000e-01)	Acc 0.767578 (0.756267)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.756865)
Training Loss of Epoch 96: 0.22990638284179254
Training Acc of Epoch 96: 0.7563675050813008
Testing Acc of Epoch 96: 0.7568652173913043
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.4196e-01 (2.4196e-01)	Acc 0.759766 (0.759766)
Epoch: [97][300/616]	Loss 2.2706e-01 (2.3016e-01)	Acc 0.768555 (0.756142)
Epoch: [97][600/616]	Loss 2.1942e-01 (2.2994e-01)	Acc 0.752930 (0.756467)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755930)
Training Loss of Epoch 97: 0.22988955993962482
Training Acc of Epoch 97: 0.7565564659552846
Testing Acc of Epoch 97: 0.7559304347826087
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4042e-01 (2.4042e-01)	Acc 0.744141 (0.744141)
Epoch: [98][300/616]	Loss 2.3441e-01 (2.2943e-01)	Acc 0.758789 (0.757469)
Epoch: [98][600/616]	Loss 2.3486e-01 (2.3001e-01)	Acc 0.767578 (0.756438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756317)
Training Loss of Epoch 98: 0.22995116170344315
Training Acc of Epoch 98: 0.7565294715447154
Testing Acc of Epoch 98: 0.7563173913043478
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3634e-01 (2.3634e-01)	Acc 0.747070 (0.747070)
Epoch: [99][300/616]	Loss 2.4527e-01 (2.3055e-01)	Acc 0.727539 (0.755473)
Epoch: [99][600/616]	Loss 2.2706e-01 (2.3004e-01)	Acc 0.745117 (0.756387)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756739)
Training Loss of Epoch 99: 0.2301537511309957
Training Acc of Epoch 99: 0.7562547637195122
Testing Acc of Epoch 99: 0.7567391304347826
Early stopping not satisfied.
