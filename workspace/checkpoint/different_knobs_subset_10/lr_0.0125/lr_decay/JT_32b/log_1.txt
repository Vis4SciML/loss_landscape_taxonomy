train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_32b
different_width False
resnet18_width 64
weight_precision 32
bias_precision 32
act_precision 35
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_32b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_32b
------------------------------------------------------
using FP32 model
---------------------- Model -------------------------
ThreeLayerMLP(
  (dense_1): Linear(in_features=16, out_features=64, bias=True)
  (dense_2): Linear(in_features=64, out_features=32, bias=True)
  (dense_3): Linear(in_features=32, out_features=32, bias=True)
  (dense_4): Linear(in_features=32, out_features=5, bias=True)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0441e-01 (5.0441e-01)	Acc 0.199219 (0.199219)
Epoch: [0][300/616]	Loss 2.3405e-01 (2.6935e-01)	Acc 0.756836 (0.715175)
Epoch: [0][600/616]	Loss 2.3221e-01 (2.5593e-01)	Acc 0.760742 (0.730375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.745587)
Training Loss of Epoch 0: 0.2555999309066834
Training Acc of Epoch 0: 0.7307307545731707
Testing Acc of Epoch 0: 0.7455869565217391
Model with the best training loss saved! The loss is 0.2555999309066834
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4017e-01 (2.4017e-01)	Acc 0.748047 (0.748047)
Epoch: [1][300/616]	Loss 2.4110e-01 (2.3875e-01)	Acc 0.739258 (0.748482)
Epoch: [1][600/616]	Loss 2.4417e-01 (2.3808e-01)	Acc 0.746094 (0.749335)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751396)
Training Loss of Epoch 1: 0.23808200846358044
Training Acc of Epoch 1: 0.7493775406504065
Testing Acc of Epoch 1: 0.751395652173913
Model with the best training loss saved! The loss is 0.23808200846358044
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3382e-01 (2.3382e-01)	Acc 0.752930 (0.752930)
Epoch: [2][300/616]	Loss 2.3370e-01 (2.3545e-01)	Acc 0.752930 (0.751791)
Epoch: [2][600/616]	Loss 2.3540e-01 (2.3580e-01)	Acc 0.767578 (0.751362)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.751004)
Training Loss of Epoch 2: 0.23576661408916721
Training Acc of Epoch 2: 0.7513386051829268
Testing Acc of Epoch 2: 0.7510043478260869
Model with the best training loss saved! The loss is 0.23576661408916721
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4576e-01 (2.4576e-01)	Acc 0.725586 (0.725586)
Epoch: [3][300/616]	Loss 2.2897e-01 (2.3451e-01)	Acc 0.748047 (0.752102)
Epoch: [3][600/616]	Loss 2.5727e-01 (2.3467e-01)	Acc 0.726562 (0.752293)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.753726)
Training Loss of Epoch 3: 0.23458739279246912
Training Acc of Epoch 3: 0.7523501016260162
Testing Acc of Epoch 3: 0.7537260869565218
Model with the best training loss saved! The loss is 0.23458739279246912
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4118e-01 (2.4118e-01)	Acc 0.726562 (0.726562)
Epoch: [4][300/616]	Loss 2.3137e-01 (2.3391e-01)	Acc 0.757812 (0.753556)
Epoch: [4][600/616]	Loss 2.2688e-01 (2.3416e-01)	Acc 0.753906 (0.752852)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753257)
Training Loss of Epoch 4: 0.234058905326254
Training Acc of Epoch 4: 0.7529741488821138
Testing Acc of Epoch 4: 0.7532565217391304
Model with the best training loss saved! The loss is 0.234058905326254
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.2203e-01 (2.2203e-01)	Acc 0.777344 (0.777344)
Epoch: [5][300/616]	Loss 2.4447e-01 (2.3403e-01)	Acc 0.737305 (0.752680)
Epoch: [5][600/616]	Loss 2.1875e-01 (2.3411e-01)	Acc 0.775391 (0.752756)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.754230)
Training Loss of Epoch 5: 0.23407594566422749
Training Acc of Epoch 5: 0.752832825203252
Testing Acc of Epoch 5: 0.7542304347826086
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3581e-01 (2.3581e-01)	Acc 0.753906 (0.753906)
Epoch: [6][300/616]	Loss 2.3100e-01 (2.3374e-01)	Acc 0.756836 (0.752287)
Epoch: [6][600/616]	Loss 2.4349e-01 (2.3328e-01)	Acc 0.730469 (0.753258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752878)
Training Loss of Epoch 6: 0.23323286359872275
Training Acc of Epoch 6: 0.7533552464430894
Testing Acc of Epoch 6: 0.7528782608695652
Model with the best training loss saved! The loss is 0.23323286359872275
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5132e-01 (2.5132e-01)	Acc 0.725586 (0.725586)
Epoch: [7][300/616]	Loss 2.3916e-01 (2.3266e-01)	Acc 0.739258 (0.754442)
Epoch: [7][600/616]	Loss 2.1290e-01 (2.3339e-01)	Acc 0.763672 (0.753498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756117)
Training Loss of Epoch 7: 0.23342552155983157
Training Acc of Epoch 7: 0.7534489329268292
Testing Acc of Epoch 7: 0.7561173913043479
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.1787e-01 (2.1787e-01)	Acc 0.767578 (0.767578)
Epoch: [8][300/616]	Loss 2.2879e-01 (2.3381e-01)	Acc 0.757812 (0.753215)
Epoch: [8][600/616]	Loss 2.3416e-01 (2.3318e-01)	Acc 0.756836 (0.753372)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754674)
Training Loss of Epoch 8: 0.23310718957970783
Training Acc of Epoch 8: 0.7534648119918699
Testing Acc of Epoch 8: 0.7546739130434783
Model with the best training loss saved! The loss is 0.23310718957970783
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4476e-01 (2.4476e-01)	Acc 0.735352 (0.735352)
Epoch: [9][300/616]	Loss 2.4065e-01 (2.3283e-01)	Acc 0.742188 (0.753754)
Epoch: [9][600/616]	Loss 2.2656e-01 (2.3299e-01)	Acc 0.765625 (0.753781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753326)
Training Loss of Epoch 9: 0.23290555796972134
Training Acc of Epoch 9: 0.7539459476626016
Testing Acc of Epoch 9: 0.7533260869565217
Model with the best training loss saved! The loss is 0.23290555796972134
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3141e-01 (2.3141e-01)	Acc 0.748047 (0.748047)
Epoch: [10][300/616]	Loss 2.3720e-01 (2.3322e-01)	Acc 0.749023 (0.753439)
Epoch: [10][600/616]	Loss 2.3800e-01 (2.3287e-01)	Acc 0.751953 (0.753549)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752343)
Training Loss of Epoch 10: 0.23287837958432794
Training Acc of Epoch 10: 0.7535076854674797
Testing Acc of Epoch 10: 0.7523434782608696
Model with the best training loss saved! The loss is 0.23287837958432794
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4646e-01 (2.4646e-01)	Acc 0.742188 (0.742188)
Epoch: [11][300/616]	Loss 2.4103e-01 (2.3291e-01)	Acc 0.754883 (0.753789)
Epoch: [11][600/616]	Loss 2.4660e-01 (2.3287e-01)	Acc 0.737305 (0.754108)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754057)
Training Loss of Epoch 11: 0.2328560415806809
Training Acc of Epoch 11: 0.754125381097561
Testing Acc of Epoch 11: 0.7540565217391304
Model with the best training loss saved! The loss is 0.2328560415806809
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3290e-01 (2.3290e-01)	Acc 0.751953 (0.751953)
Epoch: [12][300/616]	Loss 2.1871e-01 (2.3228e-01)	Acc 0.774414 (0.754821)
Epoch: [12][600/616]	Loss 2.2515e-01 (2.3294e-01)	Acc 0.755859 (0.753932)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755487)
Training Loss of Epoch 12: 0.23291587439494404
Training Acc of Epoch 12: 0.7538967225609756
Testing Acc of Epoch 12: 0.7554869565217391
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3505e-01 (2.3505e-01)	Acc 0.751953 (0.751953)
Epoch: [13][300/616]	Loss 2.3605e-01 (2.3333e-01)	Acc 0.738281 (0.753050)
Epoch: [13][600/616]	Loss 2.2754e-01 (2.3277e-01)	Acc 0.754883 (0.753908)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754987)
Training Loss of Epoch 13: 0.2328666435993784
Training Acc of Epoch 13: 0.7537855691056911
Testing Acc of Epoch 13: 0.7549869565217391
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2524e-01 (2.2524e-01)	Acc 0.755859 (0.755859)
Epoch: [14][300/616]	Loss 2.4491e-01 (2.3221e-01)	Acc 0.741211 (0.753965)
Epoch: [14][600/616]	Loss 2.0645e-01 (2.3248e-01)	Acc 0.782227 (0.753965)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753209)
Training Loss of Epoch 14: 0.2324572713879066
Training Acc of Epoch 14: 0.7540571011178862
Testing Acc of Epoch 14: 0.753208695652174
Model with the best training loss saved! The loss is 0.2324572713879066
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3538e-01 (2.3538e-01)	Acc 0.766602 (0.766602)
Epoch: [15][300/616]	Loss 2.2704e-01 (2.3277e-01)	Acc 0.763672 (0.753978)
Epoch: [15][600/616]	Loss 2.4194e-01 (2.3284e-01)	Acc 0.738281 (0.753918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756761)
Training Loss of Epoch 15: 0.2328091894223438
Training Acc of Epoch 15: 0.7539189532520325
Testing Acc of Epoch 15: 0.7567608695652174
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2367e-01 (2.2367e-01)	Acc 0.770508 (0.770508)
Epoch: [16][300/616]	Loss 2.2956e-01 (2.3273e-01)	Acc 0.751953 (0.754211)
Epoch: [16][600/616]	Loss 2.5045e-01 (2.3235e-01)	Acc 0.736328 (0.754540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753087)
Training Loss of Epoch 16: 0.2324416446007364
Training Acc of Epoch 16: 0.7544604293699188
Testing Acc of Epoch 16: 0.7530869565217392
Model with the best training loss saved! The loss is 0.2324416446007364
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.2578e-01 (2.2578e-01)	Acc 0.764648 (0.764648)
Epoch: [17][300/616]	Loss 2.1754e-01 (2.3257e-01)	Acc 0.780273 (0.754519)
Epoch: [17][600/616]	Loss 2.3433e-01 (2.3288e-01)	Acc 0.771484 (0.754015)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756709)
Training Loss of Epoch 17: 0.23290668805924858
Training Acc of Epoch 17: 0.7539586509146341
Testing Acc of Epoch 17: 0.7567086956521739
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4626e-01 (2.4626e-01)	Acc 0.740234 (0.740234)
Epoch: [18][300/616]	Loss 2.2319e-01 (2.3248e-01)	Acc 0.751953 (0.754386)
Epoch: [18][600/616]	Loss 2.2289e-01 (2.3249e-01)	Acc 0.759766 (0.754226)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.754596)
Training Loss of Epoch 18: 0.23248547865607874
Training Acc of Epoch 18: 0.7542143038617887
Testing Acc of Epoch 18: 0.754595652173913
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3218e-01 (2.3218e-01)	Acc 0.752930 (0.752930)
Epoch: [19][300/616]	Loss 2.4274e-01 (2.3241e-01)	Acc 0.748047 (0.754412)
Epoch: [19][600/616]	Loss 2.4264e-01 (2.3259e-01)	Acc 0.748047 (0.754033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755230)
Training Loss of Epoch 19: 0.2325460794253078
Training Acc of Epoch 19: 0.7541396722560976
Testing Acc of Epoch 19: 0.7552304347826087
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.2676e-01 (2.2676e-01)	Acc 0.766602 (0.766602)
Epoch: [20][300/616]	Loss 2.3785e-01 (2.3238e-01)	Acc 0.754883 (0.753871)
Epoch: [20][600/616]	Loss 2.3095e-01 (2.3256e-01)	Acc 0.749023 (0.754033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752422)
Training Loss of Epoch 20: 0.2325361769616119
Training Acc of Epoch 20: 0.7540078760162602
Testing Acc of Epoch 20: 0.7524217391304348
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3551e-01 (2.3551e-01)	Acc 0.759766 (0.759766)
Epoch: [21][300/616]	Loss 2.3136e-01 (2.3241e-01)	Acc 0.753906 (0.753845)
Epoch: [21][600/616]	Loss 2.2972e-01 (2.3243e-01)	Acc 0.764648 (0.753885)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754065)
Training Loss of Epoch 21: 0.23245632326215263
Training Acc of Epoch 21: 0.7538983104674797
Testing Acc of Epoch 21: 0.7540652173913044
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.2672e-01 (2.2672e-01)	Acc 0.767578 (0.767578)
Epoch: [22][300/616]	Loss 2.2634e-01 (2.3256e-01)	Acc 0.777344 (0.754292)
Epoch: [22][600/616]	Loss 2.4734e-01 (2.3240e-01)	Acc 0.745117 (0.754324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755013)
Training Loss of Epoch 22: 0.2324215188501327
Training Acc of Epoch 22: 0.7542873475609756
Testing Acc of Epoch 22: 0.7550130434782608
Model with the best training loss saved! The loss is 0.2324215188501327
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3393e-01 (2.3393e-01)	Acc 0.745117 (0.745117)
Epoch: [23][300/616]	Loss 2.2691e-01 (2.3189e-01)	Acc 0.768555 (0.754442)
Epoch: [23][600/616]	Loss 2.3011e-01 (2.3222e-01)	Acc 0.756836 (0.754425)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753491)
Training Loss of Epoch 23: 0.23227527897532393
Training Acc of Epoch 23: 0.7543730945121951
Testing Acc of Epoch 23: 0.7534913043478261
Model with the best training loss saved! The loss is 0.23227527897532393
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2725e-01 (2.2725e-01)	Acc 0.763672 (0.763672)
Epoch: [24][300/616]	Loss 2.2952e-01 (2.3264e-01)	Acc 0.749023 (0.754085)
Epoch: [24][600/616]	Loss 2.2995e-01 (2.3257e-01)	Acc 0.757812 (0.754231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752752)
Training Loss of Epoch 24: 0.23254723657922047
Training Acc of Epoch 24: 0.7542762322154472
Testing Acc of Epoch 24: 0.7527521739130435
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.2377e-01 (2.2377e-01)	Acc 0.763672 (0.763672)
Epoch: [25][300/616]	Loss 2.2712e-01 (2.3303e-01)	Acc 0.764648 (0.753650)
Epoch: [25][600/616]	Loss 2.5653e-01 (2.3230e-01)	Acc 0.732422 (0.754668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754922)
Training Loss of Epoch 25: 0.23227496772277645
Training Acc of Epoch 25: 0.7547240218495935
Testing Acc of Epoch 25: 0.7549217391304348
Model with the best training loss saved! The loss is 0.23227496772277645
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3504e-01 (2.3504e-01)	Acc 0.751953 (0.751953)
Epoch: [26][300/616]	Loss 2.3628e-01 (2.3271e-01)	Acc 0.757812 (0.753747)
Epoch: [26][600/616]	Loss 2.3312e-01 (2.3248e-01)	Acc 0.749023 (0.754122)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750678)
Training Loss of Epoch 26: 0.23254192718645422
Training Acc of Epoch 26: 0.7540205792682927
Testing Acc of Epoch 26: 0.7506782608695652
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.3459e-01 (2.3459e-01)	Acc 0.735352 (0.735352)
Epoch: [27][300/616]	Loss 2.2908e-01 (2.3252e-01)	Acc 0.752930 (0.753974)
Epoch: [27][600/616]	Loss 2.3091e-01 (2.3243e-01)	Acc 0.756836 (0.754316)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756043)
Training Loss of Epoch 27: 0.2324390860107856
Training Acc of Epoch 27: 0.7542365345528456
Testing Acc of Epoch 27: 0.7560434782608696
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3868e-01 (2.3868e-01)	Acc 0.738281 (0.738281)
Epoch: [28][300/616]	Loss 2.2367e-01 (2.3275e-01)	Acc 0.763672 (0.753420)
Epoch: [28][600/616]	Loss 2.4975e-01 (2.3220e-01)	Acc 0.737305 (0.754654)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756126)
Training Loss of Epoch 28: 0.23223955003711266
Training Acc of Epoch 28: 0.7545779344512196
Testing Acc of Epoch 28: 0.7561260869565217
Model with the best training loss saved! The loss is 0.23223955003711266
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.3643e-01 (2.3643e-01)	Acc 0.764648 (0.764648)
Epoch: [29][300/616]	Loss 2.3735e-01 (2.3195e-01)	Acc 0.740234 (0.754831)
Epoch: [29][600/616]	Loss 2.2662e-01 (2.3241e-01)	Acc 0.767578 (0.754278)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.756265)
Training Loss of Epoch 29: 0.23232099682819551
Training Acc of Epoch 29: 0.7543492759146342
Testing Acc of Epoch 29: 0.7562652173913044
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2626e-01 (2.2626e-01)	Acc 0.771484 (0.771484)
Epoch: [30][300/616]	Loss 2.3334e-01 (2.3242e-01)	Acc 0.752930 (0.754344)
Epoch: [30][600/616]	Loss 2.3305e-01 (2.3220e-01)	Acc 0.754883 (0.754262)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754391)
Training Loss of Epoch 30: 0.23214925626913707
Training Acc of Epoch 30: 0.7542603531504065
Testing Acc of Epoch 30: 0.7543913043478261
Model with the best training loss saved! The loss is 0.23214925626913707
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.4313e-01 (2.4313e-01)	Acc 0.736328 (0.736328)
Epoch: [31][300/616]	Loss 2.5324e-01 (2.3252e-01)	Acc 0.725586 (0.753747)
Epoch: [31][600/616]	Loss 2.2464e-01 (2.3243e-01)	Acc 0.774414 (0.753942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755891)
Training Loss of Epoch 31: 0.23237537761044696
Training Acc of Epoch 31: 0.7540380462398374
Testing Acc of Epoch 31: 0.755891304347826
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.1689e-01 (2.1689e-01)	Acc 0.785156 (0.785156)
Epoch: [32][300/616]	Loss 2.2359e-01 (2.3197e-01)	Acc 0.755859 (0.754179)
Epoch: [32][600/616]	Loss 2.2476e-01 (2.3224e-01)	Acc 0.753906 (0.754353)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757030)
Training Loss of Epoch 32: 0.23220483832727604
Training Acc of Epoch 32: 0.7544016768292683
Testing Acc of Epoch 32: 0.7570304347826087
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2076e-01 (2.2076e-01)	Acc 0.750977 (0.750977)
Epoch: [33][300/616]	Loss 2.2486e-01 (2.3206e-01)	Acc 0.756836 (0.754516)
Epoch: [33][600/616]	Loss 2.3048e-01 (2.3246e-01)	Acc 0.757812 (0.754519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756496)
Training Loss of Epoch 33: 0.23243567965379575
Training Acc of Epoch 33: 0.754517594004065
Testing Acc of Epoch 33: 0.756495652173913
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4832e-01 (2.4832e-01)	Acc 0.737305 (0.737305)
Epoch: [34][300/616]	Loss 2.3439e-01 (2.3255e-01)	Acc 0.751953 (0.754406)
Epoch: [34][600/616]	Loss 2.1735e-01 (2.3233e-01)	Acc 0.776367 (0.754511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755304)
Training Loss of Epoch 34: 0.23248234238566423
Training Acc of Epoch 34: 0.7542857596544715
Testing Acc of Epoch 34: 0.7553043478260869
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3201e-01 (2.3201e-01)	Acc 0.762695 (0.762695)
Epoch: [35][300/616]	Loss 2.4620e-01 (2.3218e-01)	Acc 0.740234 (0.754727)
Epoch: [35][600/616]	Loss 2.4826e-01 (2.3202e-01)	Acc 0.743164 (0.754834)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753439)
Training Loss of Epoch 35: 0.2321614596659575
Training Acc of Epoch 35: 0.7546700330284553
Testing Acc of Epoch 35: 0.7534391304347826
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3510e-01 (2.3510e-01)	Acc 0.757812 (0.757812)
Epoch: [36][300/616]	Loss 2.3338e-01 (2.3178e-01)	Acc 0.756836 (0.755470)
Epoch: [36][600/616]	Loss 2.4983e-01 (2.3222e-01)	Acc 0.730469 (0.754949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753643)
Training Loss of Epoch 36: 0.23229127041692657
Training Acc of Epoch 36: 0.754809768800813
Testing Acc of Epoch 36: 0.7536434782608695
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3700e-01 (2.3700e-01)	Acc 0.743164 (0.743164)
Epoch: [37][300/616]	Loss 2.2808e-01 (2.3232e-01)	Acc 0.755859 (0.754140)
Epoch: [37][600/616]	Loss 2.1539e-01 (2.3199e-01)	Acc 0.776367 (0.754418)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756126)
Training Loss of Epoch 37: 0.23198922300726418
Training Acc of Epoch 37: 0.7544937754065041
Testing Acc of Epoch 37: 0.7561260869565217
Model with the best training loss saved! The loss is 0.23198922300726418
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.2900e-01 (2.2900e-01)	Acc 0.763672 (0.763672)
Epoch: [38][300/616]	Loss 2.1900e-01 (2.3201e-01)	Acc 0.762695 (0.754776)
Epoch: [38][600/616]	Loss 2.3155e-01 (2.3214e-01)	Acc 0.755859 (0.755031)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754904)
Training Loss of Epoch 38: 0.23222402060419564
Training Acc of Epoch 38: 0.7548716971544716
Testing Acc of Epoch 38: 0.754904347826087
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2677e-01 (2.2677e-01)	Acc 0.765625 (0.765625)
Epoch: [39][300/616]	Loss 2.2430e-01 (2.3294e-01)	Acc 0.773438 (0.753965)
Epoch: [39][600/616]	Loss 2.4452e-01 (2.3240e-01)	Acc 0.739258 (0.754626)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755787)
Training Loss of Epoch 39: 0.23239613407995643
Training Acc of Epoch 39: 0.7546001651422765
Testing Acc of Epoch 39: 0.7557869565217391
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4421e-01 (2.4421e-01)	Acc 0.749023 (0.749023)
Epoch: [40][300/616]	Loss 2.3552e-01 (2.3193e-01)	Acc 0.755859 (0.755142)
Epoch: [40][600/616]	Loss 2.2948e-01 (2.3257e-01)	Acc 0.760742 (0.754316)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755987)
Training Loss of Epoch 40: 0.23246833951008028
Training Acc of Epoch 40: 0.7544778963414634
Testing Acc of Epoch 40: 0.7559869565217391
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.2358e-01 (2.2358e-01)	Acc 0.764648 (0.764648)
Epoch: [41][300/616]	Loss 2.2552e-01 (2.3244e-01)	Acc 0.767578 (0.754312)
Epoch: [41][600/616]	Loss 2.3251e-01 (2.3251e-01)	Acc 0.754883 (0.754327)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749183)
Training Loss of Epoch 41: 0.2324879914522171
Training Acc of Epoch 41: 0.7543413363821139
Testing Acc of Epoch 41: 0.7491826086956521
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3082e-01 (2.3082e-01)	Acc 0.756836 (0.756836)
Epoch: [42][300/616]	Loss 2.3668e-01 (2.3246e-01)	Acc 0.737305 (0.754227)
Epoch: [42][600/616]	Loss 2.3018e-01 (2.3245e-01)	Acc 0.752930 (0.754238)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756448)
Training Loss of Epoch 42: 0.2324554844358103
Training Acc of Epoch 42: 0.7541698424796748
Testing Acc of Epoch 42: 0.7564478260869565
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3297e-01 (2.3297e-01)	Acc 0.760742 (0.760742)
Epoch: [43][300/616]	Loss 2.2490e-01 (2.3182e-01)	Acc 0.763672 (0.754328)
Epoch: [43][600/616]	Loss 2.1679e-01 (2.3210e-01)	Acc 0.775391 (0.754516)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.756552)
Training Loss of Epoch 43: 0.23209145943808362
Training Acc of Epoch 43: 0.7544810721544716
Testing Acc of Epoch 43: 0.7565521739130435
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3701e-01 (2.3701e-01)	Acc 0.753906 (0.753906)
Epoch: [44][300/616]	Loss 2.2881e-01 (2.3313e-01)	Acc 0.766602 (0.753520)
Epoch: [44][600/616]	Loss 2.4492e-01 (2.3249e-01)	Acc 0.748047 (0.754642)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755717)
Training Loss of Epoch 44: 0.232439361718612
Training Acc of Epoch 44: 0.754689087906504
Testing Acc of Epoch 44: 0.7557173913043478
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3562e-01 (2.3562e-01)	Acc 0.741211 (0.741211)
Epoch: [45][300/616]	Loss 2.2990e-01 (2.3361e-01)	Acc 0.764648 (0.753218)
Epoch: [45][600/616]	Loss 2.3813e-01 (2.3253e-01)	Acc 0.753906 (0.754384)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754952)
Training Loss of Epoch 45: 0.23249164213979148
Training Acc of Epoch 45: 0.7543969131097561
Testing Acc of Epoch 45: 0.7549521739130435
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4407e-01 (2.4407e-01)	Acc 0.741211 (0.741211)
Epoch: [46][300/616]	Loss 2.2824e-01 (2.3247e-01)	Acc 0.762695 (0.754328)
Epoch: [46][600/616]	Loss 2.1994e-01 (2.3258e-01)	Acc 0.770508 (0.754324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755435)
Training Loss of Epoch 46: 0.23261553545792898
Training Acc of Epoch 46: 0.7543000508130081
Testing Acc of Epoch 46: 0.7554347826086957
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2448e-01 (2.2448e-01)	Acc 0.763672 (0.763672)
Epoch: [47][300/616]	Loss 2.4064e-01 (2.3253e-01)	Acc 0.753906 (0.753887)
Epoch: [47][600/616]	Loss 2.3501e-01 (2.3268e-01)	Acc 0.738281 (0.754169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752583)
Training Loss of Epoch 47: 0.23258962158749744
Training Acc of Epoch 47: 0.754149199695122
Testing Acc of Epoch 47: 0.7525826086956522
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.2980e-01 (2.2980e-01)	Acc 0.752930 (0.752930)
Epoch: [48][300/616]	Loss 2.2226e-01 (2.3193e-01)	Acc 0.762695 (0.754581)
Epoch: [48][600/616]	Loss 2.3130e-01 (2.3238e-01)	Acc 0.769531 (0.754088)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754900)
Training Loss of Epoch 48: 0.23240891462419092
Training Acc of Epoch 48: 0.7540475736788618
Testing Acc of Epoch 48: 0.7549
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4289e-01 (2.4289e-01)	Acc 0.730469 (0.730469)
Epoch: [49][300/616]	Loss 2.2426e-01 (2.3249e-01)	Acc 0.763672 (0.753650)
Epoch: [49][600/616]	Loss 2.4201e-01 (2.3226e-01)	Acc 0.752930 (0.754434)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755309)
Training Loss of Epoch 49: 0.23226170106147362
Training Acc of Epoch 49: 0.7544254954268292
Testing Acc of Epoch 49: 0.755308695652174
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4092e-01 (2.4092e-01)	Acc 0.730469 (0.730469)
Epoch: [50][300/616]	Loss 2.3000e-01 (2.3203e-01)	Acc 0.750000 (0.754886)
Epoch: [50][600/616]	Loss 2.4721e-01 (2.3262e-01)	Acc 0.734375 (0.754321)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755148)
Training Loss of Epoch 50: 0.23258067703828578
Training Acc of Epoch 50: 0.7544143800813008
Testing Acc of Epoch 50: 0.7551478260869565
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2331e-01 (2.2331e-01)	Acc 0.765625 (0.765625)
Epoch: [51][300/616]	Loss 2.3307e-01 (2.3264e-01)	Acc 0.754883 (0.755058)
Epoch: [51][600/616]	Loss 2.2331e-01 (2.3243e-01)	Acc 0.766602 (0.754364)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755709)
Training Loss of Epoch 51: 0.23244237972468865
Training Acc of Epoch 51: 0.7544175558943089
Testing Acc of Epoch 51: 0.7557086956521739
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.3893e-01 (2.3893e-01)	Acc 0.750977 (0.750977)
Epoch: [52][300/616]	Loss 2.3519e-01 (2.3247e-01)	Acc 0.754883 (0.754104)
Epoch: [52][600/616]	Loss 2.4222e-01 (2.3211e-01)	Acc 0.742188 (0.754594)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755061)
Training Loss of Epoch 52: 0.23218026318685794
Training Acc of Epoch 52: 0.7545366488821138
Testing Acc of Epoch 52: 0.7550608695652173
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3853e-01 (2.3853e-01)	Acc 0.744141 (0.744141)
Epoch: [53][300/616]	Loss 2.2715e-01 (2.3195e-01)	Acc 0.763672 (0.754763)
Epoch: [53][600/616]	Loss 2.4307e-01 (2.3218e-01)	Acc 0.747070 (0.754845)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754391)
Training Loss of Epoch 53: 0.2322260725062068
Training Acc of Epoch 53: 0.7548018292682926
Testing Acc of Epoch 53: 0.7543913043478261
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3872e-01 (2.3872e-01)	Acc 0.738281 (0.738281)
Epoch: [54][300/616]	Loss 2.3880e-01 (2.3290e-01)	Acc 0.736328 (0.753812)
Epoch: [54][600/616]	Loss 2.3264e-01 (2.3255e-01)	Acc 0.753906 (0.754257)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755009)
Training Loss of Epoch 54: 0.23262996300449215
Training Acc of Epoch 54: 0.7541444359756098
Testing Acc of Epoch 54: 0.7550086956521739
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3363e-01 (2.3363e-01)	Acc 0.752930 (0.752930)
Epoch: [55][300/616]	Loss 2.3571e-01 (2.3271e-01)	Acc 0.746094 (0.753981)
Epoch: [55][600/616]	Loss 2.4382e-01 (2.3237e-01)	Acc 0.739258 (0.754517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753896)
Training Loss of Epoch 55: 0.23246963334277393
Training Acc of Epoch 55: 0.7544223196138211
Testing Acc of Epoch 55: 0.7538956521739131
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2346e-01 (2.2346e-01)	Acc 0.766602 (0.766602)
Epoch: [56][300/616]	Loss 2.3230e-01 (2.3246e-01)	Acc 0.757812 (0.754523)
Epoch: [56][600/616]	Loss 2.1217e-01 (2.3239e-01)	Acc 0.779297 (0.754030)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754787)
Training Loss of Epoch 56: 0.23238961720854287
Training Acc of Epoch 56: 0.754125381097561
Testing Acc of Epoch 56: 0.7547869565217391
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3139e-01 (2.3139e-01)	Acc 0.748047 (0.748047)
Epoch: [57][300/616]	Loss 2.4071e-01 (2.3221e-01)	Acc 0.740234 (0.754253)
Epoch: [57][600/616]	Loss 2.4461e-01 (2.3237e-01)	Acc 0.726562 (0.754303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756326)
Training Loss of Epoch 57: 0.2323112774912904
Training Acc of Epoch 57: 0.7544286712398374
Testing Acc of Epoch 57: 0.7563260869565217
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.4561e-01 (2.4561e-01)	Acc 0.726562 (0.726562)
Epoch: [58][300/616]	Loss 2.2396e-01 (2.3166e-01)	Acc 0.778320 (0.755405)
Epoch: [58][600/616]	Loss 2.2621e-01 (2.3209e-01)	Acc 0.763672 (0.754683)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752735)
Training Loss of Epoch 58: 0.2320694614958957
Training Acc of Epoch 58: 0.754735137195122
Testing Acc of Epoch 58: 0.7527347826086956
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.1753e-01 (2.1753e-01)	Acc 0.778320 (0.778320)
Epoch: [59][300/616]	Loss 2.3368e-01 (2.3256e-01)	Acc 0.753906 (0.754500)
Epoch: [59][600/616]	Loss 2.2363e-01 (2.3242e-01)	Acc 0.759766 (0.754194)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754804)
Training Loss of Epoch 59: 0.23238794747891464
Training Acc of Epoch 59: 0.7542333587398374
Testing Acc of Epoch 59: 0.754804347826087
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3420e-01 (2.3420e-01)	Acc 0.758789 (0.758789)
Epoch: [60][300/616]	Loss 2.3644e-01 (2.3234e-01)	Acc 0.753906 (0.754078)
Epoch: [60][600/616]	Loss 2.3388e-01 (2.3239e-01)	Acc 0.747070 (0.754698)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754604)
Training Loss of Epoch 60: 0.23245528563251341
Training Acc of Epoch 60: 0.7545493521341463
Testing Acc of Epoch 60: 0.754604347826087
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.1832e-01 (2.1832e-01)	Acc 0.768555 (0.768555)
Epoch: [61][300/616]	Loss 2.3157e-01 (2.3199e-01)	Acc 0.752930 (0.755110)
Epoch: [61][600/616]	Loss 2.3926e-01 (2.3244e-01)	Acc 0.742188 (0.754278)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.753591)
Training Loss of Epoch 61: 0.23236467431231242
Training Acc of Epoch 61: 0.7543191056910569
Testing Acc of Epoch 61: 0.7535913043478261
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.1309e-01 (2.1309e-01)	Acc 0.781250 (0.781250)
Epoch: [62][300/616]	Loss 2.3813e-01 (2.3215e-01)	Acc 0.746094 (0.754802)
Epoch: [62][600/616]	Loss 2.4213e-01 (2.3236e-01)	Acc 0.735352 (0.754407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754965)
Training Loss of Epoch 62: 0.23231296282473618
Training Acc of Epoch 62: 0.7545302972560975
Testing Acc of Epoch 62: 0.7549652173913044
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3025e-01 (2.3025e-01)	Acc 0.749023 (0.749023)
Epoch: [63][300/616]	Loss 2.4837e-01 (2.3236e-01)	Acc 0.730469 (0.755045)
Epoch: [63][600/616]	Loss 2.3671e-01 (2.3251e-01)	Acc 0.741211 (0.754407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755830)
Training Loss of Epoch 63: 0.23247726957003276
Training Acc of Epoch 63: 0.7544699568089431
Testing Acc of Epoch 63: 0.7558304347826087
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3306e-01 (2.3306e-01)	Acc 0.754883 (0.754883)
Epoch: [64][300/616]	Loss 2.2585e-01 (2.3289e-01)	Acc 0.760742 (0.753776)
Epoch: [64][600/616]	Loss 2.3886e-01 (2.3232e-01)	Acc 0.751953 (0.754589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749074)
Training Loss of Epoch 64: 0.23230018489729098
Training Acc of Epoch 64: 0.7546366869918699
Testing Acc of Epoch 64: 0.7490739130434783
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.2720e-01 (2.2720e-01)	Acc 0.761719 (0.761719)
Epoch: [65][300/616]	Loss 2.1678e-01 (2.3268e-01)	Acc 0.774414 (0.753773)
Epoch: [65][600/616]	Loss 2.6138e-01 (2.3226e-01)	Acc 0.721680 (0.754711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.757139)
Training Loss of Epoch 65: 0.23232541547073582
Training Acc of Epoch 65: 0.7546430386178862
Testing Acc of Epoch 65: 0.7571391304347826
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3111e-01 (2.3111e-01)	Acc 0.750000 (0.750000)
Epoch: [66][300/616]	Loss 2.2947e-01 (2.3203e-01)	Acc 0.766602 (0.754805)
Epoch: [66][600/616]	Loss 2.1544e-01 (2.3244e-01)	Acc 0.768555 (0.754431)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751978)
Training Loss of Epoch 66: 0.23250641585365545
Training Acc of Epoch 66: 0.7543349847560976
Testing Acc of Epoch 66: 0.7519782608695652
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.2168e-01 (2.2168e-01)	Acc 0.756836 (0.756836)
Epoch: [67][300/616]	Loss 2.2767e-01 (2.3268e-01)	Acc 0.766602 (0.754565)
Epoch: [67][600/616]	Loss 2.2416e-01 (2.3238e-01)	Acc 0.768555 (0.754592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755757)
Training Loss of Epoch 67: 0.23244016676898893
Training Acc of Epoch 67: 0.7545239456300813
Testing Acc of Epoch 67: 0.7557565217391304
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3989e-01 (2.3989e-01)	Acc 0.745117 (0.745117)
Epoch: [68][300/616]	Loss 2.3926e-01 (2.3291e-01)	Acc 0.748047 (0.754150)
Epoch: [68][600/616]	Loss 2.2215e-01 (2.3266e-01)	Acc 0.769531 (0.754303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752791)
Training Loss of Epoch 68: 0.2326187373176823
Training Acc of Epoch 68: 0.7544302591463414
Testing Acc of Epoch 68: 0.752791304347826
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4105e-01 (2.4105e-01)	Acc 0.745117 (0.745117)
Epoch: [69][300/616]	Loss 2.3056e-01 (2.3220e-01)	Acc 0.759766 (0.754510)
Epoch: [69][600/616]	Loss 2.4578e-01 (2.3259e-01)	Acc 0.732422 (0.754121)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755243)
Training Loss of Epoch 69: 0.23261565231695408
Training Acc of Epoch 69: 0.7541206173780488
Testing Acc of Epoch 69: 0.7552434782608696
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3155e-01 (2.3155e-01)	Acc 0.758789 (0.758789)
Epoch: [70][300/616]	Loss 2.3993e-01 (2.3181e-01)	Acc 0.744141 (0.755541)
Epoch: [70][600/616]	Loss 2.3127e-01 (2.3253e-01)	Acc 0.749023 (0.754629)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755535)
Training Loss of Epoch 70: 0.2324574301882488
Training Acc of Epoch 70: 0.7547303734756098
Testing Acc of Epoch 70: 0.7555347826086957
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.3147e-01 (2.3147e-01)	Acc 0.764648 (0.764648)
Epoch: [71][300/616]	Loss 2.3239e-01 (2.3308e-01)	Acc 0.749023 (0.753394)
Epoch: [71][600/616]	Loss 2.2881e-01 (2.3260e-01)	Acc 0.766602 (0.754139)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753939)
Training Loss of Epoch 71: 0.23258730812770564
Training Acc of Epoch 71: 0.7542111280487804
Testing Acc of Epoch 71: 0.7539391304347826
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.3245e-01 (2.3245e-01)	Acc 0.753906 (0.753906)
Epoch: [72][300/616]	Loss 2.3115e-01 (2.3208e-01)	Acc 0.756836 (0.754604)
Epoch: [72][600/616]	Loss 2.3571e-01 (2.3255e-01)	Acc 0.760742 (0.754083)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752870)
Training Loss of Epoch 72: 0.2324980654852177
Training Acc of Epoch 72: 0.7541523755081301
Testing Acc of Epoch 72: 0.7528695652173913
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.3133e-01 (2.3133e-01)	Acc 0.752930 (0.752930)
Epoch: [73][300/616]	Loss 2.3768e-01 (2.3209e-01)	Acc 0.745117 (0.754698)
Epoch: [73][600/616]	Loss 2.3598e-01 (2.3232e-01)	Acc 0.748047 (0.754540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.756261)
Training Loss of Epoch 73: 0.23236543291467962
Training Acc of Epoch 73: 0.7544985391260163
Testing Acc of Epoch 73: 0.7562608695652174
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.2697e-01 (2.2697e-01)	Acc 0.747070 (0.747070)
Epoch: [74][300/616]	Loss 2.3634e-01 (2.3224e-01)	Acc 0.750977 (0.754516)
Epoch: [74][600/616]	Loss 2.1918e-01 (2.3217e-01)	Acc 0.768555 (0.754509)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755839)
Training Loss of Epoch 74: 0.2321449819134503
Training Acc of Epoch 74: 0.7545509400406504
Testing Acc of Epoch 74: 0.7558391304347826
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.2415e-01 (2.2415e-01)	Acc 0.761719 (0.761719)
Epoch: [75][300/616]	Loss 2.1820e-01 (2.2801e-01)	Acc 0.781250 (0.758095)
Epoch: [75][600/616]	Loss 2.3579e-01 (2.2850e-01)	Acc 0.750000 (0.757621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.758687)
Training Loss of Epoch 75: 0.2285766022234428
Training Acc of Epoch 75: 0.7574996824186991
Testing Acc of Epoch 75: 0.7586869565217391
Model with the best training loss saved! The loss is 0.2285766022234428
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3898e-01 (2.3898e-01)	Acc 0.749023 (0.749023)
Epoch: [76][300/616]	Loss 2.1204e-01 (2.2837e-01)	Acc 0.770508 (0.757975)
Epoch: [76][600/616]	Loss 2.2289e-01 (2.2824e-01)	Acc 0.773438 (0.757977)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757913)
Training Loss of Epoch 76: 0.22820160401061299
Training Acc of Epoch 76: 0.7579665269308943
Testing Acc of Epoch 76: 0.7579130434782608
Model with the best training loss saved! The loss is 0.22820160401061299
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.2542e-01 (2.2542e-01)	Acc 0.755859 (0.755859)
Epoch: [77][300/616]	Loss 2.2441e-01 (2.2831e-01)	Acc 0.766602 (0.758023)
Epoch: [77][600/616]	Loss 2.2514e-01 (2.2818e-01)	Acc 0.765625 (0.758079)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759339)
Training Loss of Epoch 77: 0.2281640255111989
Training Acc of Epoch 77: 0.7581173780487804
Testing Acc of Epoch 77: 0.7593391304347826
Model with the best training loss saved! The loss is 0.2281640255111989
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3378e-01 (2.3378e-01)	Acc 0.727539 (0.727539)
Epoch: [78][300/616]	Loss 2.2213e-01 (2.2716e-01)	Acc 0.779297 (0.759042)
Epoch: [78][600/616]	Loss 2.2559e-01 (2.2797e-01)	Acc 0.760742 (0.758347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.758057)
Training Loss of Epoch 78: 0.2280851352747863
Training Acc of Epoch 78: 0.7582205919715447
Testing Acc of Epoch 78: 0.7580565217391304
Model with the best training loss saved! The loss is 0.2280851352747863
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2922e-01 (2.2922e-01)	Acc 0.756836 (0.756836)
Epoch: [79][300/616]	Loss 2.3074e-01 (2.2799e-01)	Acc 0.744141 (0.758257)
Epoch: [79][600/616]	Loss 2.2257e-01 (2.2800e-01)	Acc 0.768555 (0.758303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.758691)
Training Loss of Epoch 79: 0.2279193872601036
Training Acc of Epoch 79: 0.7583444486788617
Testing Acc of Epoch 79: 0.7586913043478261
Model with the best training loss saved! The loss is 0.2279193872601036
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2674e-01 (2.2674e-01)	Acc 0.753906 (0.753906)
Epoch: [80][300/616]	Loss 2.1893e-01 (2.2819e-01)	Acc 0.772461 (0.757926)
Epoch: [80][600/616]	Loss 2.3944e-01 (2.2795e-01)	Acc 0.747070 (0.758448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.758596)
Training Loss of Epoch 80: 0.22790434721524153
Training Acc of Epoch 80: 0.7584762449186991
Testing Acc of Epoch 80: 0.758595652173913
Model with the best training loss saved! The loss is 0.22790434721524153
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3173e-01 (2.3173e-01)	Acc 0.755859 (0.755859)
Epoch: [81][300/616]	Loss 2.1928e-01 (2.2762e-01)	Acc 0.768555 (0.759198)
Epoch: [81][600/616]	Loss 2.2852e-01 (2.2790e-01)	Acc 0.746094 (0.758592)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758970)
Training Loss of Epoch 81: 0.22786855152467403
Training Acc of Epoch 81: 0.7586112169715448
Testing Acc of Epoch 81: 0.7589695652173913
Model with the best training loss saved! The loss is 0.22786855152467403
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3257e-01 (2.3257e-01)	Acc 0.749023 (0.749023)
Epoch: [82][300/616]	Loss 2.2858e-01 (2.2725e-01)	Acc 0.754883 (0.759603)
Epoch: [82][600/616]	Loss 2.2575e-01 (2.2778e-01)	Acc 0.753906 (0.758744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.759065)
Training Loss of Epoch 82: 0.2277265095371541
Training Acc of Epoch 82: 0.7587477769308943
Testing Acc of Epoch 82: 0.7590652173913044
Model with the best training loss saved! The loss is 0.2277265095371541
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.2618e-01 (2.2618e-01)	Acc 0.761719 (0.761719)
Epoch: [83][300/616]	Loss 2.0106e-01 (2.2781e-01)	Acc 0.790039 (0.758796)
Epoch: [83][600/616]	Loss 2.2481e-01 (2.2776e-01)	Acc 0.755859 (0.758591)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759361)
Training Loss of Epoch 83: 0.22768572384749003
Training Acc of Epoch 83: 0.7586810848577236
Testing Acc of Epoch 83: 0.7593608695652174
Model with the best training loss saved! The loss is 0.22768572384749003
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3712e-01 (2.3712e-01)	Acc 0.742188 (0.742188)
Epoch: [84][300/616]	Loss 2.3541e-01 (2.2756e-01)	Acc 0.752930 (0.758289)
Epoch: [84][600/616]	Loss 2.1761e-01 (2.2776e-01)	Acc 0.767578 (0.758493)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759126)
Training Loss of Epoch 84: 0.22768352373344142
Training Acc of Epoch 84: 0.7586001016260162
Testing Acc of Epoch 84: 0.7591260869565217
Model with the best training loss saved! The loss is 0.22768352373344142
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2407e-01 (2.2407e-01)	Acc 0.768555 (0.768555)
Epoch: [85][300/616]	Loss 2.3777e-01 (2.2766e-01)	Acc 0.749023 (0.758578)
Epoch: [85][600/616]	Loss 2.1773e-01 (2.2750e-01)	Acc 0.779297 (0.759062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759448)
Training Loss of Epoch 85: 0.22757896453869053
Training Acc of Epoch 85: 0.7589129192073171
Testing Acc of Epoch 85: 0.7594478260869565
Model with the best training loss saved! The loss is 0.22757896453869053
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3483e-01 (2.3483e-01)	Acc 0.742188 (0.742188)
Epoch: [86][300/616]	Loss 2.1854e-01 (2.2727e-01)	Acc 0.774414 (0.759649)
Epoch: [86][600/616]	Loss 2.3502e-01 (2.2756e-01)	Acc 0.756836 (0.758787)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759761)
Training Loss of Epoch 86: 0.22756128359616287
Training Acc of Epoch 86: 0.7588319359756097
Testing Acc of Epoch 86: 0.7597608695652174
Model with the best training loss saved! The loss is 0.22756128359616287
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.2016e-01 (2.2016e-01)	Acc 0.777344 (0.777344)
Epoch: [87][300/616]	Loss 2.2817e-01 (2.2753e-01)	Acc 0.757812 (0.758942)
Epoch: [87][600/616]	Loss 2.4128e-01 (2.2747e-01)	Acc 0.731445 (0.758822)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758996)
Training Loss of Epoch 87: 0.22751268509442243
Training Acc of Epoch 87: 0.7588081173780488
Testing Acc of Epoch 87: 0.7589956521739131
Model with the best training loss saved! The loss is 0.22751268509442243
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.2791e-01 (2.2791e-01)	Acc 0.755859 (0.755859)
Epoch: [88][300/616]	Loss 2.1992e-01 (2.2779e-01)	Acc 0.770508 (0.758841)
Epoch: [88][600/616]	Loss 2.0107e-01 (2.2740e-01)	Acc 0.792969 (0.759213)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.759891)
Training Loss of Epoch 88: 0.22755228319788368
Training Acc of Epoch 88: 0.7590272484756098
Testing Acc of Epoch 88: 0.759891304347826
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.3012e-01 (2.3012e-01)	Acc 0.754883 (0.754883)
Epoch: [89][300/616]	Loss 2.2492e-01 (2.2747e-01)	Acc 0.761719 (0.759159)
Epoch: [89][600/616]	Loss 2.3175e-01 (2.2738e-01)	Acc 0.751953 (0.759069)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759574)
Training Loss of Epoch 89: 0.22747857938937055
Training Acc of Epoch 89: 0.7589764354674797
Testing Acc of Epoch 89: 0.7595739130434782
Model with the best training loss saved! The loss is 0.22747857938937055
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.1943e-01 (2.1943e-01)	Acc 0.770508 (0.770508)
Epoch: [90][300/616]	Loss 2.1777e-01 (2.2743e-01)	Acc 0.780273 (0.759292)
Epoch: [90][600/616]	Loss 2.3199e-01 (2.2753e-01)	Acc 0.752930 (0.759073)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759413)
Training Loss of Epoch 90: 0.22748944962896953
Training Acc of Epoch 90: 0.7590717098577235
Testing Acc of Epoch 90: 0.7594130434782609
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2356e-01 (2.2356e-01)	Acc 0.758789 (0.758789)
Epoch: [91][300/616]	Loss 2.2251e-01 (2.2759e-01)	Acc 0.766602 (0.758776)
Epoch: [91][600/616]	Loss 2.1002e-01 (2.2741e-01)	Acc 0.780273 (0.758867)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.759574)
Training Loss of Epoch 91: 0.2274338503920935
Training Acc of Epoch 91: 0.7588525787601627
Testing Acc of Epoch 91: 0.7595739130434782
Model with the best training loss saved! The loss is 0.2274338503920935
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.4050e-01 (2.4050e-01)	Acc 0.745117 (0.745117)
Epoch: [92][300/616]	Loss 2.3187e-01 (2.2719e-01)	Acc 0.749023 (0.759201)
Epoch: [92][600/616]	Loss 2.2611e-01 (2.2742e-01)	Acc 0.760742 (0.758984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.759243)
Training Loss of Epoch 92: 0.22740053619311107
Training Acc of Epoch 92: 0.7590145452235773
Testing Acc of Epoch 92: 0.7592434782608696
Model with the best training loss saved! The loss is 0.22740053619311107
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2599e-01 (2.2599e-01)	Acc 0.763672 (0.763672)
Epoch: [93][300/616]	Loss 2.2070e-01 (2.2757e-01)	Acc 0.775391 (0.758942)
Epoch: [93][600/616]	Loss 2.2504e-01 (2.2748e-01)	Acc 0.764648 (0.758815)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.760074)
Training Loss of Epoch 93: 0.2274272962556622
Training Acc of Epoch 93: 0.7589303861788618
Testing Acc of Epoch 93: 0.7600739130434783
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.1770e-01 (2.1770e-01)	Acc 0.769531 (0.769531)
Epoch: [94][300/616]	Loss 2.3375e-01 (2.2741e-01)	Acc 0.745117 (0.758961)
Epoch: [94][600/616]	Loss 2.1590e-01 (2.2744e-01)	Acc 0.771484 (0.758994)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.759643)
Training Loss of Epoch 94: 0.22744276896724855
Training Acc of Epoch 94: 0.7590399517276423
Testing Acc of Epoch 94: 0.7596434782608695
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3564e-01 (2.3564e-01)	Acc 0.753906 (0.753906)
Epoch: [95][300/616]	Loss 2.2117e-01 (2.2750e-01)	Acc 0.768555 (0.759019)
Epoch: [95][600/616]	Loss 2.2945e-01 (2.2749e-01)	Acc 0.750000 (0.758986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.759209)
Training Loss of Epoch 95: 0.22741745744294267
Training Acc of Epoch 95: 0.7590907647357723
Testing Acc of Epoch 95: 0.7592086956521739
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2774e-01 (2.2774e-01)	Acc 0.756836 (0.756836)
Epoch: [96][300/616]	Loss 2.2823e-01 (2.2715e-01)	Acc 0.752930 (0.759373)
Epoch: [96][600/616]	Loss 2.4172e-01 (2.2728e-01)	Acc 0.734375 (0.759130)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.759217)
Training Loss of Epoch 96: 0.22731454403904394
Training Acc of Epoch 96: 0.7591209349593496
Testing Acc of Epoch 96: 0.7592173913043478
Model with the best training loss saved! The loss is 0.22731454403904394
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.3582e-01 (2.3582e-01)	Acc 0.750000 (0.750000)
Epoch: [97][300/616]	Loss 2.4267e-01 (2.2706e-01)	Acc 0.742188 (0.759253)
Epoch: [97][600/616]	Loss 2.2049e-01 (2.2737e-01)	Acc 0.785156 (0.758991)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758687)
Training Loss of Epoch 97: 0.227347304738634
Training Acc of Epoch 97: 0.7590367759146341
Testing Acc of Epoch 97: 0.7586869565217391
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.1937e-01 (2.1937e-01)	Acc 0.774414 (0.774414)
Epoch: [98][300/616]	Loss 2.1088e-01 (2.2691e-01)	Acc 0.781250 (0.759681)
Epoch: [98][600/616]	Loss 2.2938e-01 (2.2729e-01)	Acc 0.746094 (0.759012)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.759496)
Training Loss of Epoch 98: 0.22728946414904866
Training Acc of Epoch 98: 0.7590256605691057
Testing Acc of Epoch 98: 0.759495652173913
Model with the best training loss saved! The loss is 0.22728946414904866
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3560e-01 (2.3560e-01)	Acc 0.762695 (0.762695)
Epoch: [99][300/616]	Loss 2.3711e-01 (2.2705e-01)	Acc 0.757812 (0.759720)
Epoch: [99][600/616]	Loss 2.3035e-01 (2.2732e-01)	Acc 0.758789 (0.759095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.759791)
Training Loss of Epoch 99: 0.22733267848569202
Training Acc of Epoch 99: 0.7590193089430894
Testing Acc of Epoch 99: 0.759791304347826
Early stopping not satisfied.
