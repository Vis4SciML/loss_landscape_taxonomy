train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_10b
different_width False
resnet18_width 64
weight_precision 10
bias_precision 10
act_precision 13
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_10b/
file_prefix exp_3
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_10b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=13, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=10, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=13, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=10, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=13, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=10, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=13, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=10, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0153e-01 (5.0153e-01)	Acc 0.089844 (0.089844)
Epoch: [0][300/616]	Loss 2.4671e-01 (2.7641e-01)	Acc 0.750977 (0.707236)
Epoch: [0][600/616]	Loss 2.3987e-01 (2.6048e-01)	Acc 0.757812 (0.725843)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748065)
Training Loss of Epoch 0: 0.26007163633660574
Training Acc of Epoch 0: 0.7263052591463415
Testing Acc of Epoch 0: 0.7480652173913044
Model with the best training loss saved! The loss is 0.26007163633660574
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4713e-01 (2.4713e-01)	Acc 0.748047 (0.748047)
Epoch: [1][300/616]	Loss 2.3088e-01 (2.4144e-01)	Acc 0.757812 (0.746662)
Epoch: [1][600/616]	Loss 2.3002e-01 (2.4013e-01)	Acc 0.748047 (0.747873)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.749700)
Training Loss of Epoch 1: 0.24006813185486367
Training Acc of Epoch 1: 0.7479452489837398
Testing Acc of Epoch 1: 0.7497
Model with the best training loss saved! The loss is 0.24006813185486367
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3662e-01 (2.3662e-01)	Acc 0.754883 (0.754883)
Epoch: [2][300/616]	Loss 2.4735e-01 (2.3869e-01)	Acc 0.735352 (0.748981)
Epoch: [2][600/616]	Loss 2.1264e-01 (2.3780e-01)	Acc 0.772461 (0.749691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752243)
Training Loss of Epoch 2: 0.2377007493885552
Training Acc of Epoch 2: 0.7498348577235773
Testing Acc of Epoch 2: 0.7522434782608696
Model with the best training loss saved! The loss is 0.2377007493885552
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3362e-01 (2.3362e-01)	Acc 0.761719 (0.761719)
Epoch: [3][300/616]	Loss 2.1799e-01 (2.3719e-01)	Acc 0.769531 (0.750188)
Epoch: [3][600/616]	Loss 2.3675e-01 (2.3647e-01)	Acc 0.751953 (0.751133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.750939)
Training Loss of Epoch 3: 0.23642476588729922
Training Acc of Epoch 3: 0.7511909298780488
Testing Acc of Epoch 3: 0.7509391304347826
Model with the best training loss saved! The loss is 0.23642476588729922
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3552e-01 (2.3552e-01)	Acc 0.736328 (0.736328)
Epoch: [4][300/616]	Loss 2.3505e-01 (2.3564e-01)	Acc 0.746094 (0.751019)
Epoch: [4][600/616]	Loss 2.3344e-01 (2.3560e-01)	Acc 0.763672 (0.751732)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754674)
Training Loss of Epoch 4: 0.23551378126551464
Training Acc of Epoch 4: 0.7518483231707317
Testing Acc of Epoch 4: 0.7546739130434783
Model with the best training loss saved! The loss is 0.23551378126551464
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3893e-01 (2.3893e-01)	Acc 0.748047 (0.748047)
Epoch: [5][300/616]	Loss 2.4747e-01 (2.3545e-01)	Acc 0.748047 (0.752031)
Epoch: [5][600/616]	Loss 2.4265e-01 (2.3525e-01)	Acc 0.741211 (0.752085)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753691)
Training Loss of Epoch 5: 0.2351750501287662
Training Acc of Epoch 5: 0.7521770198170732
Testing Acc of Epoch 5: 0.753691304347826
Model with the best training loss saved! The loss is 0.2351750501287662
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.1608e-01 (2.1608e-01)	Acc 0.779297 (0.779297)
Epoch: [6][300/616]	Loss 2.3728e-01 (2.3527e-01)	Acc 0.762695 (0.751992)
Epoch: [6][600/616]	Loss 2.3699e-01 (2.3491e-01)	Acc 0.750000 (0.752327)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751822)
Training Loss of Epoch 6: 0.23498424298879578
Training Acc of Epoch 6: 0.7521801956300813
Testing Acc of Epoch 6: 0.7518217391304348
Model with the best training loss saved! The loss is 0.23498424298879578
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2694e-01 (2.2694e-01)	Acc 0.773438 (0.773438)
Epoch: [7][300/616]	Loss 2.3453e-01 (2.3521e-01)	Acc 0.745117 (0.751976)
Epoch: [7][600/616]	Loss 2.3382e-01 (2.3452e-01)	Acc 0.753906 (0.752746)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754183)
Training Loss of Epoch 7: 0.23449919240746073
Training Acc of Epoch 7: 0.7527613694105691
Testing Acc of Epoch 7: 0.7541826086956521
Model with the best training loss saved! The loss is 0.23449919240746073
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4189e-01 (2.4189e-01)	Acc 0.744141 (0.744141)
Epoch: [8][300/616]	Loss 2.2303e-01 (2.3478e-01)	Acc 0.753906 (0.752388)
Epoch: [8][600/616]	Loss 2.2772e-01 (2.3419e-01)	Acc 0.756836 (0.753121)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.754083)
Training Loss of Epoch 8: 0.23423963484725333
Training Acc of Epoch 8: 0.7530043191056911
Testing Acc of Epoch 8: 0.7540826086956521
Model with the best training loss saved! The loss is 0.23423963484725333
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4710e-01 (2.4710e-01)	Acc 0.732422 (0.732422)
Epoch: [9][300/616]	Loss 2.3661e-01 (2.3425e-01)	Acc 0.748047 (0.753043)
Epoch: [9][600/616]	Loss 2.2159e-01 (2.3429e-01)	Acc 0.762695 (0.752949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755470)
Training Loss of Epoch 9: 0.23418970655619614
Training Acc of Epoch 9: 0.7530535442073171
Testing Acc of Epoch 9: 0.7554695652173913
Model with the best training loss saved! The loss is 0.23418970655619614
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.4015e-01 (2.4015e-01)	Acc 0.739258 (0.739258)
Epoch: [10][300/616]	Loss 2.2358e-01 (2.3468e-01)	Acc 0.763672 (0.752832)
Epoch: [10][600/616]	Loss 2.3570e-01 (2.3403e-01)	Acc 0.763672 (0.753201)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755970)
Training Loss of Epoch 10: 0.23402414496352034
Training Acc of Epoch 10: 0.7533091971544715
Testing Acc of Epoch 10: 0.7559695652173913
Model with the best training loss saved! The loss is 0.23402414496352034
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4315e-01 (2.4315e-01)	Acc 0.739258 (0.739258)
Epoch: [11][300/616]	Loss 2.3382e-01 (2.3366e-01)	Acc 0.761719 (0.753695)
Epoch: [11][600/616]	Loss 2.2689e-01 (2.3400e-01)	Acc 0.753906 (0.753108)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754522)
Training Loss of Epoch 11: 0.23396018171698096
Training Acc of Epoch 11: 0.7530900660569105
Testing Acc of Epoch 11: 0.7545217391304347
Model with the best training loss saved! The loss is 0.23396018171698096
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3938e-01 (2.3938e-01)	Acc 0.747070 (0.747070)
Epoch: [12][300/616]	Loss 2.4102e-01 (2.3270e-01)	Acc 0.736328 (0.754146)
Epoch: [12][600/616]	Loss 2.3030e-01 (2.3344e-01)	Acc 0.747070 (0.753637)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752465)
Training Loss of Epoch 12: 0.2333937720796926
Training Acc of Epoch 12: 0.7536855309959349
Testing Acc of Epoch 12: 0.7524652173913043
Model with the best training loss saved! The loss is 0.2333937720796926
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3909e-01 (2.3909e-01)	Acc 0.740234 (0.740234)
Epoch: [13][300/616]	Loss 2.3469e-01 (2.3338e-01)	Acc 0.751953 (0.753725)
Epoch: [13][600/616]	Loss 2.3759e-01 (2.3377e-01)	Acc 0.762695 (0.753091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751674)
Training Loss of Epoch 13: 0.23370648033735228
Training Acc of Epoch 13: 0.7531551702235773
Testing Acc of Epoch 13: 0.7516739130434783
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3975e-01 (2.3975e-01)	Acc 0.757812 (0.757812)
Epoch: [14][300/616]	Loss 2.3604e-01 (2.3296e-01)	Acc 0.745117 (0.754221)
Epoch: [14][600/616]	Loss 2.2508e-01 (2.3355e-01)	Acc 0.753906 (0.753313)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.753930)
Training Loss of Epoch 14: 0.23351744432759478
Training Acc of Epoch 14: 0.7533330157520325
Testing Acc of Epoch 14: 0.7539304347826087
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3704e-01 (2.3704e-01)	Acc 0.756836 (0.756836)
Epoch: [15][300/616]	Loss 2.4271e-01 (2.3365e-01)	Acc 0.741211 (0.752839)
Epoch: [15][600/616]	Loss 2.2244e-01 (2.3347e-01)	Acc 0.769531 (0.753290)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753474)
Training Loss of Epoch 15: 0.2335610355545835
Training Acc of Epoch 15: 0.7531535823170732
Testing Acc of Epoch 15: 0.7534739130434782
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3530e-01 (2.3530e-01)	Acc 0.751953 (0.751953)
Epoch: [16][300/616]	Loss 2.3846e-01 (2.3390e-01)	Acc 0.736328 (0.752725)
Epoch: [16][600/616]	Loss 2.3700e-01 (2.3328e-01)	Acc 0.767578 (0.753505)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751813)
Training Loss of Epoch 16: 0.2332593238208352
Training Acc of Epoch 16: 0.7535727896341463
Testing Acc of Epoch 16: 0.7518130434782608
Model with the best training loss saved! The loss is 0.2332593238208352
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4310e-01 (2.4310e-01)	Acc 0.744141 (0.744141)
Epoch: [17][300/616]	Loss 2.3849e-01 (2.3404e-01)	Acc 0.744141 (0.752888)
Epoch: [17][600/616]	Loss 2.2639e-01 (2.3393e-01)	Acc 0.759766 (0.753024)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754435)
Training Loss of Epoch 17: 0.23382710402573997
Training Acc of Epoch 17: 0.7531408790650407
Testing Acc of Epoch 17: 0.7544347826086957
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.2804e-01 (2.2804e-01)	Acc 0.765625 (0.765625)
Epoch: [18][300/616]	Loss 2.4339e-01 (2.3383e-01)	Acc 0.754883 (0.752735)
Epoch: [18][600/616]	Loss 2.4016e-01 (2.3324e-01)	Acc 0.738281 (0.753690)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752848)
Training Loss of Epoch 18: 0.2332020732930036
Training Acc of Epoch 18: 0.7537411077235773
Testing Acc of Epoch 18: 0.7528478260869566
Model with the best training loss saved! The loss is 0.2332020732930036
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.2859e-01 (2.2859e-01)	Acc 0.758789 (0.758789)
Epoch: [19][300/616]	Loss 2.4030e-01 (2.3368e-01)	Acc 0.752930 (0.753601)
Epoch: [19][600/616]	Loss 2.3707e-01 (2.3355e-01)	Acc 0.740234 (0.753442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752917)
Training Loss of Epoch 19: 0.23352281744402598
Training Acc of Epoch 19: 0.7534727515243902
Testing Acc of Epoch 19: 0.7529173913043479
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4071e-01 (2.4071e-01)	Acc 0.743164 (0.743164)
Epoch: [20][300/616]	Loss 2.2752e-01 (2.3392e-01)	Acc 0.754883 (0.752514)
Epoch: [20][600/616]	Loss 2.2618e-01 (2.3377e-01)	Acc 0.762695 (0.752959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752496)
Training Loss of Epoch 20: 0.2338911626639405
Training Acc of Epoch 20: 0.7527804242886179
Testing Acc of Epoch 20: 0.752495652173913
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.2418e-01 (2.2418e-01)	Acc 0.772461 (0.772461)
Epoch: [21][300/616]	Loss 2.4921e-01 (2.3352e-01)	Acc 0.729492 (0.753167)
Epoch: [21][600/616]	Loss 2.4721e-01 (2.3357e-01)	Acc 0.726562 (0.753045)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.752835)
Training Loss of Epoch 21: 0.23357012082890766
Training Acc of Epoch 21: 0.7531075330284552
Testing Acc of Epoch 21: 0.7528347826086956
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3604e-01 (2.3604e-01)	Acc 0.751953 (0.751953)
Epoch: [22][300/616]	Loss 2.3175e-01 (2.3260e-01)	Acc 0.750977 (0.754114)
Epoch: [22][600/616]	Loss 2.3620e-01 (2.3338e-01)	Acc 0.751953 (0.753576)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755843)
Training Loss of Epoch 22: 0.23329422665320762
Training Acc of Epoch 22: 0.7536474212398374
Testing Acc of Epoch 22: 0.7558434782608696
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2050e-01 (2.2050e-01)	Acc 0.776367 (0.776367)
Epoch: [23][300/616]	Loss 2.4144e-01 (2.3415e-01)	Acc 0.746094 (0.752709)
Epoch: [23][600/616]	Loss 2.3483e-01 (2.3364e-01)	Acc 0.759766 (0.753404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754113)
Training Loss of Epoch 23: 0.23357100937424635
Training Acc of Epoch 23: 0.753490218495935
Testing Acc of Epoch 23: 0.7541130434782609
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.4617e-01 (2.4617e-01)	Acc 0.731445 (0.731445)
Epoch: [24][300/616]	Loss 2.4207e-01 (2.3304e-01)	Acc 0.731445 (0.753984)
Epoch: [24][600/616]	Loss 2.3622e-01 (2.3330e-01)	Acc 0.746094 (0.753916)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756065)
Training Loss of Epoch 24: 0.23340810951663227
Training Acc of Epoch 24: 0.7537744537601626
Testing Acc of Epoch 24: 0.7560652173913044
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.2707e-01 (2.2707e-01)	Acc 0.756836 (0.756836)
Epoch: [25][300/616]	Loss 2.2990e-01 (2.3245e-01)	Acc 0.746094 (0.754909)
Epoch: [25][600/616]	Loss 2.3437e-01 (2.3296e-01)	Acc 0.757812 (0.753830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750713)
Training Loss of Epoch 25: 0.2330752540652345
Training Acc of Epoch 25: 0.7537935086382114
Testing Acc of Epoch 25: 0.7507130434782608
Model with the best training loss saved! The loss is 0.2330752540652345
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4525e-01 (2.4525e-01)	Acc 0.741211 (0.741211)
Epoch: [26][300/616]	Loss 2.3513e-01 (2.3265e-01)	Acc 0.755859 (0.753913)
Epoch: [26][600/616]	Loss 2.3107e-01 (2.3336e-01)	Acc 0.760742 (0.753217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748696)
Training Loss of Epoch 26: 0.23332614101530091
Training Acc of Epoch 26: 0.7532885543699187
Testing Acc of Epoch 26: 0.7486956521739131
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.2462e-01 (2.2462e-01)	Acc 0.769531 (0.769531)
Epoch: [27][300/616]	Loss 2.3780e-01 (2.3415e-01)	Acc 0.754883 (0.752446)
Epoch: [27][600/616]	Loss 2.2125e-01 (2.3347e-01)	Acc 0.778320 (0.753287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753752)
Training Loss of Epoch 27: 0.2335098055804648
Training Acc of Epoch 27: 0.7532806148373984
Testing Acc of Epoch 27: 0.7537521739130435
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3781e-01 (2.3781e-01)	Acc 0.758789 (0.758789)
Epoch: [28][300/616]	Loss 2.3685e-01 (2.3245e-01)	Acc 0.750000 (0.754730)
Epoch: [28][600/616]	Loss 2.3588e-01 (2.3319e-01)	Acc 0.753906 (0.753729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755039)
Training Loss of Epoch 28: 0.23329594198281203
Training Acc of Epoch 28: 0.7536172510162602
Testing Acc of Epoch 28: 0.7550391304347827
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4285e-01 (2.4285e-01)	Acc 0.735352 (0.735352)
Epoch: [29][300/616]	Loss 2.3018e-01 (2.3323e-01)	Acc 0.759766 (0.754000)
Epoch: [29][600/616]	Loss 2.3455e-01 (2.3323e-01)	Acc 0.748047 (0.753507)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.749696)
Training Loss of Epoch 29: 0.23331188736407737
Training Acc of Epoch 29: 0.7534012957317073
Testing Acc of Epoch 29: 0.7496956521739131
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4520e-01 (2.4520e-01)	Acc 0.734375 (0.734375)
Epoch: [30][300/616]	Loss 2.3799e-01 (2.3290e-01)	Acc 0.736328 (0.754445)
Epoch: [30][600/616]	Loss 2.4195e-01 (2.3380e-01)	Acc 0.735352 (0.753167)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.754808 (0.756148)
Training Loss of Epoch 30: 0.23379731340621546
Training Acc of Epoch 30: 0.7531583460365854
Testing Acc of Epoch 30: 0.7561478260869565
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3146e-01 (2.3146e-01)	Acc 0.754883 (0.754883)
Epoch: [31][300/616]	Loss 2.2301e-01 (2.3344e-01)	Acc 0.766602 (0.753592)
Epoch: [31][600/616]	Loss 2.4102e-01 (2.3341e-01)	Acc 0.736328 (0.753458)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.751448)
Training Loss of Epoch 31: 0.23337848516014534
Training Acc of Epoch 31: 0.7535219766260163
Testing Acc of Epoch 31: 0.7514478260869565
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3983e-01 (2.3983e-01)	Acc 0.748047 (0.748047)
Epoch: [32][300/616]	Loss 2.2753e-01 (2.3354e-01)	Acc 0.774414 (0.753364)
Epoch: [32][600/616]	Loss 2.5294e-01 (2.3310e-01)	Acc 0.726562 (0.753955)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.755600)
Training Loss of Epoch 32: 0.23310021794423824
Training Acc of Epoch 32: 0.7539808816056911
Testing Acc of Epoch 32: 0.7556
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.1712e-01 (2.1712e-01)	Acc 0.765625 (0.765625)
Epoch: [33][300/616]	Loss 2.4578e-01 (2.3313e-01)	Acc 0.735352 (0.753030)
Epoch: [33][600/616]	Loss 2.3152e-01 (2.3372e-01)	Acc 0.756836 (0.753104)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.751022)
Training Loss of Epoch 33: 0.23373780713333348
Training Acc of Epoch 33: 0.7531408790650407
Testing Acc of Epoch 33: 0.7510217391304348
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.2032e-01 (2.2032e-01)	Acc 0.777344 (0.777344)
Epoch: [34][300/616]	Loss 2.3531e-01 (2.3355e-01)	Acc 0.751953 (0.754140)
Epoch: [34][600/616]	Loss 2.2951e-01 (2.3357e-01)	Acc 0.757812 (0.753524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754239)
Training Loss of Epoch 34: 0.23355449140071868
Training Acc of Epoch 34: 0.7535426194105691
Testing Acc of Epoch 34: 0.7542391304347826
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3456e-01 (2.3456e-01)	Acc 0.753906 (0.753906)
Epoch: [35][300/616]	Loss 2.2999e-01 (2.3339e-01)	Acc 0.753906 (0.753043)
Epoch: [35][600/616]	Loss 2.4186e-01 (2.3357e-01)	Acc 0.750000 (0.753021)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751100)
Training Loss of Epoch 35: 0.23354539657995954
Training Acc of Epoch 35: 0.7530694232723577
Testing Acc of Epoch 35: 0.7511
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3768e-01 (2.3768e-01)	Acc 0.755859 (0.755859)
Epoch: [36][300/616]	Loss 2.1728e-01 (2.3331e-01)	Acc 0.761719 (0.753468)
Epoch: [36][600/616]	Loss 2.1633e-01 (2.3347e-01)	Acc 0.770508 (0.753139)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754109)
Training Loss of Epoch 36: 0.2335251914291847
Training Acc of Epoch 36: 0.7531361153455285
Testing Acc of Epoch 36: 0.7541086956521739
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.1994e-01 (2.1994e-01)	Acc 0.771484 (0.771484)
Epoch: [37][300/616]	Loss 2.2960e-01 (2.3370e-01)	Acc 0.769531 (0.752589)
Epoch: [37][600/616]	Loss 2.2377e-01 (2.3335e-01)	Acc 0.750977 (0.753258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754474)
Training Loss of Epoch 37: 0.23338838941682644
Training Acc of Epoch 37: 0.7532028074186992
Testing Acc of Epoch 37: 0.7544739130434782
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3451e-01 (2.3451e-01)	Acc 0.747070 (0.747070)
Epoch: [38][300/616]	Loss 2.3680e-01 (2.3329e-01)	Acc 0.758789 (0.752858)
Epoch: [38][600/616]	Loss 2.2051e-01 (2.3337e-01)	Acc 0.775391 (0.753524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752839)
Training Loss of Epoch 38: 0.23334279527993707
Training Acc of Epoch 38: 0.7535696138211382
Testing Acc of Epoch 38: 0.7528391304347826
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.4071e-01 (2.4071e-01)	Acc 0.740234 (0.740234)
Epoch: [39][300/616]	Loss 2.3993e-01 (2.3272e-01)	Acc 0.740234 (0.753812)
Epoch: [39][600/616]	Loss 2.4949e-01 (2.3329e-01)	Acc 0.738281 (0.753468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753396)
Training Loss of Epoch 39: 0.23328089692243717
Training Acc of Epoch 39: 0.7534711636178861
Testing Acc of Epoch 39: 0.753395652173913
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4236e-01 (2.4236e-01)	Acc 0.737305 (0.737305)
Epoch: [40][300/616]	Loss 2.5193e-01 (2.3379e-01)	Acc 0.734375 (0.753186)
Epoch: [40][600/616]	Loss 2.2407e-01 (2.3337e-01)	Acc 0.759766 (0.753237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753152)
Training Loss of Epoch 40: 0.23345212134403912
Training Acc of Epoch 40: 0.753125
Testing Acc of Epoch 40: 0.7531521739130435
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4204e-01 (2.4204e-01)	Acc 0.741211 (0.741211)
Epoch: [41][300/616]	Loss 2.3126e-01 (2.3247e-01)	Acc 0.759766 (0.754124)
Epoch: [41][600/616]	Loss 2.4413e-01 (2.3327e-01)	Acc 0.754883 (0.753287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756065)
Training Loss of Epoch 41: 0.2332838365702125
Training Acc of Epoch 41: 0.7533663617886179
Testing Acc of Epoch 41: 0.7560652173913044
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.2834e-01 (2.2834e-01)	Acc 0.759766 (0.759766)
Epoch: [42][300/616]	Loss 2.3094e-01 (2.3364e-01)	Acc 0.759766 (0.753650)
Epoch: [42][600/616]	Loss 2.1941e-01 (2.3333e-01)	Acc 0.770508 (0.753445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754100)
Training Loss of Epoch 42: 0.23342938675143854
Training Acc of Epoch 42: 0.7533266641260162
Testing Acc of Epoch 42: 0.7541
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.4552e-01 (2.4552e-01)	Acc 0.737305 (0.737305)
Epoch: [43][300/616]	Loss 2.1801e-01 (2.3371e-01)	Acc 0.771484 (0.752716)
Epoch: [43][600/616]	Loss 2.2629e-01 (2.3382e-01)	Acc 0.771484 (0.752899)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755300)
Training Loss of Epoch 43: 0.23382501490717011
Training Acc of Epoch 43: 0.7529328633130081
Testing Acc of Epoch 43: 0.7553
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.2553e-01 (2.2553e-01)	Acc 0.756836 (0.756836)
Epoch: [44][300/616]	Loss 2.4143e-01 (2.3397e-01)	Acc 0.733398 (0.752829)
Epoch: [44][600/616]	Loss 2.3257e-01 (2.3332e-01)	Acc 0.752930 (0.753474)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752565)
Training Loss of Epoch 44: 0.23327863819715453
Training Acc of Epoch 44: 0.7535076854674797
Testing Acc of Epoch 44: 0.7525652173913043
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2012e-01 (2.2012e-01)	Acc 0.759766 (0.759766)
Epoch: [45][300/616]	Loss 2.3530e-01 (2.3243e-01)	Acc 0.757812 (0.754163)
Epoch: [45][600/616]	Loss 2.2958e-01 (2.3283e-01)	Acc 0.755859 (0.753880)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754891)
Training Loss of Epoch 45: 0.23289448485626438
Training Acc of Epoch 45: 0.7537728658536585
Testing Acc of Epoch 45: 0.7548913043478261
Model with the best training loss saved! The loss is 0.23289448485626438
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.2019e-01 (2.2019e-01)	Acc 0.772461 (0.772461)
Epoch: [46][300/616]	Loss 2.3502e-01 (2.3398e-01)	Acc 0.754883 (0.752252)
Epoch: [46][600/616]	Loss 2.2326e-01 (2.3321e-01)	Acc 0.761719 (0.753416)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751878)
Training Loss of Epoch 46: 0.23330256594874996
Training Acc of Epoch 46: 0.7533123729674797
Testing Acc of Epoch 46: 0.7518782608695652
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.5292e-01 (2.5292e-01)	Acc 0.733398 (0.733398)
Epoch: [47][300/616]	Loss 2.4975e-01 (2.3274e-01)	Acc 0.731445 (0.753738)
Epoch: [47][600/616]	Loss 2.3113e-01 (2.3297e-01)	Acc 0.757812 (0.753747)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753087)
Training Loss of Epoch 47: 0.23294711626642117
Training Acc of Epoch 47: 0.7536966463414634
Testing Acc of Epoch 47: 0.7530869565217392
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4908e-01 (2.4908e-01)	Acc 0.738281 (0.738281)
Epoch: [48][300/616]	Loss 2.3748e-01 (2.3349e-01)	Acc 0.746094 (0.753342)
Epoch: [48][600/616]	Loss 2.1788e-01 (2.3374e-01)	Acc 0.762695 (0.753117)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755270)
Training Loss of Epoch 48: 0.23373014398706637
Training Acc of Epoch 48: 0.7531932799796748
Testing Acc of Epoch 48: 0.7552695652173913
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3185e-01 (2.3185e-01)	Acc 0.765625 (0.765625)
Epoch: [49][300/616]	Loss 2.4108e-01 (2.3364e-01)	Acc 0.751953 (0.753150)
Epoch: [49][600/616]	Loss 2.4097e-01 (2.3345e-01)	Acc 0.750000 (0.753229)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755652)
Training Loss of Epoch 49: 0.23336926701107646
Training Acc of Epoch 49: 0.7533298399390244
Testing Acc of Epoch 49: 0.7556521739130435
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.3056e-01 (2.3056e-01)	Acc 0.761719 (0.761719)
Epoch: [50][300/616]	Loss 2.2088e-01 (2.3312e-01)	Acc 0.772461 (0.753494)
Epoch: [50][600/616]	Loss 2.2550e-01 (2.3367e-01)	Acc 0.768555 (0.753092)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754183)
Training Loss of Epoch 50: 0.2335655024623483
Training Acc of Epoch 50: 0.7532043953252032
Testing Acc of Epoch 50: 0.7541826086956521
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2251e-01 (2.2251e-01)	Acc 0.765625 (0.765625)
Epoch: [51][300/616]	Loss 2.2853e-01 (2.3331e-01)	Acc 0.759766 (0.753270)
Epoch: [51][600/616]	Loss 2.5145e-01 (2.3319e-01)	Acc 0.714844 (0.753632)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756435)
Training Loss of Epoch 51: 0.23324982051442308
Training Acc of Epoch 51: 0.753541031504065
Testing Acc of Epoch 51: 0.7564347826086957
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.1886e-01 (2.1886e-01)	Acc 0.782227 (0.782227)
Epoch: [52][300/616]	Loss 2.4036e-01 (2.3407e-01)	Acc 0.742188 (0.752693)
Epoch: [52][600/616]	Loss 2.2764e-01 (2.3295e-01)	Acc 0.767578 (0.754122)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750987)
Training Loss of Epoch 52: 0.23297377904740776
Training Acc of Epoch 52: 0.7540682164634146
Testing Acc of Epoch 52: 0.7509869565217391
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.2512e-01 (2.2512e-01)	Acc 0.753906 (0.753906)
Epoch: [53][300/616]	Loss 2.4266e-01 (2.3287e-01)	Acc 0.741211 (0.753751)
Epoch: [53][600/616]	Loss 2.2525e-01 (2.3307e-01)	Acc 0.770508 (0.753638)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751696)
Training Loss of Epoch 53: 0.23301196224321194
Training Acc of Epoch 53: 0.7537681021341464
Testing Acc of Epoch 53: 0.7516956521739131
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3845e-01 (2.3845e-01)	Acc 0.753906 (0.753906)
Epoch: [54][300/616]	Loss 2.4097e-01 (2.3244e-01)	Acc 0.756836 (0.754377)
Epoch: [54][600/616]	Loss 2.4041e-01 (2.3329e-01)	Acc 0.741211 (0.753438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754852)
Training Loss of Epoch 54: 0.23323793789235556
Training Acc of Epoch 54: 0.7534886305894309
Testing Acc of Epoch 54: 0.7548521739130435
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3465e-01 (2.3465e-01)	Acc 0.757812 (0.757812)
Epoch: [55][300/616]	Loss 2.4178e-01 (2.3310e-01)	Acc 0.748047 (0.753144)
Epoch: [55][600/616]	Loss 2.1374e-01 (2.3324e-01)	Acc 0.783203 (0.753352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750961)
Training Loss of Epoch 55: 0.2333739254775086
Training Acc of Epoch 55: 0.7532298018292682
Testing Acc of Epoch 55: 0.7509608695652173
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2720e-01 (2.2720e-01)	Acc 0.762695 (0.762695)
Epoch: [56][300/616]	Loss 2.3348e-01 (2.3323e-01)	Acc 0.760742 (0.753433)
Epoch: [56][600/616]	Loss 2.2209e-01 (2.3350e-01)	Acc 0.771484 (0.753172)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754017)
Training Loss of Epoch 56: 0.233465132965305
Training Acc of Epoch 56: 0.7532504446138212
Testing Acc of Epoch 56: 0.7540173913043479
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3961e-01 (2.3961e-01)	Acc 0.747070 (0.747070)
Epoch: [57][300/616]	Loss 2.5529e-01 (2.3316e-01)	Acc 0.723633 (0.753118)
Epoch: [57][600/616]	Loss 2.3175e-01 (2.3307e-01)	Acc 0.759766 (0.753560)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755243)
Training Loss of Epoch 57: 0.23298808120615114
Training Acc of Epoch 57: 0.7536744156504065
Testing Acc of Epoch 57: 0.7552434782608696
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3226e-01 (2.3226e-01)	Acc 0.762695 (0.762695)
Epoch: [58][300/616]	Loss 2.4005e-01 (2.3336e-01)	Acc 0.743164 (0.753215)
Epoch: [58][600/616]	Loss 2.3631e-01 (2.3336e-01)	Acc 0.747070 (0.753585)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.755070)
Training Loss of Epoch 58: 0.23330806052781702
Training Acc of Epoch 58: 0.7536013719512196
Testing Acc of Epoch 58: 0.7550695652173913
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3742e-01 (2.3742e-01)	Acc 0.755859 (0.755859)
Epoch: [59][300/616]	Loss 2.3878e-01 (2.3330e-01)	Acc 0.744141 (0.753274)
Epoch: [59][600/616]	Loss 2.3735e-01 (2.3312e-01)	Acc 0.751953 (0.753646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754061)
Training Loss of Epoch 59: 0.23314552384663403
Training Acc of Epoch 59: 0.7535918445121951
Testing Acc of Epoch 59: 0.7540608695652173
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.2748e-01 (2.2748e-01)	Acc 0.757812 (0.757812)
Epoch: [60][300/616]	Loss 2.3377e-01 (2.3307e-01)	Acc 0.745117 (0.754373)
Epoch: [60][600/616]	Loss 2.2666e-01 (2.3324e-01)	Acc 0.763672 (0.753666)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.756643)
Training Loss of Epoch 60: 0.23332484725529584
Training Acc of Epoch 60: 0.7535997840447154
Testing Acc of Epoch 60: 0.7566434782608695
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.2639e-01 (2.2639e-01)	Acc 0.758789 (0.758789)
Epoch: [61][300/616]	Loss 2.2787e-01 (2.3313e-01)	Acc 0.768555 (0.753751)
Epoch: [61][600/616]	Loss 2.2407e-01 (2.3345e-01)	Acc 0.762695 (0.753198)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756122)
Training Loss of Epoch 61: 0.23346293813329402
Training Acc of Epoch 61: 0.7532345655487804
Testing Acc of Epoch 61: 0.7561217391304348
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4127e-01 (2.4127e-01)	Acc 0.727539 (0.727539)
Epoch: [62][300/616]	Loss 2.3683e-01 (2.3335e-01)	Acc 0.733398 (0.753283)
Epoch: [62][600/616]	Loss 2.2365e-01 (2.3329e-01)	Acc 0.762695 (0.753614)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752904)
Training Loss of Epoch 62: 0.23321979041022015
Training Acc of Epoch 62: 0.7536839430894309
Testing Acc of Epoch 62: 0.752904347826087
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3452e-01 (2.3452e-01)	Acc 0.747070 (0.747070)
Epoch: [63][300/616]	Loss 2.2224e-01 (2.3327e-01)	Acc 0.771484 (0.753410)
Epoch: [63][600/616]	Loss 2.2592e-01 (2.3319e-01)	Acc 0.762695 (0.753542)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755491)
Training Loss of Epoch 63: 0.23313115088920283
Training Acc of Epoch 63: 0.7536029598577236
Testing Acc of Epoch 63: 0.7554913043478261
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.2721e-01 (2.2721e-01)	Acc 0.769531 (0.769531)
Epoch: [64][300/616]	Loss 2.1885e-01 (2.3271e-01)	Acc 0.778320 (0.754581)
Epoch: [64][600/616]	Loss 2.4376e-01 (2.3320e-01)	Acc 0.734375 (0.753560)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755287)
Training Loss of Epoch 64: 0.2332597757015771
Training Acc of Epoch 64: 0.7534505208333333
Testing Acc of Epoch 64: 0.7552869565217392
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3815e-01 (2.3815e-01)	Acc 0.750977 (0.750977)
Epoch: [65][300/616]	Loss 2.2210e-01 (2.3339e-01)	Acc 0.766602 (0.753699)
Epoch: [65][600/616]	Loss 2.2676e-01 (2.3330e-01)	Acc 0.755859 (0.753455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755600)
Training Loss of Epoch 65: 0.23332588796208545
Training Acc of Epoch 65: 0.7533346036585366
Testing Acc of Epoch 65: 0.7556
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.2898e-01 (2.2898e-01)	Acc 0.762695 (0.762695)
Epoch: [66][300/616]	Loss 2.4225e-01 (2.3280e-01)	Acc 0.743164 (0.753738)
Epoch: [66][600/616]	Loss 2.3855e-01 (2.3310e-01)	Acc 0.750000 (0.753497)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755535)
Training Loss of Epoch 66: 0.23311489190512558
Training Acc of Epoch 66: 0.7534505208333333
Testing Acc of Epoch 66: 0.7555347826086957
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3565e-01 (2.3565e-01)	Acc 0.747070 (0.747070)
Epoch: [67][300/616]	Loss 2.4395e-01 (2.3286e-01)	Acc 0.752930 (0.753939)
Epoch: [67][600/616]	Loss 2.3787e-01 (2.3316e-01)	Acc 0.763672 (0.753687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754100)
Training Loss of Epoch 67: 0.23323635119732802
Training Acc of Epoch 67: 0.7535807291666666
Testing Acc of Epoch 67: 0.7541
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.4001e-01 (2.4001e-01)	Acc 0.748047 (0.748047)
Epoch: [68][300/616]	Loss 2.3929e-01 (2.3308e-01)	Acc 0.740234 (0.753715)
Epoch: [68][600/616]	Loss 2.3707e-01 (2.3332e-01)	Acc 0.746094 (0.753445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752448)
Training Loss of Epoch 68: 0.23336394196603355
Training Acc of Epoch 68: 0.7533758892276423
Testing Acc of Epoch 68: 0.7524478260869565
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.5187e-01 (2.5187e-01)	Acc 0.736328 (0.736328)
Epoch: [69][300/616]	Loss 2.1816e-01 (2.3265e-01)	Acc 0.767578 (0.754455)
Epoch: [69][600/616]	Loss 2.3486e-01 (2.3306e-01)	Acc 0.752930 (0.753661)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752952)
Training Loss of Epoch 69: 0.23307576753744266
Training Acc of Epoch 69: 0.7536553607723577
Testing Acc of Epoch 69: 0.7529521739130435
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3064e-01 (2.3064e-01)	Acc 0.761719 (0.761719)
Epoch: [70][300/616]	Loss 2.2713e-01 (2.3302e-01)	Acc 0.764648 (0.753712)
Epoch: [70][600/616]	Loss 2.2768e-01 (2.3336e-01)	Acc 0.758789 (0.753352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756609)
Training Loss of Epoch 70: 0.23322763486606318
Training Acc of Epoch 70: 0.7534505208333333
Testing Acc of Epoch 70: 0.7566086956521739
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4116e-01 (2.4116e-01)	Acc 0.755859 (0.755859)
Epoch: [71][300/616]	Loss 2.4732e-01 (2.3362e-01)	Acc 0.723633 (0.751807)
Epoch: [71][600/616]	Loss 2.3440e-01 (2.3332e-01)	Acc 0.750000 (0.752957)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754613)
Training Loss of Epoch 71: 0.23338153396195513
Training Acc of Epoch 71: 0.7529074568089431
Testing Acc of Epoch 71: 0.7546130434782609
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.2861e-01 (2.2861e-01)	Acc 0.757812 (0.757812)
Epoch: [72][300/616]	Loss 2.1241e-01 (2.3326e-01)	Acc 0.787109 (0.753488)
Epoch: [72][600/616]	Loss 2.2596e-01 (2.3313e-01)	Acc 0.767578 (0.753875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749226)
Training Loss of Epoch 72: 0.23319872498996858
Training Acc of Epoch 72: 0.7538109756097561
Testing Acc of Epoch 72: 0.7492260869565217
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.3431e-01 (2.3431e-01)	Acc 0.760742 (0.760742)
Epoch: [73][300/616]	Loss 2.4222e-01 (2.3311e-01)	Acc 0.738281 (0.753955)
Epoch: [73][600/616]	Loss 2.3693e-01 (2.3293e-01)	Acc 0.736328 (0.754150)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754970)
Training Loss of Epoch 73: 0.2330220040993962
Training Acc of Epoch 73: 0.7539538871951219
Testing Acc of Epoch 73: 0.7549695652173913
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4521e-01 (2.4521e-01)	Acc 0.746094 (0.746094)
Epoch: [74][300/616]	Loss 2.0501e-01 (2.3382e-01)	Acc 0.780273 (0.753270)
Epoch: [74][600/616]	Loss 2.3070e-01 (2.3342e-01)	Acc 0.755859 (0.753269)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.750609)
Training Loss of Epoch 74: 0.23332477357329393
Training Acc of Epoch 74: 0.7533806529471545
Testing Acc of Epoch 74: 0.7506086956521739
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.1943e-01 (2.1943e-01)	Acc 0.773438 (0.773438)
Epoch: [75][300/616]	Loss 2.2937e-01 (2.2917e-01)	Acc 0.754883 (0.756985)
Epoch: [75][600/616]	Loss 2.2010e-01 (2.2912e-01)	Acc 0.771484 (0.756781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756978)
Training Loss of Epoch 75: 0.22912367936072311
Training Acc of Epoch 75: 0.7567914761178862
Testing Acc of Epoch 75: 0.7569782608695652
Model with the best training loss saved! The loss is 0.22912367936072311
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2903e-01 (2.2903e-01)	Acc 0.760742 (0.760742)
Epoch: [76][300/616]	Loss 2.2340e-01 (2.2917e-01)	Acc 0.772461 (0.756544)
Epoch: [76][600/616]	Loss 2.2368e-01 (2.2870e-01)	Acc 0.771484 (0.757533)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757843)
Training Loss of Epoch 76: 0.22872369633457526
Training Acc of Epoch 76: 0.7574377540650407
Testing Acc of Epoch 76: 0.7578434782608696
Model with the best training loss saved! The loss is 0.22872369633457526
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.2631e-01 (2.2631e-01)	Acc 0.750000 (0.750000)
Epoch: [77][300/616]	Loss 2.2596e-01 (2.2811e-01)	Acc 0.752930 (0.758186)
Epoch: [77][600/616]	Loss 2.2736e-01 (2.2875e-01)	Acc 0.750000 (0.757215)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758204)
Training Loss of Epoch 77: 0.22870148611747151
Training Acc of Epoch 77: 0.7573266006097561
Testing Acc of Epoch 77: 0.7582043478260869
Model with the best training loss saved! The loss is 0.22870148611747151
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.2131e-01 (2.2131e-01)	Acc 0.767578 (0.767578)
Epoch: [78][300/616]	Loss 2.3250e-01 (2.2898e-01)	Acc 0.749023 (0.757008)
Epoch: [78][600/616]	Loss 2.2280e-01 (2.2861e-01)	Acc 0.760742 (0.757673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758113)
Training Loss of Epoch 78: 0.22860220040732282
Training Acc of Epoch 78: 0.7576822916666667
Testing Acc of Epoch 78: 0.7581130434782609
Model with the best training loss saved! The loss is 0.22860220040732282
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.1401e-01 (2.1401e-01)	Acc 0.769531 (0.769531)
Epoch: [79][300/616]	Loss 2.2529e-01 (2.2879e-01)	Acc 0.753906 (0.757209)
Epoch: [79][600/616]	Loss 2.2973e-01 (2.2859e-01)	Acc 0.759766 (0.757502)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.758174)
Training Loss of Epoch 79: 0.22855723953343987
Training Acc of Epoch 79: 0.7575774898373984
Testing Acc of Epoch 79: 0.7581739130434783
Model with the best training loss saved! The loss is 0.22855723953343987
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4171e-01 (2.4171e-01)	Acc 0.738281 (0.738281)
Epoch: [80][300/616]	Loss 2.2722e-01 (2.2757e-01)	Acc 0.762695 (0.758779)
Epoch: [80][600/616]	Loss 2.3135e-01 (2.2853e-01)	Acc 0.748047 (0.757600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.758604)
Training Loss of Epoch 80: 0.22850970797907047
Training Acc of Epoch 80: 0.7576822916666667
Testing Acc of Epoch 80: 0.758604347826087
Model with the best training loss saved! The loss is 0.22850970797907047
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2101e-01 (2.2101e-01)	Acc 0.775391 (0.775391)
Epoch: [81][300/616]	Loss 2.2639e-01 (2.2837e-01)	Acc 0.754883 (0.757670)
Epoch: [81][600/616]	Loss 2.3050e-01 (2.2850e-01)	Acc 0.763672 (0.757772)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758978)
Training Loss of Epoch 81: 0.22855256267679416
Training Acc of Epoch 81: 0.7576743521341464
Testing Acc of Epoch 81: 0.7589782608695652
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2855e-01 (2.2855e-01)	Acc 0.748047 (0.748047)
Epoch: [82][300/616]	Loss 2.3406e-01 (2.2874e-01)	Acc 0.748047 (0.757793)
Epoch: [82][600/616]	Loss 2.1886e-01 (2.2848e-01)	Acc 0.775391 (0.757947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757657)
Training Loss of Epoch 82: 0.22846326968534206
Training Acc of Epoch 82: 0.7579871697154471
Testing Acc of Epoch 82: 0.7576565217391305
Model with the best training loss saved! The loss is 0.22846326968534206
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3665e-01 (2.3665e-01)	Acc 0.754883 (0.754883)
Epoch: [83][300/616]	Loss 2.2499e-01 (2.2861e-01)	Acc 0.767578 (0.757595)
Epoch: [83][600/616]	Loss 2.2379e-01 (2.2852e-01)	Acc 0.765625 (0.757795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758843)
Training Loss of Epoch 83: 0.2284536472665585
Training Acc of Epoch 83: 0.7578426702235772
Testing Acc of Epoch 83: 0.7588434782608696
Model with the best training loss saved! The loss is 0.2284536472665585
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3418e-01 (2.3418e-01)	Acc 0.751953 (0.751953)
Epoch: [84][300/616]	Loss 2.3523e-01 (2.2837e-01)	Acc 0.743164 (0.757903)
Epoch: [84][600/616]	Loss 2.2765e-01 (2.2855e-01)	Acc 0.757812 (0.757655)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758278)
Training Loss of Epoch 84: 0.22846520921079125
Training Acc of Epoch 84: 0.7577823297764228
Testing Acc of Epoch 84: 0.7582782608695652
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2935e-01 (2.2935e-01)	Acc 0.756836 (0.756836)
Epoch: [85][300/616]	Loss 2.1832e-01 (2.2876e-01)	Acc 0.773438 (0.757728)
Epoch: [85][600/616]	Loss 2.2704e-01 (2.2831e-01)	Acc 0.753906 (0.757863)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758770)
Training Loss of Epoch 85: 0.22842148821043776
Training Acc of Epoch 85: 0.7577521595528456
Testing Acc of Epoch 85: 0.7587695652173913
Model with the best training loss saved! The loss is 0.22842148821043776
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.5038e-01 (2.5038e-01)	Acc 0.729492 (0.729492)
Epoch: [86][300/616]	Loss 2.4197e-01 (2.2847e-01)	Acc 0.755859 (0.757851)
Epoch: [86][600/616]	Loss 2.2894e-01 (2.2844e-01)	Acc 0.750000 (0.757923)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758457)
Training Loss of Epoch 86: 0.2284602566947782
Training Acc of Epoch 86: 0.7578728404471544
Testing Acc of Epoch 86: 0.7584565217391305
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3012e-01 (2.3012e-01)	Acc 0.761719 (0.761719)
Epoch: [87][300/616]	Loss 2.2236e-01 (2.2808e-01)	Acc 0.760742 (0.757754)
Epoch: [87][600/616]	Loss 2.3198e-01 (2.2840e-01)	Acc 0.757812 (0.757949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757804)
Training Loss of Epoch 87: 0.22838776654344264
Training Acc of Epoch 87: 0.757934768800813
Testing Acc of Epoch 87: 0.757804347826087
Model with the best training loss saved! The loss is 0.22838776654344264
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4501e-01 (2.4501e-01)	Acc 0.735352 (0.735352)
Epoch: [88][300/616]	Loss 2.2321e-01 (2.2808e-01)	Acc 0.772461 (0.758380)
Epoch: [88][600/616]	Loss 2.3733e-01 (2.2833e-01)	Acc 0.747070 (0.757869)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758013)
Training Loss of Epoch 88: 0.22833766249137197
Training Acc of Epoch 88: 0.757883955792683
Testing Acc of Epoch 88: 0.7580130434782608
Model with the best training loss saved! The loss is 0.22833766249137197
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.3695e-01 (2.3695e-01)	Acc 0.759766 (0.759766)
Epoch: [89][300/616]	Loss 2.3054e-01 (2.2838e-01)	Acc 0.750977 (0.758108)
Epoch: [89][600/616]	Loss 2.2616e-01 (2.2834e-01)	Acc 0.765625 (0.758100)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757891)
Training Loss of Epoch 89: 0.22838518571078292
Training Acc of Epoch 89: 0.7580395706300813
Testing Acc of Epoch 89: 0.757891304347826
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.1321e-01 (2.1321e-01)	Acc 0.776367 (0.776367)
Epoch: [90][300/616]	Loss 2.4045e-01 (2.2834e-01)	Acc 0.757812 (0.758010)
Epoch: [90][600/616]	Loss 2.2957e-01 (2.2830e-01)	Acc 0.757812 (0.758129)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758574)
Training Loss of Epoch 90: 0.22832736188803263
Training Acc of Epoch 90: 0.7580649771341463
Testing Acc of Epoch 90: 0.7585739130434782
Model with the best training loss saved! The loss is 0.22832736188803263
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.4159e-01 (2.4159e-01)	Acc 0.753906 (0.753906)
Epoch: [91][300/616]	Loss 2.2914e-01 (2.2773e-01)	Acc 0.750000 (0.758604)
Epoch: [91][600/616]	Loss 2.1246e-01 (2.2840e-01)	Acc 0.768555 (0.757926)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758243)
Training Loss of Epoch 91: 0.2283514015315994
Training Acc of Epoch 91: 0.7579919334349593
Testing Acc of Epoch 91: 0.7582434782608696
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2940e-01 (2.2940e-01)	Acc 0.747070 (0.747070)
Epoch: [92][300/616]	Loss 2.3168e-01 (2.2834e-01)	Acc 0.751953 (0.757907)
Epoch: [92][600/616]	Loss 2.1360e-01 (2.2826e-01)	Acc 0.785156 (0.758144)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758152)
Training Loss of Epoch 92: 0.22827533912367937
Training Acc of Epoch 92: 0.758128493394309
Testing Acc of Epoch 92: 0.7581521739130435
Model with the best training loss saved! The loss is 0.22827533912367937
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3911e-01 (2.3911e-01)	Acc 0.746094 (0.746094)
Epoch: [93][300/616]	Loss 2.3139e-01 (2.2830e-01)	Acc 0.757812 (0.757910)
Epoch: [93][600/616]	Loss 2.4001e-01 (2.2831e-01)	Acc 0.740234 (0.758037)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758317)
Training Loss of Epoch 93: 0.22831236826695078
Training Acc of Epoch 93: 0.758055449695122
Testing Acc of Epoch 93: 0.7583173913043478
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.1678e-01 (2.1678e-01)	Acc 0.771484 (0.771484)
Epoch: [94][300/616]	Loss 2.2693e-01 (2.2865e-01)	Acc 0.765625 (0.757413)
Epoch: [94][600/616]	Loss 2.2037e-01 (2.2840e-01)	Acc 0.756836 (0.757916)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758378)
Training Loss of Epoch 94: 0.22841320306789584
Training Acc of Epoch 94: 0.7578601371951219
Testing Acc of Epoch 94: 0.7583782608695652
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3291e-01 (2.3291e-01)	Acc 0.761719 (0.761719)
Epoch: [95][300/616]	Loss 2.2903e-01 (2.2819e-01)	Acc 0.759766 (0.758182)
Epoch: [95][600/616]	Loss 2.2029e-01 (2.2838e-01)	Acc 0.765625 (0.757858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758696)
Training Loss of Epoch 95: 0.228356943963989
Training Acc of Epoch 95: 0.7579300050813008
Testing Acc of Epoch 95: 0.758695652173913
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2897e-01 (2.2897e-01)	Acc 0.753906 (0.753906)
Epoch: [96][300/616]	Loss 2.3768e-01 (2.2765e-01)	Acc 0.734375 (0.758685)
Epoch: [96][600/616]	Loss 2.3037e-01 (2.2823e-01)	Acc 0.752930 (0.758167)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757996)
Training Loss of Epoch 96: 0.22832467984862445
Training Acc of Epoch 96: 0.7580490980691057
Testing Acc of Epoch 96: 0.7579956521739131
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.1415e-01 (2.1415e-01)	Acc 0.780273 (0.780273)
Epoch: [97][300/616]	Loss 2.3113e-01 (2.2854e-01)	Acc 0.755859 (0.757410)
Epoch: [97][600/616]	Loss 2.2065e-01 (2.2836e-01)	Acc 0.750977 (0.757951)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758822)
Training Loss of Epoch 97: 0.22832023901183432
Training Acc of Epoch 97: 0.7579744664634146
Testing Acc of Epoch 97: 0.7588217391304348
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4235e-01 (2.4235e-01)	Acc 0.738281 (0.738281)
Epoch: [98][300/616]	Loss 2.2254e-01 (2.2844e-01)	Acc 0.771484 (0.757965)
Epoch: [98][600/616]	Loss 2.2445e-01 (2.2833e-01)	Acc 0.763672 (0.757934)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757843)
Training Loss of Epoch 98: 0.22820690501996171
Training Acc of Epoch 98: 0.7580792682926829
Testing Acc of Epoch 98: 0.7578434782608696
Model with the best training loss saved! The loss is 0.22820690501996171
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3683e-01 (2.3683e-01)	Acc 0.739258 (0.739258)
Epoch: [99][300/616]	Loss 2.4044e-01 (2.2780e-01)	Acc 0.733398 (0.758040)
Epoch: [99][600/616]	Loss 2.4537e-01 (2.2816e-01)	Acc 0.746094 (0.758033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759187)
Training Loss of Epoch 99: 0.2282772198682878
Training Acc of Epoch 99: 0.7579077743902439
Testing Acc of Epoch 99: 0.7591869565217392
Early stopping not satisfied.
