train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.0125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0125/lr_decay/JT_6b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0139e-01 (5.0139e-01)	Acc 0.143555 (0.143555)
Epoch: [0][300/616]	Loss 2.2524e-01 (2.7960e-01)	Acc 0.765625 (0.702726)
Epoch: [0][600/616]	Loss 2.5045e-01 (2.6256e-01)	Acc 0.745117 (0.722531)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745943)
Training Loss of Epoch 0: 0.262182786576147
Training Acc of Epoch 0: 0.7230119410569106
Testing Acc of Epoch 0: 0.7459434782608696
Model with the best training loss saved! The loss is 0.262182786576147
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4410e-01 (2.4410e-01)	Acc 0.742188 (0.742188)
Epoch: [1][300/616]	Loss 2.3370e-01 (2.4180e-01)	Acc 0.764648 (0.746470)
Epoch: [1][600/616]	Loss 2.4257e-01 (2.4153e-01)	Acc 0.744141 (0.746370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.749413)
Training Loss of Epoch 1: 0.2415454021798886
Training Acc of Epoch 1: 0.7463287601626016
Testing Acc of Epoch 1: 0.7494130434782609
Model with the best training loss saved! The loss is 0.2415454021798886
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4025e-01 (2.4025e-01)	Acc 0.741211 (0.741211)
Epoch: [2][300/616]	Loss 2.3120e-01 (2.4097e-01)	Acc 0.762695 (0.746746)
Epoch: [2][600/616]	Loss 2.3310e-01 (2.3999e-01)	Acc 0.758789 (0.747636)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752078)
Training Loss of Epoch 2: 0.2398723884568951
Training Acc of Epoch 2: 0.747751524390244
Testing Acc of Epoch 2: 0.7520782608695652
Model with the best training loss saved! The loss is 0.2398723884568951
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3677e-01 (2.3677e-01)	Acc 0.755859 (0.755859)
Epoch: [3][300/616]	Loss 2.4463e-01 (2.3898e-01)	Acc 0.750000 (0.748693)
Epoch: [3][600/616]	Loss 2.4253e-01 (2.3880e-01)	Acc 0.739258 (0.748645)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750352)
Training Loss of Epoch 3: 0.2388942842561055
Training Acc of Epoch 3: 0.7485518292682927
Testing Acc of Epoch 3: 0.7503521739130434
Model with the best training loss saved! The loss is 0.2388942842561055
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3708e-01 (2.3708e-01)	Acc 0.744141 (0.744141)
Epoch: [4][300/616]	Loss 2.4665e-01 (2.3911e-01)	Acc 0.755859 (0.748761)
Epoch: [4][600/616]	Loss 2.4854e-01 (2.3941e-01)	Acc 0.740234 (0.748458)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750109)
Training Loss of Epoch 4: 0.2394181794509655
Training Acc of Epoch 4: 0.7484486153455284
Testing Acc of Epoch 4: 0.7501086956521739
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3385e-01 (2.3385e-01)	Acc 0.753906 (0.753906)
Epoch: [5][300/616]	Loss 2.2232e-01 (2.3994e-01)	Acc 0.777344 (0.747894)
Epoch: [5][600/616]	Loss 2.2802e-01 (2.3976e-01)	Acc 0.752930 (0.748024)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750965)
Training Loss of Epoch 5: 0.23974359159062547
Training Acc of Epoch 5: 0.7480659298780488
Testing Acc of Epoch 5: 0.7509652173913044
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.2262e-01 (2.2262e-01)	Acc 0.769531 (0.769531)
Epoch: [6][300/616]	Loss 2.3980e-01 (2.4076e-01)	Acc 0.757812 (0.746477)
Epoch: [6][600/616]	Loss 2.3827e-01 (2.4094e-01)	Acc 0.756836 (0.747116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748257)
Training Loss of Epoch 6: 0.24082313163009117
Training Acc of Epoch 6: 0.7471909933943089
Testing Acc of Epoch 6: 0.7482565217391305
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2998e-01 (2.2998e-01)	Acc 0.757812 (0.757812)
Epoch: [7][300/616]	Loss 2.3603e-01 (2.3986e-01)	Acc 0.756836 (0.747570)
Epoch: [7][600/616]	Loss 2.5304e-01 (2.4021e-01)	Acc 0.731445 (0.747369)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748574)
Training Loss of Epoch 7: 0.24014247992659002
Training Acc of Epoch 7: 0.7474339430894309
Testing Acc of Epoch 7: 0.7485739130434783
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5464e-01 (2.5464e-01)	Acc 0.746094 (0.746094)
Epoch: [8][300/616]	Loss 2.4080e-01 (2.4052e-01)	Acc 0.737305 (0.747051)
Epoch: [8][600/616]	Loss 2.3930e-01 (2.4081e-01)	Acc 0.763672 (0.746965)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752126)
Training Loss of Epoch 8: 0.24088763829653825
Training Acc of Epoch 8: 0.7469051702235773
Testing Acc of Epoch 8: 0.7521260869565217
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4607e-01 (2.4607e-01)	Acc 0.750977 (0.750977)
Epoch: [9][300/616]	Loss 2.3961e-01 (2.4077e-01)	Acc 0.749023 (0.746973)
Epoch: [9][600/616]	Loss 2.5870e-01 (2.4099e-01)	Acc 0.732422 (0.746981)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748535)
Training Loss of Epoch 9: 0.24105451383241794
Training Acc of Epoch 9: 0.7468321265243902
Testing Acc of Epoch 9: 0.7485347826086957
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.4833e-01 (2.4833e-01)	Acc 0.730469 (0.730469)
Epoch: [10][300/616]	Loss 2.3308e-01 (2.4148e-01)	Acc 0.766602 (0.745004)
Epoch: [10][600/616]	Loss 2.4840e-01 (2.4109e-01)	Acc 0.748047 (0.746029)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.748787)
Training Loss of Epoch 10: 0.24102537505994967
Training Acc of Epoch 10: 0.7461604420731708
Testing Acc of Epoch 10: 0.7487869565217391
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3735e-01 (2.3735e-01)	Acc 0.756836 (0.756836)
Epoch: [11][300/616]	Loss 2.1739e-01 (2.4372e-01)	Acc 0.773438 (0.744374)
Epoch: [11][600/616]	Loss 2.3395e-01 (2.4286e-01)	Acc 0.758789 (0.744620)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747170)
Training Loss of Epoch 11: 0.2428055901110657
Training Acc of Epoch 11: 0.7447202108739838
Testing Acc of Epoch 11: 0.7471695652173913
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4309e-01 (2.4309e-01)	Acc 0.751953 (0.751953)
Epoch: [12][300/616]	Loss 2.7055e-01 (2.4317e-01)	Acc 0.703125 (0.744085)
Epoch: [12][600/616]	Loss 2.2826e-01 (2.4286e-01)	Acc 0.768555 (0.744830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745161)
Training Loss of Epoch 12: 0.2427982081000398
Training Acc of Epoch 12: 0.7449075838414634
Testing Acc of Epoch 12: 0.7451608695652174
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.2265e-01 (2.2265e-01)	Acc 0.769531 (0.769531)
Epoch: [13][300/616]	Loss 2.2880e-01 (2.4169e-01)	Acc 0.755859 (0.745737)
Epoch: [13][600/616]	Loss 2.4206e-01 (2.4258e-01)	Acc 0.739258 (0.744864)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.741083)
Training Loss of Epoch 13: 0.24261810699614084
Training Acc of Epoch 13: 0.7449377540650407
Testing Acc of Epoch 13: 0.7410826086956521
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3939e-01 (2.3939e-01)	Acc 0.746094 (0.746094)
Epoch: [14][300/616]	Loss 2.3133e-01 (2.4520e-01)	Acc 0.766602 (0.743151)
Epoch: [14][600/616]	Loss 2.7050e-01 (2.4467e-01)	Acc 0.704102 (0.743029)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743587)
Training Loss of Epoch 14: 0.24492736519352207
Training Acc of Epoch 14: 0.7426892784552845
Testing Acc of Epoch 14: 0.7435869565217391
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4643e-01 (2.4643e-01)	Acc 0.737305 (0.737305)
Epoch: [15][300/616]	Loss 2.4950e-01 (2.4393e-01)	Acc 0.737305 (0.743381)
Epoch: [15][600/616]	Loss 2.4240e-01 (2.4329e-01)	Acc 0.735352 (0.744059)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751243)
Training Loss of Epoch 15: 0.24319847092880467
Training Acc of Epoch 15: 0.744115218495935
Testing Acc of Epoch 15: 0.7512434782608696
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4310e-01 (2.4310e-01)	Acc 0.737305 (0.737305)
Epoch: [16][300/616]	Loss 2.3907e-01 (2.4267e-01)	Acc 0.739258 (0.744400)
Epoch: [16][600/616]	Loss 2.3599e-01 (2.4243e-01)	Acc 0.750977 (0.745013)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747061)
Training Loss of Epoch 16: 0.2423228758621991
Training Acc of Epoch 16: 0.7451918191056911
Testing Acc of Epoch 16: 0.7470608695652174
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3234e-01 (2.3234e-01)	Acc 0.759766 (0.759766)
Epoch: [17][300/616]	Loss 2.4754e-01 (2.4306e-01)	Acc 0.737305 (0.745396)
Epoch: [17][600/616]	Loss 2.4801e-01 (2.4320e-01)	Acc 0.738281 (0.744324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749343)
Training Loss of Epoch 17: 0.24309648774503692
Training Acc of Epoch 17: 0.7444852007113821
Testing Acc of Epoch 17: 0.7493434782608696
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3692e-01 (2.3692e-01)	Acc 0.753906 (0.753906)
Epoch: [18][300/616]	Loss 2.3594e-01 (2.4347e-01)	Acc 0.758789 (0.744176)
Epoch: [18][600/616]	Loss 2.5826e-01 (2.4325e-01)	Acc 0.734375 (0.744469)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.745917)
Training Loss of Epoch 18: 0.2431867841298018
Training Acc of Epoch 18: 0.7445217225609756
Testing Acc of Epoch 18: 0.7459173913043479
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4084e-01 (2.4084e-01)	Acc 0.750977 (0.750977)
Epoch: [19][300/616]	Loss 2.4626e-01 (2.4460e-01)	Acc 0.749023 (0.742671)
Epoch: [19][600/616]	Loss 2.4571e-01 (2.4478e-01)	Acc 0.732422 (0.742930)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.746935)
Training Loss of Epoch 19: 0.24479045344562064
Training Acc of Epoch 19: 0.7429623983739837
Testing Acc of Epoch 19: 0.7469347826086956
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4245e-01 (2.4245e-01)	Acc 0.752930 (0.752930)
Epoch: [20][300/616]	Loss 2.2358e-01 (2.4377e-01)	Acc 0.763672 (0.743411)
Epoch: [20][600/616]	Loss 2.7283e-01 (2.4365e-01)	Acc 0.708984 (0.743489)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748652)
Training Loss of Epoch 20: 0.2437243332707785
Training Acc of Epoch 20: 0.7434244791666667
Testing Acc of Epoch 20: 0.7486521739130435
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4696e-01 (2.4696e-01)	Acc 0.737305 (0.737305)
Epoch: [21][300/616]	Loss 2.4650e-01 (2.4275e-01)	Acc 0.746094 (0.745364)
Epoch: [21][600/616]	Loss 2.3931e-01 (2.4322e-01)	Acc 0.753906 (0.744596)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.738065)
Training Loss of Epoch 21: 0.24329969153656222
Training Acc of Epoch 21: 0.7444788490853659
Testing Acc of Epoch 21: 0.7380652173913044
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.6139e-01 (2.6139e-01)	Acc 0.728516 (0.728516)
Epoch: [22][300/616]	Loss 2.3343e-01 (2.4099e-01)	Acc 0.763672 (0.746097)
Epoch: [22][600/616]	Loss 2.2709e-01 (2.4094e-01)	Acc 0.759766 (0.746279)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.739861)
Training Loss of Epoch 22: 0.240930323242172
Training Acc of Epoch 22: 0.7463001778455285
Testing Acc of Epoch 22: 0.7398608695652173
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.4651e-01 (2.4651e-01)	Acc 0.722656 (0.722656)
Epoch: [23][300/616]	Loss 2.4126e-01 (2.4128e-01)	Acc 0.751953 (0.746103)
Epoch: [23][600/616]	Loss 2.3685e-01 (2.4119e-01)	Acc 0.752930 (0.746337)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.738152)
Training Loss of Epoch 23: 0.24115280370886732
Training Acc of Epoch 23: 0.7463557545731707
Testing Acc of Epoch 23: 0.7381521739130434
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.6431e-01 (2.6431e-01)	Acc 0.723633 (0.723633)
Epoch: [24][300/616]	Loss 2.3795e-01 (2.4122e-01)	Acc 0.762695 (0.746746)
Epoch: [24][600/616]	Loss 2.3676e-01 (2.4150e-01)	Acc 0.747070 (0.746217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744926)
Training Loss of Epoch 24: 0.24155630869109457
Training Acc of Epoch 24: 0.7461604420731708
Testing Acc of Epoch 24: 0.7449260869565217
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3802e-01 (2.3802e-01)	Acc 0.748047 (0.748047)
Epoch: [25][300/616]	Loss 2.3939e-01 (2.4257e-01)	Acc 0.758789 (0.744176)
Epoch: [25][600/616]	Loss 2.3037e-01 (2.4174e-01)	Acc 0.771484 (0.745390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.748935)
Training Loss of Epoch 25: 0.2416392770966863
Training Acc of Epoch 25: 0.7455300431910569
Testing Acc of Epoch 25: 0.7489347826086956
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3433e-01 (2.3433e-01)	Acc 0.749023 (0.749023)
Epoch: [26][300/616]	Loss 2.5153e-01 (2.4339e-01)	Acc 0.728516 (0.743784)
Epoch: [26][600/616]	Loss 2.2907e-01 (2.4307e-01)	Acc 0.762695 (0.744214)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.743935)
Training Loss of Epoch 26: 0.2430223159431442
Training Acc of Epoch 26: 0.7442200203252033
Testing Acc of Epoch 26: 0.7439347826086956
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.2796e-01 (2.2796e-01)	Acc 0.762695 (0.762695)
Epoch: [27][300/616]	Loss 2.4219e-01 (2.4397e-01)	Acc 0.745117 (0.744251)
Epoch: [27][600/616]	Loss 2.2734e-01 (2.4363e-01)	Acc 0.772461 (0.744472)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749639)
Training Loss of Epoch 27: 0.2436350799672972
Training Acc of Epoch 27: 0.7444661458333334
Testing Acc of Epoch 27: 0.7496391304347826
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.1170e-01 (2.1170e-01)	Acc 0.788086 (0.788086)
Epoch: [28][300/616]	Loss 2.4465e-01 (2.4352e-01)	Acc 0.731445 (0.744212)
Epoch: [28][600/616]	Loss 2.5708e-01 (2.4371e-01)	Acc 0.724609 (0.743832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747283)
Training Loss of Epoch 28: 0.24388385591952783
Training Acc of Epoch 28: 0.743676956300813
Testing Acc of Epoch 28: 0.7472826086956522
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4681e-01 (2.4681e-01)	Acc 0.743164 (0.743164)
Epoch: [29][300/616]	Loss 2.4498e-01 (2.4285e-01)	Acc 0.751953 (0.744692)
Epoch: [29][600/616]	Loss 2.6629e-01 (2.4266e-01)	Acc 0.728516 (0.745099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.745617)
Training Loss of Epoch 29: 0.24275676579010197
Training Acc of Epoch 29: 0.7449314024390243
Testing Acc of Epoch 29: 0.7456173913043478
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4540e-01 (2.4540e-01)	Acc 0.751953 (0.751953)
Epoch: [30][300/616]	Loss 2.4431e-01 (2.4418e-01)	Acc 0.730469 (0.743424)
Epoch: [30][600/616]	Loss 2.5855e-01 (2.4425e-01)	Acc 0.717773 (0.743232)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.741091)
Training Loss of Epoch 30: 0.24421888197340616
Training Acc of Epoch 30: 0.7432291666666667
Testing Acc of Epoch 30: 0.7410913043478261
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.5100e-01 (2.5100e-01)	Acc 0.738281 (0.738281)
Epoch: [31][300/616]	Loss 2.6925e-01 (2.4385e-01)	Acc 0.707031 (0.743320)
Epoch: [31][600/616]	Loss 2.4286e-01 (2.4363e-01)	Acc 0.745117 (0.744102)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.746474)
Training Loss of Epoch 31: 0.2437315668517012
Training Acc of Epoch 31: 0.7439834222560976
Testing Acc of Epoch 31: 0.7464739130434782
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4547e-01 (2.4547e-01)	Acc 0.732422 (0.732422)
Epoch: [32][300/616]	Loss 2.3226e-01 (2.4323e-01)	Acc 0.756836 (0.744390)
Epoch: [32][600/616]	Loss 2.4713e-01 (2.4299e-01)	Acc 0.744141 (0.744740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.748757)
Training Loss of Epoch 32: 0.2428836274437788
Training Acc of Epoch 32: 0.744823424796748
Testing Acc of Epoch 32: 0.7487565217391304
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2764e-01 (2.2764e-01)	Acc 0.762695 (0.762695)
Epoch: [33][300/616]	Loss 2.5369e-01 (2.4334e-01)	Acc 0.736328 (0.743414)
Epoch: [33][600/616]	Loss 2.4068e-01 (2.4320e-01)	Acc 0.741211 (0.743959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749630)
Training Loss of Epoch 33: 0.24320204175584684
Training Acc of Epoch 33: 0.7439611915650407
Testing Acc of Epoch 33: 0.7496304347826087
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4010e-01 (2.4010e-01)	Acc 0.747070 (0.747070)
Epoch: [34][300/616]	Loss 2.4487e-01 (2.4449e-01)	Acc 0.730469 (0.742742)
Epoch: [34][600/616]	Loss 2.3965e-01 (2.4356e-01)	Acc 0.743164 (0.743700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747248)
Training Loss of Epoch 34: 0.24347136926844837
Training Acc of Epoch 34: 0.7437785823170732
Testing Acc of Epoch 34: 0.7472478260869565
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4011e-01 (2.4011e-01)	Acc 0.744141 (0.744141)
Epoch: [35][300/616]	Loss 2.5348e-01 (2.4367e-01)	Acc 0.727539 (0.744001)
Epoch: [35][600/616]	Loss 2.5865e-01 (2.4373e-01)	Acc 0.711914 (0.743673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745657)
Training Loss of Epoch 35: 0.24364757608107435
Training Acc of Epoch 35: 0.7437198297764228
Testing Acc of Epoch 35: 0.7456565217391304
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.5620e-01 (2.5620e-01)	Acc 0.734375 (0.734375)
Epoch: [36][300/616]	Loss 2.5129e-01 (2.4267e-01)	Acc 0.732422 (0.745432)
Epoch: [36][600/616]	Loss 2.3886e-01 (2.4275e-01)	Acc 0.748047 (0.744807)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749657)
Training Loss of Epoch 36: 0.24276844757359203
Training Acc of Epoch 36: 0.7447487931910569
Testing Acc of Epoch 36: 0.7496565217391304
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4617e-01 (2.4617e-01)	Acc 0.734375 (0.734375)
Epoch: [37][300/616]	Loss 2.4665e-01 (2.4265e-01)	Acc 0.745117 (0.744890)
Epoch: [37][600/616]	Loss 2.2474e-01 (2.4260e-01)	Acc 0.767578 (0.745091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.743861)
Training Loss of Epoch 37: 0.24251647453482558
Training Acc of Epoch 37: 0.7452124618902439
Testing Acc of Epoch 37: 0.7438608695652174
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3626e-01 (2.3626e-01)	Acc 0.756836 (0.756836)
Epoch: [38][300/616]	Loss 2.4408e-01 (2.4243e-01)	Acc 0.738281 (0.744549)
Epoch: [38][600/616]	Loss 2.4075e-01 (2.4302e-01)	Acc 0.752930 (0.743799)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750704)
Training Loss of Epoch 38: 0.24287774126704145
Training Acc of Epoch 38: 0.7440485264227642
Testing Acc of Epoch 38: 0.750704347826087
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.3457e-01 (2.3457e-01)	Acc 0.747070 (0.747070)
Epoch: [39][300/616]	Loss 2.4942e-01 (2.4536e-01)	Acc 0.733398 (0.741675)
Epoch: [39][600/616]	Loss 2.5716e-01 (2.4416e-01)	Acc 0.716797 (0.743383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.732387)
Training Loss of Epoch 39: 0.2440859241214225
Training Acc of Epoch 39: 0.7435070503048781
Testing Acc of Epoch 39: 0.7323869565217391
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.5648e-01 (2.5648e-01)	Acc 0.728516 (0.728516)
Epoch: [40][300/616]	Loss 2.5389e-01 (2.4453e-01)	Acc 0.740234 (0.743034)
Epoch: [40][600/616]	Loss 2.3983e-01 (2.4413e-01)	Acc 0.754883 (0.743193)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746817)
Training Loss of Epoch 40: 0.2442917092059686
Training Acc of Epoch 40: 0.7428734756097561
Testing Acc of Epoch 40: 0.7468173913043479
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4199e-01 (2.4199e-01)	Acc 0.742188 (0.742188)
Epoch: [41][300/616]	Loss 2.3132e-01 (2.4367e-01)	Acc 0.751953 (0.743917)
Epoch: [41][600/616]	Loss 2.4514e-01 (2.4486e-01)	Acc 0.742188 (0.742480)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.746965)
Training Loss of Epoch 41: 0.24466266644194842
Training Acc of Epoch 41: 0.7428051956300813
Testing Acc of Epoch 41: 0.7469652173913044
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3368e-01 (2.3368e-01)	Acc 0.752930 (0.752930)
Epoch: [42][300/616]	Loss 2.3142e-01 (2.4315e-01)	Acc 0.750000 (0.743667)
Epoch: [42][600/616]	Loss 2.2601e-01 (2.4383e-01)	Acc 0.772461 (0.743234)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.748061)
Training Loss of Epoch 42: 0.24385952670884325
Training Acc of Epoch 42: 0.7431735899390244
Testing Acc of Epoch 42: 0.7480608695652174
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3901e-01 (2.3901e-01)	Acc 0.760742 (0.760742)
Epoch: [43][300/616]	Loss 2.4542e-01 (2.4476e-01)	Acc 0.736328 (0.742071)
Epoch: [43][600/616]	Loss 2.5198e-01 (2.4464e-01)	Acc 0.738281 (0.742563)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.747826)
Training Loss of Epoch 43: 0.24465646927918844
Training Acc of Epoch 43: 0.7425574822154472
Testing Acc of Epoch 43: 0.7478260869565218
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4409e-01 (2.4409e-01)	Acc 0.748047 (0.748047)
Epoch: [44][300/616]	Loss 2.7196e-01 (2.4319e-01)	Acc 0.710938 (0.744475)
Epoch: [44][600/616]	Loss 2.3230e-01 (2.4351e-01)	Acc 0.751953 (0.743977)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.744813)
Training Loss of Epoch 44: 0.24340021658719072
Training Acc of Epoch 44: 0.7441723831300813
Testing Acc of Epoch 44: 0.7448130434782608
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.6138e-01 (2.6138e-01)	Acc 0.720703 (0.720703)
Epoch: [45][300/616]	Loss 2.4080e-01 (2.4322e-01)	Acc 0.742188 (0.744144)
Epoch: [45][600/616]	Loss 2.3783e-01 (2.4383e-01)	Acc 0.744141 (0.743494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746330)
Training Loss of Epoch 45: 0.24374764513194078
Training Acc of Epoch 45: 0.7436245553861789
Testing Acc of Epoch 45: 0.7463304347826087
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.3544e-01 (2.3544e-01)	Acc 0.750000 (0.750000)
Epoch: [46][300/616]	Loss 2.4085e-01 (2.4357e-01)	Acc 0.755859 (0.743722)
Epoch: [46][600/616]	Loss 2.4617e-01 (2.4377e-01)	Acc 0.728516 (0.743772)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.747930)
Training Loss of Epoch 46: 0.24366457784563544
Training Acc of Epoch 46: 0.7438881478658537
Testing Acc of Epoch 46: 0.7479304347826087
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4695e-01 (2.4695e-01)	Acc 0.756836 (0.756836)
Epoch: [47][300/616]	Loss 2.4882e-01 (2.4363e-01)	Acc 0.730469 (0.743508)
Epoch: [47][600/616]	Loss 2.4775e-01 (2.4318e-01)	Acc 0.734375 (0.744113)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.747096)
Training Loss of Epoch 47: 0.24305839066098375
Training Acc of Epoch 47: 0.7442597179878049
Testing Acc of Epoch 47: 0.747095652173913
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4477e-01 (2.4477e-01)	Acc 0.750000 (0.750000)
Epoch: [48][300/616]	Loss 2.3136e-01 (2.4382e-01)	Acc 0.757812 (0.744098)
Epoch: [48][600/616]	Loss 2.5931e-01 (2.4382e-01)	Acc 0.722656 (0.743925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.732739)
Training Loss of Epoch 48: 0.2438791152907581
Training Acc of Epoch 48: 0.743824631605691
Testing Acc of Epoch 48: 0.7327391304347826
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.7435e-01 (2.7435e-01)	Acc 0.703125 (0.703125)
Epoch: [49][300/616]	Loss 2.2658e-01 (2.4571e-01)	Acc 0.761719 (0.741808)
Epoch: [49][600/616]	Loss 2.3756e-01 (2.4521e-01)	Acc 0.750000 (0.742241)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.743891)
Training Loss of Epoch 49: 0.24513656465503258
Training Acc of Epoch 49: 0.742236725101626
Testing Acc of Epoch 49: 0.7438913043478261
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4950e-01 (2.4950e-01)	Acc 0.729492 (0.729492)
Epoch: [50][300/616]	Loss 2.5325e-01 (2.4448e-01)	Acc 0.732422 (0.743703)
Epoch: [50][600/616]	Loss 2.4044e-01 (2.4413e-01)	Acc 0.743164 (0.743409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740409)
Training Loss of Epoch 50: 0.24411791027561436
Training Acc of Epoch 50: 0.7433530233739838
Testing Acc of Epoch 50: 0.7404086956521739
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.4816e-01 (2.4816e-01)	Acc 0.741211 (0.741211)
Epoch: [51][300/616]	Loss 2.5991e-01 (2.4446e-01)	Acc 0.728516 (0.743430)
Epoch: [51][600/616]	Loss 2.4497e-01 (2.4371e-01)	Acc 0.734375 (0.743754)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.740587)
Training Loss of Epoch 51: 0.24371395862199427
Training Acc of Epoch 51: 0.7438008130081301
Testing Acc of Epoch 51: 0.7405869565217391
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4752e-01 (2.4752e-01)	Acc 0.748047 (0.748047)
Epoch: [52][300/616]	Loss 2.6364e-01 (2.4461e-01)	Acc 0.719727 (0.742473)
Epoch: [52][600/616]	Loss 2.2233e-01 (2.4434e-01)	Acc 0.773438 (0.743119)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.747965)
Training Loss of Epoch 52: 0.2442566855409281
Training Acc of Epoch 52: 0.7432974466463415
Testing Acc of Epoch 52: 0.7479652173913044
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.4452e-01 (2.4452e-01)	Acc 0.740234 (0.740234)
Epoch: [53][300/616]	Loss 2.3040e-01 (2.4501e-01)	Acc 0.762695 (0.742408)
Epoch: [53][600/616]	Loss 2.5554e-01 (2.4375e-01)	Acc 0.728516 (0.743668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.719861)
Training Loss of Epoch 53: 0.2437719879596214
Training Acc of Epoch 53: 0.7436563135162602
Testing Acc of Epoch 53: 0.7198608695652174
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.6944e-01 (2.6944e-01)	Acc 0.714844 (0.714844)
Epoch: [54][300/616]	Loss 2.3549e-01 (2.4556e-01)	Acc 0.751953 (0.741376)
Epoch: [54][600/616]	Loss 2.3915e-01 (2.4474e-01)	Acc 0.748047 (0.742620)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.677885 (0.714548)
Training Loss of Epoch 54: 0.24469789649412885
Training Acc of Epoch 54: 0.7426654598577236
Testing Acc of Epoch 54: 0.7145478260869566
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.8153e-01 (2.8153e-01)	Acc 0.696289 (0.696289)
Epoch: [55][300/616]	Loss 2.2456e-01 (2.4498e-01)	Acc 0.767578 (0.742827)
Epoch: [55][600/616]	Loss 2.4941e-01 (2.4452e-01)	Acc 0.753906 (0.743125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746283)
Training Loss of Epoch 55: 0.2443857298876212
Training Acc of Epoch 55: 0.743310149898374
Testing Acc of Epoch 55: 0.7462826086956522
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5415e-01 (2.5415e-01)	Acc 0.734375 (0.734375)
Epoch: [56][300/616]	Loss 2.2806e-01 (2.4385e-01)	Acc 0.762695 (0.743203)
Epoch: [56][600/616]	Loss 2.3951e-01 (2.4435e-01)	Acc 0.765625 (0.742803)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748517)
Training Loss of Epoch 56: 0.2443723261598649
Training Acc of Epoch 56: 0.7426924542682927
Testing Acc of Epoch 56: 0.7485173913043478
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5303e-01 (2.5303e-01)	Acc 0.721680 (0.721680)
Epoch: [57][300/616]	Loss 2.4475e-01 (2.4515e-01)	Acc 0.738281 (0.742473)
Epoch: [57][600/616]	Loss 2.3376e-01 (2.4413e-01)	Acc 0.747070 (0.743400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.731461)
Training Loss of Epoch 57: 0.24394627053563187
Training Acc of Epoch 57: 0.7437007748983739
Testing Acc of Epoch 57: 0.7314608695652174
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3782e-01 (2.3782e-01)	Acc 0.753906 (0.753906)
Epoch: [58][300/616]	Loss 2.3645e-01 (2.4478e-01)	Acc 0.744141 (0.742746)
Epoch: [58][600/616]	Loss 2.5773e-01 (2.4465e-01)	Acc 0.732422 (0.742690)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748735)
Training Loss of Epoch 58: 0.2445162325855193
Training Acc of Epoch 58: 0.7428560086382113
Testing Acc of Epoch 58: 0.7487347826086956
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.4606e-01 (2.4606e-01)	Acc 0.723633 (0.723633)
Epoch: [59][300/616]	Loss 2.3471e-01 (2.4373e-01)	Acc 0.755859 (0.743216)
Epoch: [59][600/616]	Loss 2.3526e-01 (2.4385e-01)	Acc 0.754883 (0.743122)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747048)
Training Loss of Epoch 59: 0.24392666131015717
Training Acc of Epoch 59: 0.7430529090447154
Testing Acc of Epoch 59: 0.7470478260869565
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3841e-01 (2.3841e-01)	Acc 0.744141 (0.744141)
Epoch: [60][300/616]	Loss 2.3183e-01 (2.4441e-01)	Acc 0.760742 (0.742674)
Epoch: [60][600/616]	Loss 2.3689e-01 (2.4457e-01)	Acc 0.741211 (0.742660)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742004)
Training Loss of Epoch 60: 0.24474371577181467
Training Acc of Epoch 60: 0.7424892022357723
Testing Acc of Epoch 60: 0.7420043478260869
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.5439e-01 (2.5439e-01)	Acc 0.736328 (0.736328)
Epoch: [61][300/616]	Loss 2.3406e-01 (2.4500e-01)	Acc 0.753906 (0.742385)
Epoch: [61][600/616]	Loss 2.5067e-01 (2.4481e-01)	Acc 0.737305 (0.742574)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.723304)
Training Loss of Epoch 61: 0.24485695686766773
Training Acc of Epoch 61: 0.7425431910569106
Testing Acc of Epoch 61: 0.723304347826087
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.5986e-01 (2.5986e-01)	Acc 0.718750 (0.718750)
Epoch: [62][300/616]	Loss 2.2471e-01 (2.4564e-01)	Acc 0.791016 (0.742038)
Epoch: [62][600/616]	Loss 2.5496e-01 (2.4539e-01)	Acc 0.726562 (0.741838)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749765)
Training Loss of Epoch 62: 0.24534151331195986
Training Acc of Epoch 62: 0.7419397865853659
Testing Acc of Epoch 62: 0.7497652173913043
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.5047e-01 (2.5047e-01)	Acc 0.738281 (0.738281)
Epoch: [63][300/616]	Loss 2.2810e-01 (2.4675e-01)	Acc 0.761719 (0.741214)
Epoch: [63][600/616]	Loss 2.3426e-01 (2.4651e-01)	Acc 0.761719 (0.741045)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.744583)
Training Loss of Epoch 63: 0.24650718705440924
Training Acc of Epoch 63: 0.741039443597561
Testing Acc of Epoch 63: 0.7445826086956522
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5645e-01 (2.5645e-01)	Acc 0.750000 (0.750000)
Epoch: [64][300/616]	Loss 2.4428e-01 (2.4457e-01)	Acc 0.751953 (0.742979)
Epoch: [64][600/616]	Loss 2.2962e-01 (2.4555e-01)	Acc 0.757812 (0.741382)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747078)
Training Loss of Epoch 64: 0.24537653021696137
Training Acc of Epoch 64: 0.7415507494918699
Testing Acc of Epoch 64: 0.7470782608695652
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4189e-01 (2.4189e-01)	Acc 0.753906 (0.753906)
Epoch: [65][300/616]	Loss 2.3816e-01 (2.4338e-01)	Acc 0.750977 (0.744390)
Epoch: [65][600/616]	Loss 2.7784e-01 (2.4426e-01)	Acc 0.704102 (0.742976)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.748565)
Training Loss of Epoch 65: 0.24428112334836788
Training Acc of Epoch 65: 0.7429512830284553
Testing Acc of Epoch 65: 0.7485652173913043
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3627e-01 (2.3627e-01)	Acc 0.746094 (0.746094)
Epoch: [66][300/616]	Loss 2.2902e-01 (2.4541e-01)	Acc 0.768555 (0.741639)
Epoch: [66][600/616]	Loss 2.5702e-01 (2.4427e-01)	Acc 0.718750 (0.743141)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744230)
Training Loss of Epoch 66: 0.24426661979376785
Training Acc of Epoch 66: 0.7431100736788618
Testing Acc of Epoch 66: 0.7442304347826086
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.6985e-01 (2.6985e-01)	Acc 0.720703 (0.720703)
Epoch: [67][300/616]	Loss 2.5416e-01 (2.4423e-01)	Acc 0.724609 (0.743443)
Epoch: [67][600/616]	Loss 2.4146e-01 (2.4455e-01)	Acc 0.746094 (0.742875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.750713)
Training Loss of Epoch 67: 0.24442789244457958
Training Acc of Epoch 67: 0.743041793699187
Testing Acc of Epoch 67: 0.7507130434782608
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.2667e-01 (2.2667e-01)	Acc 0.761719 (0.761719)
Epoch: [68][300/616]	Loss 2.3597e-01 (2.4409e-01)	Acc 0.761719 (0.743599)
Epoch: [68][600/616]	Loss 2.4111e-01 (2.4446e-01)	Acc 0.740234 (0.742729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749857)
Training Loss of Epoch 68: 0.24441240788475285
Training Acc of Epoch 68: 0.7427385035569106
Testing Acc of Epoch 68: 0.7498565217391304
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.3913e-01 (2.3913e-01)	Acc 0.748047 (0.748047)
Epoch: [69][300/616]	Loss 2.3116e-01 (2.4577e-01)	Acc 0.757812 (0.741548)
Epoch: [69][600/616]	Loss 2.3718e-01 (2.4533e-01)	Acc 0.750977 (0.741957)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.737491)
Training Loss of Epoch 69: 0.24532488538482325
Training Acc of Epoch 69: 0.7419318470528455
Testing Acc of Epoch 69: 0.7374913043478261
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3601e-01 (2.3601e-01)	Acc 0.743164 (0.743164)
Epoch: [70][300/616]	Loss 2.3439e-01 (2.4533e-01)	Acc 0.762695 (0.741775)
Epoch: [70][600/616]	Loss 2.5386e-01 (2.4522e-01)	Acc 0.741211 (0.741897)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.746643)
Training Loss of Epoch 70: 0.24526240440403543
Training Acc of Epoch 70: 0.7418556275406504
Testing Acc of Epoch 70: 0.7466434782608695
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5203e-01 (2.5203e-01)	Acc 0.725586 (0.725586)
Epoch: [71][300/616]	Loss 2.5055e-01 (2.4527e-01)	Acc 0.743164 (0.741587)
Epoch: [71][600/616]	Loss 2.4929e-01 (2.4541e-01)	Acc 0.747070 (0.741687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.749296)
Training Loss of Epoch 71: 0.2452788720043694
Training Acc of Epoch 71: 0.7418397484756097
Testing Acc of Epoch 71: 0.749295652173913
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4510e-01 (2.4510e-01)	Acc 0.743164 (0.743164)
Epoch: [72][300/616]	Loss 2.4534e-01 (2.4400e-01)	Acc 0.746094 (0.743297)
Epoch: [72][600/616]	Loss 2.3897e-01 (2.4417e-01)	Acc 0.759766 (0.743470)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.746074)
Training Loss of Epoch 72: 0.24413054723565172
Training Acc of Epoch 72: 0.743529280995935
Testing Acc of Epoch 72: 0.7460739130434783
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.5481e-01 (2.5481e-01)	Acc 0.724609 (0.724609)
Epoch: [73][300/616]	Loss 2.3495e-01 (2.4462e-01)	Acc 0.751953 (0.742966)
Epoch: [73][600/616]	Loss 2.3310e-01 (2.4454e-01)	Acc 0.762695 (0.742771)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.745291)
Training Loss of Epoch 73: 0.24473654848288715
Training Acc of Epoch 73: 0.7425304878048781
Testing Acc of Epoch 73: 0.7452913043478261
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3634e-01 (2.3634e-01)	Acc 0.750977 (0.750977)
Epoch: [74][300/616]	Loss 2.4770e-01 (2.4523e-01)	Acc 0.740234 (0.741811)
Epoch: [74][600/616]	Loss 2.4120e-01 (2.4483e-01)	Acc 0.757812 (0.742231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.733743)
Training Loss of Epoch 74: 0.24476741215562434
Training Acc of Epoch 74: 0.7422256097560975
Testing Acc of Epoch 74: 0.7337434782608696
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.5498e-01 (2.5498e-01)	Acc 0.741211 (0.741211)
Epoch: [75][300/616]	Loss 2.5287e-01 (2.3837e-01)	Acc 0.713867 (0.748680)
Epoch: [75][600/616]	Loss 2.4169e-01 (2.3834e-01)	Acc 0.738281 (0.748261)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.751948)
Training Loss of Epoch 75: 0.23830062705811447
Training Acc of Epoch 75: 0.7482390116869919
Testing Acc of Epoch 75: 0.7519478260869565
Model with the best training loss saved! The loss is 0.23830062705811447
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.1875e-01 (2.1875e-01)	Acc 0.779297 (0.779297)
Epoch: [76][300/616]	Loss 2.4610e-01 (2.3800e-01)	Acc 0.735352 (0.748654)
Epoch: [76][600/616]	Loss 2.5775e-01 (2.3845e-01)	Acc 0.729492 (0.748385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747674)
Training Loss of Epoch 76: 0.23858153601972068
Training Acc of Epoch 76: 0.7482326600609757
Testing Acc of Epoch 76: 0.7476739130434783
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.5666e-01 (2.5666e-01)	Acc 0.715820 (0.715820)
Epoch: [77][300/616]	Loss 2.3393e-01 (2.3851e-01)	Acc 0.758789 (0.748939)
Epoch: [77][600/616]	Loss 2.3898e-01 (2.3891e-01)	Acc 0.755859 (0.747792)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.737043)
Training Loss of Epoch 77: 0.2389066820464483
Training Acc of Epoch 77: 0.7477022992886179
Testing Acc of Epoch 77: 0.7370434782608696
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3007e-01 (2.3007e-01)	Acc 0.764648 (0.764648)
Epoch: [78][300/616]	Loss 2.2627e-01 (2.3943e-01)	Acc 0.757812 (0.747271)
Epoch: [78][600/616]	Loss 2.3367e-01 (2.3940e-01)	Acc 0.753906 (0.747455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749778)
Training Loss of Epoch 78: 0.23926506006136172
Training Acc of Epoch 78: 0.7476276676829269
Testing Acc of Epoch 78: 0.7497782608695652
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2114e-01 (2.2114e-01)	Acc 0.770508 (0.770508)
Epoch: [79][300/616]	Loss 2.2424e-01 (2.3885e-01)	Acc 0.760742 (0.747826)
Epoch: [79][600/616]	Loss 2.4856e-01 (2.3909e-01)	Acc 0.747070 (0.747621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.751400)
Training Loss of Epoch 79: 0.23907820727767015
Training Acc of Epoch 79: 0.7476705411585366
Testing Acc of Epoch 79: 0.7514
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4687e-01 (2.4687e-01)	Acc 0.743164 (0.743164)
Epoch: [80][300/616]	Loss 2.4256e-01 (2.3838e-01)	Acc 0.735352 (0.748420)
Epoch: [80][600/616]	Loss 2.5125e-01 (2.3884e-01)	Acc 0.738281 (0.747938)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.735304)
Training Loss of Epoch 80: 0.23884313278566532
Training Acc of Epoch 80: 0.747827743902439
Testing Acc of Epoch 80: 0.735304347826087
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4361e-01 (2.4361e-01)	Acc 0.747070 (0.747070)
Epoch: [81][300/616]	Loss 2.3968e-01 (2.3824e-01)	Acc 0.747070 (0.748482)
Epoch: [81][600/616]	Loss 2.2692e-01 (2.3906e-01)	Acc 0.761719 (0.747724)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749843)
Training Loss of Epoch 81: 0.2390919331854921
Training Acc of Epoch 81: 0.7476181402439024
Testing Acc of Epoch 81: 0.7498434782608696
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3439e-01 (2.3439e-01)	Acc 0.749023 (0.749023)
Epoch: [82][300/616]	Loss 2.4694e-01 (2.3980e-01)	Acc 0.740234 (0.746470)
Epoch: [82][600/616]	Loss 2.4035e-01 (2.3940e-01)	Acc 0.749023 (0.747231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752339)
Training Loss of Epoch 82: 0.23938221177919125
Training Acc of Epoch 82: 0.7472878556910569
Testing Acc of Epoch 82: 0.7523391304347826
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3957e-01 (2.3957e-01)	Acc 0.743164 (0.743164)
Epoch: [83][300/616]	Loss 2.3108e-01 (2.3970e-01)	Acc 0.759766 (0.747275)
Epoch: [83][600/616]	Loss 2.2350e-01 (2.3955e-01)	Acc 0.762695 (0.747228)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.750987)
Training Loss of Epoch 83: 0.23954736476506644
Training Acc of Epoch 83: 0.7471973450203252
Testing Acc of Epoch 83: 0.7509869565217391
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.2360e-01 (2.2360e-01)	Acc 0.773438 (0.773438)
Epoch: [84][300/616]	Loss 2.3850e-01 (2.3984e-01)	Acc 0.750000 (0.746564)
Epoch: [84][600/616]	Loss 2.3170e-01 (2.3989e-01)	Acc 0.759766 (0.746934)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.747017)
Training Loss of Epoch 84: 0.2398461044319277
Training Acc of Epoch 84: 0.7468956427845529
Testing Acc of Epoch 84: 0.7470173913043479
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1540e-01 (2.1540e-01)	Acc 0.784180 (0.784180)
Epoch: [85][300/616]	Loss 2.4077e-01 (2.3911e-01)	Acc 0.739258 (0.747593)
Epoch: [85][600/616]	Loss 2.2954e-01 (2.3878e-01)	Acc 0.762695 (0.747881)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752035)
Training Loss of Epoch 85: 0.23874798271714187
Training Acc of Epoch 85: 0.7478849085365854
Testing Acc of Epoch 85: 0.7520347826086956
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3711e-01 (2.3711e-01)	Acc 0.747070 (0.747070)
Epoch: [86][300/616]	Loss 2.4278e-01 (2.3806e-01)	Acc 0.748047 (0.749251)
Epoch: [86][600/616]	Loss 2.3907e-01 (2.3950e-01)	Acc 0.740234 (0.747459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750813)
Training Loss of Epoch 86: 0.23952590152015532
Training Acc of Epoch 86: 0.7474879319105691
Testing Acc of Epoch 86: 0.7508130434782608
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3375e-01 (2.3375e-01)	Acc 0.750977 (0.750977)
Epoch: [87][300/616]	Loss 2.5262e-01 (2.3901e-01)	Acc 0.725586 (0.747602)
Epoch: [87][600/616]	Loss 2.4162e-01 (2.3948e-01)	Acc 0.741211 (0.747498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749065)
Training Loss of Epoch 87: 0.2394310827662305
Training Acc of Epoch 87: 0.7475133384146342
Testing Acc of Epoch 87: 0.7490652173913044
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4023e-01 (2.4023e-01)	Acc 0.747070 (0.747070)
Epoch: [88][300/616]	Loss 2.4117e-01 (2.3936e-01)	Acc 0.745117 (0.748050)
Epoch: [88][600/616]	Loss 2.2569e-01 (2.3990e-01)	Acc 0.765625 (0.747262)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749426)
Training Loss of Epoch 88: 0.2397096457035561
Training Acc of Epoch 88: 0.7475117505081301
Testing Acc of Epoch 88: 0.7494260869565217
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.4711e-01 (2.4711e-01)	Acc 0.741211 (0.741211)
Epoch: [89][300/616]	Loss 2.4164e-01 (2.3899e-01)	Acc 0.745117 (0.748070)
Epoch: [89][600/616]	Loss 2.3085e-01 (2.3883e-01)	Acc 0.750000 (0.747917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.749287)
Training Loss of Epoch 89: 0.23890613610666941
Training Acc of Epoch 89: 0.7478880843495935
Testing Acc of Epoch 89: 0.7492869565217392
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3692e-01 (2.3692e-01)	Acc 0.753906 (0.753906)
Epoch: [90][300/616]	Loss 2.3704e-01 (2.3794e-01)	Acc 0.755859 (0.748800)
Epoch: [90][600/616]	Loss 2.3995e-01 (2.3917e-01)	Acc 0.723633 (0.747511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749530)
Training Loss of Epoch 90: 0.2390727726666908
Training Acc of Epoch 90: 0.7476292555894309
Testing Acc of Epoch 90: 0.7495304347826087
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.3801e-01 (2.3801e-01)	Acc 0.744141 (0.744141)
Epoch: [91][300/616]	Loss 2.4793e-01 (2.3984e-01)	Acc 0.739258 (0.747262)
Epoch: [91][600/616]	Loss 2.4670e-01 (2.3997e-01)	Acc 0.735352 (0.746918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.750039)
Training Loss of Epoch 91: 0.23994496377503
Training Acc of Epoch 91: 0.7469607469512195
Testing Acc of Epoch 91: 0.7500391304347827
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3346e-01 (2.3346e-01)	Acc 0.756836 (0.756836)
Epoch: [92][300/616]	Loss 2.4975e-01 (2.3971e-01)	Acc 0.734375 (0.747343)
Epoch: [92][600/616]	Loss 2.3964e-01 (2.3981e-01)	Acc 0.743164 (0.746942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.746091)
Training Loss of Epoch 92: 0.2399618315260585
Training Acc of Epoch 92: 0.7467257367886179
Testing Acc of Epoch 92: 0.7460913043478261
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3905e-01 (2.3905e-01)	Acc 0.750977 (0.750977)
Epoch: [93][300/616]	Loss 2.4395e-01 (2.4008e-01)	Acc 0.742188 (0.746626)
Epoch: [93][600/616]	Loss 2.4689e-01 (2.3923e-01)	Acc 0.732422 (0.747662)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750530)
Training Loss of Epoch 93: 0.23931516079398674
Training Acc of Epoch 93: 0.7475276295731708
Testing Acc of Epoch 93: 0.7505304347826087
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2208e-01 (2.2208e-01)	Acc 0.762695 (0.762695)
Epoch: [94][300/616]	Loss 2.5021e-01 (2.3901e-01)	Acc 0.728516 (0.747849)
Epoch: [94][600/616]	Loss 2.4475e-01 (2.3959e-01)	Acc 0.733398 (0.747324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747839)
Training Loss of Epoch 94: 0.23959211799187388
Training Acc of Epoch 94: 0.7472973831300813
Testing Acc of Epoch 94: 0.7478391304347826
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.1764e-01 (2.1764e-01)	Acc 0.769531 (0.769531)
Epoch: [95][300/616]	Loss 2.1921e-01 (2.3966e-01)	Acc 0.779297 (0.747262)
Epoch: [95][600/616]	Loss 2.7622e-01 (2.3969e-01)	Acc 0.685547 (0.747306)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752922)
Training Loss of Epoch 95: 0.23972715430143404
Training Acc of Epoch 95: 0.747217987804878
Testing Acc of Epoch 95: 0.7529217391304348
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.2482e-01 (2.2482e-01)	Acc 0.774414 (0.774414)
Epoch: [96][300/616]	Loss 2.3538e-01 (2.4113e-01)	Acc 0.751953 (0.745964)
Epoch: [96][600/616]	Loss 2.3792e-01 (2.4014e-01)	Acc 0.760742 (0.747226)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746209)
Training Loss of Epoch 96: 0.2400415038190237
Training Acc of Epoch 96: 0.7472624491869919
Testing Acc of Epoch 96: 0.746208695652174
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.5048e-01 (2.5048e-01)	Acc 0.747070 (0.747070)
Epoch: [97][300/616]	Loss 2.3036e-01 (2.3862e-01)	Acc 0.757812 (0.748173)
Epoch: [97][600/616]	Loss 2.3390e-01 (2.3896e-01)	Acc 0.745117 (0.748138)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749957)
Training Loss of Epoch 97: 0.23896561085693235
Training Acc of Epoch 97: 0.7481770833333333
Testing Acc of Epoch 97: 0.7499565217391304
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.2038e-01 (2.2038e-01)	Acc 0.772461 (0.772461)
Epoch: [98][300/616]	Loss 2.4672e-01 (2.3973e-01)	Acc 0.738281 (0.747018)
Epoch: [98][600/616]	Loss 2.3601e-01 (2.4002e-01)	Acc 0.745117 (0.746576)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751748)
Training Loss of Epoch 98: 0.23989638943013136
Training Acc of Epoch 98: 0.7467940167682927
Testing Acc of Epoch 98: 0.7517478260869566
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2897e-01 (2.2897e-01)	Acc 0.750000 (0.750000)
Epoch: [99][300/616]	Loss 2.4835e-01 (2.3915e-01)	Acc 0.737305 (0.748242)
Epoch: [99][600/616]	Loss 2.1863e-01 (2.3943e-01)	Acc 0.772461 (0.747481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748643)
Training Loss of Epoch 99: 0.23944534581851185
Training Acc of Epoch 99: 0.7474148882113821
Testing Acc of Epoch 99: 0.7486434782608695
Early stopping not satisfied.
