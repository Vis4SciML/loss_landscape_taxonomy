train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_8b
different_width False
resnet18_width 64
weight_precision 8
bias_precision 8
act_precision 11
batch_norm False
dropout False
exp_num 5
lr 0.0015625
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.0015625/lr_decay/JT_8b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_8b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.0015625
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0015e-01 (5.0015e-01)	Acc 0.200195 (0.200195)
Epoch: [0][300/616]	Loss 2.6790e-01 (3.2199e-01)	Acc 0.727539 (0.651919)
Epoch: [0][600/616]	Loss 2.5667e-01 (2.9073e-01)	Acc 0.731445 (0.690680)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.735926)
Training Loss of Epoch 0: 0.28980189460079847
Training Acc of Epoch 0: 0.6917841717479675
Testing Acc of Epoch 0: 0.7359260869565217
Model with the best training loss saved! The loss is 0.28980189460079847
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4201e-01 (2.4201e-01)	Acc 0.740234 (0.740234)
Epoch: [1][300/616]	Loss 2.3505e-01 (2.4671e-01)	Acc 0.751953 (0.741724)
Epoch: [1][600/616]	Loss 2.2769e-01 (2.4479e-01)	Acc 0.770508 (0.743650)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745813)
Training Loss of Epoch 1: 0.2447238663347756
Training Acc of Epoch 1: 0.7435864456300812
Testing Acc of Epoch 1: 0.7458130434782608
Model with the best training loss saved! The loss is 0.2447238663347756
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5237e-01 (2.5237e-01)	Acc 0.738281 (0.738281)
Epoch: [2][300/616]	Loss 2.4648e-01 (2.3916e-01)	Acc 0.740234 (0.748018)
Epoch: [2][600/616]	Loss 2.4570e-01 (2.3884e-01)	Acc 0.736328 (0.748126)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749483)
Training Loss of Epoch 2: 0.2387555190702764
Training Acc of Epoch 2: 0.748240599593496
Testing Acc of Epoch 2: 0.7494826086956522
Model with the best training loss saved! The loss is 0.2387555190702764
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4121e-01 (2.4121e-01)	Acc 0.747070 (0.747070)
Epoch: [3][300/616]	Loss 2.4406e-01 (2.3698e-01)	Acc 0.750000 (0.750519)
Epoch: [3][600/616]	Loss 2.3201e-01 (2.3628e-01)	Acc 0.765625 (0.750668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752313)
Training Loss of Epoch 3: 0.23621752795165146
Training Acc of Epoch 3: 0.7507939532520326
Testing Acc of Epoch 3: 0.7523130434782609
Model with the best training loss saved! The loss is 0.23621752795165146
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3456e-01 (2.3456e-01)	Acc 0.763672 (0.763672)
Epoch: [4][300/616]	Loss 2.2335e-01 (2.3447e-01)	Acc 0.769531 (0.752670)
Epoch: [4][600/616]	Loss 2.4515e-01 (2.3426e-01)	Acc 0.737305 (0.752728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753530)
Training Loss of Epoch 4: 0.23417430742969358
Training Acc of Epoch 4: 0.7528836382113822
Testing Acc of Epoch 4: 0.7535304347826087
Model with the best training loss saved! The loss is 0.23417430742969358
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3915e-01 (2.3915e-01)	Acc 0.745117 (0.745117)
Epoch: [5][300/616]	Loss 2.2904e-01 (2.3326e-01)	Acc 0.755859 (0.753592)
Epoch: [5][600/616]	Loss 2.2643e-01 (2.3298e-01)	Acc 0.762695 (0.754067)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755348)
Training Loss of Epoch 5: 0.23293350515811423
Training Acc of Epoch 5: 0.7542095401422764
Testing Acc of Epoch 5: 0.7553478260869565
Model with the best training loss saved! The loss is 0.23293350515811423
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3165e-01 (2.3165e-01)	Acc 0.759766 (0.759766)
Epoch: [6][300/616]	Loss 2.2595e-01 (2.3175e-01)	Acc 0.778320 (0.755363)
Epoch: [6][600/616]	Loss 2.2475e-01 (2.3159e-01)	Acc 0.753906 (0.755533)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757213)
Training Loss of Epoch 6: 0.2316089333557501
Training Acc of Epoch 6: 0.7554274644308943
Testing Acc of Epoch 6: 0.7572130434782609
Model with the best training loss saved! The loss is 0.2316089333557501
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.3324e-01 (2.3324e-01)	Acc 0.751953 (0.751953)
Epoch: [7][300/616]	Loss 2.2940e-01 (2.3067e-01)	Acc 0.750977 (0.756025)
Epoch: [7][600/616]	Loss 2.2829e-01 (2.3049e-01)	Acc 0.767578 (0.756165)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756283)
Training Loss of Epoch 7: 0.2305724947190866
Training Acc of Epoch 7: 0.7560705665650407
Testing Acc of Epoch 7: 0.7562826086956522
Model with the best training loss saved! The loss is 0.2305724947190866
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3514e-01 (2.3514e-01)	Acc 0.750000 (0.750000)
Epoch: [8][300/616]	Loss 2.2429e-01 (2.2985e-01)	Acc 0.754883 (0.756606)
Epoch: [8][600/616]	Loss 2.3047e-01 (2.2993e-01)	Acc 0.754883 (0.756761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757252)
Training Loss of Epoch 8: 0.23002191488820362
Training Acc of Epoch 8: 0.756737487296748
Testing Acc of Epoch 8: 0.7572521739130434
Model with the best training loss saved! The loss is 0.23002191488820362
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3124e-01 (2.3124e-01)	Acc 0.773438 (0.773438)
Epoch: [9][300/616]	Loss 2.3220e-01 (2.3037e-01)	Acc 0.762695 (0.756194)
Epoch: [9][600/616]	Loss 2.1541e-01 (2.2981e-01)	Acc 0.778320 (0.756989)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757770)
Training Loss of Epoch 9: 0.22971921870863535
Training Acc of Epoch 9: 0.7570661839430894
Testing Acc of Epoch 9: 0.7577695652173912
Model with the best training loss saved! The loss is 0.22971921870863535
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2118e-01 (2.2118e-01)	Acc 0.767578 (0.767578)
Epoch: [10][300/616]	Loss 2.3587e-01 (2.2932e-01)	Acc 0.747070 (0.758007)
Epoch: [10][600/616]	Loss 2.2026e-01 (2.2952e-01)	Acc 0.770508 (0.757258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.758961)
Training Loss of Epoch 10: 0.22953257236054272
Training Acc of Epoch 10: 0.7572710238821139
Testing Acc of Epoch 10: 0.7589608695652174
Model with the best training loss saved! The loss is 0.22953257236054272
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3598e-01 (2.3598e-01)	Acc 0.751953 (0.751953)
Epoch: [11][300/616]	Loss 2.2521e-01 (2.2967e-01)	Acc 0.767578 (0.756683)
Epoch: [11][600/616]	Loss 2.1238e-01 (2.2937e-01)	Acc 0.782227 (0.757169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758687)
Training Loss of Epoch 11: 0.22937994710798185
Training Acc of Epoch 11: 0.7570407774390244
Testing Acc of Epoch 11: 0.7586869565217391
Model with the best training loss saved! The loss is 0.22937994710798185
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3651e-01 (2.3651e-01)	Acc 0.759766 (0.759766)
Epoch: [12][300/616]	Loss 2.2886e-01 (2.2922e-01)	Acc 0.762695 (0.757404)
Epoch: [12][600/616]	Loss 2.3324e-01 (2.2929e-01)	Acc 0.750977 (0.757276)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.758152)
Training Loss of Epoch 12: 0.22920723203721086
Training Acc of Epoch 12: 0.7573440675813008
Testing Acc of Epoch 12: 0.7581521739130435
Model with the best training loss saved! The loss is 0.22920723203721086
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4238e-01 (2.4238e-01)	Acc 0.733398 (0.733398)
Epoch: [13][300/616]	Loss 2.2345e-01 (2.2967e-01)	Acc 0.756836 (0.756959)
Epoch: [13][600/616]	Loss 2.4355e-01 (2.2915e-01)	Acc 0.745117 (0.757520)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757730)
Training Loss of Epoch 13: 0.22914276411378287
Training Acc of Epoch 13: 0.757518737296748
Testing Acc of Epoch 13: 0.7577304347826087
Model with the best training loss saved! The loss is 0.22914276411378287
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3744e-01 (2.3744e-01)	Acc 0.745117 (0.745117)
Epoch: [14][300/616]	Loss 2.4434e-01 (2.2941e-01)	Acc 0.737305 (0.756456)
Epoch: [14][600/616]	Loss 2.3777e-01 (2.2889e-01)	Acc 0.741211 (0.757748)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.758326)
Training Loss of Epoch 14: 0.22892581364972805
Training Acc of Epoch 14: 0.7576981707317073
Testing Acc of Epoch 14: 0.7583260869565217
Model with the best training loss saved! The loss is 0.22892581364972805
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.2033e-01 (2.2033e-01)	Acc 0.774414 (0.774414)
Epoch: [15][300/616]	Loss 2.3194e-01 (2.2881e-01)	Acc 0.755859 (0.758189)
Epoch: [15][600/616]	Loss 2.1141e-01 (2.2887e-01)	Acc 0.777344 (0.757907)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.759383)
Training Loss of Epoch 15: 0.2288709507482808
Training Acc of Epoch 15: 0.7579061864837399
Testing Acc of Epoch 15: 0.7593826086956522
Model with the best training loss saved! The loss is 0.2288709507482808
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2513e-01 (2.2513e-01)	Acc 0.761719 (0.761719)
Epoch: [16][300/616]	Loss 2.2490e-01 (2.2985e-01)	Acc 0.758789 (0.756683)
Epoch: [16][600/616]	Loss 2.3765e-01 (2.2873e-01)	Acc 0.741211 (0.757939)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759530)
Training Loss of Epoch 16: 0.22878919127510816
Training Acc of Epoch 16: 0.7579125381097561
Testing Acc of Epoch 16: 0.7595304347826087
Model with the best training loss saved! The loss is 0.22878919127510816
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3647e-01 (2.3647e-01)	Acc 0.742188 (0.742188)
Epoch: [17][300/616]	Loss 2.2268e-01 (2.2860e-01)	Acc 0.763672 (0.758072)
Epoch: [17][600/616]	Loss 2.1735e-01 (2.2853e-01)	Acc 0.771484 (0.758253)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757813)
Training Loss of Epoch 17: 0.22863260267711266
Training Acc of Epoch 17: 0.7582015370934959
Testing Acc of Epoch 17: 0.7578130434782608
Model with the best training loss saved! The loss is 0.22863260267711266
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3457e-01 (2.3457e-01)	Acc 0.747070 (0.747070)
Epoch: [18][300/616]	Loss 2.2351e-01 (2.2858e-01)	Acc 0.768555 (0.757884)
Epoch: [18][600/616]	Loss 2.0987e-01 (2.2850e-01)	Acc 0.773438 (0.757850)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759830)
Training Loss of Epoch 18: 0.2284907974847933
Training Acc of Epoch 18: 0.7578649009146341
Testing Acc of Epoch 18: 0.7598304347826087
Model with the best training loss saved! The loss is 0.2284907974847933
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.2386e-01 (2.2386e-01)	Acc 0.758789 (0.758789)
Epoch: [19][300/616]	Loss 2.4827e-01 (2.2907e-01)	Acc 0.737305 (0.757456)
Epoch: [19][600/616]	Loss 2.3305e-01 (2.2879e-01)	Acc 0.739258 (0.758087)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758200)
Training Loss of Epoch 19: 0.22876040455771657
Training Acc of Epoch 19: 0.758082444105691
Testing Acc of Epoch 19: 0.7582
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.3330e-01 (2.3330e-01)	Acc 0.750000 (0.750000)
Epoch: [20][300/616]	Loss 2.2298e-01 (2.2819e-01)	Acc 0.764648 (0.758899)
Epoch: [20][600/616]	Loss 2.2643e-01 (2.2854e-01)	Acc 0.760742 (0.758425)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757009)
Training Loss of Epoch 20: 0.22846866584405667
Training Acc of Epoch 20: 0.7584841844512196
Testing Acc of Epoch 20: 0.7570086956521739
Model with the best training loss saved! The loss is 0.22846866584405667
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.1321e-01 (2.1321e-01)	Acc 0.771484 (0.771484)
Epoch: [21][300/616]	Loss 2.3615e-01 (2.2822e-01)	Acc 0.760742 (0.758478)
Epoch: [21][600/616]	Loss 2.3544e-01 (2.2847e-01)	Acc 0.751953 (0.758274)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756252)
Training Loss of Epoch 21: 0.22852928958288052
Training Acc of Epoch 21: 0.7581650152439025
Testing Acc of Epoch 21: 0.7562521739130434
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.1755e-01 (2.1755e-01)	Acc 0.774414 (0.774414)
Epoch: [22][300/616]	Loss 2.1610e-01 (2.2814e-01)	Acc 0.775391 (0.758552)
Epoch: [22][600/616]	Loss 2.4015e-01 (2.2842e-01)	Acc 0.749023 (0.758396)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758422)
Training Loss of Epoch 22: 0.22843412662424692
Training Acc of Epoch 22: 0.7583920858739838
Testing Acc of Epoch 22: 0.7584217391304348
Model with the best training loss saved! The loss is 0.22843412662424692
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3834e-01 (2.3834e-01)	Acc 0.747070 (0.747070)
Epoch: [23][300/616]	Loss 2.2803e-01 (2.2841e-01)	Acc 0.757812 (0.758497)
Epoch: [23][600/616]	Loss 2.2905e-01 (2.2849e-01)	Acc 0.746094 (0.758237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758526)
Training Loss of Epoch 23: 0.228439036520516
Training Acc of Epoch 23: 0.7582412347560976
Testing Acc of Epoch 23: 0.7585260869565218
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.3040e-01 (2.3040e-01)	Acc 0.754883 (0.754883)
Epoch: [24][300/616]	Loss 2.3515e-01 (2.2807e-01)	Acc 0.751953 (0.758461)
Epoch: [24][600/616]	Loss 2.2242e-01 (2.2832e-01)	Acc 0.754883 (0.758246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759257)
Training Loss of Epoch 24: 0.22831852351262316
Training Acc of Epoch 24: 0.7582428226626017
Testing Acc of Epoch 24: 0.7592565217391304
Model with the best training loss saved! The loss is 0.22831852351262316
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.1409e-01 (2.1409e-01)	Acc 0.775391 (0.775391)
Epoch: [25][300/616]	Loss 2.2216e-01 (2.2849e-01)	Acc 0.760742 (0.758624)
Epoch: [25][600/616]	Loss 2.1730e-01 (2.2827e-01)	Acc 0.758789 (0.758462)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758591)
Training Loss of Epoch 25: 0.22832838008558848
Training Acc of Epoch 25: 0.7584159044715447
Testing Acc of Epoch 25: 0.7585913043478261
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.1167e-01 (2.1167e-01)	Acc 0.788086 (0.788086)
Epoch: [26][300/616]	Loss 2.4334e-01 (2.2815e-01)	Acc 0.740234 (0.758176)
Epoch: [26][600/616]	Loss 2.1832e-01 (2.2834e-01)	Acc 0.771484 (0.758596)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759309)
Training Loss of Epoch 26: 0.22833487229134009
Training Acc of Epoch 26: 0.7585334095528455
Testing Acc of Epoch 26: 0.759308695652174
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.3335e-01 (2.3335e-01)	Acc 0.737305 (0.737305)
Epoch: [27][300/616]	Loss 2.3359e-01 (2.2806e-01)	Acc 0.759766 (0.759169)
Epoch: [27][600/616]	Loss 2.4136e-01 (2.2805e-01)	Acc 0.738281 (0.758890)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758570)
Training Loss of Epoch 27: 0.22815582793417985
Training Acc of Epoch 27: 0.7587446011178862
Testing Acc of Epoch 27: 0.7585695652173913
Model with the best training loss saved! The loss is 0.22815582793417985
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3184e-01 (2.3184e-01)	Acc 0.745117 (0.745117)
Epoch: [28][300/616]	Loss 2.3534e-01 (2.2859e-01)	Acc 0.750977 (0.758023)
Epoch: [28][600/616]	Loss 2.4134e-01 (2.2830e-01)	Acc 0.745117 (0.758380)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758900)
Training Loss of Epoch 28: 0.22825962549787226
Training Acc of Epoch 28: 0.7584079649390244
Testing Acc of Epoch 28: 0.7589
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.0815e-01 (2.0815e-01)	Acc 0.797852 (0.797852)
Epoch: [29][300/616]	Loss 2.2630e-01 (2.2778e-01)	Acc 0.761719 (0.758929)
Epoch: [29][600/616]	Loss 2.2082e-01 (2.2823e-01)	Acc 0.757812 (0.758700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757861)
Training Loss of Epoch 29: 0.22819369692143385
Training Acc of Epoch 29: 0.75873824949187
Testing Acc of Epoch 29: 0.7578608695652174
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2485e-01 (2.2485e-01)	Acc 0.763672 (0.763672)
Epoch: [30][300/616]	Loss 2.3564e-01 (2.2876e-01)	Acc 0.739258 (0.757423)
Epoch: [30][600/616]	Loss 2.3304e-01 (2.2821e-01)	Acc 0.740234 (0.758105)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758496)
Training Loss of Epoch 30: 0.22824771724096157
Training Acc of Epoch 30: 0.7580268673780488
Testing Acc of Epoch 30: 0.758495652173913
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.0911e-01 (2.0911e-01)	Acc 0.784180 (0.784180)
Epoch: [31][300/616]	Loss 2.2946e-01 (2.2901e-01)	Acc 0.746094 (0.757933)
Epoch: [31][600/616]	Loss 2.3925e-01 (2.2825e-01)	Acc 0.739258 (0.758449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760196)
Training Loss of Epoch 31: 0.22818272705000592
Training Acc of Epoch 31: 0.7585032393292683
Testing Acc of Epoch 31: 0.760195652173913
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.2026e-01 (2.2026e-01)	Acc 0.765625 (0.765625)
Epoch: [32][300/616]	Loss 2.3305e-01 (2.2723e-01)	Acc 0.756836 (0.759354)
Epoch: [32][600/616]	Loss 2.1441e-01 (2.2807e-01)	Acc 0.759766 (0.758683)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757570)
Training Loss of Epoch 32: 0.22805868777317728
Training Acc of Epoch 32: 0.7586731453252032
Testing Acc of Epoch 32: 0.7575695652173913
Model with the best training loss saved! The loss is 0.22805868777317728
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.1705e-01 (2.1705e-01)	Acc 0.764648 (0.764648)
Epoch: [33][300/616]	Loss 2.5258e-01 (2.2817e-01)	Acc 0.725586 (0.758546)
Epoch: [33][600/616]	Loss 2.2586e-01 (2.2802e-01)	Acc 0.760742 (0.758396)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759417)
Training Loss of Epoch 33: 0.22796364886973933
Training Acc of Epoch 33: 0.7585111788617886
Testing Acc of Epoch 33: 0.7594173913043478
Model with the best training loss saved! The loss is 0.22796364886973933
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.1356e-01 (2.1356e-01)	Acc 0.788086 (0.788086)
Epoch: [34][300/616]	Loss 2.1490e-01 (2.2701e-01)	Acc 0.778320 (0.759675)
Epoch: [34][600/616]	Loss 2.2822e-01 (2.2796e-01)	Acc 0.768555 (0.758716)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.757191)
Training Loss of Epoch 34: 0.2279882799803726
Training Acc of Epoch 34: 0.758717606707317
Testing Acc of Epoch 34: 0.7571913043478261
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2933e-01 (2.2933e-01)	Acc 0.740234 (0.740234)
Epoch: [35][300/616]	Loss 2.2905e-01 (2.2763e-01)	Acc 0.748047 (0.759224)
Epoch: [35][600/616]	Loss 2.1596e-01 (2.2785e-01)	Acc 0.774414 (0.758831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759687)
Training Loss of Epoch 35: 0.22789646876536734
Training Acc of Epoch 35: 0.758812881097561
Testing Acc of Epoch 35: 0.7596869565217391
Model with the best training loss saved! The loss is 0.22789646876536734
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.2531e-01 (2.2531e-01)	Acc 0.757812 (0.757812)
Epoch: [36][300/616]	Loss 2.2561e-01 (2.2817e-01)	Acc 0.759766 (0.758539)
Epoch: [36][600/616]	Loss 2.3202e-01 (2.2807e-01)	Acc 0.744141 (0.758656)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759330)
Training Loss of Epoch 36: 0.22801922133298425
Training Acc of Epoch 36: 0.75873824949187
Testing Acc of Epoch 36: 0.7593304347826086
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3862e-01 (2.3862e-01)	Acc 0.750000 (0.750000)
Epoch: [37][300/616]	Loss 2.1582e-01 (2.2779e-01)	Acc 0.775391 (0.758818)
Epoch: [37][600/616]	Loss 2.3244e-01 (2.2809e-01)	Acc 0.757812 (0.758521)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757670)
Training Loss of Epoch 37: 0.2279932090906593
Training Acc of Epoch 37: 0.7586223323170732
Testing Acc of Epoch 37: 0.7576695652173913
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.2744e-01 (2.2744e-01)	Acc 0.765625 (0.765625)
Epoch: [38][300/616]	Loss 2.1674e-01 (2.2864e-01)	Acc 0.778320 (0.758212)
Epoch: [38][600/616]	Loss 2.2455e-01 (2.2804e-01)	Acc 0.757812 (0.758687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.759378)
Training Loss of Epoch 38: 0.22804191892709189
Training Acc of Epoch 38: 0.7586890243902439
Testing Acc of Epoch 38: 0.7593782608695652
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2282e-01 (2.2282e-01)	Acc 0.769531 (0.769531)
Epoch: [39][300/616]	Loss 2.2074e-01 (2.2773e-01)	Acc 0.772461 (0.758757)
Epoch: [39][600/616]	Loss 2.1902e-01 (2.2802e-01)	Acc 0.770508 (0.758449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758230)
Training Loss of Epoch 39: 0.22802465230953403
Training Acc of Epoch 39: 0.7584111407520325
Testing Acc of Epoch 39: 0.7582304347826087
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4171e-01 (2.4171e-01)	Acc 0.731445 (0.731445)
Epoch: [40][300/616]	Loss 2.2992e-01 (2.2850e-01)	Acc 0.761719 (0.758140)
Epoch: [40][600/616]	Loss 2.3343e-01 (2.2783e-01)	Acc 0.746094 (0.758700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759013)
Training Loss of Epoch 40: 0.2277720615146606
Training Acc of Epoch 40: 0.7587144308943089
Testing Acc of Epoch 40: 0.7590130434782608
Model with the best training loss saved! The loss is 0.2277720615146606
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4611e-01 (2.4611e-01)	Acc 0.733398 (0.733398)
Epoch: [41][300/616]	Loss 2.3326e-01 (2.2770e-01)	Acc 0.752930 (0.759075)
Epoch: [41][600/616]	Loss 2.2398e-01 (2.2792e-01)	Acc 0.763672 (0.758865)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759261)
Training Loss of Epoch 41: 0.22797898032316347
Training Acc of Epoch 41: 0.7588097052845528
Testing Acc of Epoch 41: 0.7592608695652174
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.4567e-01 (2.4567e-01)	Acc 0.744141 (0.744141)
Epoch: [42][300/616]	Loss 2.0791e-01 (2.2843e-01)	Acc 0.791016 (0.758147)
Epoch: [42][600/616]	Loss 2.1473e-01 (2.2793e-01)	Acc 0.785156 (0.758721)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759148)
Training Loss of Epoch 42: 0.2279062271118164
Training Acc of Epoch 42: 0.7587779471544716
Testing Acc of Epoch 42: 0.7591478260869565
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3147e-01 (2.3147e-01)	Acc 0.746094 (0.746094)
Epoch: [43][300/616]	Loss 2.2192e-01 (2.2792e-01)	Acc 0.773438 (0.758656)
Epoch: [43][600/616]	Loss 2.2121e-01 (2.2800e-01)	Acc 0.743164 (0.758563)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759978)
Training Loss of Epoch 43: 0.22793102230482953
Training Acc of Epoch 43: 0.7586350355691057
Testing Acc of Epoch 43: 0.7599782608695652
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3096e-01 (2.3096e-01)	Acc 0.750000 (0.750000)
Epoch: [44][300/616]	Loss 2.2292e-01 (2.2787e-01)	Acc 0.759766 (0.758899)
Epoch: [44][600/616]	Loss 2.3828e-01 (2.2780e-01)	Acc 0.742188 (0.758748)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760574)
Training Loss of Epoch 44: 0.22787908630642464
Training Acc of Epoch 44: 0.7586747332317073
Testing Acc of Epoch 44: 0.7605739130434782
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2442e-01 (2.2442e-01)	Acc 0.770508 (0.770508)
Epoch: [45][300/616]	Loss 2.3460e-01 (2.2784e-01)	Acc 0.748047 (0.758478)
Epoch: [45][600/616]	Loss 2.2871e-01 (2.2777e-01)	Acc 0.761719 (0.758986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757348)
Training Loss of Epoch 45: 0.22782632982343193
Training Acc of Epoch 45: 0.7588954522357724
Testing Acc of Epoch 45: 0.7573478260869565
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.2105e-01 (2.2105e-01)	Acc 0.771484 (0.771484)
Epoch: [46][300/616]	Loss 2.1928e-01 (2.2750e-01)	Acc 0.767578 (0.759710)
Epoch: [46][600/616]	Loss 2.4066e-01 (2.2802e-01)	Acc 0.757812 (0.758744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758365)
Training Loss of Epoch 46: 0.2279653148922494
Training Acc of Epoch 46: 0.7588081173780488
Testing Acc of Epoch 46: 0.7583652173913044
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2086e-01 (2.2086e-01)	Acc 0.756836 (0.756836)
Epoch: [47][300/616]	Loss 2.2932e-01 (2.2800e-01)	Acc 0.772461 (0.758364)
Epoch: [47][600/616]	Loss 2.2628e-01 (2.2807e-01)	Acc 0.764648 (0.758313)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.758643)
Training Loss of Epoch 47: 0.22799154175975458
Training Acc of Epoch 47: 0.7584301956300813
Testing Acc of Epoch 47: 0.7586434782608695
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.1849e-01 (2.1849e-01)	Acc 0.768555 (0.768555)
Epoch: [48][300/616]	Loss 2.3537e-01 (2.2731e-01)	Acc 0.737305 (0.759337)
Epoch: [48][600/616]	Loss 2.1137e-01 (2.2776e-01)	Acc 0.781250 (0.758781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758600)
Training Loss of Epoch 48: 0.2277436997347731
Training Acc of Epoch 48: 0.758885924796748
Testing Acc of Epoch 48: 0.7586
Model with the best training loss saved! The loss is 0.2277436997347731
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3279e-01 (2.3279e-01)	Acc 0.747070 (0.747070)
Epoch: [49][300/616]	Loss 2.3068e-01 (2.2757e-01)	Acc 0.756836 (0.759117)
Epoch: [49][600/616]	Loss 2.3348e-01 (2.2774e-01)	Acc 0.746094 (0.758716)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757248)
Training Loss of Epoch 49: 0.2275829367036742
Training Acc of Epoch 49: 0.7589494410569105
Testing Acc of Epoch 49: 0.7572478260869565
Model with the best training loss saved! The loss is 0.2275829367036742
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2647e-01 (2.2647e-01)	Acc 0.760742 (0.760742)
Epoch: [50][300/616]	Loss 2.3380e-01 (2.2699e-01)	Acc 0.749023 (0.759681)
Epoch: [50][600/616]	Loss 2.2375e-01 (2.2791e-01)	Acc 0.773438 (0.758570)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760061)
Training Loss of Epoch 50: 0.22781206672269155
Training Acc of Epoch 50: 0.7586683816056911
Testing Acc of Epoch 50: 0.7600608695652173
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2590e-01 (2.2590e-01)	Acc 0.757812 (0.757812)
Epoch: [51][300/616]	Loss 2.2462e-01 (2.2797e-01)	Acc 0.763672 (0.758711)
Epoch: [51][600/616]	Loss 2.3117e-01 (2.2787e-01)	Acc 0.753906 (0.758558)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760204)
Training Loss of Epoch 51: 0.22774402516159584
Training Acc of Epoch 51: 0.7586858485772358
Testing Acc of Epoch 51: 0.7602043478260869
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.1857e-01 (2.1857e-01)	Acc 0.780273 (0.780273)
Epoch: [52][300/616]	Loss 2.3420e-01 (2.2763e-01)	Acc 0.748047 (0.758968)
Epoch: [52][600/616]	Loss 2.2837e-01 (2.2755e-01)	Acc 0.750977 (0.759121)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760491)
Training Loss of Epoch 52: 0.22759613595842346
Training Acc of Epoch 52: 0.7589875508130082
Testing Acc of Epoch 52: 0.7604913043478261
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.2669e-01 (2.2669e-01)	Acc 0.750000 (0.750000)
Epoch: [53][300/616]	Loss 2.2831e-01 (2.2730e-01)	Acc 0.763672 (0.758964)
Epoch: [53][600/616]	Loss 2.2036e-01 (2.2768e-01)	Acc 0.772461 (0.758851)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758465)
Training Loss of Epoch 53: 0.22771710641985018
Training Acc of Epoch 53: 0.7588366996951219
Testing Acc of Epoch 53: 0.7584652173913043
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.2964e-01 (2.2964e-01)	Acc 0.750977 (0.750977)
Epoch: [54][300/616]	Loss 2.3035e-01 (2.2797e-01)	Acc 0.740234 (0.758708)
Epoch: [54][600/616]	Loss 2.2391e-01 (2.2768e-01)	Acc 0.776367 (0.758976)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758339)
Training Loss of Epoch 54: 0.22773139106064308
Training Acc of Epoch 54: 0.7589383257113821
Testing Acc of Epoch 54: 0.7583391304347826
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.2994e-01 (2.2994e-01)	Acc 0.751953 (0.751953)
Epoch: [55][300/616]	Loss 2.3626e-01 (2.2725e-01)	Acc 0.769531 (0.759633)
Epoch: [55][600/616]	Loss 2.2561e-01 (2.2758e-01)	Acc 0.753906 (0.759090)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760283)
Training Loss of Epoch 55: 0.2276958138235216
Training Acc of Epoch 55: 0.7589224466463415
Testing Acc of Epoch 55: 0.7602826086956522
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2704e-01 (2.2704e-01)	Acc 0.763672 (0.763672)
Epoch: [56][300/616]	Loss 2.3451e-01 (2.2763e-01)	Acc 0.735352 (0.759055)
Epoch: [56][600/616]	Loss 2.1641e-01 (2.2760e-01)	Acc 0.779297 (0.759085)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.758587)
Training Loss of Epoch 56: 0.2276183122299551
Training Acc of Epoch 56: 0.7590510670731707
Testing Acc of Epoch 56: 0.7585869565217391
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3410e-01 (2.3410e-01)	Acc 0.747070 (0.747070)
Epoch: [57][300/616]	Loss 2.2536e-01 (2.2855e-01)	Acc 0.764648 (0.757621)
Epoch: [57][600/616]	Loss 2.2415e-01 (2.2790e-01)	Acc 0.763672 (0.758796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759935)
Training Loss of Epoch 57: 0.22781299146694867
Training Acc of Epoch 57: 0.7589065675813008
Testing Acc of Epoch 57: 0.7599347826086956
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.1858e-01 (2.1858e-01)	Acc 0.766602 (0.766602)
Epoch: [58][300/616]	Loss 2.2661e-01 (2.2781e-01)	Acc 0.760742 (0.758568)
Epoch: [58][600/616]	Loss 2.2180e-01 (2.2760e-01)	Acc 0.760742 (0.758875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.760230)
Training Loss of Epoch 58: 0.22749390413121479
Training Acc of Epoch 58: 0.7590891768292682
Testing Acc of Epoch 58: 0.7602304347826087
Model with the best training loss saved! The loss is 0.22749390413121479
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.2179e-01 (2.2179e-01)	Acc 0.767578 (0.767578)
Epoch: [59][300/616]	Loss 2.2230e-01 (2.2824e-01)	Acc 0.761719 (0.758604)
Epoch: [59][600/616]	Loss 2.4008e-01 (2.2770e-01)	Acc 0.742188 (0.758922)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759852)
Training Loss of Epoch 59: 0.2277166408494236
Training Acc of Epoch 59: 0.7589287982723577
Testing Acc of Epoch 59: 0.7598521739130435
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.2903e-01 (2.2903e-01)	Acc 0.744141 (0.744141)
Epoch: [60][300/616]	Loss 2.4395e-01 (2.2824e-01)	Acc 0.744141 (0.758004)
Epoch: [60][600/616]	Loss 2.1950e-01 (2.2755e-01)	Acc 0.762695 (0.759069)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759713)
Training Loss of Epoch 60: 0.22766428965378582
Training Acc of Epoch 60: 0.7589526168699187
Testing Acc of Epoch 60: 0.7597130434782609
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3798e-01 (2.3798e-01)	Acc 0.738281 (0.738281)
Epoch: [61][300/616]	Loss 2.3125e-01 (2.2761e-01)	Acc 0.758789 (0.758847)
Epoch: [61][600/616]	Loss 2.4307e-01 (2.2774e-01)	Acc 0.746094 (0.758895)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758848)
Training Loss of Epoch 61: 0.22774033124853926
Training Acc of Epoch 61: 0.7589002159552846
Testing Acc of Epoch 61: 0.7588478260869566
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.1207e-01 (2.1207e-01)	Acc 0.776367 (0.776367)
Epoch: [62][300/616]	Loss 2.2455e-01 (2.2731e-01)	Acc 0.752930 (0.759143)
Epoch: [62][600/616]	Loss 2.2812e-01 (2.2752e-01)	Acc 0.765625 (0.758995)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760348)
Training Loss of Epoch 62: 0.22759370391930991
Training Acc of Epoch 62: 0.7587970020325203
Testing Acc of Epoch 62: 0.7603478260869565
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.2563e-01 (2.2563e-01)	Acc 0.766602 (0.766602)
Epoch: [63][300/616]	Loss 2.2866e-01 (2.2736e-01)	Acc 0.747070 (0.758916)
Epoch: [63][600/616]	Loss 2.4232e-01 (2.2778e-01)	Acc 0.738281 (0.758997)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.758713)
Training Loss of Epoch 63: 0.2276456301774436
Training Acc of Epoch 63: 0.7591193470528456
Testing Acc of Epoch 63: 0.7587130434782609
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.2971e-01 (2.2971e-01)	Acc 0.757812 (0.757812)
Epoch: [64][300/616]	Loss 2.4000e-01 (2.2763e-01)	Acc 0.737305 (0.759114)
Epoch: [64][600/616]	Loss 2.2732e-01 (2.2775e-01)	Acc 0.768555 (0.758989)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759861)
Training Loss of Epoch 64: 0.2277852085790014
Training Acc of Epoch 64: 0.7589081554878049
Testing Acc of Epoch 64: 0.7598608695652174
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3429e-01 (2.3429e-01)	Acc 0.748047 (0.748047)
Epoch: [65][300/616]	Loss 2.1642e-01 (2.2756e-01)	Acc 0.775391 (0.758932)
Epoch: [65][600/616]	Loss 2.1852e-01 (2.2774e-01)	Acc 0.767578 (0.758932)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758335)
Training Loss of Epoch 65: 0.22767972645720816
Training Acc of Epoch 65: 0.7590542428861788
Testing Acc of Epoch 65: 0.7583347826086957
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3163e-01 (2.3163e-01)	Acc 0.757812 (0.757812)
Epoch: [66][300/616]	Loss 2.3523e-01 (2.2802e-01)	Acc 0.751953 (0.758056)
Epoch: [66][600/616]	Loss 2.1245e-01 (2.2764e-01)	Acc 0.767578 (0.758799)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759870)
Training Loss of Epoch 66: 0.22766062428311604
Training Acc of Epoch 66: 0.7588351117886178
Testing Acc of Epoch 66: 0.7598695652173914
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3028e-01 (2.3028e-01)	Acc 0.744141 (0.744141)
Epoch: [67][300/616]	Loss 2.3849e-01 (2.2784e-01)	Acc 0.733398 (0.758880)
Epoch: [67][600/616]	Loss 2.2695e-01 (2.2766e-01)	Acc 0.757812 (0.759002)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.759743)
Training Loss of Epoch 67: 0.22771295587706372
Training Acc of Epoch 67: 0.7589415015243902
Testing Acc of Epoch 67: 0.7597434782608695
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.2966e-01 (2.2966e-01)	Acc 0.751953 (0.751953)
Epoch: [68][300/616]	Loss 2.0053e-01 (2.2638e-01)	Acc 0.791992 (0.760123)
Epoch: [68][600/616]	Loss 2.2697e-01 (2.2760e-01)	Acc 0.756836 (0.759119)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.759348)
Training Loss of Epoch 68: 0.22764062285423278
Training Acc of Epoch 68: 0.7590097815040651
Testing Acc of Epoch 68: 0.7593478260869565
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.3642e-01 (2.3642e-01)	Acc 0.757812 (0.757812)
Epoch: [69][300/616]	Loss 2.4032e-01 (2.2757e-01)	Acc 0.757812 (0.759032)
Epoch: [69][600/616]	Loss 2.2652e-01 (2.2764e-01)	Acc 0.778320 (0.759067)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758826)
Training Loss of Epoch 69: 0.22777355400527396
Training Acc of Epoch 69: 0.7588652820121952
Testing Acc of Epoch 69: 0.7588260869565218
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3747e-01 (2.3747e-01)	Acc 0.757812 (0.757812)
Epoch: [70][300/616]	Loss 2.1992e-01 (2.2718e-01)	Acc 0.764648 (0.759467)
Epoch: [70][600/616]	Loss 2.3590e-01 (2.2764e-01)	Acc 0.747070 (0.759046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758970)
Training Loss of Epoch 70: 0.22766665334139413
Training Acc of Epoch 70: 0.7590494791666667
Testing Acc of Epoch 70: 0.7589695652173913
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.2296e-01 (2.2296e-01)	Acc 0.753906 (0.753906)
Epoch: [71][300/616]	Loss 2.2878e-01 (2.2816e-01)	Acc 0.751953 (0.758163)
Epoch: [71][600/616]	Loss 2.3184e-01 (2.2761e-01)	Acc 0.758789 (0.759086)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760104)
Training Loss of Epoch 71: 0.22757296722109724
Training Acc of Epoch 71: 0.7591114075203252
Testing Acc of Epoch 71: 0.7601043478260869
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.0473e-01 (2.0473e-01)	Acc 0.795898 (0.795898)
Epoch: [72][300/616]	Loss 2.2031e-01 (2.2747e-01)	Acc 0.777344 (0.758831)
Epoch: [72][600/616]	Loss 2.0605e-01 (2.2739e-01)	Acc 0.788086 (0.759134)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758835)
Training Loss of Epoch 72: 0.22741274303052483
Training Acc of Epoch 72: 0.7591225228658537
Testing Acc of Epoch 72: 0.7588347826086956
Model with the best training loss saved! The loss is 0.22741274303052483
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.2685e-01 (2.2685e-01)	Acc 0.755859 (0.755859)
Epoch: [73][300/616]	Loss 2.2279e-01 (2.2803e-01)	Acc 0.760742 (0.758250)
Epoch: [73][600/616]	Loss 2.2447e-01 (2.2774e-01)	Acc 0.768555 (0.758974)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759570)
Training Loss of Epoch 73: 0.2277198836570833
Training Acc of Epoch 73: 0.7588938643292683
Testing Acc of Epoch 73: 0.7595695652173913
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.1163e-01 (2.1163e-01)	Acc 0.786133 (0.786133)
Epoch: [74][300/616]	Loss 2.3345e-01 (2.2719e-01)	Acc 0.742188 (0.759240)
Epoch: [74][600/616]	Loss 2.1846e-01 (2.2742e-01)	Acc 0.769531 (0.759249)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.759848)
Training Loss of Epoch 74: 0.2274549976354692
Training Acc of Epoch 74: 0.7591717479674797
Testing Acc of Epoch 74: 0.7598478260869566
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.2989e-01 (2.2989e-01)	Acc 0.764648 (0.764648)
Epoch: [75][300/616]	Loss 2.3107e-01 (2.2681e-01)	Acc 0.748047 (0.759259)
Epoch: [75][600/616]	Loss 2.3860e-01 (2.2632e-01)	Acc 0.736328 (0.760017)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.756410 (0.760491)
Training Loss of Epoch 75: 0.22615601720848705
Training Acc of Epoch 75: 0.7602007113821139
Testing Acc of Epoch 75: 0.7604913043478261
Model with the best training loss saved! The loss is 0.22615601720848705
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3463e-01 (2.3463e-01)	Acc 0.754883 (0.754883)
Epoch: [76][300/616]	Loss 2.3352e-01 (2.2612e-01)	Acc 0.757812 (0.760243)
Epoch: [76][600/616]	Loss 2.1770e-01 (2.2604e-01)	Acc 0.784180 (0.760295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.761130)
Training Loss of Epoch 76: 0.22599085440965203
Training Acc of Epoch 76: 0.7603293318089431
Testing Acc of Epoch 76: 0.7611304347826087
Model with the best training loss saved! The loss is 0.22599085440965203
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.1784e-01 (2.1784e-01)	Acc 0.769531 (0.769531)
Epoch: [77][300/616]	Loss 2.3242e-01 (2.2559e-01)	Acc 0.749023 (0.760823)
Epoch: [77][600/616]	Loss 2.3964e-01 (2.2597e-01)	Acc 0.745117 (0.760282)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760835)
Training Loss of Epoch 77: 0.22596833676826664
Training Acc of Epoch 77: 0.7603198043699188
Testing Acc of Epoch 77: 0.7608347826086956
Model with the best training loss saved! The loss is 0.22596833676826664
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.2406e-01 (2.2406e-01)	Acc 0.750977 (0.750977)
Epoch: [78][300/616]	Loss 2.3030e-01 (2.2624e-01)	Acc 0.759766 (0.760551)
Epoch: [78][600/616]	Loss 2.4284e-01 (2.2598e-01)	Acc 0.741211 (0.760227)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760652)
Training Loss of Epoch 78: 0.2260000969336285
Training Acc of Epoch 78: 0.7602626397357723
Testing Acc of Epoch 78: 0.7606521739130435
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2255e-01 (2.2255e-01)	Acc 0.763672 (0.763672)
Epoch: [79][300/616]	Loss 2.3337e-01 (2.2594e-01)	Acc 0.739258 (0.760369)
Epoch: [79][600/616]	Loss 2.1891e-01 (2.2600e-01)	Acc 0.762695 (0.760203)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.761135)
Training Loss of Epoch 79: 0.2259350198555768
Training Acc of Epoch 79: 0.7603182164634147
Testing Acc of Epoch 79: 0.7611347826086956
Model with the best training loss saved! The loss is 0.2259350198555768
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3128e-01 (2.3128e-01)	Acc 0.755859 (0.755859)
Epoch: [80][300/616]	Loss 2.2772e-01 (2.2548e-01)	Acc 0.759766 (0.760664)
Epoch: [80][600/616]	Loss 2.3021e-01 (2.2599e-01)	Acc 0.754883 (0.760362)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760500)
Training Loss of Epoch 80: 0.22593229645636023
Training Acc of Epoch 80: 0.7604182545731707
Testing Acc of Epoch 80: 0.7605
Model with the best training loss saved! The loss is 0.22593229645636023
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2900e-01 (2.2900e-01)	Acc 0.750000 (0.750000)
Epoch: [81][300/616]	Loss 2.2272e-01 (2.2570e-01)	Acc 0.758789 (0.760648)
Epoch: [81][600/616]	Loss 2.3144e-01 (2.2593e-01)	Acc 0.741211 (0.760388)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760674)
Training Loss of Epoch 81: 0.2259418609423366
Training Acc of Epoch 81: 0.7602912220528455
Testing Acc of Epoch 81: 0.7606739130434783
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.1959e-01 (2.1959e-01)	Acc 0.777344 (0.777344)
Epoch: [82][300/616]	Loss 2.2536e-01 (2.2628e-01)	Acc 0.759766 (0.760256)
Epoch: [82][600/616]	Loss 2.3357e-01 (2.2599e-01)	Acc 0.743164 (0.760180)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.760974)
Training Loss of Epoch 82: 0.22590556910367515
Training Acc of Epoch 82: 0.7603166285569106
Testing Acc of Epoch 82: 0.7609739130434783
Model with the best training loss saved! The loss is 0.22590556910367515
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3008e-01 (2.3008e-01)	Acc 0.746094 (0.746094)
Epoch: [83][300/616]	Loss 2.1429e-01 (2.2610e-01)	Acc 0.775391 (0.760100)
Epoch: [83][600/616]	Loss 2.2606e-01 (2.2595e-01)	Acc 0.760742 (0.760320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.760639)
Training Loss of Epoch 83: 0.2259414058632967
Training Acc of Epoch 83: 0.7603467987804878
Testing Acc of Epoch 83: 0.7606391304347826
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.1620e-01 (2.1620e-01)	Acc 0.773438 (0.773438)
Epoch: [84][300/616]	Loss 2.4172e-01 (2.2599e-01)	Acc 0.742188 (0.760434)
Epoch: [84][600/616]	Loss 2.2324e-01 (2.2603e-01)	Acc 0.764648 (0.760287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.760630)
Training Loss of Epoch 84: 0.22591951050894046
Training Acc of Epoch 84: 0.7604341336382113
Testing Acc of Epoch 84: 0.7606304347826087
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2577e-01 (2.2577e-01)	Acc 0.750977 (0.750977)
Epoch: [85][300/616]	Loss 2.3892e-01 (2.2573e-01)	Acc 0.755859 (0.760453)
Epoch: [85][600/616]	Loss 2.2450e-01 (2.2595e-01)	Acc 0.751953 (0.760342)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.761070)
Training Loss of Epoch 85: 0.22590284393570287
Training Acc of Epoch 85: 0.7603785569105691
Testing Acc of Epoch 85: 0.7610695652173913
Model with the best training loss saved! The loss is 0.22590284393570287
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.2606e-01 (2.2606e-01)	Acc 0.757812 (0.757812)
Epoch: [86][300/616]	Loss 2.3789e-01 (2.2615e-01)	Acc 0.743164 (0.759889)
Epoch: [86][600/616]	Loss 2.2219e-01 (2.2590e-01)	Acc 0.770508 (0.760346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.761065)
Training Loss of Epoch 86: 0.22591040015705233
Training Acc of Epoch 86: 0.7603690294715447
Testing Acc of Epoch 86: 0.7610652173913044
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3186e-01 (2.3186e-01)	Acc 0.748047 (0.748047)
Epoch: [87][300/616]	Loss 2.2956e-01 (2.2566e-01)	Acc 0.756836 (0.761239)
Epoch: [87][600/616]	Loss 2.2625e-01 (2.2601e-01)	Acc 0.760742 (0.760333)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.760996)
Training Loss of Epoch 87: 0.22596841202518805
Training Acc of Epoch 87: 0.7603864964430894
Testing Acc of Epoch 87: 0.7609956521739131
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.1880e-01 (2.1880e-01)	Acc 0.771484 (0.771484)
Epoch: [88][300/616]	Loss 2.1197e-01 (2.2587e-01)	Acc 0.775391 (0.760739)
Epoch: [88][600/616]	Loss 2.2147e-01 (2.2591e-01)	Acc 0.764648 (0.760404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.760474)
Training Loss of Epoch 88: 0.22589427162476672
Training Acc of Epoch 88: 0.7604436610772358
Testing Acc of Epoch 88: 0.7604739130434782
Model with the best training loss saved! The loss is 0.22589427162476672
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.4115e-01 (2.4115e-01)	Acc 0.735352 (0.735352)
Epoch: [89][300/616]	Loss 2.3125e-01 (2.2562e-01)	Acc 0.750977 (0.760973)
Epoch: [89][600/616]	Loss 2.2734e-01 (2.2601e-01)	Acc 0.764648 (0.760242)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.761383)
Training Loss of Epoch 89: 0.22590313263055756
Training Acc of Epoch 89: 0.7604373094512196
Testing Acc of Epoch 89: 0.7613826086956522
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.1729e-01 (2.1729e-01)	Acc 0.765625 (0.765625)
Epoch: [90][300/616]	Loss 2.3056e-01 (2.2649e-01)	Acc 0.760742 (0.759672)
Epoch: [90][600/616]	Loss 2.0999e-01 (2.2598e-01)	Acc 0.786133 (0.760471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.760896)
Training Loss of Epoch 90: 0.22589607527100944
Training Acc of Epoch 90: 0.7604944740853659
Testing Acc of Epoch 90: 0.7608956521739131
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2526e-01 (2.2526e-01)	Acc 0.764648 (0.764648)
Epoch: [91][300/616]	Loss 2.2843e-01 (2.2610e-01)	Acc 0.737305 (0.760191)
Epoch: [91][600/616]	Loss 2.1486e-01 (2.2590e-01)	Acc 0.768555 (0.760342)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760870)
Training Loss of Epoch 91: 0.22588756704718116
Training Acc of Epoch 91: 0.7603642657520325
Testing Acc of Epoch 91: 0.7608695652173914
Model with the best training loss saved! The loss is 0.22588756704718116
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2597e-01 (2.2597e-01)	Acc 0.752930 (0.752930)
Epoch: [92][300/616]	Loss 2.1631e-01 (2.2619e-01)	Acc 0.766602 (0.760054)
Epoch: [92][600/616]	Loss 2.1810e-01 (2.2598e-01)	Acc 0.776367 (0.760217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.761409)
Training Loss of Epoch 92: 0.22594239639072883
Training Acc of Epoch 92: 0.7602578760162602
Testing Acc of Epoch 92: 0.761408695652174
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3022e-01 (2.3022e-01)	Acc 0.769531 (0.769531)
Epoch: [93][300/616]	Loss 2.1958e-01 (2.2566e-01)	Acc 0.755859 (0.760950)
Epoch: [93][600/616]	Loss 2.2625e-01 (2.2589e-01)	Acc 0.758789 (0.760560)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.761248)
Training Loss of Epoch 93: 0.22582645525292652
Training Acc of Epoch 93: 0.7606612042682926
Testing Acc of Epoch 93: 0.7612478260869565
Model with the best training loss saved! The loss is 0.22582645525292652
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3882e-01 (2.3882e-01)	Acc 0.735352 (0.735352)
Epoch: [94][300/616]	Loss 2.1831e-01 (2.2615e-01)	Acc 0.775391 (0.760181)
Epoch: [94][600/616]	Loss 2.2774e-01 (2.2589e-01)	Acc 0.749023 (0.760602)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.760557)
Training Loss of Epoch 94: 0.22584790262749524
Training Acc of Epoch 94: 0.7606389735772358
Testing Acc of Epoch 94: 0.7605565217391305
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3521e-01 (2.3521e-01)	Acc 0.762695 (0.762695)
Epoch: [95][300/616]	Loss 2.3516e-01 (2.2610e-01)	Acc 0.749023 (0.760655)
Epoch: [95][600/616]	Loss 2.1820e-01 (2.2588e-01)	Acc 0.765625 (0.760407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.761235)
Training Loss of Epoch 95: 0.22587744344056138
Training Acc of Epoch 95: 0.7604230182926829
Testing Acc of Epoch 95: 0.7612347826086957
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.1465e-01 (2.1465e-01)	Acc 0.779297 (0.779297)
Epoch: [96][300/616]	Loss 2.2243e-01 (2.2646e-01)	Acc 0.756836 (0.759328)
Epoch: [96][600/616]	Loss 2.3250e-01 (2.2581e-01)	Acc 0.757812 (0.760287)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.760983)
Training Loss of Epoch 96: 0.22582775296234503
Training Acc of Epoch 96: 0.7602864583333333
Testing Acc of Epoch 96: 0.7609826086956522
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2605e-01 (2.2605e-01)	Acc 0.764648 (0.764648)
Epoch: [97][300/616]	Loss 2.2138e-01 (2.2555e-01)	Acc 0.766602 (0.760966)
Epoch: [97][600/616]	Loss 2.2770e-01 (2.2587e-01)	Acc 0.748047 (0.760320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.760817)
Training Loss of Epoch 97: 0.22585513121713469
Training Acc of Epoch 97: 0.7603626778455285
Testing Acc of Epoch 97: 0.7608173913043478
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.1792e-01 (2.1792e-01)	Acc 0.769531 (0.769531)
Epoch: [98][300/616]	Loss 2.1661e-01 (2.2562e-01)	Acc 0.765625 (0.761151)
Epoch: [98][600/616]	Loss 2.3580e-01 (2.2578e-01)	Acc 0.747070 (0.760768)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.761448)
Training Loss of Epoch 98: 0.2258498960151905
Training Acc of Epoch 98: 0.7605929242886179
Testing Acc of Epoch 98: 0.7614478260869565
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3966e-01 (2.3966e-01)	Acc 0.748047 (0.748047)
Epoch: [99][300/616]	Loss 2.1780e-01 (2.2522e-01)	Acc 0.764648 (0.761631)
Epoch: [99][600/616]	Loss 2.2923e-01 (2.2580e-01)	Acc 0.757812 (0.760628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.761200)
Training Loss of Epoch 99: 0.22583206761174085
Training Acc of Epoch 99: 0.7605722815040651
Testing Acc of Epoch 99: 0.7612
Early stopping not satisfied.
