train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_9b
different_width False
resnet18_width 64
weight_precision 9
bias_precision 9
act_precision 12
batch_norm False
dropout False
exp_num 5
lr 0.003125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.003125/lr_decay/JT_9b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_9b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=12, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=9, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=12, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=9, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=12, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=9, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=12, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=9, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.003125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9850e-01 (4.9850e-01)	Acc 0.196289 (0.196289)
Epoch: [0][300/616]	Loss 2.6213e-01 (2.9362e-01)	Acc 0.726562 (0.687383)
Epoch: [0][600/616]	Loss 2.3815e-01 (2.7137e-01)	Acc 0.750000 (0.713737)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744952)
Training Loss of Epoch 0: 0.27068271217792017
Training Acc of Epoch 0: 0.714548399390244
Testing Acc of Epoch 0: 0.7449521739130435
Model with the best training loss saved! The loss is 0.27068271217792017
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.2860e-01 (2.2860e-01)	Acc 0.768555 (0.768555)
Epoch: [1][300/616]	Loss 2.3787e-01 (2.4113e-01)	Acc 0.747070 (0.746808)
Epoch: [1][600/616]	Loss 2.4301e-01 (2.3879e-01)	Acc 0.735352 (0.749088)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753070)
Training Loss of Epoch 1: 0.23863381820965587
Training Acc of Epoch 1: 0.7492044588414634
Testing Acc of Epoch 1: 0.7530695652173913
Model with the best training loss saved! The loss is 0.23863381820965587
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3505e-01 (2.3505e-01)	Acc 0.751953 (0.751953)
Epoch: [2][300/616]	Loss 2.2810e-01 (2.3515e-01)	Acc 0.768555 (0.752424)
Epoch: [2][600/616]	Loss 2.2301e-01 (2.3473e-01)	Acc 0.779297 (0.752535)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751104)
Training Loss of Epoch 2: 0.23468881518375584
Training Acc of Epoch 2: 0.752661331300813
Testing Acc of Epoch 2: 0.7511043478260869
Model with the best training loss saved! The loss is 0.23468881518375584
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3407e-01 (2.3407e-01)	Acc 0.751953 (0.751953)
Epoch: [3][300/616]	Loss 2.2948e-01 (2.3321e-01)	Acc 0.755859 (0.754617)
Epoch: [3][600/616]	Loss 2.3187e-01 (2.3311e-01)	Acc 0.743164 (0.753919)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755487)
Training Loss of Epoch 3: 0.23304212701514485
Training Acc of Epoch 3: 0.7539951727642277
Testing Acc of Epoch 3: 0.7554869565217391
Model with the best training loss saved! The loss is 0.23304212701514485
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.2549e-01 (2.2549e-01)	Acc 0.767578 (0.767578)
Epoch: [4][300/616]	Loss 2.2604e-01 (2.3167e-01)	Acc 0.771484 (0.755950)
Epoch: [4][600/616]	Loss 2.1938e-01 (2.3183e-01)	Acc 0.774414 (0.755391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757026)
Training Loss of Epoch 4: 0.23197744273073306
Training Acc of Epoch 4: 0.7552289761178862
Testing Acc of Epoch 4: 0.7570260869565217
Model with the best training loss saved! The loss is 0.23197744273073306
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3484e-01 (2.3484e-01)	Acc 0.756836 (0.756836)
Epoch: [5][300/616]	Loss 2.4379e-01 (2.3307e-01)	Acc 0.740234 (0.753939)
Epoch: [5][600/616]	Loss 2.3786e-01 (2.3185e-01)	Acc 0.734375 (0.755414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757926)
Training Loss of Epoch 5: 0.23180350176687162
Training Acc of Epoch 5: 0.7555386178861788
Testing Acc of Epoch 5: 0.7579260869565217
Model with the best training loss saved! The loss is 0.23180350176687162
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.2744e-01 (2.2744e-01)	Acc 0.754883 (0.754883)
Epoch: [6][300/616]	Loss 2.2978e-01 (2.3126e-01)	Acc 0.752930 (0.756171)
Epoch: [6][600/616]	Loss 2.3860e-01 (2.3128e-01)	Acc 0.750000 (0.755804)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.756683)
Training Loss of Epoch 6: 0.23124979661732184
Training Acc of Epoch 6: 0.7558133257113822
Testing Acc of Epoch 6: 0.7566826086956522
Model with the best training loss saved! The loss is 0.23124979661732184
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.1425e-01 (2.1425e-01)	Acc 0.782227 (0.782227)
Epoch: [7][300/616]	Loss 2.2696e-01 (2.3086e-01)	Acc 0.770508 (0.756486)
Epoch: [7][600/616]	Loss 2.1792e-01 (2.3110e-01)	Acc 0.761719 (0.756142)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.757226)
Training Loss of Epoch 7: 0.23113366033488172
Training Acc of Epoch 7: 0.7560689786585366
Testing Acc of Epoch 7: 0.7572260869565217
Model with the best training loss saved! The loss is 0.23113366033488172
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3292e-01 (2.3292e-01)	Acc 0.750977 (0.750977)
Epoch: [8][300/616]	Loss 2.3172e-01 (2.3134e-01)	Acc 0.757812 (0.755253)
Epoch: [8][600/616]	Loss 2.4115e-01 (2.3094e-01)	Acc 0.743164 (0.756062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.757252)
Training Loss of Epoch 8: 0.2309329334071012
Training Acc of Epoch 8: 0.7560610391260163
Testing Acc of Epoch 8: 0.7572521739130434
Model with the best training loss saved! The loss is 0.2309329334071012
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.2516e-01 (2.2516e-01)	Acc 0.762695 (0.762695)
Epoch: [9][300/616]	Loss 2.2973e-01 (2.3095e-01)	Acc 0.758789 (0.756434)
Epoch: [9][600/616]	Loss 2.2638e-01 (2.3102e-01)	Acc 0.768555 (0.756140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756713)
Training Loss of Epoch 9: 0.231038109073794
Training Acc of Epoch 9: 0.7561070884146341
Testing Acc of Epoch 9: 0.7567130434782608
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.1263e-01 (2.1263e-01)	Acc 0.779297 (0.779297)
Epoch: [10][300/616]	Loss 2.3530e-01 (2.2996e-01)	Acc 0.736328 (0.756946)
Epoch: [10][600/616]	Loss 2.3434e-01 (2.3055e-01)	Acc 0.757812 (0.756600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754809)
Training Loss of Epoch 10: 0.23056918896310696
Training Acc of Epoch 10: 0.7565262957317073
Testing Acc of Epoch 10: 0.7548086956521739
Model with the best training loss saved! The loss is 0.23056918896310696
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5053e-01 (2.5053e-01)	Acc 0.745117 (0.745117)
Epoch: [11][300/616]	Loss 2.2919e-01 (2.3071e-01)	Acc 0.764648 (0.756878)
Epoch: [11][600/616]	Loss 2.4223e-01 (2.3064e-01)	Acc 0.737305 (0.756730)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757278)
Training Loss of Epoch 11: 0.23058281533601807
Training Acc of Epoch 11: 0.756786712398374
Testing Acc of Epoch 11: 0.7572782608695652
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4013e-01 (2.4013e-01)	Acc 0.743164 (0.743164)
Epoch: [12][300/616]	Loss 2.4233e-01 (2.3079e-01)	Acc 0.735352 (0.756479)
Epoch: [12][600/616]	Loss 2.3164e-01 (2.3059e-01)	Acc 0.750977 (0.756553)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754809)
Training Loss of Epoch 12: 0.2305956458900033
Training Acc of Epoch 12: 0.7565707571138212
Testing Acc of Epoch 12: 0.7548086956521739
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.2928e-01 (2.2928e-01)	Acc 0.765625 (0.765625)
Epoch: [13][300/616]	Loss 2.3688e-01 (2.2996e-01)	Acc 0.753906 (0.756752)
Epoch: [13][600/616]	Loss 2.1289e-01 (2.3028e-01)	Acc 0.780273 (0.756933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754952)
Training Loss of Epoch 13: 0.230368455299517
Training Acc of Epoch 13: 0.7568279979674797
Testing Acc of Epoch 13: 0.7549521739130435
Model with the best training loss saved! The loss is 0.230368455299517
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2633e-01 (2.2633e-01)	Acc 0.751953 (0.751953)
Epoch: [14][300/616]	Loss 2.2197e-01 (2.3080e-01)	Acc 0.758789 (0.756463)
Epoch: [14][600/616]	Loss 2.3375e-01 (2.3052e-01)	Acc 0.738281 (0.756646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757574)
Training Loss of Epoch 14: 0.23050631054533208
Training Acc of Epoch 14: 0.7566723831300813
Testing Acc of Epoch 14: 0.7575739130434782
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3023e-01 (2.3023e-01)	Acc 0.755859 (0.755859)
Epoch: [15][300/616]	Loss 2.2214e-01 (2.2964e-01)	Acc 0.757812 (0.757180)
Epoch: [15][600/616]	Loss 2.3157e-01 (2.3035e-01)	Acc 0.755859 (0.756626)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757491)
Training Loss of Epoch 15: 0.23024188204994048
Training Acc of Epoch 15: 0.7567803607723578
Testing Acc of Epoch 15: 0.7574913043478261
Model with the best training loss saved! The loss is 0.23024188204994048
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2013e-01 (2.2013e-01)	Acc 0.773438 (0.773438)
Epoch: [16][300/616]	Loss 2.3126e-01 (2.2945e-01)	Acc 0.754883 (0.758072)
Epoch: [16][600/616]	Loss 2.4203e-01 (2.3023e-01)	Acc 0.750977 (0.756977)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758096)
Training Loss of Epoch 16: 0.23020979931684044
Training Acc of Epoch 16: 0.7570900025406504
Testing Acc of Epoch 16: 0.7580956521739131
Model with the best training loss saved! The loss is 0.23020979931684044
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3021e-01 (2.3021e-01)	Acc 0.750000 (0.750000)
Epoch: [17][300/616]	Loss 2.3076e-01 (2.3061e-01)	Acc 0.759766 (0.756258)
Epoch: [17][600/616]	Loss 2.3125e-01 (2.3040e-01)	Acc 0.759766 (0.756916)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758100)
Training Loss of Epoch 17: 0.2302775585796775
Training Acc of Epoch 17: 0.7570931783536585
Testing Acc of Epoch 17: 0.7581
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3106e-01 (2.3106e-01)	Acc 0.752930 (0.752930)
Epoch: [18][300/616]	Loss 2.3403e-01 (2.2985e-01)	Acc 0.764648 (0.757303)
Epoch: [18][600/616]	Loss 2.2659e-01 (2.3019e-01)	Acc 0.760742 (0.757180)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.758139)
Training Loss of Epoch 18: 0.2301868261845131
Training Acc of Epoch 18: 0.7571551067073171
Testing Acc of Epoch 18: 0.7581391304347826
Model with the best training loss saved! The loss is 0.2301868261845131
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3220e-01 (2.3220e-01)	Acc 0.763672 (0.763672)
Epoch: [19][300/616]	Loss 2.3554e-01 (2.2958e-01)	Acc 0.747070 (0.757482)
Epoch: [19][600/616]	Loss 2.3010e-01 (2.3017e-01)	Acc 0.759766 (0.757037)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758539)
Training Loss of Epoch 19: 0.23014594493358115
Training Acc of Epoch 19: 0.7570630081300813
Testing Acc of Epoch 19: 0.7585391304347826
Model with the best training loss saved! The loss is 0.23014594493358115
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.3623e-01 (2.3623e-01)	Acc 0.742188 (0.742188)
Epoch: [20][300/616]	Loss 2.2375e-01 (2.2995e-01)	Acc 0.765625 (0.756245)
Epoch: [20][600/616]	Loss 2.3141e-01 (2.3003e-01)	Acc 0.758789 (0.756920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756622)
Training Loss of Epoch 20: 0.23004728638059724
Training Acc of Epoch 20: 0.756859756097561
Testing Acc of Epoch 20: 0.7566217391304347
Model with the best training loss saved! The loss is 0.23004728638059724
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3291e-01 (2.3291e-01)	Acc 0.755859 (0.755859)
Epoch: [21][300/616]	Loss 2.1010e-01 (2.3093e-01)	Acc 0.774414 (0.755486)
Epoch: [21][600/616]	Loss 2.2494e-01 (2.2999e-01)	Acc 0.755859 (0.757081)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757461)
Training Loss of Epoch 21: 0.2300286008332803
Training Acc of Epoch 21: 0.7571201727642276
Testing Acc of Epoch 21: 0.7574608695652174
Model with the best training loss saved! The loss is 0.2300286008332803
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3918e-01 (2.3918e-01)	Acc 0.756836 (0.756836)
Epoch: [22][300/616]	Loss 2.3993e-01 (2.2952e-01)	Acc 0.746094 (0.757923)
Epoch: [22][600/616]	Loss 2.1758e-01 (2.2977e-01)	Acc 0.770508 (0.757179)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.753205 (0.756000)
Training Loss of Epoch 22: 0.22975517428987394
Training Acc of Epoch 22: 0.7572265625
Testing Acc of Epoch 22: 0.756
Model with the best training loss saved! The loss is 0.22975517428987394
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3886e-01 (2.3886e-01)	Acc 0.749023 (0.749023)
Epoch: [23][300/616]	Loss 2.3181e-01 (2.2949e-01)	Acc 0.758789 (0.758027)
Epoch: [23][600/616]	Loss 2.3267e-01 (2.2995e-01)	Acc 0.746094 (0.757312)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.757143)
Training Loss of Epoch 23: 0.22997984515457617
Training Acc of Epoch 23: 0.7572487931910569
Testing Acc of Epoch 23: 0.7571434782608696
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2058e-01 (2.2058e-01)	Acc 0.777344 (0.777344)
Epoch: [24][300/616]	Loss 2.2505e-01 (2.2915e-01)	Acc 0.764648 (0.757641)
Epoch: [24][600/616]	Loss 2.2806e-01 (2.2991e-01)	Acc 0.761719 (0.757314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.755996)
Training Loss of Epoch 24: 0.2298844937628847
Training Acc of Epoch 24: 0.7573456554878049
Testing Acc of Epoch 24: 0.7559956521739131
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3400e-01 (2.3400e-01)	Acc 0.764648 (0.764648)
Epoch: [25][300/616]	Loss 2.3420e-01 (2.2967e-01)	Acc 0.760742 (0.757751)
Epoch: [25][600/616]	Loss 2.3148e-01 (2.2981e-01)	Acc 0.770508 (0.757359)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757970)
Training Loss of Epoch 25: 0.22981201175267135
Training Acc of Epoch 25: 0.757348831300813
Testing Acc of Epoch 25: 0.7579695652173913
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3651e-01 (2.3651e-01)	Acc 0.754883 (0.754883)
Epoch: [26][300/616]	Loss 2.1986e-01 (2.3000e-01)	Acc 0.766602 (0.757767)
Epoch: [26][600/616]	Loss 2.2633e-01 (2.2979e-01)	Acc 0.771484 (0.757375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.757104)
Training Loss of Epoch 26: 0.22983910540739697
Training Acc of Epoch 26: 0.757325012703252
Testing Acc of Epoch 26: 0.7571043478260869
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.3395e-01 (2.3395e-01)	Acc 0.743164 (0.743164)
Epoch: [27][300/616]	Loss 2.3063e-01 (2.2994e-01)	Acc 0.758789 (0.757138)
Epoch: [27][600/616]	Loss 2.3504e-01 (2.3000e-01)	Acc 0.752930 (0.757068)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.753205 (0.758165)
Training Loss of Epoch 27: 0.22998153678769986
Training Acc of Epoch 27: 0.7571106453252032
Testing Acc of Epoch 27: 0.7581652173913044
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4082e-01 (2.4082e-01)	Acc 0.732422 (0.732422)
Epoch: [28][300/616]	Loss 2.2614e-01 (2.2986e-01)	Acc 0.753906 (0.756687)
Epoch: [28][600/616]	Loss 2.4717e-01 (2.2958e-01)	Acc 0.743164 (0.757442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757230)
Training Loss of Epoch 28: 0.2295619858958857
Training Acc of Epoch 28: 0.7574917428861788
Testing Acc of Epoch 28: 0.7572304347826087
Model with the best training loss saved! The loss is 0.2295619858958857
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.2770e-01 (2.2770e-01)	Acc 0.753906 (0.753906)
Epoch: [29][300/616]	Loss 2.2812e-01 (2.3004e-01)	Acc 0.761719 (0.756739)
Epoch: [29][600/616]	Loss 2.2770e-01 (2.2969e-01)	Acc 0.759766 (0.757169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756526)
Training Loss of Epoch 29: 0.22970165171758916
Training Acc of Epoch 29: 0.7571455792682927
Testing Acc of Epoch 29: 0.7565260869565218
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2435e-01 (2.2435e-01)	Acc 0.758789 (0.758789)
Epoch: [30][300/616]	Loss 2.3102e-01 (2.2930e-01)	Acc 0.753906 (0.757456)
Epoch: [30][600/616]	Loss 2.3442e-01 (2.2942e-01)	Acc 0.750977 (0.757920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758848)
Training Loss of Epoch 30: 0.2293697935536625
Training Acc of Epoch 30: 0.7579204776422764
Testing Acc of Epoch 30: 0.7588478260869566
Model with the best training loss saved! The loss is 0.2293697935536625
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3163e-01 (2.3163e-01)	Acc 0.752930 (0.752930)
Epoch: [31][300/616]	Loss 2.4264e-01 (2.3032e-01)	Acc 0.746094 (0.756943)
Epoch: [31][600/616]	Loss 2.4168e-01 (2.2969e-01)	Acc 0.738281 (0.757284)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758239)
Training Loss of Epoch 31: 0.2296985339343063
Training Acc of Epoch 31: 0.7573424796747967
Testing Acc of Epoch 31: 0.7582391304347826
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.2768e-01 (2.2768e-01)	Acc 0.755859 (0.755859)
Epoch: [32][300/616]	Loss 2.3050e-01 (2.2954e-01)	Acc 0.751953 (0.757540)
Epoch: [32][600/616]	Loss 2.1192e-01 (2.2975e-01)	Acc 0.791992 (0.757323)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756687)
Training Loss of Epoch 32: 0.22977115599120534
Training Acc of Epoch 32: 0.7573377159552845
Testing Acc of Epoch 32: 0.7566869565217391
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.3180e-01 (2.3180e-01)	Acc 0.763672 (0.763672)
Epoch: [33][300/616]	Loss 2.1708e-01 (2.2948e-01)	Acc 0.775391 (0.757472)
Epoch: [33][600/616]	Loss 2.2311e-01 (2.2968e-01)	Acc 0.769531 (0.757436)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758457)
Training Loss of Epoch 33: 0.22962892695171078
Training Acc of Epoch 33: 0.7574980945121951
Testing Acc of Epoch 33: 0.7584565217391305
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4564e-01 (2.4564e-01)	Acc 0.750977 (0.750977)
Epoch: [34][300/616]	Loss 2.2234e-01 (2.2881e-01)	Acc 0.757812 (0.758283)
Epoch: [34][600/616]	Loss 2.1609e-01 (2.2962e-01)	Acc 0.771484 (0.757320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756296)
Training Loss of Epoch 34: 0.2296763355896725
Training Acc of Epoch 34: 0.7572186229674797
Testing Acc of Epoch 34: 0.756295652173913
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3008e-01 (2.3008e-01)	Acc 0.754883 (0.754883)
Epoch: [35][300/616]	Loss 2.2115e-01 (2.2964e-01)	Acc 0.758789 (0.757569)
Epoch: [35][600/616]	Loss 2.2201e-01 (2.2954e-01)	Acc 0.763672 (0.757553)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.758365)
Training Loss of Epoch 35: 0.2295259314823926
Training Acc of Epoch 35: 0.757567962398374
Testing Acc of Epoch 35: 0.7583652173913044
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.2434e-01 (2.2434e-01)	Acc 0.757812 (0.757812)
Epoch: [36][300/616]	Loss 2.2419e-01 (2.3028e-01)	Acc 0.764648 (0.756375)
Epoch: [36][600/616]	Loss 2.5471e-01 (2.2956e-01)	Acc 0.731445 (0.757543)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755748)
Training Loss of Epoch 36: 0.22958218731531282
Training Acc of Epoch 36: 0.7576044842479674
Testing Acc of Epoch 36: 0.7557478260869566
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.2247e-01 (2.2247e-01)	Acc 0.780273 (0.780273)
Epoch: [37][300/616]	Loss 2.3581e-01 (2.2896e-01)	Acc 0.757812 (0.758095)
Epoch: [37][600/616]	Loss 2.1392e-01 (2.2948e-01)	Acc 0.781250 (0.757419)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758709)
Training Loss of Epoch 37: 0.22950278711997396
Training Acc of Epoch 37: 0.7573869410569106
Testing Acc of Epoch 37: 0.7587086956521739
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.2143e-01 (2.2143e-01)	Acc 0.765625 (0.765625)
Epoch: [38][300/616]	Loss 2.3220e-01 (2.2984e-01)	Acc 0.768555 (0.757420)
Epoch: [38][600/616]	Loss 2.1864e-01 (2.2957e-01)	Acc 0.760742 (0.757267)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757670)
Training Loss of Epoch 38: 0.2295398205034132
Training Acc of Epoch 38: 0.757348831300813
Testing Acc of Epoch 38: 0.7576695652173913
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2394e-01 (2.2394e-01)	Acc 0.754883 (0.754883)
Epoch: [39][300/616]	Loss 2.1958e-01 (2.3006e-01)	Acc 0.775391 (0.756648)
Epoch: [39][600/616]	Loss 2.3123e-01 (2.2935e-01)	Acc 0.746094 (0.757702)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.757148)
Training Loss of Epoch 39: 0.22946901074269924
Training Acc of Epoch 39: 0.757518737296748
Testing Acc of Epoch 39: 0.7571478260869565
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.1995e-01 (2.1995e-01)	Acc 0.773438 (0.773438)
Epoch: [40][300/616]	Loss 2.2778e-01 (2.2961e-01)	Acc 0.767578 (0.757472)
Epoch: [40][600/616]	Loss 2.2479e-01 (2.2953e-01)	Acc 0.761719 (0.757595)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756717)
Training Loss of Epoch 40: 0.22958251906604302
Training Acc of Epoch 40: 0.757517149390244
Testing Acc of Epoch 40: 0.7567173913043478
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.1783e-01 (2.1783e-01)	Acc 0.783203 (0.783203)
Epoch: [41][300/616]	Loss 2.2921e-01 (2.2909e-01)	Acc 0.765625 (0.758147)
Epoch: [41][600/616]	Loss 2.2095e-01 (2.2942e-01)	Acc 0.763672 (0.757575)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759165)
Training Loss of Epoch 41: 0.22943892885998982
Training Acc of Epoch 41: 0.7576743521341464
Testing Acc of Epoch 41: 0.7591652173913044
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.2917e-01 (2.2917e-01)	Acc 0.754883 (0.754883)
Epoch: [42][300/616]	Loss 2.2666e-01 (2.2873e-01)	Acc 0.770508 (0.758416)
Epoch: [42][600/616]	Loss 2.3424e-01 (2.2912e-01)	Acc 0.755859 (0.758116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758704)
Training Loss of Epoch 42: 0.22914626918187955
Training Acc of Epoch 42: 0.7580522738821138
Testing Acc of Epoch 42: 0.758704347826087
Model with the best training loss saved! The loss is 0.22914626918187955
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.2012e-01 (2.2012e-01)	Acc 0.761719 (0.761719)
Epoch: [43][300/616]	Loss 2.4192e-01 (2.3016e-01)	Acc 0.744141 (0.756294)
Epoch: [43][600/616]	Loss 2.4003e-01 (2.2947e-01)	Acc 0.741211 (0.757338)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758074)
Training Loss of Epoch 43: 0.22949863960587882
Training Acc of Epoch 43: 0.7572837271341464
Testing Acc of Epoch 43: 0.7580739130434783
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3891e-01 (2.3891e-01)	Acc 0.742188 (0.742188)
Epoch: [44][300/616]	Loss 2.3245e-01 (2.2938e-01)	Acc 0.757812 (0.757235)
Epoch: [44][600/616]	Loss 2.1712e-01 (2.2944e-01)	Acc 0.778320 (0.757361)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758339)
Training Loss of Epoch 44: 0.2294073179485352
Training Acc of Epoch 44: 0.7574298145325203
Testing Acc of Epoch 44: 0.7583391304347826
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2207e-01 (2.2207e-01)	Acc 0.766602 (0.766602)
Epoch: [45][300/616]	Loss 2.3238e-01 (2.2915e-01)	Acc 0.765625 (0.757660)
Epoch: [45][600/616]	Loss 2.4095e-01 (2.2932e-01)	Acc 0.752930 (0.757562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.757426)
Training Loss of Epoch 45: 0.22940259580689717
Training Acc of Epoch 45: 0.7575092098577236
Testing Acc of Epoch 45: 0.7574260869565217
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4117e-01 (2.4117e-01)	Acc 0.742188 (0.742188)
Epoch: [46][300/616]	Loss 2.3270e-01 (2.2984e-01)	Acc 0.752930 (0.757420)
Epoch: [46][600/616]	Loss 2.3354e-01 (2.2936e-01)	Acc 0.746094 (0.757811)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758300)
Training Loss of Epoch 46: 0.22936268443983746
Training Acc of Epoch 46: 0.7577918572154472
Testing Acc of Epoch 46: 0.7583
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2930e-01 (2.2930e-01)	Acc 0.767578 (0.767578)
Epoch: [47][300/616]	Loss 2.2402e-01 (2.2914e-01)	Acc 0.771484 (0.758143)
Epoch: [47][600/616]	Loss 2.1630e-01 (2.2931e-01)	Acc 0.782227 (0.757795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758226)
Training Loss of Epoch 47: 0.22935362133553358
Training Acc of Epoch 47: 0.7576711763211382
Testing Acc of Epoch 47: 0.7582260869565217
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.2130e-01 (2.2130e-01)	Acc 0.762695 (0.762695)
Epoch: [48][300/616]	Loss 2.1632e-01 (2.2875e-01)	Acc 0.771484 (0.757761)
Epoch: [48][600/616]	Loss 2.3073e-01 (2.2929e-01)	Acc 0.754883 (0.757666)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757974)
Training Loss of Epoch 48: 0.22928791634920168
Training Acc of Epoch 48: 0.7576632367886179
Testing Acc of Epoch 48: 0.7579739130434783
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.2522e-01 (2.2522e-01)	Acc 0.757812 (0.757812)
Epoch: [49][300/616]	Loss 2.2877e-01 (2.2944e-01)	Acc 0.755859 (0.757452)
Epoch: [49][600/616]	Loss 2.2008e-01 (2.2933e-01)	Acc 0.773438 (0.757621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.758757)
Training Loss of Epoch 49: 0.22933694112107036
Training Acc of Epoch 49: 0.7575854293699187
Testing Acc of Epoch 49: 0.7587565217391304
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2456e-01 (2.2456e-01)	Acc 0.762695 (0.762695)
Epoch: [50][300/616]	Loss 2.2803e-01 (2.2891e-01)	Acc 0.754883 (0.757949)
Epoch: [50][600/616]	Loss 2.3924e-01 (2.2898e-01)	Acc 0.743164 (0.757980)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756639)
Training Loss of Epoch 50: 0.2290905635773651
Training Acc of Epoch 50: 0.757861725101626
Testing Acc of Epoch 50: 0.7566391304347826
Model with the best training loss saved! The loss is 0.2290905635773651
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.1933e-01 (2.1933e-01)	Acc 0.770508 (0.770508)
Epoch: [51][300/616]	Loss 2.3378e-01 (2.2966e-01)	Acc 0.748047 (0.758053)
Epoch: [51][600/616]	Loss 2.3083e-01 (2.2930e-01)	Acc 0.764648 (0.757902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758939)
Training Loss of Epoch 51: 0.2293296165340315
Training Acc of Epoch 51: 0.7578172637195122
Testing Acc of Epoch 51: 0.7589391304347826
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4084e-01 (2.4084e-01)	Acc 0.743164 (0.743164)
Epoch: [52][300/616]	Loss 2.2919e-01 (2.2871e-01)	Acc 0.757812 (0.758212)
Epoch: [52][600/616]	Loss 2.1599e-01 (2.2912e-01)	Acc 0.779297 (0.757661)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.758139)
Training Loss of Epoch 52: 0.22921198936012702
Training Acc of Epoch 52: 0.757569550304878
Testing Acc of Epoch 52: 0.7581391304347826
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.1900e-01 (2.1900e-01)	Acc 0.763672 (0.763672)
Epoch: [53][300/616]	Loss 2.4504e-01 (2.2934e-01)	Acc 0.727539 (0.757147)
Epoch: [53][600/616]	Loss 2.3512e-01 (2.2951e-01)	Acc 0.753906 (0.757375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757557)
Training Loss of Epoch 53: 0.22937787570119875
Training Acc of Epoch 53: 0.7574933307926829
Testing Acc of Epoch 53: 0.7575565217391305
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3286e-01 (2.3286e-01)	Acc 0.751953 (0.751953)
Epoch: [54][300/616]	Loss 2.1965e-01 (2.2918e-01)	Acc 0.765625 (0.757504)
Epoch: [54][600/616]	Loss 2.4605e-01 (2.2943e-01)	Acc 0.740234 (0.757341)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758739)
Training Loss of Epoch 54: 0.2294552391137534
Training Acc of Epoch 54: 0.7573885289634147
Testing Acc of Epoch 54: 0.7587391304347826
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.1473e-01 (2.1473e-01)	Acc 0.778320 (0.778320)
Epoch: [55][300/616]	Loss 2.2461e-01 (2.2931e-01)	Acc 0.755859 (0.757624)
Epoch: [55][600/616]	Loss 2.4148e-01 (2.2911e-01)	Acc 0.743164 (0.757798)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758783)
Training Loss of Epoch 55: 0.22913431956516048
Training Acc of Epoch 55: 0.757836318597561
Testing Acc of Epoch 55: 0.7587826086956522
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2626e-01 (2.2626e-01)	Acc 0.766602 (0.766602)
Epoch: [56][300/616]	Loss 2.1454e-01 (2.2856e-01)	Acc 0.785156 (0.759081)
Epoch: [56][600/616]	Loss 2.2657e-01 (2.2917e-01)	Acc 0.757812 (0.757639)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758630)
Training Loss of Epoch 56: 0.22921870009201328
Training Acc of Epoch 56: 0.7575790777439024
Testing Acc of Epoch 56: 0.7586304347826087
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3564e-01 (2.3564e-01)	Acc 0.745117 (0.745117)
Epoch: [57][300/616]	Loss 2.2051e-01 (2.2967e-01)	Acc 0.779297 (0.757115)
Epoch: [57][600/616]	Loss 2.2540e-01 (2.2945e-01)	Acc 0.766602 (0.757452)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758591)
Training Loss of Epoch 57: 0.2293927353329775
Training Acc of Epoch 57: 0.7574552210365854
Testing Acc of Epoch 57: 0.7585913043478261
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3695e-01 (2.3695e-01)	Acc 0.751953 (0.751953)
Epoch: [58][300/616]	Loss 2.3434e-01 (2.2943e-01)	Acc 0.754883 (0.757177)
Epoch: [58][600/616]	Loss 2.2194e-01 (2.2938e-01)	Acc 0.761719 (0.757530)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756191)
Training Loss of Epoch 58: 0.22935543348634146
Training Acc of Epoch 58: 0.7575409679878049
Testing Acc of Epoch 58: 0.7561913043478261
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3453e-01 (2.3453e-01)	Acc 0.750977 (0.750977)
Epoch: [59][300/616]	Loss 2.2232e-01 (2.2949e-01)	Acc 0.764648 (0.757579)
Epoch: [59][600/616]	Loss 2.5306e-01 (2.2907e-01)	Acc 0.726562 (0.758009)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759557)
Training Loss of Epoch 59: 0.22910826797407818
Training Acc of Epoch 59: 0.7579331808943089
Testing Acc of Epoch 59: 0.7595565217391305
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3169e-01 (2.3169e-01)	Acc 0.755859 (0.755859)
Epoch: [60][300/616]	Loss 2.3481e-01 (2.2915e-01)	Acc 0.745117 (0.757997)
Epoch: [60][600/616]	Loss 2.4959e-01 (2.2910e-01)	Acc 0.725586 (0.758068)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.758674)
Training Loss of Epoch 60: 0.22919153546899315
Training Acc of Epoch 60: 0.757909362296748
Testing Acc of Epoch 60: 0.7586739130434783
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3376e-01 (2.3376e-01)	Acc 0.755859 (0.755859)
Epoch: [61][300/616]	Loss 2.2592e-01 (2.2903e-01)	Acc 0.758789 (0.758416)
Epoch: [61][600/616]	Loss 2.3203e-01 (2.2942e-01)	Acc 0.759766 (0.757746)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758970)
Training Loss of Epoch 61: 0.22941868445737576
Training Acc of Epoch 61: 0.7576552972560976
Testing Acc of Epoch 61: 0.7589695652173913
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.2603e-01 (2.2603e-01)	Acc 0.769531 (0.769531)
Epoch: [62][300/616]	Loss 2.1273e-01 (2.3015e-01)	Acc 0.774414 (0.756794)
Epoch: [62][600/616]	Loss 2.2441e-01 (2.2925e-01)	Acc 0.761719 (0.757913)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757739)
Training Loss of Epoch 62: 0.22923739421658398
Training Acc of Epoch 62: 0.7579506478658536
Testing Acc of Epoch 62: 0.7577391304347826
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.2820e-01 (2.2820e-01)	Acc 0.765625 (0.765625)
Epoch: [63][300/616]	Loss 2.3632e-01 (2.2925e-01)	Acc 0.749023 (0.757942)
Epoch: [63][600/616]	Loss 2.4440e-01 (2.2934e-01)	Acc 0.739258 (0.757723)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.758878)
Training Loss of Epoch 63: 0.22928475369282855
Training Acc of Epoch 63: 0.7577839176829269
Testing Acc of Epoch 63: 0.7588782608695652
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.1667e-01 (2.1667e-01)	Acc 0.767578 (0.767578)
Epoch: [64][300/616]	Loss 2.3053e-01 (2.2923e-01)	Acc 0.750000 (0.757871)
Epoch: [64][600/616]	Loss 2.2850e-01 (2.2949e-01)	Acc 0.758789 (0.757289)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758096)
Training Loss of Epoch 64: 0.22939021892663908
Training Acc of Epoch 64: 0.7574520452235772
Testing Acc of Epoch 64: 0.7580956521739131
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.1675e-01 (2.1675e-01)	Acc 0.776367 (0.776367)
Epoch: [65][300/616]	Loss 2.1015e-01 (2.2927e-01)	Acc 0.776367 (0.757735)
Epoch: [65][600/616]	Loss 2.2339e-01 (2.2934e-01)	Acc 0.769531 (0.757796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756791)
Training Loss of Epoch 65: 0.22930368296499173
Training Acc of Epoch 65: 0.7578093241869919
Testing Acc of Epoch 65: 0.756791304347826
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4208e-01 (2.4208e-01)	Acc 0.752930 (0.752930)
Epoch: [66][300/616]	Loss 2.2399e-01 (2.2914e-01)	Acc 0.756836 (0.757514)
Epoch: [66][600/616]	Loss 2.3135e-01 (2.2927e-01)	Acc 0.750977 (0.757722)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758496)
Training Loss of Epoch 66: 0.22923611727187304
Training Acc of Epoch 66: 0.7578474339430894
Testing Acc of Epoch 66: 0.758495652173913
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.3581e-01 (2.3581e-01)	Acc 0.750000 (0.750000)
Epoch: [67][300/616]	Loss 2.4910e-01 (2.2864e-01)	Acc 0.732422 (0.758825)
Epoch: [67][600/616]	Loss 2.3327e-01 (2.2897e-01)	Acc 0.743164 (0.757973)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756683)
Training Loss of Epoch 67: 0.2290686734081284
Training Acc of Epoch 67: 0.7578998348577236
Testing Acc of Epoch 67: 0.7566826086956522
Model with the best training loss saved! The loss is 0.2290686734081284
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.2707e-01 (2.2707e-01)	Acc 0.761719 (0.761719)
Epoch: [68][300/616]	Loss 2.3176e-01 (2.2935e-01)	Acc 0.744141 (0.758046)
Epoch: [68][600/616]	Loss 2.2471e-01 (2.2935e-01)	Acc 0.769531 (0.757741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.757839)
Training Loss of Epoch 68: 0.2293852502010702
Training Acc of Epoch 68: 0.757690231199187
Testing Acc of Epoch 68: 0.7578391304347826
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.2156e-01 (2.2156e-01)	Acc 0.766602 (0.766602)
Epoch: [69][300/616]	Loss 2.1847e-01 (2.2914e-01)	Acc 0.772461 (0.758066)
Epoch: [69][600/616]	Loss 2.2884e-01 (2.2930e-01)	Acc 0.743164 (0.757761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.759074)
Training Loss of Epoch 69: 0.2292443623630012
Training Acc of Epoch 69: 0.7578013846544716
Testing Acc of Epoch 69: 0.7590739130434783
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4140e-01 (2.4140e-01)	Acc 0.736328 (0.736328)
Epoch: [70][300/616]	Loss 2.3509e-01 (2.2900e-01)	Acc 0.751953 (0.758679)
Epoch: [70][600/616]	Loss 2.3072e-01 (2.2943e-01)	Acc 0.752930 (0.757678)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758374)
Training Loss of Epoch 70: 0.22941817122746289
Training Acc of Epoch 70: 0.7577299288617886
Testing Acc of Epoch 70: 0.7583739130434782
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.1135e-01 (2.1135e-01)	Acc 0.779297 (0.779297)
Epoch: [71][300/616]	Loss 2.1686e-01 (2.2956e-01)	Acc 0.773438 (0.757430)
Epoch: [71][600/616]	Loss 2.2217e-01 (2.2931e-01)	Acc 0.758789 (0.757434)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756804)
Training Loss of Epoch 71: 0.2293040629809465
Training Acc of Epoch 71: 0.7575139735772358
Testing Acc of Epoch 71: 0.756804347826087
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.3435e-01 (2.3435e-01)	Acc 0.752930 (0.752930)
Epoch: [72][300/616]	Loss 2.2176e-01 (2.2986e-01)	Acc 0.753906 (0.757290)
Epoch: [72][600/616]	Loss 2.2396e-01 (2.2917e-01)	Acc 0.752930 (0.758029)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758265)
Training Loss of Epoch 72: 0.22921201060942517
Training Acc of Epoch 72: 0.7579395325203252
Testing Acc of Epoch 72: 0.7582652173913044
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4186e-01 (2.4186e-01)	Acc 0.739258 (0.739258)
Epoch: [73][300/616]	Loss 2.2087e-01 (2.2965e-01)	Acc 0.767578 (0.757225)
Epoch: [73][600/616]	Loss 2.3467e-01 (2.2924e-01)	Acc 0.740234 (0.757822)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758557)
Training Loss of Epoch 73: 0.2292358038871269
Training Acc of Epoch 73: 0.7578204395325203
Testing Acc of Epoch 73: 0.7585565217391305
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.2226e-01 (2.2226e-01)	Acc 0.765625 (0.765625)
Epoch: [74][300/616]	Loss 2.2442e-01 (2.2952e-01)	Acc 0.765625 (0.757641)
Epoch: [74][600/616]	Loss 2.3293e-01 (2.2932e-01)	Acc 0.753906 (0.757713)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758335)
Training Loss of Epoch 74: 0.22940254853508338
Training Acc of Epoch 74: 0.757666412601626
Testing Acc of Epoch 74: 0.7583347826086957
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.2065e-01 (2.2065e-01)	Acc 0.760742 (0.760742)
Epoch: [75][300/616]	Loss 2.2491e-01 (2.2802e-01)	Acc 0.757812 (0.757770)
Epoch: [75][600/616]	Loss 2.2626e-01 (2.2746e-01)	Acc 0.753906 (0.759233)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760126)
Training Loss of Epoch 75: 0.2274193766155863
Training Acc of Epoch 75: 0.7592019181910569
Testing Acc of Epoch 75: 0.7601260869565217
Model with the best training loss saved! The loss is 0.2274193766155863
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.2272e-01 (2.2272e-01)	Acc 0.767578 (0.767578)
Epoch: [76][300/616]	Loss 2.2715e-01 (2.2717e-01)	Acc 0.764648 (0.759461)
Epoch: [76][600/616]	Loss 2.2899e-01 (2.2726e-01)	Acc 0.757812 (0.759514)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.759365)
Training Loss of Epoch 76: 0.22724918051948392
Training Acc of Epoch 76: 0.7595099720528455
Testing Acc of Epoch 76: 0.7593652173913044
Model with the best training loss saved! The loss is 0.22724918051948392
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.1537e-01 (2.1537e-01)	Acc 0.777344 (0.777344)
Epoch: [77][300/616]	Loss 2.1430e-01 (2.2699e-01)	Acc 0.770508 (0.759393)
Epoch: [77][600/616]	Loss 2.2435e-01 (2.2714e-01)	Acc 0.753906 (0.759476)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.760604)
Training Loss of Epoch 77: 0.22716669038059265
Training Acc of Epoch 77: 0.7594734502032521
Testing Acc of Epoch 77: 0.760604347826087
Model with the best training loss saved! The loss is 0.22716669038059265
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.1549e-01 (2.1549e-01)	Acc 0.770508 (0.770508)
Epoch: [78][300/616]	Loss 2.2888e-01 (2.2691e-01)	Acc 0.756836 (0.759730)
Epoch: [78][600/616]	Loss 2.2412e-01 (2.2715e-01)	Acc 0.759766 (0.759541)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760626)
Training Loss of Epoch 78: 0.22713798570439098
Training Acc of Epoch 78: 0.759594131097561
Testing Acc of Epoch 78: 0.7606260869565218
Model with the best training loss saved! The loss is 0.22713798570439098
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.1394e-01 (2.1394e-01)	Acc 0.768555 (0.768555)
Epoch: [79][300/616]	Loss 2.2924e-01 (2.2725e-01)	Acc 0.751953 (0.759522)
Epoch: [79][600/616]	Loss 2.3807e-01 (2.2703e-01)	Acc 0.727539 (0.759658)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759943)
Training Loss of Epoch 79: 0.22706145162020272
Training Acc of Epoch 79: 0.7596608231707317
Testing Acc of Epoch 79: 0.7599434782608696
Model with the best training loss saved! The loss is 0.22706145162020272
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3393e-01 (2.3393e-01)	Acc 0.743164 (0.743164)
Epoch: [80][300/616]	Loss 2.2911e-01 (2.2751e-01)	Acc 0.760742 (0.759295)
Epoch: [80][600/616]	Loss 2.1451e-01 (2.2706e-01)	Acc 0.783203 (0.759691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.759804)
Training Loss of Epoch 80: 0.22708692465855823
Training Acc of Epoch 80: 0.7596306529471545
Testing Acc of Epoch 80: 0.759804347826087
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2534e-01 (2.2534e-01)	Acc 0.765625 (0.765625)
Epoch: [81][300/616]	Loss 2.2250e-01 (2.2686e-01)	Acc 0.774414 (0.760084)
Epoch: [81][600/616]	Loss 2.2923e-01 (2.2701e-01)	Acc 0.760742 (0.759628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760778)
Training Loss of Epoch 81: 0.22705611724679062
Training Acc of Epoch 81: 0.7596227134146342
Testing Acc of Epoch 81: 0.7607782608695652
Model with the best training loss saved! The loss is 0.22705611724679062
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.4515e-01 (2.4515e-01)	Acc 0.730469 (0.730469)
Epoch: [82][300/616]	Loss 2.1967e-01 (2.2681e-01)	Acc 0.757812 (0.759954)
Epoch: [82][600/616]	Loss 2.2817e-01 (2.2705e-01)	Acc 0.747070 (0.759683)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760330)
Training Loss of Epoch 82: 0.22701934457309847
Training Acc of Epoch 82: 0.759694169207317
Testing Acc of Epoch 82: 0.7603304347826086
Model with the best training loss saved! The loss is 0.22701934457309847
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.1857e-01 (2.1857e-01)	Acc 0.773438 (0.773438)
Epoch: [83][300/616]	Loss 2.3106e-01 (2.2715e-01)	Acc 0.753906 (0.759659)
Epoch: [83][600/616]	Loss 2.4535e-01 (2.2707e-01)	Acc 0.733398 (0.759764)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759900)
Training Loss of Epoch 83: 0.22697955852116997
Training Acc of Epoch 83: 0.759837080792683
Testing Acc of Epoch 83: 0.7599
Model with the best training loss saved! The loss is 0.22697955852116997
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.2012e-01 (2.2012e-01)	Acc 0.774414 (0.774414)
Epoch: [84][300/616]	Loss 2.3526e-01 (2.2687e-01)	Acc 0.738281 (0.760028)
Epoch: [84][600/616]	Loss 2.2600e-01 (2.2697e-01)	Acc 0.757812 (0.759884)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.760048)
Training Loss of Epoch 84: 0.22697931656507941
Training Acc of Epoch 84: 0.7598196138211382
Testing Acc of Epoch 84: 0.7600478260869565
Model with the best training loss saved! The loss is 0.22697931656507941
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2443e-01 (2.2443e-01)	Acc 0.756836 (0.756836)
Epoch: [85][300/616]	Loss 2.3651e-01 (2.2657e-01)	Acc 0.759766 (0.760291)
Epoch: [85][600/616]	Loss 2.2042e-01 (2.2691e-01)	Acc 0.769531 (0.759775)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760357)
Training Loss of Epoch 85: 0.22694106111681558
Training Acc of Epoch 85: 0.7597973831300813
Testing Acc of Epoch 85: 0.7603565217391305
Model with the best training loss saved! The loss is 0.22694106111681558
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3528e-01 (2.3528e-01)	Acc 0.739258 (0.739258)
Epoch: [86][300/616]	Loss 2.2625e-01 (2.2718e-01)	Acc 0.758789 (0.759370)
Epoch: [86][600/616]	Loss 2.1989e-01 (2.2707e-01)	Acc 0.766602 (0.759491)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759635)
Training Loss of Epoch 86: 0.2268973307154043
Training Acc of Epoch 86: 0.7597068724593496
Testing Acc of Epoch 86: 0.7596347826086957
Model with the best training loss saved! The loss is 0.2268973307154043
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3013e-01 (2.3013e-01)	Acc 0.750977 (0.750977)
Epoch: [87][300/616]	Loss 2.2215e-01 (2.2689e-01)	Acc 0.757812 (0.759769)
Epoch: [87][600/616]	Loss 2.2340e-01 (2.2692e-01)	Acc 0.757812 (0.759897)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.760265)
Training Loss of Epoch 87: 0.22686364619227928
Training Acc of Epoch 87: 0.7599958714430894
Testing Acc of Epoch 87: 0.7602652173913044
Model with the best training loss saved! The loss is 0.22686364619227928
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.2652e-01 (2.2652e-01)	Acc 0.756836 (0.756836)
Epoch: [88][300/616]	Loss 2.1151e-01 (2.2734e-01)	Acc 0.778320 (0.758740)
Epoch: [88][600/616]	Loss 2.3700e-01 (2.2690e-01)	Acc 0.753906 (0.759600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760478)
Training Loss of Epoch 88: 0.22684519547757095
Training Acc of Epoch 88: 0.759716399898374
Testing Acc of Epoch 88: 0.7604782608695653
Model with the best training loss saved! The loss is 0.22684519547757095
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.4309e-01 (2.4309e-01)	Acc 0.731445 (0.731445)
Epoch: [89][300/616]	Loss 2.2931e-01 (2.2672e-01)	Acc 0.760742 (0.759548)
Epoch: [89][600/616]	Loss 2.2670e-01 (2.2689e-01)	Acc 0.769531 (0.759879)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760239)
Training Loss of Epoch 89: 0.22688250960857886
Training Acc of Epoch 89: 0.7598720147357724
Testing Acc of Epoch 89: 0.7602391304347826
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2353e-01 (2.2353e-01)	Acc 0.754883 (0.754883)
Epoch: [90][300/616]	Loss 2.1299e-01 (2.2728e-01)	Acc 0.778320 (0.759723)
Epoch: [90][600/616]	Loss 2.2416e-01 (2.2696e-01)	Acc 0.751953 (0.759710)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760470)
Training Loss of Epoch 90: 0.22685195735799588
Training Acc of Epoch 90: 0.7598688389227642
Testing Acc of Epoch 90: 0.7604695652173913
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.1995e-01 (2.1995e-01)	Acc 0.765625 (0.765625)
Epoch: [91][300/616]	Loss 2.1701e-01 (2.2693e-01)	Acc 0.775391 (0.759736)
Epoch: [91][600/616]	Loss 2.2748e-01 (2.2682e-01)	Acc 0.761719 (0.760035)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759983)
Training Loss of Epoch 91: 0.22685058572428013
Training Acc of Epoch 91: 0.7599784044715447
Testing Acc of Epoch 91: 0.7599826086956522
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.1074e-01 (2.1074e-01)	Acc 0.786133 (0.786133)
Epoch: [92][300/616]	Loss 2.2167e-01 (2.2708e-01)	Acc 0.762695 (0.759681)
Epoch: [92][600/616]	Loss 2.3225e-01 (2.2692e-01)	Acc 0.750977 (0.759925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.759648)
Training Loss of Epoch 92: 0.22688964590793703
Training Acc of Epoch 92: 0.7599577616869919
Testing Acc of Epoch 92: 0.7596478260869565
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.1079e-01 (2.1079e-01)	Acc 0.776367 (0.776367)
Epoch: [93][300/616]	Loss 2.4686e-01 (2.2666e-01)	Acc 0.731445 (0.759989)
Epoch: [93][600/616]	Loss 2.2591e-01 (2.2697e-01)	Acc 0.757812 (0.759439)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.760091)
Training Loss of Epoch 93: 0.22694279992483496
Training Acc of Epoch 93: 0.759571900406504
Testing Acc of Epoch 93: 0.7600913043478261
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2024e-01 (2.2024e-01)	Acc 0.765625 (0.765625)
Epoch: [94][300/616]	Loss 2.1581e-01 (2.2657e-01)	Acc 0.781250 (0.760135)
Epoch: [94][600/616]	Loss 2.2143e-01 (2.2695e-01)	Acc 0.773438 (0.759572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.759161)
Training Loss of Epoch 94: 0.22685350717567815
Training Acc of Epoch 94: 0.7597846798780488
Testing Acc of Epoch 94: 0.7591608695652174
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3934e-01 (2.3934e-01)	Acc 0.742188 (0.742188)
Epoch: [95][300/616]	Loss 2.2953e-01 (2.2699e-01)	Acc 0.760742 (0.760116)
Epoch: [95][600/616]	Loss 2.2526e-01 (2.2691e-01)	Acc 0.758789 (0.759858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760057)
Training Loss of Epoch 95: 0.22687658418000228
Training Acc of Epoch 95: 0.7599577616869919
Testing Acc of Epoch 95: 0.7600565217391304
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3354e-01 (2.3354e-01)	Acc 0.745117 (0.745117)
Epoch: [96][300/616]	Loss 2.3723e-01 (2.2636e-01)	Acc 0.753906 (0.760333)
Epoch: [96][600/616]	Loss 2.3228e-01 (2.2679e-01)	Acc 0.744141 (0.759809)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.760343)
Training Loss of Epoch 96: 0.22683998973873573
Training Acc of Epoch 96: 0.7597513338414634
Testing Acc of Epoch 96: 0.7603434782608696
Model with the best training loss saved! The loss is 0.22683998973873573
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.3219e-01 (2.3219e-01)	Acc 0.754883 (0.754883)
Epoch: [97][300/616]	Loss 2.2407e-01 (2.2728e-01)	Acc 0.766602 (0.759386)
Epoch: [97][600/616]	Loss 2.3559e-01 (2.2670e-01)	Acc 0.743164 (0.759956)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.760126)
Training Loss of Epoch 97: 0.22672852066958823
Training Acc of Epoch 97: 0.7599244156504065
Testing Acc of Epoch 97: 0.7601260869565217
Model with the best training loss saved! The loss is 0.22672852066958823
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.2769e-01 (2.2769e-01)	Acc 0.750977 (0.750977)
Epoch: [98][300/616]	Loss 2.1498e-01 (2.2672e-01)	Acc 0.771484 (0.759973)
Epoch: [98][600/616]	Loss 2.2659e-01 (2.2674e-01)	Acc 0.772461 (0.760001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.759722)
Training Loss of Epoch 98: 0.22677790137325846
Training Acc of Epoch 98: 0.7599529979674797
Testing Acc of Epoch 98: 0.7597217391304348
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2482e-01 (2.2482e-01)	Acc 0.756836 (0.756836)
Epoch: [99][300/616]	Loss 2.1710e-01 (2.2661e-01)	Acc 0.761719 (0.759973)
Epoch: [99][600/616]	Loss 2.1972e-01 (2.2672e-01)	Acc 0.766602 (0.759923)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.760417)
Training Loss of Epoch 99: 0.22678221856675496
Training Acc of Epoch 99: 0.7598116742886178
Testing Acc of Epoch 99: 0.7604173913043478
Early stopping not satisfied.
