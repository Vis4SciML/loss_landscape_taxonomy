train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.003125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.003125/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.003125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0015e-01 (5.0015e-01)	Acc 0.194336 (0.194336)
Epoch: [0][300/616]	Loss 2.7812e-01 (2.9402e-01)	Acc 0.693359 (0.687880)
Epoch: [0][600/616]	Loss 2.3148e-01 (2.7123e-01)	Acc 0.753906 (0.713835)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.744239)
Training Loss of Epoch 0: 0.27065035979437635
Training Acc of Epoch 0: 0.7145118775406504
Testing Acc of Epoch 0: 0.7442391304347826
Model with the best training loss saved! The loss is 0.27065035979437635
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4984e-01 (2.4984e-01)	Acc 0.745117 (0.745117)
Epoch: [1][300/616]	Loss 2.5352e-01 (2.4257e-01)	Acc 0.727539 (0.745941)
Epoch: [1][600/616]	Loss 2.6474e-01 (2.4117e-01)	Acc 0.719727 (0.746963)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749696)
Training Loss of Epoch 1: 0.2411267416506279
Training Acc of Epoch 1: 0.7469702743902439
Testing Acc of Epoch 1: 0.7496956521739131
Model with the best training loss saved! The loss is 0.2411267416506279
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4210e-01 (2.4210e-01)	Acc 0.747070 (0.747070)
Epoch: [2][300/616]	Loss 2.2953e-01 (2.3870e-01)	Acc 0.760742 (0.748641)
Epoch: [2][600/616]	Loss 2.2210e-01 (2.3735e-01)	Acc 0.764648 (0.749802)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752361)
Training Loss of Epoch 2: 0.23733464995535408
Training Acc of Epoch 2: 0.749780868902439
Testing Acc of Epoch 2: 0.7523608695652174
Model with the best training loss saved! The loss is 0.23733464995535408
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4901e-01 (2.4901e-01)	Acc 0.746094 (0.746094)
Epoch: [3][300/616]	Loss 2.3977e-01 (2.3616e-01)	Acc 0.746094 (0.751210)
Epoch: [3][600/616]	Loss 2.2992e-01 (2.3550e-01)	Acc 0.756836 (0.751612)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753691)
Training Loss of Epoch 3: 0.23545938417678927
Training Acc of Epoch 3: 0.7516942962398374
Testing Acc of Epoch 3: 0.753691304347826
Model with the best training loss saved! The loss is 0.23545938417678927
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3781e-01 (2.3781e-01)	Acc 0.740234 (0.740234)
Epoch: [4][300/616]	Loss 2.0966e-01 (2.3403e-01)	Acc 0.781250 (0.753030)
Epoch: [4][600/616]	Loss 2.2686e-01 (2.3400e-01)	Acc 0.764648 (0.752332)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754004)
Training Loss of Epoch 4: 0.23401848172269216
Training Acc of Epoch 4: 0.7522722942073171
Testing Acc of Epoch 4: 0.7540043478260869
Model with the best training loss saved! The loss is 0.23401848172269216
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.2955e-01 (2.2955e-01)	Acc 0.754883 (0.754883)
Epoch: [5][300/616]	Loss 2.3185e-01 (2.3399e-01)	Acc 0.743164 (0.752427)
Epoch: [5][600/616]	Loss 2.2474e-01 (2.3352e-01)	Acc 0.765625 (0.753048)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755635)
Training Loss of Epoch 5: 0.23353791786887781
Training Acc of Epoch 5: 0.7530122586382114
Testing Acc of Epoch 5: 0.7556347826086957
Model with the best training loss saved! The loss is 0.23353791786887781
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3872e-01 (2.3872e-01)	Acc 0.753906 (0.753906)
Epoch: [6][300/616]	Loss 2.2620e-01 (2.3433e-01)	Acc 0.755859 (0.752193)
Epoch: [6][600/616]	Loss 2.4169e-01 (2.3324e-01)	Acc 0.745117 (0.753248)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754704)
Training Loss of Epoch 6: 0.23321037617156176
Training Acc of Epoch 6: 0.7532282139227642
Testing Acc of Epoch 6: 0.754704347826087
Model with the best training loss saved! The loss is 0.23321037617156176
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4165e-01 (2.4165e-01)	Acc 0.746094 (0.746094)
Epoch: [7][300/616]	Loss 2.3854e-01 (2.3279e-01)	Acc 0.755859 (0.753692)
Epoch: [7][600/616]	Loss 2.2369e-01 (2.3234e-01)	Acc 0.770508 (0.754174)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755396)
Training Loss of Epoch 7: 0.23237607651125125
Training Acc of Epoch 7: 0.7541460238821138
Testing Acc of Epoch 7: 0.755395652173913
Model with the best training loss saved! The loss is 0.23237607651125125
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3354e-01 (2.3354e-01)	Acc 0.748047 (0.748047)
Epoch: [8][300/616]	Loss 2.3483e-01 (2.3265e-01)	Acc 0.754883 (0.754383)
Epoch: [8][600/616]	Loss 2.3292e-01 (2.3245e-01)	Acc 0.747070 (0.754083)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753135)
Training Loss of Epoch 8: 0.23239595914274697
Training Acc of Epoch 8: 0.7541428480691057
Testing Acc of Epoch 8: 0.7531347826086957
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4030e-01 (2.4030e-01)	Acc 0.747070 (0.747070)
Epoch: [9][300/616]	Loss 2.1244e-01 (2.3268e-01)	Acc 0.791016 (0.754098)
Epoch: [9][600/616]	Loss 2.2474e-01 (2.3227e-01)	Acc 0.762695 (0.754457)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756609)
Training Loss of Epoch 9: 0.23230392126048482
Training Acc of Epoch 9: 0.7543619791666667
Testing Acc of Epoch 9: 0.7566086956521739
Model with the best training loss saved! The loss is 0.23230392126048482
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2527e-01 (2.2527e-01)	Acc 0.765625 (0.765625)
Epoch: [10][300/616]	Loss 2.2196e-01 (2.3303e-01)	Acc 0.763672 (0.753296)
Epoch: [10][600/616]	Loss 2.2599e-01 (2.3224e-01)	Acc 0.759766 (0.754335)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752917)
Training Loss of Epoch 10: 0.23215573187766037
Training Acc of Epoch 10: 0.7544350228658536
Testing Acc of Epoch 10: 0.7529173913043479
Model with the best training loss saved! The loss is 0.23215573187766037
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.2086e-01 (2.2086e-01)	Acc 0.768555 (0.768555)
Epoch: [11][300/616]	Loss 2.2878e-01 (2.3203e-01)	Acc 0.754883 (0.754549)
Epoch: [11][600/616]	Loss 2.4071e-01 (2.3228e-01)	Acc 0.740234 (0.754158)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754548)
Training Loss of Epoch 11: 0.23225003551176893
Training Acc of Epoch 11: 0.7541841336382114
Testing Acc of Epoch 11: 0.7545478260869565
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4207e-01 (2.4207e-01)	Acc 0.735352 (0.735352)
Epoch: [12][300/616]	Loss 2.4820e-01 (2.3204e-01)	Acc 0.738281 (0.755236)
Epoch: [12][600/616]	Loss 2.3358e-01 (2.3247e-01)	Acc 0.755859 (0.754095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753057)
Training Loss of Epoch 12: 0.23244894822923148
Training Acc of Epoch 12: 0.7541539634146341
Testing Acc of Epoch 12: 0.7530565217391304
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4307e-01 (2.4307e-01)	Acc 0.747070 (0.747070)
Epoch: [13][300/616]	Loss 2.3015e-01 (2.3191e-01)	Acc 0.750977 (0.755444)
Epoch: [13][600/616]	Loss 2.4883e-01 (2.3199e-01)	Acc 0.726562 (0.754415)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755609)
Training Loss of Epoch 13: 0.23198921867017824
Training Acc of Epoch 13: 0.754420731707317
Testing Acc of Epoch 13: 0.7556086956521739
Model with the best training loss saved! The loss is 0.23198921867017824
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3214e-01 (2.3214e-01)	Acc 0.765625 (0.765625)
Epoch: [14][300/616]	Loss 2.3876e-01 (2.3162e-01)	Acc 0.755859 (0.755165)
Epoch: [14][600/616]	Loss 2.2870e-01 (2.3190e-01)	Acc 0.759766 (0.754499)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756357)
Training Loss of Epoch 14: 0.23186468433558458
Training Acc of Epoch 14: 0.7545699949186991
Testing Acc of Epoch 14: 0.7563565217391305
Model with the best training loss saved! The loss is 0.23186468433558458
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3289e-01 (2.3289e-01)	Acc 0.751953 (0.751953)
Epoch: [15][300/616]	Loss 2.3260e-01 (2.3106e-01)	Acc 0.743164 (0.756025)
Epoch: [15][600/616]	Loss 2.3667e-01 (2.3180e-01)	Acc 0.750000 (0.754902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755530)
Training Loss of Epoch 15: 0.2318854174962858
Training Acc of Epoch 15: 0.7549018673780488
Testing Acc of Epoch 15: 0.7555304347826087
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2504e-01 (2.2504e-01)	Acc 0.748047 (0.748047)
Epoch: [16][300/616]	Loss 2.1963e-01 (2.3158e-01)	Acc 0.774414 (0.755123)
Epoch: [16][600/616]	Loss 2.3366e-01 (2.3221e-01)	Acc 0.749023 (0.754381)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755704)
Training Loss of Epoch 16: 0.2321761628476585
Training Acc of Epoch 16: 0.7544286712398374
Testing Acc of Epoch 16: 0.755704347826087
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4224e-01 (2.4224e-01)	Acc 0.749023 (0.749023)
Epoch: [17][300/616]	Loss 2.2946e-01 (2.3189e-01)	Acc 0.763672 (0.755042)
Epoch: [17][600/616]	Loss 2.2534e-01 (2.3210e-01)	Acc 0.765625 (0.754743)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755135)
Training Loss of Epoch 17: 0.23202747649293604
Training Acc of Epoch 17: 0.7548653455284553
Testing Acc of Epoch 17: 0.7551347826086956
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4853e-01 (2.4853e-01)	Acc 0.743164 (0.743164)
Epoch: [18][300/616]	Loss 2.3616e-01 (2.3238e-01)	Acc 0.744141 (0.754373)
Epoch: [18][600/616]	Loss 2.1223e-01 (2.3222e-01)	Acc 0.786133 (0.754597)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753517)
Training Loss of Epoch 18: 0.2322252996326462
Training Acc of Epoch 18: 0.7546350990853659
Testing Acc of Epoch 18: 0.7535173913043478
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3313e-01 (2.3313e-01)	Acc 0.757812 (0.757812)
Epoch: [19][300/616]	Loss 2.3038e-01 (2.3218e-01)	Acc 0.754883 (0.754643)
Epoch: [19][600/616]	Loss 2.4311e-01 (2.3193e-01)	Acc 0.750977 (0.754572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754326)
Training Loss of Epoch 19: 0.23202927771622572
Training Acc of Epoch 19: 0.7544604293699188
Testing Acc of Epoch 19: 0.7543260869565217
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.1661e-01 (2.1661e-01)	Acc 0.778320 (0.778320)
Epoch: [20][300/616]	Loss 2.3271e-01 (2.3257e-01)	Acc 0.752930 (0.754104)
Epoch: [20][600/616]	Loss 2.3661e-01 (2.3227e-01)	Acc 0.753906 (0.754507)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753643)
Training Loss of Epoch 20: 0.23219272460394758
Training Acc of Epoch 20: 0.7545858739837399
Testing Acc of Epoch 20: 0.7536434782608695
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3779e-01 (2.3779e-01)	Acc 0.739258 (0.739258)
Epoch: [21][300/616]	Loss 2.2317e-01 (2.3295e-01)	Acc 0.774414 (0.752729)
Epoch: [21][600/616]	Loss 2.3784e-01 (2.3205e-01)	Acc 0.745117 (0.754199)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754983)
Training Loss of Epoch 21: 0.23209644238638685
Training Acc of Epoch 21: 0.7541857215447154
Testing Acc of Epoch 21: 0.7549826086956521
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3322e-01 (2.3322e-01)	Acc 0.750000 (0.750000)
Epoch: [22][300/616]	Loss 2.2170e-01 (2.3179e-01)	Acc 0.761719 (0.754948)
Epoch: [22][600/616]	Loss 2.2135e-01 (2.3208e-01)	Acc 0.775391 (0.754551)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755861)
Training Loss of Epoch 22: 0.2321015448589635
Training Acc of Epoch 22: 0.7545461763211382
Testing Acc of Epoch 22: 0.7558608695652174
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3472e-01 (2.3472e-01)	Acc 0.746094 (0.746094)
Epoch: [23][300/616]	Loss 2.1045e-01 (2.3218e-01)	Acc 0.784180 (0.754970)
Epoch: [23][600/616]	Loss 2.3125e-01 (2.3250e-01)	Acc 0.756836 (0.754077)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755535)
Training Loss of Epoch 23: 0.23249888003357058
Training Acc of Epoch 23: 0.7540809197154471
Testing Acc of Epoch 23: 0.7555347826086957
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.4505e-01 (2.4505e-01)	Acc 0.739258 (0.739258)
Epoch: [24][300/616]	Loss 2.2456e-01 (2.3238e-01)	Acc 0.755859 (0.754325)
Epoch: [24][600/616]	Loss 2.2374e-01 (2.3204e-01)	Acc 0.776367 (0.754572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.756022)
Training Loss of Epoch 24: 0.23212965065386237
Training Acc of Epoch 24: 0.7543524517276423
Testing Acc of Epoch 24: 0.7560217391304348
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.3936e-01 (2.3936e-01)	Acc 0.743164 (0.743164)
Epoch: [25][300/616]	Loss 2.3819e-01 (2.3256e-01)	Acc 0.748047 (0.754120)
Epoch: [25][600/616]	Loss 2.3248e-01 (2.3221e-01)	Acc 0.760742 (0.754621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.756130)
Training Loss of Epoch 25: 0.23228361652634008
Training Acc of Epoch 25: 0.7545525279471544
Testing Acc of Epoch 25: 0.7561304347826087
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.5558e-01 (2.5558e-01)	Acc 0.723633 (0.723633)
Epoch: [26][300/616]	Loss 2.3608e-01 (2.3264e-01)	Acc 0.763672 (0.753608)
Epoch: [26][600/616]	Loss 2.3960e-01 (2.3200e-01)	Acc 0.742188 (0.754542)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754791)
Training Loss of Epoch 26: 0.23203649554795366
Training Acc of Epoch 26: 0.7545525279471544
Testing Acc of Epoch 26: 0.754791304347826
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.2699e-01 (2.2699e-01)	Acc 0.764648 (0.764648)
Epoch: [27][300/616]	Loss 2.4713e-01 (2.3186e-01)	Acc 0.741211 (0.754445)
Epoch: [27][600/616]	Loss 2.1499e-01 (2.3207e-01)	Acc 0.775391 (0.754629)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753730)
Training Loss of Epoch 27: 0.23205663632086623
Training Acc of Epoch 27: 0.7546255716463415
Testing Acc of Epoch 27: 0.7537304347826087
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.2459e-01 (2.2459e-01)	Acc 0.766602 (0.766602)
Epoch: [28][300/616]	Loss 2.2082e-01 (2.3278e-01)	Acc 0.767578 (0.754007)
Epoch: [28][600/616]	Loss 2.5203e-01 (2.3209e-01)	Acc 0.718750 (0.754402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754952)
Training Loss of Epoch 28: 0.23195714802761389
Training Acc of Epoch 28: 0.7545318851626016
Testing Acc of Epoch 28: 0.7549521739130435
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.4728e-01 (2.4728e-01)	Acc 0.739258 (0.739258)
Epoch: [29][300/616]	Loss 2.3326e-01 (2.3173e-01)	Acc 0.753906 (0.754468)
Epoch: [29][600/616]	Loss 2.2126e-01 (2.3191e-01)	Acc 0.764648 (0.754496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756026)
Training Loss of Epoch 29: 0.23185637098986928
Training Acc of Epoch 29: 0.7544953633130081
Testing Acc of Epoch 29: 0.7560260869565217
Model with the best training loss saved! The loss is 0.23185637098986928
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2873e-01 (2.2873e-01)	Acc 0.763672 (0.763672)
Epoch: [30][300/616]	Loss 2.2957e-01 (2.3149e-01)	Acc 0.750977 (0.755366)
Epoch: [30][600/616]	Loss 2.3167e-01 (2.3196e-01)	Acc 0.764648 (0.754460)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755313)
Training Loss of Epoch 30: 0.2319307231079272
Training Acc of Epoch 30: 0.7544620172764228
Testing Acc of Epoch 30: 0.7553130434782609
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.2820e-01 (2.2820e-01)	Acc 0.753906 (0.753906)
Epoch: [31][300/616]	Loss 2.4089e-01 (2.3281e-01)	Acc 0.736328 (0.753699)
Epoch: [31][600/616]	Loss 2.2535e-01 (2.3246e-01)	Acc 0.767578 (0.754228)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755857)
Training Loss of Epoch 31: 0.23260275702166364
Training Acc of Epoch 31: 0.7540952108739838
Testing Acc of Epoch 31: 0.7558565217391304
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.2097e-01 (2.2097e-01)	Acc 0.769531 (0.769531)
Epoch: [32][300/616]	Loss 2.2513e-01 (2.3277e-01)	Acc 0.765625 (0.753407)
Epoch: [32][600/616]	Loss 2.2842e-01 (2.3253e-01)	Acc 0.759766 (0.753953)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754365)
Training Loss of Epoch 32: 0.23250442501006088
Training Acc of Epoch 32: 0.7540158155487805
Testing Acc of Epoch 32: 0.7543652173913044
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4000e-01 (2.4000e-01)	Acc 0.735352 (0.735352)
Epoch: [33][300/616]	Loss 2.3663e-01 (2.3327e-01)	Acc 0.745117 (0.753141)
Epoch: [33][600/616]	Loss 2.2531e-01 (2.3284e-01)	Acc 0.764648 (0.753601)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755887)
Training Loss of Epoch 33: 0.23264156814997758
Training Acc of Epoch 33: 0.7538236788617886
Testing Acc of Epoch 33: 0.7558869565217391
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.2278e-01 (2.2278e-01)	Acc 0.763672 (0.763672)
Epoch: [34][300/616]	Loss 2.2974e-01 (2.3196e-01)	Acc 0.758789 (0.754630)
Epoch: [34][600/616]	Loss 2.5062e-01 (2.3233e-01)	Acc 0.722656 (0.753953)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756165)
Training Loss of Epoch 34: 0.2322436860664104
Training Acc of Epoch 34: 0.7541269690040651
Testing Acc of Epoch 34: 0.7561652173913044
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2119e-01 (2.2119e-01)	Acc 0.776367 (0.776367)
Epoch: [35][300/616]	Loss 2.2192e-01 (2.3182e-01)	Acc 0.770508 (0.754409)
Epoch: [35][600/616]	Loss 2.4196e-01 (2.3200e-01)	Acc 0.730469 (0.754428)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.757022)
Training Loss of Epoch 35: 0.2319374981692167
Training Acc of Epoch 35: 0.7544985391260163
Testing Acc of Epoch 35: 0.7570217391304348
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.2506e-01 (2.2506e-01)	Acc 0.772461 (0.772461)
Epoch: [36][300/616]	Loss 2.3355e-01 (2.3209e-01)	Acc 0.739258 (0.754649)
Epoch: [36][600/616]	Loss 2.3958e-01 (2.3198e-01)	Acc 0.741211 (0.754987)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754422)
Training Loss of Epoch 36: 0.2321037939166635
Training Acc of Epoch 36: 0.7548669334349594
Testing Acc of Epoch 36: 0.7544217391304348
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3564e-01 (2.3564e-01)	Acc 0.746094 (0.746094)
Epoch: [37][300/616]	Loss 2.2631e-01 (2.3232e-01)	Acc 0.763672 (0.753627)
Epoch: [37][600/616]	Loss 2.3073e-01 (2.3248e-01)	Acc 0.761719 (0.753827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.756709)
Training Loss of Epoch 37: 0.23252782409753256
Training Acc of Epoch 37: 0.7537268165650407
Testing Acc of Epoch 37: 0.7567086956521739
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.4082e-01 (2.4082e-01)	Acc 0.721680 (0.721680)
Epoch: [38][300/616]	Loss 2.1989e-01 (2.3196e-01)	Acc 0.767578 (0.754630)
Epoch: [38][600/616]	Loss 2.4094e-01 (2.3232e-01)	Acc 0.760742 (0.754283)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754613)
Training Loss of Epoch 38: 0.23237517056910972
Training Acc of Epoch 38: 0.7542206554878049
Testing Acc of Epoch 38: 0.7546130434782609
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.3613e-01 (2.3613e-01)	Acc 0.745117 (0.745117)
Epoch: [39][300/616]	Loss 2.4779e-01 (2.3270e-01)	Acc 0.740234 (0.753984)
Epoch: [39][600/616]	Loss 2.4285e-01 (2.3218e-01)	Acc 0.745117 (0.754569)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755196)
Training Loss of Epoch 39: 0.2320680909282793
Training Acc of Epoch 39: 0.7547335492886179
Testing Acc of Epoch 39: 0.755195652173913
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.2510e-01 (2.2510e-01)	Acc 0.756836 (0.756836)
Epoch: [40][300/616]	Loss 2.2543e-01 (2.3250e-01)	Acc 0.759766 (0.754146)
Epoch: [40][600/616]	Loss 2.3567e-01 (2.3232e-01)	Acc 0.760742 (0.754210)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756043)
Training Loss of Epoch 40: 0.23225367817936873
Training Acc of Epoch 40: 0.7543397484756098
Testing Acc of Epoch 40: 0.7560434782608696
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.2377e-01 (2.2377e-01)	Acc 0.760742 (0.760742)
Epoch: [41][300/616]	Loss 2.3079e-01 (2.3199e-01)	Acc 0.766602 (0.754977)
Epoch: [41][600/616]	Loss 2.3856e-01 (2.3220e-01)	Acc 0.746094 (0.754732)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756122)
Training Loss of Epoch 41: 0.2321102423396537
Training Acc of Epoch 41: 0.7548288236788618
Testing Acc of Epoch 41: 0.7561217391304348
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.5502e-01 (2.5502e-01)	Acc 0.728516 (0.728516)
Epoch: [42][300/616]	Loss 2.2891e-01 (2.3266e-01)	Acc 0.753906 (0.753384)
Epoch: [42][600/616]	Loss 2.4273e-01 (2.3247e-01)	Acc 0.738281 (0.754083)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748761)
Training Loss of Epoch 42: 0.23250847291655657
Training Acc of Epoch 42: 0.7540189913617886
Testing Acc of Epoch 42: 0.7487608695652174
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3041e-01 (2.3041e-01)	Acc 0.759766 (0.759766)
Epoch: [43][300/616]	Loss 2.4215e-01 (2.3264e-01)	Acc 0.741211 (0.753728)
Epoch: [43][600/616]	Loss 2.3685e-01 (2.3275e-01)	Acc 0.734375 (0.753830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754935)
Training Loss of Epoch 43: 0.2327041981181478
Training Acc of Epoch 43: 0.7538617886178862
Testing Acc of Epoch 43: 0.7549347826086956
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.2411e-01 (2.2411e-01)	Acc 0.757812 (0.757812)
Epoch: [44][300/616]	Loss 2.3663e-01 (2.3179e-01)	Acc 0.755859 (0.754711)
Epoch: [44][600/616]	Loss 2.3734e-01 (2.3267e-01)	Acc 0.744141 (0.753762)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754409)
Training Loss of Epoch 44: 0.23263327595664235
Training Acc of Epoch 44: 0.7538633765243903
Testing Acc of Epoch 44: 0.7544086956521739
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.4209e-01 (2.4209e-01)	Acc 0.735352 (0.735352)
Epoch: [45][300/616]	Loss 2.3434e-01 (2.3349e-01)	Acc 0.752930 (0.752706)
Epoch: [45][600/616]	Loss 2.3060e-01 (2.3298e-01)	Acc 0.756836 (0.753695)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756278)
Training Loss of Epoch 45: 0.23292224588917523
Training Acc of Epoch 45: 0.7537871570121951
Testing Acc of Epoch 45: 0.7562782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.2099e-01 (2.2099e-01)	Acc 0.773438 (0.773438)
Epoch: [46][300/616]	Loss 2.1913e-01 (2.3250e-01)	Acc 0.764648 (0.754792)
Epoch: [46][600/616]	Loss 2.3208e-01 (2.3253e-01)	Acc 0.764648 (0.754496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755848)
Training Loss of Epoch 46: 0.23257563923432575
Training Acc of Epoch 46: 0.754419143800813
Testing Acc of Epoch 46: 0.7558478260869566
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2955e-01 (2.2955e-01)	Acc 0.744141 (0.744141)
Epoch: [47][300/616]	Loss 2.3927e-01 (2.3218e-01)	Acc 0.742188 (0.754649)
Epoch: [47][600/616]	Loss 2.2460e-01 (2.3234e-01)	Acc 0.762695 (0.754535)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754065)
Training Loss of Epoch 47: 0.23236496472746376
Training Acc of Epoch 47: 0.7545461763211382
Testing Acc of Epoch 47: 0.7540652173913044
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4714e-01 (2.4714e-01)	Acc 0.734375 (0.734375)
Epoch: [48][300/616]	Loss 2.4757e-01 (2.3349e-01)	Acc 0.739258 (0.753059)
Epoch: [48][600/616]	Loss 2.3561e-01 (2.3281e-01)	Acc 0.754883 (0.753944)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754426)
Training Loss of Epoch 48: 0.23273613147619293
Training Acc of Epoch 48: 0.754053925304878
Testing Acc of Epoch 48: 0.7544260869565217
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4003e-01 (2.4003e-01)	Acc 0.750000 (0.750000)
Epoch: [49][300/616]	Loss 2.2290e-01 (2.3310e-01)	Acc 0.767578 (0.753971)
Epoch: [49][600/616]	Loss 2.2352e-01 (2.3293e-01)	Acc 0.772461 (0.753965)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753452)
Training Loss of Epoch 49: 0.2329307057508608
Training Acc of Epoch 49: 0.7539919969512195
Testing Acc of Epoch 49: 0.7534521739130435
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.3608e-01 (2.3608e-01)	Acc 0.744141 (0.744141)
Epoch: [50][300/616]	Loss 2.2476e-01 (2.3229e-01)	Acc 0.762695 (0.754218)
Epoch: [50][600/616]	Loss 2.3261e-01 (2.3254e-01)	Acc 0.749023 (0.754022)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.756465)
Training Loss of Epoch 50: 0.23257497183191098
Training Acc of Epoch 50: 0.7540015243902439
Testing Acc of Epoch 50: 0.7564652173913043
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.2993e-01 (2.2993e-01)	Acc 0.757812 (0.757812)
Epoch: [51][300/616]	Loss 2.2162e-01 (2.3326e-01)	Acc 0.773438 (0.753241)
Epoch: [51][600/616]	Loss 2.5411e-01 (2.3266e-01)	Acc 0.720703 (0.754030)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752617)
Training Loss of Epoch 51: 0.2325782314790943
Training Acc of Epoch 51: 0.7540856834349593
Testing Acc of Epoch 51: 0.7526173913043478
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.3795e-01 (2.3795e-01)	Acc 0.749023 (0.749023)
Epoch: [52][300/616]	Loss 2.1702e-01 (2.3264e-01)	Acc 0.773438 (0.754143)
Epoch: [52][600/616]	Loss 2.4498e-01 (2.3274e-01)	Acc 0.732422 (0.753948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.751113)
Training Loss of Epoch 52: 0.23279342011707585
Training Acc of Epoch 52: 0.7538649644308943
Testing Acc of Epoch 52: 0.7511130434782609
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3171e-01 (2.3171e-01)	Acc 0.764648 (0.764648)
Epoch: [53][300/616]	Loss 2.4445e-01 (2.3314e-01)	Acc 0.733398 (0.753712)
Epoch: [53][600/616]	Loss 2.2688e-01 (2.3297e-01)	Acc 0.759766 (0.753890)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751996)
Training Loss of Epoch 53: 0.23295043001814586
Training Acc of Epoch 53: 0.75395706300813
Testing Acc of Epoch 53: 0.7519956521739131
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.2308e-01 (2.2308e-01)	Acc 0.766602 (0.766602)
Epoch: [54][300/616]	Loss 2.2674e-01 (2.3292e-01)	Acc 0.765625 (0.753598)
Epoch: [54][600/616]	Loss 2.4014e-01 (2.3313e-01)	Acc 0.741211 (0.753591)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754517)
Training Loss of Epoch 54: 0.233138784208918
Training Acc of Epoch 54: 0.7536204268292683
Testing Acc of Epoch 54: 0.7545173913043478
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3648e-01 (2.3648e-01)	Acc 0.751953 (0.751953)
Epoch: [55][300/616]	Loss 2.5176e-01 (2.3338e-01)	Acc 0.717773 (0.753481)
Epoch: [55][600/616]	Loss 2.4588e-01 (2.3358e-01)	Acc 0.736328 (0.753450)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755952)
Training Loss of Epoch 55: 0.2335796895308223
Training Acc of Epoch 55: 0.7534870426829269
Testing Acc of Epoch 55: 0.7559521739130435
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2972e-01 (2.2972e-01)	Acc 0.750000 (0.750000)
Epoch: [56][300/616]	Loss 2.3436e-01 (2.3417e-01)	Acc 0.755859 (0.752803)
Epoch: [56][600/616]	Loss 2.2796e-01 (2.3394e-01)	Acc 0.760742 (0.752519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.753496)
Training Loss of Epoch 56: 0.23392186339308577
Training Acc of Epoch 56: 0.7526438643292683
Testing Acc of Epoch 56: 0.753495652173913
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3853e-01 (2.3853e-01)	Acc 0.762695 (0.762695)
Epoch: [57][300/616]	Loss 2.4035e-01 (2.3347e-01)	Acc 0.737305 (0.753695)
Epoch: [57][600/616]	Loss 2.3366e-01 (2.3381e-01)	Acc 0.756836 (0.752910)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.754787)
Training Loss of Epoch 57: 0.23375719259909497
Training Acc of Epoch 57: 0.7529408028455284
Testing Acc of Epoch 57: 0.7547869565217391
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.0764e-01 (2.0764e-01)	Acc 0.807617 (0.807617)
Epoch: [58][300/616]	Loss 2.4015e-01 (2.3401e-01)	Acc 0.732422 (0.752479)
Epoch: [58][600/616]	Loss 2.3256e-01 (2.3365e-01)	Acc 0.744141 (0.752967)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752657)
Training Loss of Epoch 58: 0.23357437359608285
Training Acc of Epoch 58: 0.7530583079268293
Testing Acc of Epoch 58: 0.7526565217391304
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.2426e-01 (2.2426e-01)	Acc 0.760742 (0.760742)
Epoch: [59][300/616]	Loss 2.5005e-01 (2.3488e-01)	Acc 0.723633 (0.751648)
Epoch: [59][600/616]	Loss 2.2212e-01 (2.3438e-01)	Acc 0.765625 (0.752457)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.754291)
Training Loss of Epoch 59: 0.23430508956676577
Training Acc of Epoch 59: 0.7525025406504066
Testing Acc of Epoch 59: 0.7542913043478261
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.4420e-01 (2.4420e-01)	Acc 0.748047 (0.748047)
Epoch: [60][300/616]	Loss 2.2348e-01 (2.3403e-01)	Acc 0.770508 (0.753021)
Epoch: [60][600/616]	Loss 2.3808e-01 (2.3392e-01)	Acc 0.756836 (0.753121)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752226)
Training Loss of Epoch 60: 0.23393719671218377
Training Acc of Epoch 60: 0.7531154725609757
Testing Acc of Epoch 60: 0.7522260869565217
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3732e-01 (2.3732e-01)	Acc 0.749023 (0.749023)
Epoch: [61][300/616]	Loss 2.3830e-01 (2.3494e-01)	Acc 0.736328 (0.751470)
Epoch: [61][600/616]	Loss 2.2969e-01 (2.3409e-01)	Acc 0.758789 (0.752605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753578)
Training Loss of Epoch 61: 0.23410370912009137
Training Acc of Epoch 61: 0.7526263973577236
Testing Acc of Epoch 61: 0.7535782608695653
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3927e-01 (2.3927e-01)	Acc 0.746094 (0.746094)
Epoch: [62][300/616]	Loss 2.1985e-01 (2.3387e-01)	Acc 0.750000 (0.752917)
Epoch: [62][600/616]	Loss 2.4397e-01 (2.3431e-01)	Acc 0.747070 (0.752353)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754557)
Training Loss of Epoch 62: 0.2343080230602404
Training Acc of Epoch 62: 0.7523723323170731
Testing Acc of Epoch 62: 0.7545565217391305
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3749e-01 (2.3749e-01)	Acc 0.751953 (0.751953)
Epoch: [63][300/616]	Loss 2.3616e-01 (2.3458e-01)	Acc 0.745117 (0.751982)
Epoch: [63][600/616]	Loss 2.2338e-01 (2.3432e-01)	Acc 0.764648 (0.752569)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.751400)
Training Loss of Epoch 63: 0.23434960059518736
Training Acc of Epoch 63: 0.7525009527439024
Testing Acc of Epoch 63: 0.7514
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.4182e-01 (2.4182e-01)	Acc 0.748047 (0.748047)
Epoch: [64][300/616]	Loss 2.4302e-01 (2.3437e-01)	Acc 0.742188 (0.752583)
Epoch: [64][600/616]	Loss 2.3336e-01 (2.3425e-01)	Acc 0.754883 (0.752694)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.750961)
Training Loss of Epoch 64: 0.23427163580568824
Training Acc of Epoch 64: 0.7526962652439024
Testing Acc of Epoch 64: 0.7509608695652173
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4024e-01 (2.4024e-01)	Acc 0.748047 (0.748047)
Epoch: [65][300/616]	Loss 2.3677e-01 (2.3355e-01)	Acc 0.754883 (0.753205)
Epoch: [65][600/616]	Loss 2.2715e-01 (2.3405e-01)	Acc 0.758789 (0.752307)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755904)
Training Loss of Epoch 65: 0.23404802151811802
Training Acc of Epoch 65: 0.7523882113821139
Testing Acc of Epoch 65: 0.755904347826087
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4033e-01 (2.4033e-01)	Acc 0.752930 (0.752930)
Epoch: [66][300/616]	Loss 2.2648e-01 (2.3500e-01)	Acc 0.778320 (0.751924)
Epoch: [66][600/616]	Loss 2.2422e-01 (2.3479e-01)	Acc 0.758789 (0.752358)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754065)
Training Loss of Epoch 66: 0.23479036322938718
Training Acc of Epoch 66: 0.7523786839430894
Testing Acc of Epoch 66: 0.7540652173913044
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.2140e-01 (2.2140e-01)	Acc 0.780273 (0.780273)
Epoch: [67][300/616]	Loss 2.1871e-01 (2.3406e-01)	Acc 0.771484 (0.752527)
Epoch: [67][600/616]	Loss 2.3689e-01 (2.3447e-01)	Acc 0.750000 (0.752130)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754930)
Training Loss of Epoch 67: 0.23441533889712357
Training Acc of Epoch 67: 0.7522135416666667
Testing Acc of Epoch 67: 0.7549304347826087
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.4463e-01 (2.4463e-01)	Acc 0.745117 (0.745117)
Epoch: [68][300/616]	Loss 2.3288e-01 (2.3468e-01)	Acc 0.755859 (0.751804)
Epoch: [68][600/616]	Loss 2.2028e-01 (2.3425e-01)	Acc 0.780273 (0.752237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755017)
Training Loss of Epoch 68: 0.23424826669014567
Training Acc of Epoch 68: 0.752196074695122
Testing Acc of Epoch 68: 0.7550173913043479
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.3579e-01 (2.3579e-01)	Acc 0.747070 (0.747070)
Epoch: [69][300/616]	Loss 2.2811e-01 (2.3455e-01)	Acc 0.766602 (0.751574)
Epoch: [69][600/616]	Loss 2.2369e-01 (2.3426e-01)	Acc 0.766602 (0.752447)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754630)
Training Loss of Epoch 69: 0.23421147310636878
Training Acc of Epoch 69: 0.7524628429878049
Testing Acc of Epoch 69: 0.7546304347826087
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.3419e-01 (2.3419e-01)	Acc 0.755859 (0.755859)
Epoch: [70][300/616]	Loss 2.4205e-01 (2.3364e-01)	Acc 0.738281 (0.753131)
Epoch: [70][600/616]	Loss 2.4927e-01 (2.3409e-01)	Acc 0.737305 (0.752449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755143)
Training Loss of Epoch 70: 0.23403687816325242
Training Acc of Epoch 70: 0.752564469004065
Testing Acc of Epoch 70: 0.7551434782608696
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.2063e-01 (2.2063e-01)	Acc 0.776367 (0.776367)
Epoch: [71][300/616]	Loss 2.4126e-01 (2.3376e-01)	Acc 0.737305 (0.752673)
Epoch: [71][600/616]	Loss 2.3185e-01 (2.3417e-01)	Acc 0.762695 (0.752302)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755330)
Training Loss of Epoch 71: 0.23410682285704265
Training Acc of Epoch 71: 0.752440612296748
Testing Acc of Epoch 71: 0.7553304347826088
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.2744e-01 (2.2744e-01)	Acc 0.765625 (0.765625)
Epoch: [72][300/616]	Loss 2.1681e-01 (2.3355e-01)	Acc 0.770508 (0.752813)
Epoch: [72][600/616]	Loss 2.3594e-01 (2.3363e-01)	Acc 0.757812 (0.752980)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755748)
Training Loss of Epoch 72: 0.23367668683451365
Training Acc of Epoch 72: 0.753001143292683
Testing Acc of Epoch 72: 0.7557478260869566
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4021e-01 (2.4021e-01)	Acc 0.739258 (0.739258)
Epoch: [73][300/616]	Loss 2.4207e-01 (2.3381e-01)	Acc 0.747070 (0.753617)
Epoch: [73][600/616]	Loss 2.2630e-01 (2.3395e-01)	Acc 0.761719 (0.753001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753361)
Training Loss of Epoch 73: 0.23394478070057503
Training Acc of Epoch 73: 0.7529836763211382
Testing Acc of Epoch 73: 0.7533608695652174
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3223e-01 (2.3223e-01)	Acc 0.759766 (0.759766)
Epoch: [74][300/616]	Loss 2.3097e-01 (2.3386e-01)	Acc 0.751953 (0.752878)
Epoch: [74][600/616]	Loss 2.2801e-01 (2.3399e-01)	Acc 0.759766 (0.752808)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751113)
Training Loss of Epoch 74: 0.23395982791737813
Training Acc of Epoch 74: 0.7528693470528456
Testing Acc of Epoch 74: 0.7511130434782609
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4273e-01 (2.4273e-01)	Acc 0.739258 (0.739258)
Epoch: [75][300/616]	Loss 2.2078e-01 (2.3031e-01)	Acc 0.781250 (0.756112)
Epoch: [75][600/616]	Loss 2.2599e-01 (2.3030e-01)	Acc 0.771484 (0.755853)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.757113)
Training Loss of Epoch 75: 0.23026205029914049
Training Acc of Epoch 75: 0.7558911331300813
Testing Acc of Epoch 75: 0.7571130434782609
Model with the best training loss saved! The loss is 0.23026205029914049
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.1932e-01 (2.1932e-01)	Acc 0.764648 (0.764648)
Epoch: [76][300/616]	Loss 2.3745e-01 (2.3148e-01)	Acc 0.737305 (0.754305)
Epoch: [76][600/616]	Loss 2.4122e-01 (2.3056e-01)	Acc 0.734375 (0.755791)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756357)
Training Loss of Epoch 76: 0.2305485981993559
Training Acc of Epoch 76: 0.7558689024390244
Testing Acc of Epoch 76: 0.7563565217391305
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.3049e-01 (2.3049e-01)	Acc 0.748047 (0.748047)
Epoch: [77][300/616]	Loss 2.3718e-01 (2.3112e-01)	Acc 0.744141 (0.755535)
Epoch: [77][600/616]	Loss 2.1868e-01 (2.3080e-01)	Acc 0.772461 (0.755539)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.756817)
Training Loss of Epoch 77: 0.2307703715998952
Training Acc of Epoch 77: 0.7554258765243902
Testing Acc of Epoch 77: 0.7568173913043478
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.2685e-01 (2.2685e-01)	Acc 0.761719 (0.761719)
Epoch: [78][300/616]	Loss 2.1529e-01 (2.3014e-01)	Acc 0.777344 (0.756353)
Epoch: [78][600/616]	Loss 2.3423e-01 (2.3093e-01)	Acc 0.757812 (0.755460)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.755204)
Training Loss of Epoch 78: 0.23082418715566155
Training Acc of Epoch 78: 0.755565612296748
Testing Acc of Epoch 78: 0.7552043478260869
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.1815e-01 (2.1815e-01)	Acc 0.784180 (0.784180)
Epoch: [79][300/616]	Loss 2.2661e-01 (2.3059e-01)	Acc 0.761719 (0.755691)
Epoch: [79][600/616]	Loss 2.2973e-01 (2.3106e-01)	Acc 0.760742 (0.755190)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756270)
Training Loss of Epoch 79: 0.2310499627415727
Training Acc of Epoch 79: 0.7552432672764228
Testing Acc of Epoch 79: 0.7562695652173913
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3468e-01 (2.3468e-01)	Acc 0.752930 (0.752930)
Epoch: [80][300/616]	Loss 2.3373e-01 (2.3171e-01)	Acc 0.758789 (0.754627)
Epoch: [80][600/616]	Loss 2.2987e-01 (2.3117e-01)	Acc 0.747070 (0.755375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.757365)
Training Loss of Epoch 80: 0.2311541500857206
Training Acc of Epoch 80: 0.7554401676829269
Testing Acc of Epoch 80: 0.7573652173913044
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4861e-01 (2.4861e-01)	Acc 0.729492 (0.729492)
Epoch: [81][300/616]	Loss 2.2614e-01 (2.3164e-01)	Acc 0.770508 (0.754545)
Epoch: [81][600/616]	Loss 2.4114e-01 (2.3107e-01)	Acc 0.739258 (0.755463)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755978)
Training Loss of Epoch 81: 0.2310015279103101
Training Acc of Epoch 81: 0.7555814913617886
Testing Acc of Epoch 81: 0.7559782608695652
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.4310e-01 (2.4310e-01)	Acc 0.750000 (0.750000)
Epoch: [82][300/616]	Loss 2.1950e-01 (2.3081e-01)	Acc 0.762695 (0.755700)
Epoch: [82][600/616]	Loss 2.4089e-01 (2.3099e-01)	Acc 0.746094 (0.755279)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.756461)
Training Loss of Epoch 82: 0.23097555436254516
Training Acc of Epoch 82: 0.7553178988821139
Testing Acc of Epoch 82: 0.7564608695652174
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3787e-01 (2.3787e-01)	Acc 0.749023 (0.749023)
Epoch: [83][300/616]	Loss 2.2823e-01 (2.3109e-01)	Acc 0.745117 (0.755522)
Epoch: [83][600/616]	Loss 2.3996e-01 (2.3095e-01)	Acc 0.750977 (0.755879)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755126)
Training Loss of Epoch 83: 0.23090334224507092
Training Acc of Epoch 83: 0.7559387703252033
Testing Acc of Epoch 83: 0.7551260869565217
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.4000e-01 (2.4000e-01)	Acc 0.745117 (0.745117)
Epoch: [84][300/616]	Loss 2.3200e-01 (2.3123e-01)	Acc 0.759766 (0.755499)
Epoch: [84][600/616]	Loss 2.3598e-01 (2.3087e-01)	Acc 0.751953 (0.755692)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754387)
Training Loss of Epoch 84: 0.23076383003859016
Training Acc of Epoch 84: 0.7557815675813008
Testing Acc of Epoch 84: 0.7543869565217391
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.2544e-01 (2.2544e-01)	Acc 0.769531 (0.769531)
Epoch: [85][300/616]	Loss 2.2405e-01 (2.3059e-01)	Acc 0.769531 (0.755464)
Epoch: [85][600/616]	Loss 2.3504e-01 (2.3067e-01)	Acc 0.766602 (0.755690)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756048)
Training Loss of Epoch 85: 0.23075591824888214
Training Acc of Epoch 85: 0.7555957825203252
Testing Acc of Epoch 85: 0.7560478260869565
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3300e-01 (2.3300e-01)	Acc 0.751953 (0.751953)
Epoch: [86][300/616]	Loss 2.3273e-01 (2.3077e-01)	Acc 0.758789 (0.755791)
Epoch: [86][600/616]	Loss 2.3734e-01 (2.3102e-01)	Acc 0.753906 (0.755726)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757622)
Training Loss of Epoch 86: 0.23106549164144005
Training Acc of Epoch 86: 0.7556497713414634
Testing Acc of Epoch 86: 0.7576217391304347
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.2805e-01 (2.2805e-01)	Acc 0.769531 (0.769531)
Epoch: [87][300/616]	Loss 2.3600e-01 (2.3107e-01)	Acc 0.751953 (0.755194)
Epoch: [87][600/616]	Loss 2.2557e-01 (2.3089e-01)	Acc 0.759766 (0.755495)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757122)
Training Loss of Epoch 87: 0.2308378720913476
Training Acc of Epoch 87: 0.7555227388211382
Testing Acc of Epoch 87: 0.7571217391304348
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.1460e-01 (2.1460e-01)	Acc 0.775391 (0.775391)
Epoch: [88][300/616]	Loss 2.3233e-01 (2.3118e-01)	Acc 0.764648 (0.755833)
Epoch: [88][600/616]	Loss 2.3652e-01 (2.3103e-01)	Acc 0.761719 (0.755378)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755943)
Training Loss of Epoch 88: 0.23112762417735122
Training Acc of Epoch 88: 0.7552623221544715
Testing Acc of Epoch 88: 0.7559434782608696
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2368e-01 (2.2368e-01)	Acc 0.765625 (0.765625)
Epoch: [89][300/616]	Loss 2.2839e-01 (2.3092e-01)	Acc 0.750000 (0.755908)
Epoch: [89][600/616]	Loss 2.3033e-01 (2.3108e-01)	Acc 0.753906 (0.755148)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756822)
Training Loss of Epoch 89: 0.23111832190335282
Training Acc of Epoch 89: 0.7551305259146341
Testing Acc of Epoch 89: 0.7568217391304348
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4150e-01 (2.4150e-01)	Acc 0.740234 (0.740234)
Epoch: [90][300/616]	Loss 2.3296e-01 (2.3098e-01)	Acc 0.755859 (0.755123)
Epoch: [90][600/616]	Loss 2.4761e-01 (2.3088e-01)	Acc 0.739258 (0.755479)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.756435)
Training Loss of Epoch 90: 0.23090297335047064
Training Acc of Epoch 90: 0.7554846290650407
Testing Acc of Epoch 90: 0.7564347826086957
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2934e-01 (2.2934e-01)	Acc 0.752930 (0.752930)
Epoch: [91][300/616]	Loss 2.3639e-01 (2.3165e-01)	Acc 0.746094 (0.755107)
Epoch: [91][600/616]	Loss 2.3489e-01 (2.3123e-01)	Acc 0.742188 (0.755364)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756326)
Training Loss of Epoch 91: 0.2312294827244146
Training Acc of Epoch 91: 0.7553401295731708
Testing Acc of Epoch 91: 0.7563260869565217
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.2159e-01 (2.2159e-01)	Acc 0.744141 (0.744141)
Epoch: [92][300/616]	Loss 2.3054e-01 (2.3085e-01)	Acc 0.756836 (0.755619)
Epoch: [92][600/616]	Loss 2.3426e-01 (2.3118e-01)	Acc 0.740234 (0.755073)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755422)
Training Loss of Epoch 92: 0.23114666839440665
Training Acc of Epoch 92: 0.7551479928861788
Testing Acc of Epoch 92: 0.7554217391304348
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2780e-01 (2.2780e-01)	Acc 0.764648 (0.764648)
Epoch: [93][300/616]	Loss 2.3872e-01 (2.3146e-01)	Acc 0.753906 (0.755029)
Epoch: [93][600/616]	Loss 2.3382e-01 (2.3121e-01)	Acc 0.739258 (0.755131)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.755578)
Training Loss of Epoch 93: 0.23121501679827527
Training Acc of Epoch 93: 0.7551845147357723
Testing Acc of Epoch 93: 0.7555782608695653
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3541e-01 (2.3541e-01)	Acc 0.749023 (0.749023)
Epoch: [94][300/616]	Loss 2.3356e-01 (2.3158e-01)	Acc 0.746094 (0.754133)
Epoch: [94][600/616]	Loss 2.1273e-01 (2.3107e-01)	Acc 0.785156 (0.755343)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755970)
Training Loss of Epoch 94: 0.23110359887766643
Training Acc of Epoch 94: 0.7554084095528455
Testing Acc of Epoch 94: 0.7559695652173913
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2916e-01 (2.2916e-01)	Acc 0.746094 (0.746094)
Epoch: [95][300/616]	Loss 2.1263e-01 (2.3082e-01)	Acc 0.778320 (0.755820)
Epoch: [95][600/616]	Loss 2.4478e-01 (2.3102e-01)	Acc 0.741211 (0.755625)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757970)
Training Loss of Epoch 95: 0.23103409407100056
Training Acc of Epoch 95: 0.7556037220528455
Testing Acc of Epoch 95: 0.7579695652173913
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3658e-01 (2.3658e-01)	Acc 0.741211 (0.741211)
Epoch: [96][300/616]	Loss 2.3474e-01 (2.3063e-01)	Acc 0.750977 (0.755055)
Epoch: [96][600/616]	Loss 2.4120e-01 (2.3114e-01)	Acc 0.735352 (0.754919)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.755391)
Training Loss of Epoch 96: 0.23110783482954755
Training Acc of Epoch 96: 0.7550701854674797
Testing Acc of Epoch 96: 0.7553913043478261
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2467e-01 (2.2467e-01)	Acc 0.777344 (0.777344)
Epoch: [97][300/616]	Loss 2.2955e-01 (2.3213e-01)	Acc 0.750000 (0.753348)
Epoch: [97][600/616]	Loss 2.4080e-01 (2.3145e-01)	Acc 0.738281 (0.755089)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755970)
Training Loss of Epoch 97: 0.23145920227213604
Training Acc of Epoch 97: 0.7550971798780488
Testing Acc of Epoch 97: 0.7559695652173913
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.2979e-01 (2.2979e-01)	Acc 0.750977 (0.750977)
Epoch: [98][300/616]	Loss 2.2479e-01 (2.3108e-01)	Acc 0.761719 (0.754980)
Epoch: [98][600/616]	Loss 2.4204e-01 (2.3085e-01)	Acc 0.743164 (0.755474)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756674)
Training Loss of Epoch 98: 0.23084950895328832
Training Acc of Epoch 98: 0.7554560467479675
Testing Acc of Epoch 98: 0.7566739130434783
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2115e-01 (2.2115e-01)	Acc 0.762695 (0.762695)
Epoch: [99][300/616]	Loss 2.4080e-01 (2.3095e-01)	Acc 0.747070 (0.755048)
Epoch: [99][600/616]	Loss 2.2760e-01 (2.3105e-01)	Acc 0.761719 (0.755518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.756457)
Training Loss of Epoch 99: 0.2311165440857895
Training Acc of Epoch 99: 0.7554655741869919
Testing Acc of Epoch 99: 0.7564565217391305
Early stopping not satisfied.
