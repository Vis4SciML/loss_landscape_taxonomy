train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.003125
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.003125/lr_decay/JT_6b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.003125
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0010e-01 (5.0010e-01)	Acc 0.271484 (0.271484)
Epoch: [0][300/616]	Loss 2.6397e-01 (2.9609e-01)	Acc 0.720703 (0.685793)
Epoch: [0][600/616]	Loss 2.5612e-01 (2.7335e-01)	Acc 0.725586 (0.711652)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.741004)
Training Loss of Epoch 0: 0.27274634525543306
Training Acc of Epoch 0: 0.7122903963414634
Testing Acc of Epoch 0: 0.7410043478260869
Model with the best training loss saved! The loss is 0.27274634525543306
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4547e-01 (2.4547e-01)	Acc 0.736328 (0.736328)
Epoch: [1][300/616]	Loss 2.4874e-01 (2.4275e-01)	Acc 0.736328 (0.745532)
Epoch: [1][600/616]	Loss 2.3397e-01 (2.4126e-01)	Acc 0.748047 (0.746476)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748800)
Training Loss of Epoch 1: 0.2412832276850212
Training Acc of Epoch 1: 0.7463986280487804
Testing Acc of Epoch 1: 0.7488
Model with the best training loss saved! The loss is 0.2412832276850212
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5262e-01 (2.5262e-01)	Acc 0.731445 (0.731445)
Epoch: [2][300/616]	Loss 2.3895e-01 (2.3848e-01)	Acc 0.752930 (0.749195)
Epoch: [2][600/616]	Loss 2.2882e-01 (2.3747e-01)	Acc 0.775391 (0.749787)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749661)
Training Loss of Epoch 2: 0.2374756474805072
Training Acc of Epoch 2: 0.7497729293699187
Testing Acc of Epoch 2: 0.7496608695652174
Model with the best training loss saved! The loss is 0.2374756474805072
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3722e-01 (2.3722e-01)	Acc 0.755859 (0.755859)
Epoch: [3][300/616]	Loss 2.4119e-01 (2.3535e-01)	Acc 0.735352 (0.751768)
Epoch: [3][600/616]	Loss 2.3959e-01 (2.3513e-01)	Acc 0.744141 (0.751849)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.751304)
Training Loss of Epoch 3: 0.23512739873513944
Training Acc of Epoch 3: 0.7518419715447154
Testing Acc of Epoch 3: 0.7513043478260869
Model with the best training loss saved! The loss is 0.23512739873513944
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.1343e-01 (2.1343e-01)	Acc 0.772461 (0.772461)
Epoch: [4][300/616]	Loss 2.3554e-01 (2.3448e-01)	Acc 0.755859 (0.752394)
Epoch: [4][600/616]	Loss 2.2651e-01 (2.3375e-01)	Acc 0.767578 (0.752835)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755152)
Training Loss of Epoch 4: 0.23376419927046552
Training Acc of Epoch 4: 0.7529026930894309
Testing Acc of Epoch 4: 0.7551521739130435
Model with the best training loss saved! The loss is 0.23376419927046552
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3347e-01 (2.3347e-01)	Acc 0.741211 (0.741211)
Epoch: [5][300/616]	Loss 2.5218e-01 (2.3334e-01)	Acc 0.723633 (0.753676)
Epoch: [5][600/616]	Loss 2.4807e-01 (2.3331e-01)	Acc 0.738281 (0.753534)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756530)
Training Loss of Epoch 5: 0.233218886769884
Training Acc of Epoch 5: 0.7535950203252032
Testing Acc of Epoch 5: 0.7565304347826087
Model with the best training loss saved! The loss is 0.233218886769884
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3731e-01 (2.3731e-01)	Acc 0.746094 (0.746094)
Epoch: [6][300/616]	Loss 2.2864e-01 (2.3216e-01)	Acc 0.765625 (0.754386)
Epoch: [6][600/616]	Loss 2.3091e-01 (2.3263e-01)	Acc 0.751953 (0.754163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753009)
Training Loss of Epoch 6: 0.23258863176272168
Training Acc of Epoch 6: 0.7542063643292682
Testing Acc of Epoch 6: 0.7530086956521739
Model with the best training loss saved! The loss is 0.23258863176272168
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.2719e-01 (2.2719e-01)	Acc 0.773438 (0.773438)
Epoch: [7][300/616]	Loss 2.2777e-01 (2.3196e-01)	Acc 0.762695 (0.755113)
Epoch: [7][600/616]	Loss 2.2566e-01 (2.3238e-01)	Acc 0.755859 (0.754324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756617)
Training Loss of Epoch 7: 0.23227692735873587
Training Acc of Epoch 7: 0.7545826981707318
Testing Acc of Epoch 7: 0.7566173913043478
Model with the best training loss saved! The loss is 0.23227692735873587
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4211e-01 (2.4211e-01)	Acc 0.747070 (0.747070)
Epoch: [8][300/616]	Loss 2.3414e-01 (2.3195e-01)	Acc 0.749023 (0.754886)
Epoch: [8][600/616]	Loss 2.3142e-01 (2.3225e-01)	Acc 0.750000 (0.754480)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754991)
Training Loss of Epoch 8: 0.2322463384004143
Training Acc of Epoch 8: 0.7544350228658536
Testing Acc of Epoch 8: 0.7549913043478261
Model with the best training loss saved! The loss is 0.2322463384004143
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.2106e-01 (2.2106e-01)	Acc 0.763672 (0.763672)
Epoch: [9][300/616]	Loss 2.3032e-01 (2.3159e-01)	Acc 0.762695 (0.755139)
Epoch: [9][600/616]	Loss 2.3460e-01 (2.3204e-01)	Acc 0.750000 (0.755023)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755187)
Training Loss of Epoch 9: 0.23205024616989664
Training Acc of Epoch 9: 0.7549606199186992
Testing Acc of Epoch 9: 0.7551869565217392
Model with the best training loss saved! The loss is 0.23205024616989664
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3750e-01 (2.3750e-01)	Acc 0.743164 (0.743164)
Epoch: [10][300/616]	Loss 2.3081e-01 (2.3170e-01)	Acc 0.758789 (0.755249)
Epoch: [10][600/616]	Loss 2.3080e-01 (2.3227e-01)	Acc 0.751953 (0.754748)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757530)
Training Loss of Epoch 10: 0.23233801877595545
Training Acc of Epoch 10: 0.7547129065040651
Testing Acc of Epoch 10: 0.7575304347826087
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.2046e-01 (2.2046e-01)	Acc 0.765625 (0.765625)
Epoch: [11][300/616]	Loss 2.3400e-01 (2.3351e-01)	Acc 0.740234 (0.753400)
Epoch: [11][600/616]	Loss 2.3468e-01 (2.3292e-01)	Acc 0.748047 (0.754057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756991)
Training Loss of Epoch 11: 0.23292064036780258
Training Acc of Epoch 11: 0.7540634527439024
Testing Acc of Epoch 11: 0.7569913043478261
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.3733e-01 (2.3733e-01)	Acc 0.752930 (0.752930)
Epoch: [12][300/616]	Loss 2.2407e-01 (2.3208e-01)	Acc 0.768555 (0.754867)
Epoch: [12][600/616]	Loss 2.4537e-01 (2.3242e-01)	Acc 0.739258 (0.754405)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.744817)
Training Loss of Epoch 12: 0.232418859126122
Training Acc of Epoch 12: 0.7543588033536586
Testing Acc of Epoch 12: 0.7448173913043479
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4258e-01 (2.4258e-01)	Acc 0.754883 (0.754883)
Epoch: [13][300/616]	Loss 2.3201e-01 (2.3201e-01)	Acc 0.761719 (0.754944)
Epoch: [13][600/616]	Loss 2.3618e-01 (2.3213e-01)	Acc 0.745117 (0.754665)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.756739)
Training Loss of Epoch 13: 0.23215330853694824
Training Acc of Epoch 13: 0.7546065167682927
Testing Acc of Epoch 13: 0.7567391304347826
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.0668e-01 (2.0668e-01)	Acc 0.781250 (0.781250)
Epoch: [14][300/616]	Loss 2.3620e-01 (2.3316e-01)	Acc 0.744141 (0.753595)
Epoch: [14][600/616]	Loss 2.4319e-01 (2.3254e-01)	Acc 0.741211 (0.754347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753439)
Training Loss of Epoch 14: 0.2325027022177611
Training Acc of Epoch 14: 0.7544651930894309
Testing Acc of Epoch 14: 0.7534391304347826
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.2367e-01 (2.2367e-01)	Acc 0.776367 (0.776367)
Epoch: [15][300/616]	Loss 2.2435e-01 (2.3275e-01)	Acc 0.764648 (0.753283)
Epoch: [15][600/616]	Loss 2.1376e-01 (2.3207e-01)	Acc 0.784180 (0.754527)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.758104)
Training Loss of Epoch 15: 0.23206873255047372
Training Acc of Epoch 15: 0.754612868394309
Testing Acc of Epoch 15: 0.7581043478260869
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.2612e-01 (2.2612e-01)	Acc 0.753906 (0.753906)
Epoch: [16][300/616]	Loss 2.3872e-01 (2.3172e-01)	Acc 0.754883 (0.754870)
Epoch: [16][600/616]	Loss 2.2862e-01 (2.3212e-01)	Acc 0.763672 (0.755040)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755617)
Training Loss of Epoch 16: 0.23226845039100183
Training Acc of Epoch 16: 0.754811356707317
Testing Acc of Epoch 16: 0.7556173913043478
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.1997e-01 (2.1997e-01)	Acc 0.772461 (0.772461)
Epoch: [17][300/616]	Loss 2.3938e-01 (2.3223e-01)	Acc 0.736328 (0.754841)
Epoch: [17][600/616]	Loss 2.2268e-01 (2.3238e-01)	Acc 0.769531 (0.754517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756487)
Training Loss of Epoch 17: 0.23231564542142355
Training Acc of Epoch 17: 0.7545747586382113
Testing Acc of Epoch 17: 0.7564869565217391
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4393e-01 (2.4393e-01)	Acc 0.730469 (0.730469)
Epoch: [18][300/616]	Loss 2.3363e-01 (2.3235e-01)	Acc 0.747070 (0.755295)
Epoch: [18][600/616]	Loss 2.4214e-01 (2.3217e-01)	Acc 0.737305 (0.754790)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754957)
Training Loss of Epoch 18: 0.23210123903383084
Training Acc of Epoch 18: 0.7548161204268292
Testing Acc of Epoch 18: 0.7549565217391304
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.5007e-01 (2.5007e-01)	Acc 0.721680 (0.721680)
Epoch: [19][300/616]	Loss 2.3047e-01 (2.3196e-01)	Acc 0.748047 (0.755477)
Epoch: [19][600/616]	Loss 2.3628e-01 (2.3212e-01)	Acc 0.755859 (0.754800)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.757296)
Training Loss of Epoch 19: 0.23205763106423666
Training Acc of Epoch 19: 0.7549018673780488
Testing Acc of Epoch 19: 0.757295652173913
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.1786e-01 (2.1786e-01)	Acc 0.779297 (0.779297)
Epoch: [20][300/616]	Loss 2.3704e-01 (2.3240e-01)	Acc 0.754883 (0.754247)
Epoch: [20][600/616]	Loss 2.2946e-01 (2.3225e-01)	Acc 0.761719 (0.754512)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755561)
Training Loss of Epoch 20: 0.23222839686928726
Training Acc of Epoch 20: 0.7545239456300813
Testing Acc of Epoch 20: 0.7555608695652174
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3892e-01 (2.3892e-01)	Acc 0.739258 (0.739258)
Epoch: [21][300/616]	Loss 2.2621e-01 (2.3131e-01)	Acc 0.760742 (0.755691)
Epoch: [21][600/616]	Loss 2.3781e-01 (2.3202e-01)	Acc 0.748047 (0.754875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755878)
Training Loss of Epoch 21: 0.23198968920765853
Training Acc of Epoch 21: 0.7548812245934959
Testing Acc of Epoch 21: 0.7558782608695652
Model with the best training loss saved! The loss is 0.23198968920765853
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.2053e-01 (2.2053e-01)	Acc 0.779297 (0.779297)
Epoch: [22][300/616]	Loss 2.3574e-01 (2.3226e-01)	Acc 0.759766 (0.754727)
Epoch: [22][600/616]	Loss 2.2501e-01 (2.3204e-01)	Acc 0.764648 (0.754884)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757004)
Training Loss of Epoch 22: 0.23210552862989223
Training Acc of Epoch 22: 0.7548637576219512
Testing Acc of Epoch 22: 0.7570043478260869
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3030e-01 (2.3030e-01)	Acc 0.752930 (0.752930)
Epoch: [23][300/616]	Loss 2.4441e-01 (2.3289e-01)	Acc 0.738281 (0.753757)
Epoch: [23][600/616]	Loss 2.3671e-01 (2.3256e-01)	Acc 0.743164 (0.754179)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753757)
Training Loss of Epoch 23: 0.232558909567391
Training Acc of Epoch 23: 0.7542571773373984
Testing Acc of Epoch 23: 0.7537565217391304
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2942e-01 (2.2942e-01)	Acc 0.761719 (0.761719)
Epoch: [24][300/616]	Loss 2.3221e-01 (2.3156e-01)	Acc 0.756836 (0.755162)
Epoch: [24][600/616]	Loss 2.2719e-01 (2.3249e-01)	Acc 0.774414 (0.754442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755270)
Training Loss of Epoch 24: 0.23240824101901636
Training Acc of Epoch 24: 0.7546319232723577
Testing Acc of Epoch 24: 0.7552695652173913
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.1892e-01 (2.1892e-01)	Acc 0.770508 (0.770508)
Epoch: [25][300/616]	Loss 2.3822e-01 (2.3198e-01)	Acc 0.747070 (0.755282)
Epoch: [25][600/616]	Loss 2.4834e-01 (2.3236e-01)	Acc 0.737305 (0.754530)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753309)
Training Loss of Epoch 25: 0.232407609403618
Training Acc of Epoch 25: 0.7544604293699188
Testing Acc of Epoch 25: 0.753308695652174
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3093e-01 (2.3093e-01)	Acc 0.749023 (0.749023)
Epoch: [26][300/616]	Loss 2.4002e-01 (2.3257e-01)	Acc 0.744141 (0.754575)
Epoch: [26][600/616]	Loss 2.3639e-01 (2.3275e-01)	Acc 0.750000 (0.753849)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.757535)
Training Loss of Epoch 26: 0.23260707411824202
Training Acc of Epoch 26: 0.7540126397357724
Testing Acc of Epoch 26: 0.7575347826086957
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4393e-01 (2.4393e-01)	Acc 0.737305 (0.737305)
Epoch: [27][300/616]	Loss 2.4383e-01 (2.3347e-01)	Acc 0.734375 (0.753244)
Epoch: [27][600/616]	Loss 2.3951e-01 (2.3281e-01)	Acc 0.744141 (0.754020)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.756213)
Training Loss of Epoch 27: 0.23272718891380278
Training Acc of Epoch 27: 0.7540555132113821
Testing Acc of Epoch 27: 0.7562130434782609
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.2069e-01 (2.2069e-01)	Acc 0.770508 (0.770508)
Epoch: [28][300/616]	Loss 2.3989e-01 (2.3182e-01)	Acc 0.748047 (0.755350)
Epoch: [28][600/616]	Loss 2.3025e-01 (2.3266e-01)	Acc 0.758789 (0.754464)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756052)
Training Loss of Epoch 28: 0.23254609173390922
Training Acc of Epoch 28: 0.7545509400406504
Testing Acc of Epoch 28: 0.7560521739130435
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.3299e-01 (2.3299e-01)	Acc 0.756836 (0.756836)
Epoch: [29][300/616]	Loss 2.3051e-01 (2.3256e-01)	Acc 0.762695 (0.754364)
Epoch: [29][600/616]	Loss 2.2427e-01 (2.3256e-01)	Acc 0.771484 (0.754173)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755987)
Training Loss of Epoch 29: 0.23244444730320596
Training Acc of Epoch 29: 0.7543238694105691
Testing Acc of Epoch 29: 0.7559869565217391
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2901e-01 (2.2901e-01)	Acc 0.749023 (0.749023)
Epoch: [30][300/616]	Loss 2.2653e-01 (2.3242e-01)	Acc 0.753906 (0.754201)
Epoch: [30][600/616]	Loss 2.5335e-01 (2.3229e-01)	Acc 0.719727 (0.754438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757622)
Training Loss of Epoch 30: 0.23233557677850492
Training Acc of Epoch 30: 0.7544016768292683
Testing Acc of Epoch 30: 0.7576217391304347
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.2248e-01 (2.2248e-01)	Acc 0.765625 (0.765625)
Epoch: [31][300/616]	Loss 2.3506e-01 (2.3334e-01)	Acc 0.743164 (0.753455)
Epoch: [31][600/616]	Loss 2.2744e-01 (2.3259e-01)	Acc 0.757812 (0.754064)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755670)
Training Loss of Epoch 31: 0.2325852213109412
Training Acc of Epoch 31: 0.7540856834349593
Testing Acc of Epoch 31: 0.7556695652173913
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3979e-01 (2.3979e-01)	Acc 0.744141 (0.744141)
Epoch: [32][300/616]	Loss 2.3468e-01 (2.3262e-01)	Acc 0.749023 (0.754445)
Epoch: [32][600/616]	Loss 2.3530e-01 (2.3241e-01)	Acc 0.754883 (0.754600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757239)
Training Loss of Epoch 32: 0.23240067336132855
Training Acc of Epoch 32: 0.7546811483739837
Testing Acc of Epoch 32: 0.7572391304347826
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2941e-01 (2.2941e-01)	Acc 0.760742 (0.760742)
Epoch: [33][300/616]	Loss 2.2144e-01 (2.3266e-01)	Acc 0.764648 (0.753942)
Epoch: [33][600/616]	Loss 2.3650e-01 (2.3240e-01)	Acc 0.732422 (0.754494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.755883)
Training Loss of Epoch 33: 0.23235733911273926
Training Acc of Epoch 33: 0.7545112423780488
Testing Acc of Epoch 33: 0.7558826086956522
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.2238e-01 (2.2238e-01)	Acc 0.762695 (0.762695)
Epoch: [34][300/616]	Loss 2.2992e-01 (2.3358e-01)	Acc 0.752930 (0.752686)
Epoch: [34][600/616]	Loss 2.4382e-01 (2.3304e-01)	Acc 0.734375 (0.753676)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754443)
Training Loss of Epoch 34: 0.2330471026703594
Training Acc of Epoch 34: 0.7537014100609756
Testing Acc of Epoch 34: 0.7544434782608695
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.2549e-01 (2.2549e-01)	Acc 0.763672 (0.763672)
Epoch: [35][300/616]	Loss 2.2619e-01 (2.3260e-01)	Acc 0.764648 (0.754260)
Epoch: [35][600/616]	Loss 2.2733e-01 (2.3281e-01)	Acc 0.750000 (0.753955)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755170)
Training Loss of Epoch 35: 0.23272069208021087
Training Acc of Epoch 35: 0.7541301448170732
Testing Acc of Epoch 35: 0.7551695652173913
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3797e-01 (2.3797e-01)	Acc 0.748047 (0.748047)
Epoch: [36][300/616]	Loss 2.1853e-01 (2.3279e-01)	Acc 0.766602 (0.754189)
Epoch: [36][600/616]	Loss 2.3619e-01 (2.3273e-01)	Acc 0.745117 (0.754368)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755226)
Training Loss of Epoch 36: 0.23271637474133716
Training Acc of Epoch 36: 0.7543889735772358
Testing Acc of Epoch 36: 0.7552260869565217
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3892e-01 (2.3892e-01)	Acc 0.748047 (0.748047)
Epoch: [37][300/616]	Loss 2.3880e-01 (2.3227e-01)	Acc 0.751953 (0.755522)
Epoch: [37][600/616]	Loss 2.4316e-01 (2.3290e-01)	Acc 0.750977 (0.754056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753196)
Training Loss of Epoch 37: 0.23282450301860405
Training Acc of Epoch 37: 0.754150787601626
Testing Acc of Epoch 37: 0.753195652173913
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3105e-01 (2.3105e-01)	Acc 0.756836 (0.756836)
Epoch: [38][300/616]	Loss 2.3221e-01 (2.3238e-01)	Acc 0.760742 (0.754237)
Epoch: [38][600/616]	Loss 2.3858e-01 (2.3249e-01)	Acc 0.748047 (0.754728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753530)
Training Loss of Epoch 38: 0.23252255419890086
Training Acc of Epoch 38: 0.7546589176829268
Testing Acc of Epoch 38: 0.7535304347826087
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.2480e-01 (2.2480e-01)	Acc 0.764648 (0.764648)
Epoch: [39][300/616]	Loss 2.2022e-01 (2.3322e-01)	Acc 0.774414 (0.752978)
Epoch: [39][600/616]	Loss 2.4052e-01 (2.3294e-01)	Acc 0.749023 (0.753984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756030)
Training Loss of Epoch 39: 0.23293551221126463
Training Acc of Epoch 39: 0.7539935848577236
Testing Acc of Epoch 39: 0.7560304347826087
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.1676e-01 (2.1676e-01)	Acc 0.772461 (0.772461)
Epoch: [40][300/616]	Loss 2.3549e-01 (2.3258e-01)	Acc 0.741211 (0.754172)
Epoch: [40][600/616]	Loss 2.3471e-01 (2.3312e-01)	Acc 0.757812 (0.753819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752604)
Training Loss of Epoch 40: 0.2329685307130581
Training Acc of Epoch 40: 0.7540221671747968
Testing Acc of Epoch 40: 0.752604347826087
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4281e-01 (2.4281e-01)	Acc 0.740234 (0.740234)
Epoch: [41][300/616]	Loss 2.4091e-01 (2.3294e-01)	Acc 0.745117 (0.753682)
Epoch: [41][600/616]	Loss 2.3895e-01 (2.3285e-01)	Acc 0.748047 (0.753812)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753730)
Training Loss of Epoch 41: 0.23292328436684803
Training Acc of Epoch 41: 0.7536918826219512
Testing Acc of Epoch 41: 0.7537304347826087
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.2809e-01 (2.2809e-01)	Acc 0.766602 (0.766602)
Epoch: [42][300/616]	Loss 2.3659e-01 (2.3265e-01)	Acc 0.740234 (0.755058)
Epoch: [42][600/616]	Loss 2.3104e-01 (2.3300e-01)	Acc 0.756836 (0.753869)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.753630)
Training Loss of Epoch 42: 0.2329453609338621
Training Acc of Epoch 42: 0.753955475101626
Testing Acc of Epoch 42: 0.7536304347826087
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.2935e-01 (2.2935e-01)	Acc 0.761719 (0.761719)
Epoch: [43][300/616]	Loss 2.4248e-01 (2.3275e-01)	Acc 0.741211 (0.754120)
Epoch: [43][600/616]	Loss 2.2897e-01 (2.3279e-01)	Acc 0.756836 (0.754062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754252)
Training Loss of Epoch 43: 0.23275769180883238
Training Acc of Epoch 43: 0.7540856834349593
Testing Acc of Epoch 43: 0.7542521739130434
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3057e-01 (2.3057e-01)	Acc 0.757812 (0.757812)
Epoch: [44][300/616]	Loss 2.4619e-01 (2.3296e-01)	Acc 0.735352 (0.753991)
Epoch: [44][600/616]	Loss 2.3883e-01 (2.3265e-01)	Acc 0.748047 (0.754324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756226)
Training Loss of Epoch 44: 0.2326267230801466
Training Acc of Epoch 44: 0.7543064024390244
Testing Acc of Epoch 44: 0.7562260869565217
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.1940e-01 (2.1940e-01)	Acc 0.778320 (0.778320)
Epoch: [45][300/616]	Loss 2.4228e-01 (2.3316e-01)	Acc 0.739258 (0.753935)
Epoch: [45][600/616]	Loss 2.3758e-01 (2.3294e-01)	Acc 0.750977 (0.753882)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.756417)
Training Loss of Epoch 45: 0.23301502015532516
Training Acc of Epoch 45: 0.7537792174796748
Testing Acc of Epoch 45: 0.7564173913043478
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.2611e-01 (2.2611e-01)	Acc 0.784180 (0.784180)
Epoch: [46][300/616]	Loss 2.4006e-01 (2.3256e-01)	Acc 0.731445 (0.754393)
Epoch: [46][600/616]	Loss 2.2844e-01 (2.3284e-01)	Acc 0.761719 (0.753901)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753187)
Training Loss of Epoch 46: 0.23279297591709508
Training Acc of Epoch 46: 0.7539538871951219
Testing Acc of Epoch 46: 0.7531869565217392
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2885e-01 (2.2885e-01)	Acc 0.749023 (0.749023)
Epoch: [47][300/616]	Loss 2.4863e-01 (2.3236e-01)	Acc 0.733398 (0.754769)
Epoch: [47][600/616]	Loss 2.4267e-01 (2.3267e-01)	Acc 0.750000 (0.754364)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753300)
Training Loss of Epoch 47: 0.2326614815529769
Training Acc of Epoch 47: 0.7544286712398374
Testing Acc of Epoch 47: 0.7533
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.3921e-01 (2.3921e-01)	Acc 0.750977 (0.750977)
Epoch: [48][300/616]	Loss 2.4370e-01 (2.3237e-01)	Acc 0.734375 (0.753984)
Epoch: [48][600/616]	Loss 2.2892e-01 (2.3264e-01)	Acc 0.761719 (0.753812)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755648)
Training Loss of Epoch 48: 0.23258322697829426
Training Acc of Epoch 48: 0.7538919588414634
Testing Acc of Epoch 48: 0.7556478260869566
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3255e-01 (2.3255e-01)	Acc 0.754883 (0.754883)
Epoch: [49][300/616]	Loss 2.4144e-01 (2.3282e-01)	Acc 0.742188 (0.753932)
Epoch: [49][600/616]	Loss 2.2651e-01 (2.3296e-01)	Acc 0.765625 (0.753913)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752948)
Training Loss of Epoch 49: 0.23303287036535217
Training Acc of Epoch 49: 0.7538030360772358
Testing Acc of Epoch 49: 0.7529478260869565
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2912e-01 (2.2912e-01)	Acc 0.751953 (0.751953)
Epoch: [50][300/616]	Loss 2.2686e-01 (2.3292e-01)	Acc 0.751953 (0.754078)
Epoch: [50][600/616]	Loss 2.1716e-01 (2.3323e-01)	Acc 0.774414 (0.753713)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755387)
Training Loss of Epoch 50: 0.2331498049623598
Training Acc of Epoch 50: 0.7537871570121951
Testing Acc of Epoch 50: 0.7553869565217392
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3390e-01 (2.3390e-01)	Acc 0.763672 (0.763672)
Epoch: [51][300/616]	Loss 2.4019e-01 (2.3410e-01)	Acc 0.744141 (0.752732)
Epoch: [51][600/616]	Loss 2.2639e-01 (2.3324e-01)	Acc 0.763672 (0.753583)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754974)
Training Loss of Epoch 51: 0.23315344719382805
Training Acc of Epoch 51: 0.7536426575203252
Testing Acc of Epoch 51: 0.7549739130434783
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.2662e-01 (2.2662e-01)	Acc 0.764648 (0.764648)
Epoch: [52][300/616]	Loss 2.3424e-01 (2.3261e-01)	Acc 0.760742 (0.754711)
Epoch: [52][600/616]	Loss 2.2222e-01 (2.3296e-01)	Acc 0.772461 (0.754000)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.754000)
Training Loss of Epoch 52: 0.2329883439754083
Training Acc of Epoch 52: 0.7539253048780488
Testing Acc of Epoch 52: 0.754
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5600e-01 (2.5600e-01)	Acc 0.724609 (0.724609)
Epoch: [53][300/616]	Loss 2.4162e-01 (2.3347e-01)	Acc 0.743164 (0.752518)
Epoch: [53][600/616]	Loss 2.3004e-01 (2.3314e-01)	Acc 0.748047 (0.753396)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.756043)
Training Loss of Epoch 53: 0.2330382160054959
Training Acc of Epoch 53: 0.7535966082317073
Testing Acc of Epoch 53: 0.7560434782608696
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3438e-01 (2.3438e-01)	Acc 0.742188 (0.742188)
Epoch: [54][300/616]	Loss 2.2559e-01 (2.3367e-01)	Acc 0.760742 (0.753686)
Epoch: [54][600/616]	Loss 2.4397e-01 (2.3345e-01)	Acc 0.748047 (0.753537)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.755522)
Training Loss of Epoch 54: 0.2333830988504053
Training Acc of Epoch 54: 0.7535823170731707
Testing Acc of Epoch 54: 0.7555217391304347
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3075e-01 (2.3075e-01)	Acc 0.745117 (0.745117)
Epoch: [55][300/616]	Loss 2.2041e-01 (2.3334e-01)	Acc 0.771484 (0.752845)
Epoch: [55][600/616]	Loss 2.4327e-01 (2.3335e-01)	Acc 0.733398 (0.753156)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755539)
Training Loss of Epoch 55: 0.23320051257203264
Training Acc of Epoch 55: 0.7533028455284553
Testing Acc of Epoch 55: 0.7555391304347826
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.1150e-01 (2.1150e-01)	Acc 0.768555 (0.768555)
Epoch: [56][300/616]	Loss 2.4243e-01 (2.3440e-01)	Acc 0.738281 (0.751872)
Epoch: [56][600/616]	Loss 2.1722e-01 (2.3358e-01)	Acc 0.777344 (0.752935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754735)
Training Loss of Epoch 56: 0.23342003156014574
Training Acc of Epoch 56: 0.7530980055894309
Testing Acc of Epoch 56: 0.7547347826086956
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.0593e-01 (2.0593e-01)	Acc 0.779297 (0.779297)
Epoch: [57][300/616]	Loss 2.2671e-01 (2.3388e-01)	Acc 0.766602 (0.753277)
Epoch: [57][600/616]	Loss 2.3198e-01 (2.3298e-01)	Acc 0.750000 (0.753888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754752)
Training Loss of Epoch 57: 0.23303765489803097
Training Acc of Epoch 57: 0.7537935086382114
Testing Acc of Epoch 57: 0.7547521739130435
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.2467e-01 (2.2467e-01)	Acc 0.756836 (0.756836)
Epoch: [58][300/616]	Loss 2.2742e-01 (2.3325e-01)	Acc 0.767578 (0.753939)
Epoch: [58][600/616]	Loss 2.3923e-01 (2.3359e-01)	Acc 0.750000 (0.753286)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753574)
Training Loss of Epoch 58: 0.2336132391681516
Training Acc of Epoch 58: 0.7532615599593496
Testing Acc of Epoch 58: 0.7535739130434783
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3137e-01 (2.3137e-01)	Acc 0.752930 (0.752930)
Epoch: [59][300/616]	Loss 2.2241e-01 (2.3337e-01)	Acc 0.762695 (0.753157)
Epoch: [59][600/616]	Loss 2.2697e-01 (2.3351e-01)	Acc 0.751953 (0.753008)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753891)
Training Loss of Epoch 59: 0.2334803078232742
Training Acc of Epoch 59: 0.753051956300813
Testing Acc of Epoch 59: 0.7538913043478261
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.4453e-01 (2.4453e-01)	Acc 0.734375 (0.734375)
Epoch: [60][300/616]	Loss 2.3704e-01 (2.3318e-01)	Acc 0.745117 (0.753196)
Epoch: [60][600/616]	Loss 2.2490e-01 (2.3348e-01)	Acc 0.752930 (0.753416)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.755491)
Training Loss of Epoch 60: 0.23343499496700318
Training Acc of Epoch 60: 0.7534536966463414
Testing Acc of Epoch 60: 0.7554913043478261
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3200e-01 (2.3200e-01)	Acc 0.750977 (0.750977)
Epoch: [61][300/616]	Loss 2.2546e-01 (2.3360e-01)	Acc 0.757812 (0.752774)
Epoch: [61][600/616]	Loss 2.2791e-01 (2.3334e-01)	Acc 0.776367 (0.753498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753852)
Training Loss of Epoch 61: 0.23330946806969682
Training Acc of Epoch 61: 0.753541031504065
Testing Acc of Epoch 61: 0.7538521739130435
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4213e-01 (2.4213e-01)	Acc 0.740234 (0.740234)
Epoch: [62][300/616]	Loss 2.4177e-01 (2.3390e-01)	Acc 0.760742 (0.753001)
Epoch: [62][600/616]	Loss 2.4617e-01 (2.3404e-01)	Acc 0.744141 (0.752969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.752365)
Training Loss of Epoch 62: 0.23402215688209224
Training Acc of Epoch 62: 0.7530297256097561
Testing Acc of Epoch 62: 0.7523652173913044
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3102e-01 (2.3102e-01)	Acc 0.752930 (0.752930)
Epoch: [63][300/616]	Loss 2.3599e-01 (2.3429e-01)	Acc 0.757812 (0.752031)
Epoch: [63][600/616]	Loss 2.3657e-01 (2.3413e-01)	Acc 0.729492 (0.752642)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755291)
Training Loss of Epoch 63: 0.23400797444145854
Training Acc of Epoch 63: 0.7527534298780488
Testing Acc of Epoch 63: 0.7552913043478261
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3476e-01 (2.3476e-01)	Acc 0.744141 (0.744141)
Epoch: [64][300/616]	Loss 2.1784e-01 (2.3316e-01)	Acc 0.778320 (0.753913)
Epoch: [64][600/616]	Loss 2.2878e-01 (2.3329e-01)	Acc 0.763672 (0.753654)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754800)
Training Loss of Epoch 64: 0.2333616866328852
Training Acc of Epoch 64: 0.7535870807926829
Testing Acc of Epoch 64: 0.7548
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.2921e-01 (2.2921e-01)	Acc 0.734375 (0.734375)
Epoch: [65][300/616]	Loss 2.3416e-01 (2.3440e-01)	Acc 0.750977 (0.752557)
Epoch: [65][600/616]	Loss 2.4196e-01 (2.3389e-01)	Acc 0.733398 (0.752904)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.753861)
Training Loss of Epoch 65: 0.23386425424397475
Training Acc of Epoch 65: 0.7529439786585366
Testing Acc of Epoch 65: 0.7538608695652174
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4471e-01 (2.4471e-01)	Acc 0.741211 (0.741211)
Epoch: [66][300/616]	Loss 2.2931e-01 (2.3361e-01)	Acc 0.760742 (0.752865)
Epoch: [66][600/616]	Loss 2.4026e-01 (2.3390e-01)	Acc 0.749023 (0.752728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.752513)
Training Loss of Epoch 66: 0.23399412702254163
Training Acc of Epoch 66: 0.7526883257113821
Testing Acc of Epoch 66: 0.7525130434782609
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.4161e-01 (2.4161e-01)	Acc 0.750000 (0.750000)
Epoch: [67][300/616]	Loss 2.2239e-01 (2.3414e-01)	Acc 0.766602 (0.752920)
Epoch: [67][600/616]	Loss 2.2855e-01 (2.3407e-01)	Acc 0.767578 (0.753021)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752904)
Training Loss of Epoch 67: 0.23404075098716146
Training Acc of Epoch 67: 0.7529900279471544
Testing Acc of Epoch 67: 0.752904347826087
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3939e-01 (2.3939e-01)	Acc 0.747070 (0.747070)
Epoch: [68][300/616]	Loss 2.3421e-01 (2.3456e-01)	Acc 0.745117 (0.752424)
Epoch: [68][600/616]	Loss 2.4216e-01 (2.3415e-01)	Acc 0.743164 (0.752554)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751035)
Training Loss of Epoch 68: 0.23407105498197603
Training Acc of Epoch 68: 0.7526756224593496
Testing Acc of Epoch 68: 0.7510347826086956
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.3667e-01 (2.3667e-01)	Acc 0.759766 (0.759766)
Epoch: [69][300/616]	Loss 2.4012e-01 (2.3494e-01)	Acc 0.750977 (0.752167)
Epoch: [69][600/616]	Loss 2.4102e-01 (2.3476e-01)	Acc 0.762695 (0.751926)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754939)
Training Loss of Epoch 69: 0.23464870176664213
Training Acc of Epoch 69: 0.7520579268292683
Testing Acc of Epoch 69: 0.7549391304347826
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4117e-01 (2.4117e-01)	Acc 0.739258 (0.739258)
Epoch: [70][300/616]	Loss 2.3999e-01 (2.3464e-01)	Acc 0.755859 (0.751278)
Epoch: [70][600/616]	Loss 2.3891e-01 (2.3418e-01)	Acc 0.745117 (0.752501)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.751603 (0.754030)
Training Loss of Epoch 70: 0.23429088638565404
Training Acc of Epoch 70: 0.7524564913617886
Testing Acc of Epoch 70: 0.7540304347826087
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.2906e-01 (2.2906e-01)	Acc 0.765625 (0.765625)
Epoch: [71][300/616]	Loss 2.4044e-01 (2.3409e-01)	Acc 0.748047 (0.752514)
Epoch: [71][600/616]	Loss 2.3405e-01 (2.3423e-01)	Acc 0.764648 (0.752697)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753704)
Training Loss of Epoch 71: 0.23425885729673432
Training Acc of Epoch 71: 0.7526549796747968
Testing Acc of Epoch 71: 0.753704347826087
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.5629e-01 (2.5629e-01)	Acc 0.727539 (0.727539)
Epoch: [72][300/616]	Loss 2.1845e-01 (2.3497e-01)	Acc 0.780273 (0.752050)
Epoch: [72][600/616]	Loss 2.3445e-01 (2.3418e-01)	Acc 0.751953 (0.752608)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753391)
Training Loss of Epoch 72: 0.23418164636061445
Training Acc of Epoch 72: 0.752564469004065
Testing Acc of Epoch 72: 0.7533913043478261
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.3002e-01 (2.3002e-01)	Acc 0.767578 (0.767578)
Epoch: [73][300/616]	Loss 2.2258e-01 (2.3418e-01)	Acc 0.774414 (0.752407)
Epoch: [73][600/616]	Loss 2.2576e-01 (2.3408e-01)	Acc 0.754883 (0.752611)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.755322)
Training Loss of Epoch 73: 0.23408379419063166
Training Acc of Epoch 73: 0.7527073805894309
Testing Acc of Epoch 73: 0.7553217391304348
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.2638e-01 (2.2638e-01)	Acc 0.772461 (0.772461)
Epoch: [74][300/616]	Loss 2.3121e-01 (2.3460e-01)	Acc 0.754883 (0.752193)
Epoch: [74][600/616]	Loss 2.3323e-01 (2.3480e-01)	Acc 0.728516 (0.752317)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752957)
Training Loss of Epoch 74: 0.2348922764867302
Training Acc of Epoch 74: 0.7522611788617887
Testing Acc of Epoch 74: 0.7529565217391304
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4212e-01 (2.4212e-01)	Acc 0.732422 (0.732422)
Epoch: [75][300/616]	Loss 2.2379e-01 (2.3100e-01)	Acc 0.763672 (0.755191)
Epoch: [75][600/616]	Loss 2.3922e-01 (2.3039e-01)	Acc 0.745117 (0.755538)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.756887)
Training Loss of Epoch 75: 0.23038452827348943
Training Acc of Epoch 75: 0.7554973323170732
Testing Acc of Epoch 75: 0.7568869565217391
Model with the best training loss saved! The loss is 0.23038452827348943
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3317e-01 (2.3317e-01)	Acc 0.754883 (0.754883)
Epoch: [76][300/616]	Loss 2.2890e-01 (2.3109e-01)	Acc 0.752930 (0.755178)
Epoch: [76][600/616]	Loss 2.1540e-01 (2.3108e-01)	Acc 0.774414 (0.755245)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.756065)
Training Loss of Epoch 76: 0.23103528000959536
Training Acc of Epoch 76: 0.7552829649390244
Testing Acc of Epoch 76: 0.7560652173913044
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.3215e-01 (2.3215e-01)	Acc 0.757812 (0.757812)
Epoch: [77][300/616]	Loss 2.3059e-01 (2.3094e-01)	Acc 0.752930 (0.754902)
Epoch: [77][600/616]	Loss 2.3625e-01 (2.3073e-01)	Acc 0.751953 (0.755517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755474)
Training Loss of Epoch 77: 0.2308494650978383
Training Acc of Epoch 77: 0.7553623602642277
Testing Acc of Epoch 77: 0.7554739130434782
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.3430e-01 (2.3430e-01)	Acc 0.748047 (0.748047)
Epoch: [78][300/616]	Loss 2.3907e-01 (2.3164e-01)	Acc 0.746094 (0.754983)
Epoch: [78][600/616]	Loss 2.2709e-01 (2.3109e-01)	Acc 0.753906 (0.755403)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.754526)
Training Loss of Epoch 78: 0.23109984245242143
Training Acc of Epoch 78: 0.7553258384146342
Testing Acc of Epoch 78: 0.7545260869565218
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3337e-01 (2.3337e-01)	Acc 0.752930 (0.752930)
Epoch: [79][300/616]	Loss 2.2658e-01 (2.3130e-01)	Acc 0.748047 (0.754708)
Epoch: [79][600/616]	Loss 2.3617e-01 (2.3122e-01)	Acc 0.756836 (0.755153)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753226)
Training Loss of Epoch 79: 0.23111977768622763
Training Acc of Epoch 79: 0.7553020198170731
Testing Acc of Epoch 79: 0.7532260869565217
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3751e-01 (2.3751e-01)	Acc 0.742188 (0.742188)
Epoch: [80][300/616]	Loss 2.4011e-01 (2.3170e-01)	Acc 0.745117 (0.754140)
Epoch: [80][600/616]	Loss 2.2725e-01 (2.3147e-01)	Acc 0.758789 (0.754902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753439)
Training Loss of Epoch 80: 0.23139022649303684
Training Acc of Epoch 80: 0.7549844385162602
Testing Acc of Epoch 80: 0.7534391304347826
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3584e-01 (2.3584e-01)	Acc 0.757812 (0.757812)
Epoch: [81][300/616]	Loss 2.4159e-01 (2.3138e-01)	Acc 0.729492 (0.755785)
Epoch: [81][600/616]	Loss 2.2214e-01 (2.3150e-01)	Acc 0.768555 (0.755131)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.757804)
Training Loss of Epoch 81: 0.23147298342328732
Training Acc of Epoch 81: 0.7552162728658537
Testing Acc of Epoch 81: 0.757804347826087
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2438e-01 (2.2438e-01)	Acc 0.754883 (0.754883)
Epoch: [82][300/616]	Loss 2.2031e-01 (2.3068e-01)	Acc 0.768555 (0.755895)
Epoch: [82][600/616]	Loss 2.2534e-01 (2.3083e-01)	Acc 0.774414 (0.755608)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.758465)
Training Loss of Epoch 82: 0.23088160501263005
Training Acc of Epoch 82: 0.7555719639227643
Testing Acc of Epoch 82: 0.7584652173913043
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3811e-01 (2.3811e-01)	Acc 0.761719 (0.761719)
Epoch: [83][300/616]	Loss 2.4900e-01 (2.3158e-01)	Acc 0.750977 (0.754419)
Epoch: [83][600/616]	Loss 2.3383e-01 (2.3180e-01)	Acc 0.748047 (0.754477)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.756165)
Training Loss of Epoch 83: 0.23173308639022394
Training Acc of Epoch 83: 0.7545795223577236
Testing Acc of Epoch 83: 0.7561652173913044
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3758e-01 (2.3758e-01)	Acc 0.745117 (0.745117)
Epoch: [84][300/616]	Loss 2.3049e-01 (2.3113e-01)	Acc 0.761719 (0.755558)
Epoch: [84][600/616]	Loss 2.3763e-01 (2.3126e-01)	Acc 0.752930 (0.755274)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755304)
Training Loss of Epoch 84: 0.23126375222109197
Training Acc of Epoch 84: 0.7552686737804878
Testing Acc of Epoch 84: 0.7553043478260869
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.5166e-01 (2.5166e-01)	Acc 0.728516 (0.728516)
Epoch: [85][300/616]	Loss 2.2672e-01 (2.3118e-01)	Acc 0.758789 (0.755198)
Epoch: [85][600/616]	Loss 2.1874e-01 (2.3127e-01)	Acc 0.778320 (0.755232)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754443)
Training Loss of Epoch 85: 0.2312906009394948
Training Acc of Epoch 85: 0.7552543826219512
Testing Acc of Epoch 85: 0.7544434782608695
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.4029e-01 (2.4029e-01)	Acc 0.743164 (0.743164)
Epoch: [86][300/616]	Loss 2.2550e-01 (2.3117e-01)	Acc 0.759766 (0.755597)
Epoch: [86][600/616]	Loss 2.4344e-01 (2.3105e-01)	Acc 0.740234 (0.755430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.757709)
Training Loss of Epoch 86: 0.23113895320310826
Training Acc of Epoch 86: 0.7554147611788617
Testing Acc of Epoch 86: 0.7577086956521739
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3828e-01 (2.3828e-01)	Acc 0.747070 (0.747070)
Epoch: [87][300/616]	Loss 2.2367e-01 (2.3127e-01)	Acc 0.761719 (0.755214)
Epoch: [87][600/616]	Loss 2.3388e-01 (2.3137e-01)	Acc 0.754883 (0.755231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753987)
Training Loss of Epoch 87: 0.2313538525404969
Training Acc of Epoch 87: 0.7551956300813009
Testing Acc of Epoch 87: 0.7539869565217391
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3471e-01 (2.3471e-01)	Acc 0.756836 (0.756836)
Epoch: [88][300/616]	Loss 2.2920e-01 (2.3145e-01)	Acc 0.761719 (0.755743)
Epoch: [88][600/616]	Loss 2.3171e-01 (2.3139e-01)	Acc 0.742188 (0.755183)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.757835)
Training Loss of Epoch 88: 0.23129396048503192
Training Acc of Epoch 88: 0.7552591463414634
Testing Acc of Epoch 88: 0.7578347826086956
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2286e-01 (2.2286e-01)	Acc 0.767578 (0.767578)
Epoch: [89][300/616]	Loss 2.3232e-01 (2.3141e-01)	Acc 0.765625 (0.754889)
Epoch: [89][600/616]	Loss 2.2576e-01 (2.3158e-01)	Acc 0.763672 (0.754917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.750000 (0.756317)
Training Loss of Epoch 89: 0.2315549901345881
Training Acc of Epoch 89: 0.7549923780487805
Testing Acc of Epoch 89: 0.7563173913043478
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.3140e-01 (2.3140e-01)	Acc 0.758789 (0.758789)
Epoch: [90][300/616]	Loss 2.4822e-01 (2.3110e-01)	Acc 0.730469 (0.755480)
Epoch: [90][600/616]	Loss 2.2736e-01 (2.3109e-01)	Acc 0.770508 (0.755297)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.756039)
Training Loss of Epoch 90: 0.2310482609805053
Training Acc of Epoch 90: 0.7553194867886179
Testing Acc of Epoch 90: 0.7560391304347827
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2527e-01 (2.2527e-01)	Acc 0.752930 (0.752930)
Epoch: [91][300/616]	Loss 2.2928e-01 (2.3112e-01)	Acc 0.759766 (0.755421)
Epoch: [91][600/616]	Loss 2.2934e-01 (2.3131e-01)	Acc 0.756836 (0.755170)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753265)
Training Loss of Epoch 91: 0.23130169474497075
Training Acc of Epoch 91: 0.7551051194105691
Testing Acc of Epoch 91: 0.7532652173913044
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.4210e-01 (2.4210e-01)	Acc 0.738281 (0.738281)
Epoch: [92][300/616]	Loss 2.2412e-01 (2.3088e-01)	Acc 0.764648 (0.755259)
Epoch: [92][600/616]	Loss 2.1945e-01 (2.3113e-01)	Acc 0.766602 (0.755208)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752896)
Training Loss of Epoch 92: 0.23117217450607114
Training Acc of Epoch 92: 0.7551972179878049
Testing Acc of Epoch 92: 0.7528956521739131
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3673e-01 (2.3673e-01)	Acc 0.738281 (0.738281)
Epoch: [93][300/616]	Loss 2.2626e-01 (2.3171e-01)	Acc 0.757812 (0.754565)
Epoch: [93][600/616]	Loss 2.2756e-01 (2.3150e-01)	Acc 0.756836 (0.755023)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.757096)
Training Loss of Epoch 93: 0.2314235805738263
Training Acc of Epoch 93: 0.7551114710365854
Testing Acc of Epoch 93: 0.7570956521739131
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3903e-01 (2.3903e-01)	Acc 0.751953 (0.751953)
Epoch: [94][300/616]	Loss 2.0016e-01 (2.3159e-01)	Acc 0.802734 (0.754682)
Epoch: [94][600/616]	Loss 2.1471e-01 (2.3149e-01)	Acc 0.777344 (0.755117)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755835)
Training Loss of Epoch 94: 0.2315312438864049
Training Acc of Epoch 94: 0.7550416031504065
Testing Acc of Epoch 94: 0.7558347826086956
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3381e-01 (2.3381e-01)	Acc 0.748047 (0.748047)
Epoch: [95][300/616]	Loss 2.2328e-01 (2.3160e-01)	Acc 0.761719 (0.754802)
Epoch: [95][600/616]	Loss 2.4063e-01 (2.3109e-01)	Acc 0.745117 (0.755190)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755361)
Training Loss of Epoch 95: 0.23110649583300924
Training Acc of Epoch 95: 0.7551876905487804
Testing Acc of Epoch 95: 0.7553608695652174
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3582e-01 (2.3582e-01)	Acc 0.742188 (0.742188)
Epoch: [96][300/616]	Loss 2.2899e-01 (2.3141e-01)	Acc 0.761719 (0.755256)
Epoch: [96][600/616]	Loss 2.4159e-01 (2.3129e-01)	Acc 0.747070 (0.755209)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.755065)
Training Loss of Epoch 96: 0.23128489192908372
Training Acc of Epoch 96: 0.7552464430894309
Testing Acc of Epoch 96: 0.7550652173913044
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2588e-01 (2.2588e-01)	Acc 0.773438 (0.773438)
Epoch: [97][300/616]	Loss 2.3009e-01 (2.3194e-01)	Acc 0.759766 (0.754237)
Epoch: [97][600/616]	Loss 2.3400e-01 (2.3155e-01)	Acc 0.741211 (0.754810)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752861)
Training Loss of Epoch 97: 0.23154327593198637
Training Acc of Epoch 97: 0.7548208841463414
Testing Acc of Epoch 97: 0.7528608695652174
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3137e-01 (2.3137e-01)	Acc 0.752930 (0.752930)
Epoch: [98][300/616]	Loss 2.3429e-01 (2.3153e-01)	Acc 0.763672 (0.754558)
Epoch: [98][600/616]	Loss 2.3027e-01 (2.3142e-01)	Acc 0.758789 (0.755070)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754070)
Training Loss of Epoch 98: 0.2314208349076713
Training Acc of Epoch 98: 0.7550797129065041
Testing Acc of Epoch 98: 0.7540695652173913
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.2066e-01 (2.2066e-01)	Acc 0.771484 (0.771484)
Epoch: [99][300/616]	Loss 2.3503e-01 (2.3098e-01)	Acc 0.745117 (0.755519)
Epoch: [99][600/616]	Loss 2.4822e-01 (2.3134e-01)	Acc 0.731445 (0.755001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751357)
Training Loss of Epoch 99: 0.23138535531555735
Training Acc of Epoch 99: 0.755030487804878
Testing Acc of Epoch 99: 0.7513565217391305
Early stopping not satisfied.
