train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.00625
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.00625/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.00625
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0119e-01 (5.0119e-01)	Acc 0.174805 (0.174805)
Epoch: [0][300/616]	Loss 2.5863e-01 (2.8033e-01)	Acc 0.734375 (0.702940)
Epoch: [0][600/616]	Loss 2.4628e-01 (2.6302e-01)	Acc 0.742188 (0.723053)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.746048)
Training Loss of Epoch 0: 0.2626418587638111
Training Acc of Epoch 0: 0.7234581427845529
Testing Acc of Epoch 0: 0.7460478260869565
Model with the best training loss saved! The loss is 0.2626418587638111
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.3117e-01 (2.3117e-01)	Acc 0.757812 (0.757812)
Epoch: [1][300/616]	Loss 2.5051e-01 (2.4180e-01)	Acc 0.734375 (0.746347)
Epoch: [1][600/616]	Loss 2.6635e-01 (2.4158e-01)	Acc 0.718750 (0.746411)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751435)
Training Loss of Epoch 1: 0.24155841392230212
Training Acc of Epoch 1: 0.7463605182926829
Testing Acc of Epoch 1: 0.7514347826086957
Model with the best training loss saved! The loss is 0.24155841392230212
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3469e-01 (2.3469e-01)	Acc 0.744141 (0.744141)
Epoch: [2][300/616]	Loss 2.4183e-01 (2.4033e-01)	Acc 0.724609 (0.747437)
Epoch: [2][600/616]	Loss 2.4489e-01 (2.3938e-01)	Acc 0.749023 (0.748386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746430)
Training Loss of Epoch 2: 0.23927560560102384
Training Acc of Epoch 2: 0.748510543699187
Testing Acc of Epoch 2: 0.7464304347826087
Model with the best training loss saved! The loss is 0.23927560560102384
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.2495e-01 (2.2495e-01)	Acc 0.774414 (0.774414)
Epoch: [3][300/616]	Loss 2.4895e-01 (2.3812e-01)	Acc 0.743164 (0.750217)
Epoch: [3][600/616]	Loss 2.3865e-01 (2.3755e-01)	Acc 0.750000 (0.749753)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752717)
Training Loss of Epoch 3: 0.23750268594036258
Training Acc of Epoch 3: 0.7497776930894309
Testing Acc of Epoch 3: 0.7527173913043478
Model with the best training loss saved! The loss is 0.23750268594036258
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4097e-01 (2.4097e-01)	Acc 0.748047 (0.748047)
Epoch: [4][300/616]	Loss 2.4470e-01 (2.3614e-01)	Acc 0.742188 (0.751398)
Epoch: [4][600/616]	Loss 2.3014e-01 (2.3629e-01)	Acc 0.747070 (0.751116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.751026)
Training Loss of Epoch 4: 0.23623622527452018
Training Acc of Epoch 4: 0.7512052210365854
Testing Acc of Epoch 4: 0.7510260869565217
Model with the best training loss saved! The loss is 0.23623622527452018
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3663e-01 (2.3663e-01)	Acc 0.750000 (0.750000)
Epoch: [5][300/616]	Loss 2.3544e-01 (2.3571e-01)	Acc 0.739258 (0.751239)
Epoch: [5][600/616]	Loss 2.3077e-01 (2.3570e-01)	Acc 0.768555 (0.751566)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747839)
Training Loss of Epoch 5: 0.23570370264654236
Training Acc of Epoch 5: 0.7516387195121951
Testing Acc of Epoch 5: 0.7478391304347826
Model with the best training loss saved! The loss is 0.23570370264654236
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.3100e-01 (2.3100e-01)	Acc 0.770508 (0.770508)
Epoch: [6][300/616]	Loss 2.3792e-01 (2.3559e-01)	Acc 0.738281 (0.752122)
Epoch: [6][600/616]	Loss 2.2476e-01 (2.3613e-01)	Acc 0.751953 (0.751131)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750265)
Training Loss of Epoch 6: 0.23610922277458316
Training Acc of Epoch 6: 0.7511686991869919
Testing Acc of Epoch 6: 0.7502652173913044
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4426e-01 (2.4426e-01)	Acc 0.760742 (0.760742)
Epoch: [7][300/616]	Loss 2.4641e-01 (2.3698e-01)	Acc 0.733398 (0.750548)
Epoch: [7][600/616]	Loss 2.3440e-01 (2.3718e-01)	Acc 0.743164 (0.749994)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.745187)
Training Loss of Epoch 7: 0.23711345956577518
Training Acc of Epoch 7: 0.7500428734756097
Testing Acc of Epoch 7: 0.7451869565217392
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.4466e-01 (2.4466e-01)	Acc 0.747070 (0.747070)
Epoch: [8][300/616]	Loss 2.3207e-01 (2.3621e-01)	Acc 0.750977 (0.751061)
Epoch: [8][600/616]	Loss 2.4903e-01 (2.3674e-01)	Acc 0.740234 (0.750174)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753848)
Training Loss of Epoch 8: 0.23680682175043152
Training Acc of Epoch 8: 0.750049225101626
Testing Acc of Epoch 8: 0.7538478260869566
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3679e-01 (2.3679e-01)	Acc 0.751953 (0.751953)
Epoch: [9][300/616]	Loss 2.5411e-01 (2.3588e-01)	Acc 0.739258 (0.751333)
Epoch: [9][600/616]	Loss 2.3877e-01 (2.3657e-01)	Acc 0.753906 (0.750726)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753013)
Training Loss of Epoch 9: 0.23663115181574007
Training Acc of Epoch 9: 0.7506891514227643
Testing Acc of Epoch 9: 0.7530130434782609
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2705e-01 (2.2705e-01)	Acc 0.757812 (0.757812)
Epoch: [10][300/616]	Loss 2.3036e-01 (2.3611e-01)	Acc 0.755859 (0.750863)
Epoch: [10][600/616]	Loss 2.3288e-01 (2.3641e-01)	Acc 0.747070 (0.750728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753774)
Training Loss of Epoch 10: 0.23639620094279934
Training Acc of Epoch 10: 0.7507352007113821
Testing Acc of Epoch 10: 0.7537739130434783
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4153e-01 (2.4153e-01)	Acc 0.755859 (0.755859)
Epoch: [11][300/616]	Loss 2.4014e-01 (2.3619e-01)	Acc 0.750000 (0.751048)
Epoch: [11][600/616]	Loss 2.3593e-01 (2.3596e-01)	Acc 0.750977 (0.751090)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752639)
Training Loss of Epoch 11: 0.23601455785394684
Training Acc of Epoch 11: 0.7509956173780488
Testing Acc of Epoch 11: 0.7526391304347826
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4609e-01 (2.4609e-01)	Acc 0.725586 (0.725586)
Epoch: [12][300/616]	Loss 2.4430e-01 (2.3614e-01)	Acc 0.737305 (0.750451)
Epoch: [12][600/616]	Loss 2.1254e-01 (2.3645e-01)	Acc 0.775391 (0.750554)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753335)
Training Loss of Epoch 12: 0.23659704073173243
Training Acc of Epoch 12: 0.7503810975609756
Testing Acc of Epoch 12: 0.7533347826086957
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3455e-01 (2.3455e-01)	Acc 0.755859 (0.755859)
Epoch: [13][300/616]	Loss 2.3174e-01 (2.3670e-01)	Acc 0.751953 (0.749627)
Epoch: [13][600/616]	Loss 2.3638e-01 (2.3587e-01)	Acc 0.752930 (0.751157)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752157)
Training Loss of Epoch 13: 0.23587908177841
Training Acc of Epoch 13: 0.751170287093496
Testing Acc of Epoch 13: 0.7521565217391304
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.3599e-01 (2.3599e-01)	Acc 0.768555 (0.768555)
Epoch: [14][300/616]	Loss 2.4132e-01 (2.3516e-01)	Acc 0.741211 (0.752411)
Epoch: [14][600/616]	Loss 2.4061e-01 (2.3591e-01)	Acc 0.738281 (0.751136)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.752787)
Training Loss of Epoch 14: 0.23600700652696252
Training Acc of Epoch 14: 0.7510083206300813
Testing Acc of Epoch 14: 0.7527869565217391
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4598e-01 (2.4598e-01)	Acc 0.750977 (0.750977)
Epoch: [15][300/616]	Loss 2.2738e-01 (2.3691e-01)	Acc 0.769531 (0.750091)
Epoch: [15][600/616]	Loss 2.4865e-01 (2.3653e-01)	Acc 0.732422 (0.750562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752296)
Training Loss of Epoch 15: 0.23652752603941818
Training Acc of Epoch 15: 0.7505859375
Testing Acc of Epoch 15: 0.752295652173913
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3493e-01 (2.3493e-01)	Acc 0.754883 (0.754883)
Epoch: [16][300/616]	Loss 2.4488e-01 (2.3623e-01)	Acc 0.744141 (0.750970)
Epoch: [16][600/616]	Loss 2.3685e-01 (2.3649e-01)	Acc 0.757812 (0.750825)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.745491)
Training Loss of Epoch 16: 0.23661183218161266
Training Acc of Epoch 16: 0.7506383384146341
Testing Acc of Epoch 16: 0.7454913043478261
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4365e-01 (2.4365e-01)	Acc 0.746094 (0.746094)
Epoch: [17][300/616]	Loss 2.3055e-01 (2.3796e-01)	Acc 0.762695 (0.749416)
Epoch: [17][600/616]	Loss 2.5768e-01 (2.3751e-01)	Acc 0.735352 (0.749751)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754704)
Training Loss of Epoch 17: 0.23747316186990194
Training Acc of Epoch 17: 0.7498602642276423
Testing Acc of Epoch 17: 0.754704347826087
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.3861e-01 (2.3861e-01)	Acc 0.759766 (0.759766)
Epoch: [18][300/616]	Loss 2.2314e-01 (2.3726e-01)	Acc 0.767578 (0.749812)
Epoch: [18][600/616]	Loss 2.1585e-01 (2.3700e-01)	Acc 0.776367 (0.750338)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740652)
Training Loss of Epoch 18: 0.23692858233684447
Training Acc of Epoch 18: 0.7504589049796748
Testing Acc of Epoch 18: 0.7406521739130435
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3072e-01 (2.3072e-01)	Acc 0.763672 (0.763672)
Epoch: [19][300/616]	Loss 2.1869e-01 (2.3726e-01)	Acc 0.777344 (0.750250)
Epoch: [19][600/616]	Loss 2.5175e-01 (2.3760e-01)	Acc 0.732422 (0.749626)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750457)
Training Loss of Epoch 19: 0.23759840734121276
Training Acc of Epoch 19: 0.7496538363821138
Testing Acc of Epoch 19: 0.7504565217391305
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.3318e-01 (2.3318e-01)	Acc 0.744141 (0.744141)
Epoch: [20][300/616]	Loss 2.2922e-01 (2.3572e-01)	Acc 0.759766 (0.751197)
Epoch: [20][600/616]	Loss 2.2926e-01 (2.3615e-01)	Acc 0.762695 (0.750669)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752526)
Training Loss of Epoch 20: 0.23610432162517456
Training Acc of Epoch 20: 0.7507463160569106
Testing Acc of Epoch 20: 0.7525260869565218
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.3803e-01 (2.3803e-01)	Acc 0.751953 (0.751953)
Epoch: [21][300/616]	Loss 2.2954e-01 (2.3794e-01)	Acc 0.753906 (0.748777)
Epoch: [21][600/616]	Loss 2.3369e-01 (2.3691e-01)	Acc 0.749023 (0.750175)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742661)
Training Loss of Epoch 21: 0.2368492203999341
Training Acc of Epoch 21: 0.7502715320121951
Testing Acc of Epoch 21: 0.7426608695652174
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3898e-01 (2.3898e-01)	Acc 0.744141 (0.744141)
Epoch: [22][300/616]	Loss 2.4558e-01 (2.3678e-01)	Acc 0.738281 (0.750088)
Epoch: [22][600/616]	Loss 2.3711e-01 (2.3636e-01)	Acc 0.753906 (0.750634)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754165)
Training Loss of Epoch 22: 0.23636840610969356
Training Acc of Epoch 22: 0.7506415142276422
Testing Acc of Epoch 22: 0.7541652173913044
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2741e-01 (2.2741e-01)	Acc 0.762695 (0.762695)
Epoch: [23][300/616]	Loss 2.3905e-01 (2.3817e-01)	Acc 0.756836 (0.749150)
Epoch: [23][600/616]	Loss 2.2053e-01 (2.3751e-01)	Acc 0.768555 (0.749747)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751878)
Training Loss of Epoch 23: 0.23748668080907526
Training Acc of Epoch 23: 0.7497427591463415
Testing Acc of Epoch 23: 0.7518782608695652
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2758e-01 (2.2758e-01)	Acc 0.751953 (0.751953)
Epoch: [24][300/616]	Loss 2.5000e-01 (2.3674e-01)	Acc 0.745117 (0.750208)
Epoch: [24][600/616]	Loss 2.4885e-01 (2.3736e-01)	Acc 0.748047 (0.749664)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.748057)
Training Loss of Epoch 24: 0.23755822923125292
Training Acc of Epoch 24: 0.7493870680894309
Testing Acc of Epoch 24: 0.7480565217391304
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4517e-01 (2.4517e-01)	Acc 0.743164 (0.743164)
Epoch: [25][300/616]	Loss 2.3786e-01 (2.3823e-01)	Acc 0.737305 (0.749105)
Epoch: [25][600/616]	Loss 2.4244e-01 (2.3768e-01)	Acc 0.743164 (0.749654)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.747652)
Training Loss of Epoch 25: 0.23768305567706505
Training Acc of Epoch 25: 0.7496061991869919
Testing Acc of Epoch 25: 0.7476521739130435
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.2381e-01 (2.2381e-01)	Acc 0.780273 (0.780273)
Epoch: [26][300/616]	Loss 2.4042e-01 (2.3914e-01)	Acc 0.748047 (0.747868)
Epoch: [26][600/616]	Loss 2.4350e-01 (2.3850e-01)	Acc 0.736328 (0.748473)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.744743)
Training Loss of Epoch 26: 0.23845731245308388
Training Acc of Epoch 26: 0.7485486534552845
Testing Acc of Epoch 26: 0.7447434782608696
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4617e-01 (2.4617e-01)	Acc 0.750977 (0.750977)
Epoch: [27][300/616]	Loss 2.3329e-01 (2.3794e-01)	Acc 0.752930 (0.749257)
Epoch: [27][600/616]	Loss 2.4185e-01 (2.3773e-01)	Acc 0.745117 (0.749334)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742726)
Training Loss of Epoch 27: 0.23775292303019424
Training Acc of Epoch 27: 0.7493696011178862
Testing Acc of Epoch 27: 0.7427260869565218
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4275e-01 (2.4275e-01)	Acc 0.757812 (0.757812)
Epoch: [28][300/616]	Loss 2.4836e-01 (2.3769e-01)	Acc 0.742188 (0.749864)
Epoch: [28][600/616]	Loss 2.4213e-01 (2.3764e-01)	Acc 0.734375 (0.749589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.751535)
Training Loss of Epoch 28: 0.23768823939125713
Training Acc of Epoch 28: 0.7496061991869919
Testing Acc of Epoch 28: 0.7515347826086957
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.3816e-01 (2.3816e-01)	Acc 0.748047 (0.748047)
Epoch: [29][300/616]	Loss 2.5668e-01 (2.3845e-01)	Acc 0.713867 (0.748592)
Epoch: [29][600/616]	Loss 2.3975e-01 (2.3788e-01)	Acc 0.748047 (0.749097)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750191)
Training Loss of Epoch 29: 0.23788783695639634
Training Acc of Epoch 29: 0.7491187118902439
Testing Acc of Epoch 29: 0.7501913043478261
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.5244e-01 (2.5244e-01)	Acc 0.736328 (0.736328)
Epoch: [30][300/616]	Loss 2.4012e-01 (2.3717e-01)	Acc 0.744141 (0.750039)
Epoch: [30][600/616]	Loss 2.1415e-01 (2.3689e-01)	Acc 0.780273 (0.750042)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753670)
Training Loss of Epoch 30: 0.23690107048042422
Training Acc of Epoch 30: 0.7500460492886178
Testing Acc of Epoch 30: 0.7536695652173913
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3104e-01 (2.3104e-01)	Acc 0.757812 (0.757812)
Epoch: [31][300/616]	Loss 2.3760e-01 (2.3718e-01)	Acc 0.748047 (0.749377)
Epoch: [31][600/616]	Loss 2.4449e-01 (2.3767e-01)	Acc 0.746094 (0.749345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.750996)
Training Loss of Epoch 31: 0.23769892425071903
Training Acc of Epoch 31: 0.7493457825203252
Testing Acc of Epoch 31: 0.7509956521739131
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.3621e-01 (2.3621e-01)	Acc 0.746094 (0.746094)
Epoch: [32][300/616]	Loss 2.2687e-01 (2.3844e-01)	Acc 0.762695 (0.748705)
Epoch: [32][600/616]	Loss 2.3149e-01 (2.3808e-01)	Acc 0.765625 (0.749116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753743)
Training Loss of Epoch 32: 0.2381245351419216
Training Acc of Epoch 32: 0.749072662601626
Testing Acc of Epoch 32: 0.7537434782608695
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.2243e-01 (2.2243e-01)	Acc 0.776367 (0.776367)
Epoch: [33][300/616]	Loss 2.3741e-01 (2.3769e-01)	Acc 0.759766 (0.750360)
Epoch: [33][600/616]	Loss 2.2377e-01 (2.3815e-01)	Acc 0.755859 (0.749409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750361)
Training Loss of Epoch 33: 0.23819636605619415
Training Acc of Epoch 33: 0.7493108485772357
Testing Acc of Epoch 33: 0.7503608695652174
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.5282e-01 (2.5282e-01)	Acc 0.730469 (0.730469)
Epoch: [34][300/616]	Loss 2.3859e-01 (2.3723e-01)	Acc 0.745117 (0.749289)
Epoch: [34][600/616]	Loss 2.4041e-01 (2.3786e-01)	Acc 0.749023 (0.749236)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.750230)
Training Loss of Epoch 34: 0.23799826316232603
Training Acc of Epoch 34: 0.7490663109756097
Testing Acc of Epoch 34: 0.7502304347826086
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.5618e-01 (2.5618e-01)	Acc 0.722656 (0.722656)
Epoch: [35][300/616]	Loss 2.4470e-01 (2.3776e-01)	Acc 0.750000 (0.749095)
Epoch: [35][600/616]	Loss 2.2440e-01 (2.3802e-01)	Acc 0.766602 (0.748921)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748126)
Training Loss of Epoch 35: 0.2379763025093854
Training Acc of Epoch 35: 0.7490154979674797
Testing Acc of Epoch 35: 0.7481260869565217
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.4530e-01 (2.4530e-01)	Acc 0.738281 (0.738281)
Epoch: [36][300/616]	Loss 2.3723e-01 (2.3921e-01)	Acc 0.738281 (0.747447)
Epoch: [36][600/616]	Loss 2.3148e-01 (2.3832e-01)	Acc 0.758789 (0.748937)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752630)
Training Loss of Epoch 36: 0.23814476665442552
Training Acc of Epoch 36: 0.7491234756097561
Testing Acc of Epoch 36: 0.7526304347826087
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3505e-01 (2.3505e-01)	Acc 0.741211 (0.741211)
Epoch: [37][300/616]	Loss 2.3585e-01 (2.3711e-01)	Acc 0.761719 (0.749974)
Epoch: [37][600/616]	Loss 2.4338e-01 (2.3782e-01)	Acc 0.736328 (0.749124)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749809)
Training Loss of Epoch 37: 0.23784169486867704
Training Acc of Epoch 37: 0.7491028328252033
Testing Acc of Epoch 37: 0.7498086956521739
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3795e-01 (2.3795e-01)	Acc 0.746094 (0.746094)
Epoch: [38][300/616]	Loss 2.5535e-01 (2.3772e-01)	Acc 0.721680 (0.749546)
Epoch: [38][600/616]	Loss 2.6406e-01 (2.3863e-01)	Acc 0.732422 (0.748239)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.749504)
Training Loss of Epoch 38: 0.2386219425656931
Training Acc of Epoch 38: 0.7482390116869919
Testing Acc of Epoch 38: 0.749504347826087
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.4158e-01 (2.4158e-01)	Acc 0.746094 (0.746094)
Epoch: [39][300/616]	Loss 2.3835e-01 (2.3797e-01)	Acc 0.750977 (0.749465)
Epoch: [39][600/616]	Loss 2.4044e-01 (2.3798e-01)	Acc 0.745117 (0.749373)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751978)
Training Loss of Epoch 39: 0.2379606889273093
Training Acc of Epoch 39: 0.7493283155487804
Testing Acc of Epoch 39: 0.7519782608695652
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.3141e-01 (2.3141e-01)	Acc 0.758789 (0.758789)
Epoch: [40][300/616]	Loss 2.4795e-01 (2.3914e-01)	Acc 0.741211 (0.748086)
Epoch: [40][600/616]	Loss 2.3623e-01 (2.3816e-01)	Acc 0.743164 (0.749019)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.745696)
Training Loss of Epoch 40: 0.23815080952353593
Training Acc of Epoch 40: 0.7489964430894309
Testing Acc of Epoch 40: 0.7456956521739131
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.2946e-01 (2.2946e-01)	Acc 0.754883 (0.754883)
Epoch: [41][300/616]	Loss 2.3490e-01 (2.3791e-01)	Acc 0.754883 (0.748676)
Epoch: [41][600/616]	Loss 2.3160e-01 (2.3766e-01)	Acc 0.748047 (0.749383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747513)
Training Loss of Epoch 41: 0.23762803506560443
Training Acc of Epoch 41: 0.7494124745934959
Testing Acc of Epoch 41: 0.7475130434782609
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.4828e-01 (2.4828e-01)	Acc 0.733398 (0.733398)
Epoch: [42][300/616]	Loss 2.4206e-01 (2.3757e-01)	Acc 0.740234 (0.749737)
Epoch: [42][600/616]	Loss 2.5326e-01 (2.3803e-01)	Acc 0.738281 (0.748850)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751665)
Training Loss of Epoch 42: 0.23795337560700208
Training Acc of Epoch 42: 0.7489646849593496
Testing Acc of Epoch 42: 0.7516652173913043
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.1877e-01 (2.1877e-01)	Acc 0.774414 (0.774414)
Epoch: [43][300/616]	Loss 2.4073e-01 (2.3745e-01)	Acc 0.767578 (0.749270)
Epoch: [43][600/616]	Loss 2.3537e-01 (2.3812e-01)	Acc 0.745117 (0.748822)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752848)
Training Loss of Epoch 43: 0.2382229578688862
Training Acc of Epoch 43: 0.7486931529471544
Testing Acc of Epoch 43: 0.7528478260869566
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3294e-01 (2.3294e-01)	Acc 0.735352 (0.735352)
Epoch: [44][300/616]	Loss 2.4060e-01 (2.3685e-01)	Acc 0.765625 (0.749734)
Epoch: [44][600/616]	Loss 2.4590e-01 (2.3776e-01)	Acc 0.739258 (0.748978)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752496)
Training Loss of Epoch 44: 0.23772538682309594
Training Acc of Epoch 44: 0.7491107723577236
Testing Acc of Epoch 44: 0.752495652173913
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.2863e-01 (2.2863e-01)	Acc 0.761719 (0.761719)
Epoch: [45][300/616]	Loss 2.2243e-01 (2.3915e-01)	Acc 0.773438 (0.748540)
Epoch: [45][600/616]	Loss 2.3298e-01 (2.3834e-01)	Acc 0.749023 (0.748612)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.747309)
Training Loss of Epoch 45: 0.23824305042503327
Training Acc of Epoch 45: 0.7487661966463415
Testing Acc of Epoch 45: 0.747308695652174
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4077e-01 (2.4077e-01)	Acc 0.756836 (0.756836)
Epoch: [46][300/616]	Loss 2.4349e-01 (2.3793e-01)	Acc 0.733398 (0.749429)
Epoch: [46][600/616]	Loss 2.3761e-01 (2.3785e-01)	Acc 0.752930 (0.749314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.752348)
Training Loss of Epoch 46: 0.2378603296551278
Training Acc of Epoch 46: 0.749291793699187
Testing Acc of Epoch 46: 0.7523478260869565
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.2212e-01 (2.2212e-01)	Acc 0.764648 (0.764648)
Epoch: [47][300/616]	Loss 2.5216e-01 (2.3775e-01)	Acc 0.739258 (0.749591)
Epoch: [47][600/616]	Loss 2.4504e-01 (2.3728e-01)	Acc 0.723633 (0.749760)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750935)
Training Loss of Epoch 47: 0.23723206302014793
Training Acc of Epoch 47: 0.7498332698170732
Testing Acc of Epoch 47: 0.7509347826086956
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.2661e-01 (2.2661e-01)	Acc 0.776367 (0.776367)
Epoch: [48][300/616]	Loss 2.4609e-01 (2.3744e-01)	Acc 0.742188 (0.749663)
Epoch: [48][600/616]	Loss 2.1902e-01 (2.3733e-01)	Acc 0.780273 (0.749794)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752839)
Training Loss of Epoch 48: 0.23728710740077785
Training Acc of Epoch 48: 0.7498015116869918
Testing Acc of Epoch 48: 0.7528391304347826
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.1740e-01 (2.1740e-01)	Acc 0.777344 (0.777344)
Epoch: [49][300/616]	Loss 2.3182e-01 (2.3713e-01)	Acc 0.759766 (0.749718)
Epoch: [49][600/616]	Loss 2.3076e-01 (2.3730e-01)	Acc 0.749023 (0.749651)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753617)
Training Loss of Epoch 49: 0.23742196043817007
Training Acc of Epoch 49: 0.7494331173780487
Testing Acc of Epoch 49: 0.7536173913043478
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2353e-01 (2.2353e-01)	Acc 0.777344 (0.777344)
Epoch: [50][300/616]	Loss 2.2842e-01 (2.3773e-01)	Acc 0.762695 (0.749617)
Epoch: [50][600/616]	Loss 2.4836e-01 (2.3796e-01)	Acc 0.736328 (0.749428)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749161)
Training Loss of Epoch 50: 0.2379605794098319
Training Acc of Epoch 50: 0.7493743648373984
Testing Acc of Epoch 50: 0.7491608695652174
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3744e-01 (2.3744e-01)	Acc 0.750977 (0.750977)
Epoch: [51][300/616]	Loss 2.3964e-01 (2.3738e-01)	Acc 0.742188 (0.749604)
Epoch: [51][600/616]	Loss 2.3052e-01 (2.3776e-01)	Acc 0.766602 (0.749140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748443)
Training Loss of Epoch 51: 0.2377267756113192
Training Acc of Epoch 51: 0.7492505081300813
Testing Acc of Epoch 51: 0.7484434782608695
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4468e-01 (2.4468e-01)	Acc 0.739258 (0.739258)
Epoch: [52][300/616]	Loss 2.1625e-01 (2.3740e-01)	Acc 0.782227 (0.750010)
Epoch: [52][600/616]	Loss 2.4700e-01 (2.3803e-01)	Acc 0.727539 (0.749152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.750857)
Training Loss of Epoch 52: 0.238037041217331
Training Acc of Epoch 52: 0.7491075965447155
Testing Acc of Epoch 52: 0.7508565217391304
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.2736e-01 (2.2736e-01)	Acc 0.749023 (0.749023)
Epoch: [53][300/616]	Loss 2.2866e-01 (2.3801e-01)	Acc 0.755859 (0.749020)
Epoch: [53][600/616]	Loss 2.3636e-01 (2.3793e-01)	Acc 0.746094 (0.749262)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.749878)
Training Loss of Epoch 53: 0.2378429935715063
Training Acc of Epoch 53: 0.7494172383130081
Testing Acc of Epoch 53: 0.7498782608695652
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.2638e-01 (2.2638e-01)	Acc 0.765625 (0.765625)
Epoch: [54][300/616]	Loss 2.4477e-01 (2.3996e-01)	Acc 0.757812 (0.747168)
Epoch: [54][600/616]	Loss 2.5676e-01 (2.3877e-01)	Acc 0.721680 (0.748282)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.754609)
Training Loss of Epoch 54: 0.2386603293380117
Training Acc of Epoch 54: 0.7484073297764228
Testing Acc of Epoch 54: 0.7546086956521739
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.3851e-01 (2.3851e-01)	Acc 0.752930 (0.752930)
Epoch: [55][300/616]	Loss 2.4500e-01 (2.3783e-01)	Acc 0.746094 (0.749565)
Epoch: [55][600/616]	Loss 2.2971e-01 (2.3864e-01)	Acc 0.758789 (0.748609)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.749439)
Training Loss of Epoch 55: 0.23866876858036692
Training Acc of Epoch 55: 0.7485438897357723
Testing Acc of Epoch 55: 0.7494391304347826
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4005e-01 (2.4005e-01)	Acc 0.750000 (0.750000)
Epoch: [56][300/616]	Loss 2.2471e-01 (2.3832e-01)	Acc 0.765625 (0.748731)
Epoch: [56][600/616]	Loss 2.3359e-01 (2.3837e-01)	Acc 0.753906 (0.748658)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746383)
Training Loss of Epoch 56: 0.23837496909668776
Training Acc of Epoch 56: 0.7486550431910569
Testing Acc of Epoch 56: 0.7463826086956522
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.2421e-01 (2.2421e-01)	Acc 0.764648 (0.764648)
Epoch: [57][300/616]	Loss 2.3248e-01 (2.3938e-01)	Acc 0.744141 (0.747742)
Epoch: [57][600/616]	Loss 2.3511e-01 (2.3920e-01)	Acc 0.752930 (0.748107)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.752339)
Training Loss of Epoch 57: 0.2391605524270515
Training Acc of Epoch 57: 0.7481167428861789
Testing Acc of Epoch 57: 0.7523391304347826
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.3087e-01 (2.3087e-01)	Acc 0.763672 (0.763672)
Epoch: [58][300/616]	Loss 2.4171e-01 (2.3967e-01)	Acc 0.729492 (0.747771)
Epoch: [58][600/616]	Loss 2.3557e-01 (2.3874e-01)	Acc 0.753906 (0.748369)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.752778)
Training Loss of Epoch 58: 0.23869060285207702
Training Acc of Epoch 58: 0.7484930767276423
Testing Acc of Epoch 58: 0.7527782608695652
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3820e-01 (2.3820e-01)	Acc 0.744141 (0.744141)
Epoch: [59][300/616]	Loss 2.3446e-01 (2.3930e-01)	Acc 0.762695 (0.748034)
Epoch: [59][600/616]	Loss 2.5090e-01 (2.3965e-01)	Acc 0.737305 (0.747584)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746283)
Training Loss of Epoch 59: 0.23958361756995442
Training Acc of Epoch 59: 0.7476768927845528
Testing Acc of Epoch 59: 0.7462826086956522
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5084e-01 (2.5084e-01)	Acc 0.739258 (0.739258)
Epoch: [60][300/616]	Loss 2.5129e-01 (2.3997e-01)	Acc 0.743164 (0.746668)
Epoch: [60][600/616]	Loss 2.5450e-01 (2.3953e-01)	Acc 0.734375 (0.747611)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.748339)
Training Loss of Epoch 60: 0.2395129173751769
Training Acc of Epoch 60: 0.747584794207317
Testing Acc of Epoch 60: 0.7483391304347826
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3299e-01 (2.3299e-01)	Acc 0.755859 (0.755859)
Epoch: [61][300/616]	Loss 2.4220e-01 (2.3947e-01)	Acc 0.747070 (0.747800)
Epoch: [61][600/616]	Loss 2.6940e-01 (2.3934e-01)	Acc 0.705078 (0.747673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.741952)
Training Loss of Epoch 61: 0.23950934492475617
Training Acc of Epoch 61: 0.7475260416666667
Testing Acc of Epoch 61: 0.7419521739130435
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4521e-01 (2.4521e-01)	Acc 0.753906 (0.753906)
Epoch: [62][300/616]	Loss 2.2416e-01 (2.3858e-01)	Acc 0.762695 (0.748861)
Epoch: [62][600/616]	Loss 2.2498e-01 (2.3943e-01)	Acc 0.764648 (0.747953)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746983)
Training Loss of Epoch 62: 0.2394166395916202
Training Acc of Epoch 62: 0.7479992378048781
Testing Acc of Epoch 62: 0.7469826086956521
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.5018e-01 (2.5018e-01)	Acc 0.733398 (0.733398)
Epoch: [63][300/616]	Loss 2.3978e-01 (2.3789e-01)	Acc 0.739258 (0.748955)
Epoch: [63][600/616]	Loss 2.2410e-01 (2.3830e-01)	Acc 0.779297 (0.748954)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749735)
Training Loss of Epoch 63: 0.23828811885380163
Training Acc of Epoch 63: 0.7489916793699187
Testing Acc of Epoch 63: 0.7497347826086956
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.2922e-01 (2.2922e-01)	Acc 0.767578 (0.767578)
Epoch: [64][300/616]	Loss 2.3603e-01 (2.3848e-01)	Acc 0.747070 (0.748962)
Epoch: [64][600/616]	Loss 2.5375e-01 (2.3856e-01)	Acc 0.729492 (0.748513)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.750452)
Training Loss of Epoch 64: 0.23852912669743948
Training Acc of Epoch 64: 0.748534362296748
Testing Acc of Epoch 64: 0.7504521739130435
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3482e-01 (2.3482e-01)	Acc 0.756836 (0.756836)
Epoch: [65][300/616]	Loss 2.3572e-01 (2.3886e-01)	Acc 0.750000 (0.748339)
Epoch: [65][600/616]	Loss 2.2743e-01 (2.3882e-01)	Acc 0.769531 (0.748079)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748361)
Training Loss of Epoch 65: 0.23872465023664924
Training Acc of Epoch 65: 0.7481119791666667
Testing Acc of Epoch 65: 0.7483608695652174
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.1469e-01 (2.1469e-01)	Acc 0.790039 (0.790039)
Epoch: [66][300/616]	Loss 2.5082e-01 (2.3867e-01)	Acc 0.736328 (0.748585)
Epoch: [66][600/616]	Loss 2.3554e-01 (2.3936e-01)	Acc 0.750000 (0.747452)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.737287)
Training Loss of Epoch 66: 0.2393302391457364
Training Acc of Epoch 66: 0.7475022230691057
Testing Acc of Epoch 66: 0.7372869565217391
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.5954e-01 (2.5954e-01)	Acc 0.718750 (0.718750)
Epoch: [67][300/616]	Loss 2.3456e-01 (2.3938e-01)	Acc 0.751953 (0.748063)
Epoch: [67][600/616]	Loss 2.4858e-01 (2.4000e-01)	Acc 0.718750 (0.747215)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.748687)
Training Loss of Epoch 67: 0.24002946152435087
Training Acc of Epoch 67: 0.7471655868902439
Testing Acc of Epoch 67: 0.7486869565217391
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.1944e-01 (2.1944e-01)	Acc 0.770508 (0.770508)
Epoch: [68][300/616]	Loss 2.3272e-01 (2.3941e-01)	Acc 0.776367 (0.747573)
Epoch: [68][600/616]	Loss 2.3162e-01 (2.4007e-01)	Acc 0.768555 (0.747142)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.752948)
Training Loss of Epoch 68: 0.24005324217846724
Training Acc of Epoch 68: 0.747243394308943
Testing Acc of Epoch 68: 0.7529478260869565
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.2893e-01 (2.2893e-01)	Acc 0.758789 (0.758789)
Epoch: [69][300/616]	Loss 2.2645e-01 (2.3874e-01)	Acc 0.775391 (0.748495)
Epoch: [69][600/616]	Loss 2.4423e-01 (2.3918e-01)	Acc 0.738281 (0.748144)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748452)
Training Loss of Epoch 69: 0.23912921334669843
Training Acc of Epoch 69: 0.7481723196138211
Testing Acc of Epoch 69: 0.7484521739130435
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4240e-01 (2.4240e-01)	Acc 0.735352 (0.735352)
Epoch: [70][300/616]	Loss 2.4193e-01 (2.3895e-01)	Acc 0.750977 (0.748397)
Epoch: [70][600/616]	Loss 2.2389e-01 (2.3888e-01)	Acc 0.758789 (0.748456)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751313)
Training Loss of Epoch 70: 0.238973467354852
Training Acc of Epoch 70: 0.7483755716463415
Testing Acc of Epoch 70: 0.7513130434782609
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.2471e-01 (2.2471e-01)	Acc 0.769531 (0.769531)
Epoch: [71][300/616]	Loss 2.3375e-01 (2.3896e-01)	Acc 0.749023 (0.747911)
Epoch: [71][600/616]	Loss 2.4514e-01 (2.3882e-01)	Acc 0.735352 (0.748138)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.747300)
Training Loss of Epoch 71: 0.23886552261627786
Training Acc of Epoch 71: 0.7481215066056911
Testing Acc of Epoch 71: 0.7473
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.3753e-01 (2.3753e-01)	Acc 0.741211 (0.741211)
Epoch: [72][300/616]	Loss 2.3790e-01 (2.3873e-01)	Acc 0.751953 (0.748579)
Epoch: [72][600/616]	Loss 2.4254e-01 (2.3945e-01)	Acc 0.748047 (0.747735)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.747183)
Training Loss of Epoch 72: 0.23937862968541743
Training Acc of Epoch 72: 0.7478213922764227
Testing Acc of Epoch 72: 0.7471826086956522
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4767e-01 (2.4767e-01)	Acc 0.730469 (0.730469)
Epoch: [73][300/616]	Loss 2.3902e-01 (2.3969e-01)	Acc 0.757812 (0.747865)
Epoch: [73][600/616]	Loss 2.5196e-01 (2.3918e-01)	Acc 0.720703 (0.748073)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748426)
Training Loss of Epoch 73: 0.2391621040861781
Training Acc of Epoch 73: 0.7480532266260163
Testing Acc of Epoch 73: 0.7484260869565217
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3528e-01 (2.3528e-01)	Acc 0.756836 (0.756836)
Epoch: [74][300/616]	Loss 2.4059e-01 (2.4087e-01)	Acc 0.747070 (0.746201)
Epoch: [74][600/616]	Loss 2.2724e-01 (2.3969e-01)	Acc 0.752930 (0.747358)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.751809)
Training Loss of Epoch 74: 0.2396412131505284
Training Acc of Epoch 74: 0.7474529979674797
Testing Acc of Epoch 74: 0.7518086956521739
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4017e-01 (2.4017e-01)	Acc 0.745117 (0.745117)
Epoch: [75][300/616]	Loss 2.3593e-01 (2.3345e-01)	Acc 0.756836 (0.753484)
Epoch: [75][600/616]	Loss 2.3344e-01 (2.3381e-01)	Acc 0.758789 (0.752746)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749739)
Training Loss of Epoch 75: 0.23379935728825205
Training Acc of Epoch 75: 0.7528121824186992
Testing Acc of Epoch 75: 0.7497391304347826
Model with the best training loss saved! The loss is 0.23379935728825205
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.5364e-01 (2.5364e-01)	Acc 0.723633 (0.723633)
Epoch: [76][300/616]	Loss 2.1676e-01 (2.3426e-01)	Acc 0.781250 (0.752375)
Epoch: [76][600/616]	Loss 2.4832e-01 (2.3427e-01)	Acc 0.736328 (0.752572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754200)
Training Loss of Epoch 76: 0.23429196782228423
Training Acc of Epoch 76: 0.7524834857723577
Testing Acc of Epoch 76: 0.7542
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4415e-01 (2.4415e-01)	Acc 0.743164 (0.743164)
Epoch: [77][300/616]	Loss 2.3761e-01 (2.3390e-01)	Acc 0.735352 (0.753115)
Epoch: [77][600/616]	Loss 2.4441e-01 (2.3398e-01)	Acc 0.742188 (0.752844)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.755230)
Training Loss of Epoch 77: 0.23393834078215003
Training Acc of Epoch 77: 0.7528534679878048
Testing Acc of Epoch 77: 0.7552304347826087
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4169e-01 (2.4169e-01)	Acc 0.745117 (0.745117)
Epoch: [78][300/616]	Loss 2.2785e-01 (2.3385e-01)	Acc 0.767578 (0.752745)
Epoch: [78][600/616]	Loss 2.3925e-01 (2.3402e-01)	Acc 0.732422 (0.752457)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.754296)
Training Loss of Epoch 78: 0.23395179897789065
Training Acc of Epoch 78: 0.7525771722560975
Testing Acc of Epoch 78: 0.754295652173913
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3226e-01 (2.3226e-01)	Acc 0.755859 (0.755859)
Epoch: [79][300/616]	Loss 2.3177e-01 (2.3503e-01)	Acc 0.750977 (0.752628)
Epoch: [79][600/616]	Loss 2.2269e-01 (2.3509e-01)	Acc 0.764648 (0.751995)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.753422)
Training Loss of Epoch 79: 0.23507786407703307
Training Acc of Epoch 79: 0.7520356961382114
Testing Acc of Epoch 79: 0.7534217391304348
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.1498e-01 (2.1498e-01)	Acc 0.779297 (0.779297)
Epoch: [80][300/616]	Loss 2.2999e-01 (2.3460e-01)	Acc 0.752930 (0.752242)
Epoch: [80][600/616]	Loss 2.2565e-01 (2.3474e-01)	Acc 0.752930 (0.752249)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.752226)
Training Loss of Epoch 80: 0.23477260186904814
Training Acc of Epoch 80: 0.7522452997967479
Testing Acc of Epoch 80: 0.7522260869565217
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4583e-01 (2.4583e-01)	Acc 0.732422 (0.732422)
Epoch: [81][300/616]	Loss 2.4163e-01 (2.3420e-01)	Acc 0.739258 (0.752647)
Epoch: [81][600/616]	Loss 2.2129e-01 (2.3395e-01)	Acc 0.764648 (0.753016)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752622)
Training Loss of Epoch 81: 0.23391771927112487
Training Acc of Epoch 81: 0.7530059070121952
Testing Acc of Epoch 81: 0.7526217391304347
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3979e-01 (2.3979e-01)	Acc 0.749023 (0.749023)
Epoch: [82][300/616]	Loss 2.2247e-01 (2.3402e-01)	Acc 0.766602 (0.752608)
Epoch: [82][600/616]	Loss 2.1983e-01 (2.3387e-01)	Acc 0.779297 (0.752904)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754178)
Training Loss of Epoch 82: 0.2337969469345682
Training Acc of Epoch 82: 0.7529566819105691
Testing Acc of Epoch 82: 0.7541782608695652
Model with the best training loss saved! The loss is 0.2337969469345682
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.2775e-01 (2.2775e-01)	Acc 0.748047 (0.748047)
Epoch: [83][300/616]	Loss 2.3254e-01 (2.3377e-01)	Acc 0.752930 (0.752956)
Epoch: [83][600/616]	Loss 2.2273e-01 (2.3359e-01)	Acc 0.762695 (0.753321)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755287)
Training Loss of Epoch 83: 0.2334979815696313
Training Acc of Epoch 83: 0.7533727134146342
Testing Acc of Epoch 83: 0.7552869565217392
Model with the best training loss saved! The loss is 0.2334979815696313
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3618e-01 (2.3618e-01)	Acc 0.752930 (0.752930)
Epoch: [84][300/616]	Loss 2.2844e-01 (2.3390e-01)	Acc 0.760742 (0.752615)
Epoch: [84][600/616]	Loss 2.3977e-01 (2.3339e-01)	Acc 0.746094 (0.753211)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.753222)
Training Loss of Epoch 84: 0.23344176486740267
Training Acc of Epoch 84: 0.7532186864837398
Testing Acc of Epoch 84: 0.7532217391304348
Model with the best training loss saved! The loss is 0.23344176486740267
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4007e-01 (2.4007e-01)	Acc 0.734375 (0.734375)
Epoch: [85][300/616]	Loss 2.4378e-01 (2.3393e-01)	Acc 0.748047 (0.752336)
Epoch: [85][600/616]	Loss 2.2940e-01 (2.3350e-01)	Acc 0.755859 (0.753014)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753826)
Training Loss of Epoch 85: 0.23344130457901374
Training Acc of Epoch 85: 0.7530662474593496
Testing Acc of Epoch 85: 0.7538260869565218
Model with the best training loss saved! The loss is 0.23344130457901374
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3482e-01 (2.3482e-01)	Acc 0.753906 (0.753906)
Epoch: [86][300/616]	Loss 2.2269e-01 (2.3422e-01)	Acc 0.774414 (0.752381)
Epoch: [86][600/616]	Loss 2.3511e-01 (2.3352e-01)	Acc 0.755859 (0.753381)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754435)
Training Loss of Epoch 86: 0.23353137340002914
Training Acc of Epoch 86: 0.7533298399390244
Testing Acc of Epoch 86: 0.7544347826086957
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3312e-01 (2.3312e-01)	Acc 0.762695 (0.762695)
Epoch: [87][300/616]	Loss 2.3594e-01 (2.3378e-01)	Acc 0.755859 (0.753004)
Epoch: [87][600/616]	Loss 2.2631e-01 (2.3387e-01)	Acc 0.753906 (0.753061)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754052)
Training Loss of Epoch 87: 0.23384761156105413
Training Acc of Epoch 87: 0.7531265879065041
Testing Acc of Epoch 87: 0.7540521739130435
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4591e-01 (2.4591e-01)	Acc 0.744141 (0.744141)
Epoch: [88][300/616]	Loss 2.3838e-01 (2.3372e-01)	Acc 0.750977 (0.752917)
Epoch: [88][600/616]	Loss 2.3125e-01 (2.3369e-01)	Acc 0.759766 (0.752987)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752713)
Training Loss of Epoch 88: 0.2336621465964046
Training Acc of Epoch 88: 0.7529916158536586
Testing Acc of Epoch 88: 0.7527130434782608
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2862e-01 (2.2862e-01)	Acc 0.757812 (0.757812)
Epoch: [89][300/616]	Loss 2.2547e-01 (2.3332e-01)	Acc 0.764648 (0.753416)
Epoch: [89][600/616]	Loss 2.3443e-01 (2.3385e-01)	Acc 0.741211 (0.752970)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753796)
Training Loss of Epoch 89: 0.23389857571299483
Training Acc of Epoch 89: 0.7529265116869919
Testing Acc of Epoch 89: 0.7537956521739131
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2140e-01 (2.2140e-01)	Acc 0.765625 (0.765625)
Epoch: [90][300/616]	Loss 2.2836e-01 (2.3515e-01)	Acc 0.765625 (0.751483)
Epoch: [90][600/616]	Loss 2.3230e-01 (2.3451e-01)	Acc 0.753906 (0.752192)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754735)
Training Loss of Epoch 90: 0.23446154451467158
Training Acc of Epoch 90: 0.7522151295731707
Testing Acc of Epoch 90: 0.7547347826086956
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.2915e-01 (2.2915e-01)	Acc 0.760742 (0.760742)
Epoch: [91][300/616]	Loss 2.3197e-01 (2.3383e-01)	Acc 0.757812 (0.752959)
Epoch: [91][600/616]	Loss 2.2548e-01 (2.3416e-01)	Acc 0.765625 (0.752543)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754857)
Training Loss of Epoch 91: 0.2340485910574595
Training Acc of Epoch 91: 0.7526915015243902
Testing Acc of Epoch 91: 0.7548565217391304
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.1642e-01 (2.1642e-01)	Acc 0.764648 (0.764648)
Epoch: [92][300/616]	Loss 2.3839e-01 (2.3367e-01)	Acc 0.760742 (0.753040)
Epoch: [92][600/616]	Loss 2.5583e-01 (2.3433e-01)	Acc 0.728516 (0.752216)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.752770)
Training Loss of Epoch 92: 0.23437253042934386
Training Acc of Epoch 92: 0.7522135416666667
Testing Acc of Epoch 92: 0.7527695652173914
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.4127e-01 (2.4127e-01)	Acc 0.742188 (0.742188)
Epoch: [93][300/616]	Loss 2.2554e-01 (2.3454e-01)	Acc 0.757812 (0.752680)
Epoch: [93][600/616]	Loss 2.2729e-01 (2.3405e-01)	Acc 0.750977 (0.752926)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752422)
Training Loss of Epoch 93: 0.23400807618125669
Training Acc of Epoch 93: 0.7530074949186992
Testing Acc of Epoch 93: 0.7524217391304348
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2955e-01 (2.2955e-01)	Acc 0.753906 (0.753906)
Epoch: [94][300/616]	Loss 2.1673e-01 (2.3504e-01)	Acc 0.767578 (0.751557)
Epoch: [94][600/616]	Loss 2.3565e-01 (2.3494e-01)	Acc 0.752930 (0.751708)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753778)
Training Loss of Epoch 94: 0.2348840186993281
Training Acc of Epoch 94: 0.7517736915650407
Testing Acc of Epoch 94: 0.7537782608695652
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2383e-01 (2.2383e-01)	Acc 0.772461 (0.772461)
Epoch: [95][300/616]	Loss 2.1942e-01 (2.3539e-01)	Acc 0.780273 (0.750668)
Epoch: [95][600/616]	Loss 2.2528e-01 (2.3448e-01)	Acc 0.764648 (0.752291)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.752400)
Training Loss of Epoch 95: 0.2344283957549227
Training Acc of Epoch 95: 0.7523897992886179
Testing Acc of Epoch 95: 0.7524
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3611e-01 (2.3611e-01)	Acc 0.739258 (0.739258)
Epoch: [96][300/616]	Loss 2.2851e-01 (2.3405e-01)	Acc 0.758789 (0.752248)
Epoch: [96][600/616]	Loss 2.4890e-01 (2.3443e-01)	Acc 0.737305 (0.752341)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750561)
Training Loss of Epoch 96: 0.2343849137304275
Training Acc of Epoch 96: 0.7524787220528455
Testing Acc of Epoch 96: 0.7505608695652174
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.4285e-01 (2.4285e-01)	Acc 0.727539 (0.727539)
Epoch: [97][300/616]	Loss 2.4169e-01 (2.3414e-01)	Acc 0.732422 (0.752511)
Epoch: [97][600/616]	Loss 2.3065e-01 (2.3456e-01)	Acc 0.753906 (0.752384)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.747987)
Training Loss of Epoch 97: 0.2345961919160393
Training Acc of Epoch 97: 0.7523421620934959
Testing Acc of Epoch 97: 0.7479869565217391
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4175e-01 (2.4175e-01)	Acc 0.741211 (0.741211)
Epoch: [98][300/616]	Loss 2.5215e-01 (2.3440e-01)	Acc 0.731445 (0.752751)
Epoch: [98][600/616]	Loss 2.2629e-01 (2.3481e-01)	Acc 0.766602 (0.752231)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754983)
Training Loss of Epoch 98: 0.2347370661855713
Training Acc of Epoch 98: 0.7522151295731707
Testing Acc of Epoch 98: 0.7549826086956521
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3406e-01 (2.3406e-01)	Acc 0.754883 (0.754883)
Epoch: [99][300/616]	Loss 2.4172e-01 (2.3433e-01)	Acc 0.741211 (0.752777)
Epoch: [99][600/616]	Loss 2.2643e-01 (2.3452e-01)	Acc 0.772461 (0.752528)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.749630)
Training Loss of Epoch 99: 0.23449634303891562
Training Acc of Epoch 99: 0.7525041285569106
Testing Acc of Epoch 99: 0.7496304347826087
Early stopping not satisfied.
