train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.00625
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.00625/lr_decay/JT_6b/
file_prefix exp_1
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.00625
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9987e-01 (4.9987e-01)	Acc 0.268555 (0.268555)
Epoch: [0][300/616]	Loss 2.5699e-01 (2.8352e-01)	Acc 0.722656 (0.699475)
Epoch: [0][600/616]	Loss 2.2730e-01 (2.6476e-01)	Acc 0.764648 (0.720788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747835)
Training Loss of Epoch 0: 0.2641979136118075
Training Acc of Epoch 0: 0.7214716717479674
Testing Acc of Epoch 0: 0.7478347826086956
Model with the best training loss saved! The loss is 0.2641979136118075
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.3677e-01 (2.3677e-01)	Acc 0.755859 (0.755859)
Epoch: [1][300/616]	Loss 2.4079e-01 (2.4114e-01)	Acc 0.751953 (0.746464)
Epoch: [1][600/616]	Loss 2.3048e-01 (2.3949e-01)	Acc 0.767578 (0.747750)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.752230)
Training Loss of Epoch 1: 0.23937773646377936
Training Acc of Epoch 1: 0.7478801448170732
Testing Acc of Epoch 1: 0.7522304347826086
Model with the best training loss saved! The loss is 0.23937773646377936
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.3897e-01 (2.3897e-01)	Acc 0.741211 (0.741211)
Epoch: [2][300/616]	Loss 2.3977e-01 (2.3659e-01)	Acc 0.750977 (0.751434)
Epoch: [2][600/616]	Loss 2.3994e-01 (2.3696e-01)	Acc 0.749023 (0.750034)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752339)
Training Loss of Epoch 2: 0.2370037346109142
Training Acc of Epoch 2: 0.7499221925813008
Testing Acc of Epoch 2: 0.7523391304347826
Model with the best training loss saved! The loss is 0.2370037346109142
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.5051e-01 (2.5051e-01)	Acc 0.731445 (0.731445)
Epoch: [3][300/616]	Loss 2.2243e-01 (2.3584e-01)	Acc 0.772461 (0.750941)
Epoch: [3][600/616]	Loss 2.3637e-01 (2.3581e-01)	Acc 0.753906 (0.751045)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752270)
Training Loss of Epoch 3: 0.2356987939617498
Training Acc of Epoch 3: 0.751148056402439
Testing Acc of Epoch 3: 0.7522695652173913
Model with the best training loss saved! The loss is 0.2356987939617498
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4298e-01 (2.4298e-01)	Acc 0.746094 (0.746094)
Epoch: [4][300/616]	Loss 2.4259e-01 (2.3607e-01)	Acc 0.731445 (0.751538)
Epoch: [4][600/616]	Loss 2.6125e-01 (2.3617e-01)	Acc 0.727539 (0.751128)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.748896)
Training Loss of Epoch 4: 0.2361961814446178
Training Acc of Epoch 4: 0.7511639354674797
Testing Acc of Epoch 4: 0.7488956521739131
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3967e-01 (2.3967e-01)	Acc 0.747070 (0.747070)
Epoch: [5][300/616]	Loss 2.3228e-01 (2.3550e-01)	Acc 0.754883 (0.752031)
Epoch: [5][600/616]	Loss 2.4479e-01 (2.3527e-01)	Acc 0.747070 (0.752007)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.749022)
Training Loss of Epoch 5: 0.23529417764365188
Training Acc of Epoch 5: 0.7519308943089431
Testing Acc of Epoch 5: 0.7490217391304348
Model with the best training loss saved! The loss is 0.23529417764365188
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4662e-01 (2.4662e-01)	Acc 0.746094 (0.746094)
Epoch: [6][300/616]	Loss 2.3230e-01 (2.3491e-01)	Acc 0.758789 (0.752621)
Epoch: [6][600/616]	Loss 2.3576e-01 (2.3606e-01)	Acc 0.737305 (0.751136)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.755978)
Training Loss of Epoch 6: 0.23598200022205104
Training Acc of Epoch 6: 0.7512004573170732
Testing Acc of Epoch 6: 0.7559782608695652
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.3333e-01 (2.3333e-01)	Acc 0.747070 (0.747070)
Epoch: [7][300/616]	Loss 2.2975e-01 (2.3580e-01)	Acc 0.750000 (0.750772)
Epoch: [7][600/616]	Loss 2.4582e-01 (2.3582e-01)	Acc 0.745117 (0.751373)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753735)
Training Loss of Epoch 7: 0.2358258456718631
Training Acc of Epoch 7: 0.7513401930894309
Testing Acc of Epoch 7: 0.7537347826086956
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.2585e-01 (2.2585e-01)	Acc 0.761719 (0.761719)
Epoch: [8][300/616]	Loss 2.2462e-01 (2.3504e-01)	Acc 0.762695 (0.751904)
Epoch: [8][600/616]	Loss 2.1084e-01 (2.3515e-01)	Acc 0.785156 (0.751740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753457)
Training Loss of Epoch 8: 0.23522846395891855
Training Acc of Epoch 8: 0.7516133130081301
Testing Acc of Epoch 8: 0.7534565217391305
Model with the best training loss saved! The loss is 0.23522846395891855
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.2027e-01 (2.2027e-01)	Acc 0.758789 (0.758789)
Epoch: [9][300/616]	Loss 2.4303e-01 (2.3545e-01)	Acc 0.744141 (0.751535)
Epoch: [9][600/616]	Loss 2.2948e-01 (2.3510e-01)	Acc 0.765625 (0.751908)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752196)
Training Loss of Epoch 9: 0.23512264705770383
Training Acc of Epoch 9: 0.7519451854674797
Testing Acc of Epoch 9: 0.752195652173913
Model with the best training loss saved! The loss is 0.23512264705770383
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.3253e-01 (2.3253e-01)	Acc 0.761719 (0.761719)
Epoch: [10][300/616]	Loss 2.3138e-01 (2.3449e-01)	Acc 0.759766 (0.752547)
Epoch: [10][600/616]	Loss 2.3325e-01 (2.3509e-01)	Acc 0.762695 (0.752068)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754300)
Training Loss of Epoch 10: 0.23514833859796447
Training Acc of Epoch 10: 0.7520182291666667
Testing Acc of Epoch 10: 0.7543
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3509e-01 (2.3509e-01)	Acc 0.754883 (0.754883)
Epoch: [11][300/616]	Loss 2.3980e-01 (2.3512e-01)	Acc 0.741211 (0.751869)
Epoch: [11][600/616]	Loss 2.2749e-01 (2.3491e-01)	Acc 0.769531 (0.752116)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751296)
Training Loss of Epoch 11: 0.23493897226767813
Training Acc of Epoch 11: 0.7520785696138211
Testing Acc of Epoch 11: 0.751295652173913
Model with the best training loss saved! The loss is 0.23493897226767813
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4109e-01 (2.4109e-01)	Acc 0.746094 (0.746094)
Epoch: [12][300/616]	Loss 2.2729e-01 (2.3550e-01)	Acc 0.759766 (0.751389)
Epoch: [12][600/616]	Loss 2.3547e-01 (2.3548e-01)	Acc 0.753906 (0.751345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749665)
Training Loss of Epoch 12: 0.23548814019536585
Training Acc of Epoch 12: 0.751270325203252
Testing Acc of Epoch 12: 0.7496652173913043
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4288e-01 (2.4288e-01)	Acc 0.749023 (0.749023)
Epoch: [13][300/616]	Loss 2.5016e-01 (2.3647e-01)	Acc 0.729492 (0.750191)
Epoch: [13][600/616]	Loss 2.4247e-01 (2.3559e-01)	Acc 0.730469 (0.751261)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749635)
Training Loss of Epoch 13: 0.23561450226035544
Training Acc of Epoch 13: 0.7511814024390244
Testing Acc of Epoch 13: 0.7496347826086956
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.2896e-01 (2.2896e-01)	Acc 0.750000 (0.750000)
Epoch: [14][300/616]	Loss 2.3578e-01 (2.3527e-01)	Acc 0.744141 (0.751512)
Epoch: [14][600/616]	Loss 2.3116e-01 (2.3550e-01)	Acc 0.758789 (0.751250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754483)
Training Loss of Epoch 14: 0.23565852092048986
Training Acc of Epoch 14: 0.7510654852642277
Testing Acc of Epoch 14: 0.7544826086956522
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3032e-01 (2.3032e-01)	Acc 0.751953 (0.751953)
Epoch: [15][300/616]	Loss 2.3896e-01 (2.3547e-01)	Acc 0.754883 (0.751638)
Epoch: [15][600/616]	Loss 2.3166e-01 (2.3598e-01)	Acc 0.756836 (0.750860)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753483)
Training Loss of Epoch 15: 0.23599359497306793
Training Acc of Epoch 15: 0.7508574695121951
Testing Acc of Epoch 15: 0.7534826086956522
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.3649e-01 (2.3649e-01)	Acc 0.747070 (0.747070)
Epoch: [16][300/616]	Loss 2.4197e-01 (2.3610e-01)	Acc 0.744141 (0.750454)
Epoch: [16][600/616]	Loss 2.2861e-01 (2.3566e-01)	Acc 0.759766 (0.750988)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752030)
Training Loss of Epoch 16: 0.2356731878790429
Training Acc of Epoch 16: 0.7509686229674797
Testing Acc of Epoch 16: 0.7520304347826087
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4429e-01 (2.4429e-01)	Acc 0.731445 (0.731445)
Epoch: [17][300/616]	Loss 2.2256e-01 (2.3591e-01)	Acc 0.771484 (0.751272)
Epoch: [17][600/616]	Loss 2.3883e-01 (2.3638e-01)	Acc 0.747070 (0.750637)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752509)
Training Loss of Epoch 17: 0.23631020510584358
Training Acc of Epoch 17: 0.7507748983739837
Testing Acc of Epoch 17: 0.7525086956521739
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.2896e-01 (2.2896e-01)	Acc 0.748047 (0.748047)
Epoch: [18][300/616]	Loss 2.3394e-01 (2.3603e-01)	Acc 0.744141 (0.751119)
Epoch: [18][600/616]	Loss 2.3138e-01 (2.3681e-01)	Acc 0.749023 (0.750162)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.752161)
Training Loss of Epoch 18: 0.23671491996544164
Training Acc of Epoch 18: 0.7502731199186992
Testing Acc of Epoch 18: 0.7521608695652174
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3439e-01 (2.3439e-01)	Acc 0.753906 (0.753906)
Epoch: [19][300/616]	Loss 2.3398e-01 (2.3567e-01)	Acc 0.756836 (0.751369)
Epoch: [19][600/616]	Loss 2.4422e-01 (2.3605e-01)	Acc 0.733398 (0.750858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.751409)
Training Loss of Epoch 19: 0.23610763617647373
Training Acc of Epoch 19: 0.7507637830284553
Testing Acc of Epoch 19: 0.7514086956521739
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4619e-01 (2.4619e-01)	Acc 0.731445 (0.731445)
Epoch: [20][300/616]	Loss 2.4271e-01 (2.3620e-01)	Acc 0.746094 (0.750655)
Epoch: [20][600/616]	Loss 2.2795e-01 (2.3646e-01)	Acc 0.758789 (0.750634)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747722)
Training Loss of Epoch 20: 0.2364530087971106
Training Acc of Epoch 20: 0.7506065802845528
Testing Acc of Epoch 20: 0.7477217391304348
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4744e-01 (2.4744e-01)	Acc 0.739258 (0.739258)
Epoch: [21][300/616]	Loss 2.4012e-01 (2.3602e-01)	Acc 0.739258 (0.751158)
Epoch: [21][600/616]	Loss 2.5117e-01 (2.3626e-01)	Acc 0.735352 (0.750552)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751330)
Training Loss of Epoch 21: 0.23624443983644006
Training Acc of Epoch 21: 0.7505160696138211
Testing Acc of Epoch 21: 0.7513304347826087
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.3448e-01 (2.3448e-01)	Acc 0.750977 (0.750977)
Epoch: [22][300/616]	Loss 2.4543e-01 (2.3789e-01)	Acc 0.735352 (0.749199)
Epoch: [22][600/616]	Loss 2.5515e-01 (2.3711e-01)	Acc 0.736328 (0.749919)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.750287)
Training Loss of Epoch 22: 0.2370697420787036
Training Acc of Epoch 22: 0.7499825330284553
Testing Acc of Epoch 22: 0.7502869565217392
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.4406e-01 (2.4406e-01)	Acc 0.748047 (0.748047)
Epoch: [23][300/616]	Loss 2.2723e-01 (2.3759e-01)	Acc 0.763672 (0.749838)
Epoch: [23][600/616]	Loss 2.4771e-01 (2.3692e-01)	Acc 0.737305 (0.750106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.752178)
Training Loss of Epoch 23: 0.23695815031121417
Training Acc of Epoch 23: 0.7500619283536586
Testing Acc of Epoch 23: 0.7521782608695652
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.4128e-01 (2.4128e-01)	Acc 0.731445 (0.731445)
Epoch: [24][300/616]	Loss 2.3548e-01 (2.3720e-01)	Acc 0.757812 (0.749422)
Epoch: [24][600/616]	Loss 2.3169e-01 (2.3725e-01)	Acc 0.759766 (0.749929)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.741822)
Training Loss of Epoch 24: 0.2373708484134054
Training Acc of Epoch 24: 0.7498046875
Testing Acc of Epoch 24: 0.7418217391304348
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.5575e-01 (2.5575e-01)	Acc 0.734375 (0.734375)
Epoch: [25][300/616]	Loss 2.3709e-01 (2.3738e-01)	Acc 0.753906 (0.749838)
Epoch: [25][600/616]	Loss 2.3543e-01 (2.3791e-01)	Acc 0.758789 (0.749256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749635)
Training Loss of Epoch 25: 0.23786972157838868
Training Acc of Epoch 25: 0.7492949695121951
Testing Acc of Epoch 25: 0.7496347826086956
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.5017e-01 (2.5017e-01)	Acc 0.735352 (0.735352)
Epoch: [26][300/616]	Loss 2.3306e-01 (2.3793e-01)	Acc 0.752930 (0.748676)
Epoch: [26][600/616]	Loss 2.2745e-01 (2.3783e-01)	Acc 0.764648 (0.748864)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749343)
Training Loss of Epoch 26: 0.23788864326670886
Training Acc of Epoch 26: 0.7487836636178862
Testing Acc of Epoch 26: 0.7493434782608696
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4322e-01 (2.4322e-01)	Acc 0.733398 (0.733398)
Epoch: [27][300/616]	Loss 2.1967e-01 (2.3751e-01)	Acc 0.771484 (0.749637)
Epoch: [27][600/616]	Loss 2.2868e-01 (2.3879e-01)	Acc 0.760742 (0.748468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.748739)
Training Loss of Epoch 27: 0.2388821675283153
Training Acc of Epoch 27: 0.7483517530487804
Testing Acc of Epoch 27: 0.7487391304347826
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.2517e-01 (2.2517e-01)	Acc 0.771484 (0.771484)
Epoch: [28][300/616]	Loss 2.2953e-01 (2.3859e-01)	Acc 0.765625 (0.747891)
Epoch: [28][600/616]	Loss 2.2648e-01 (2.3833e-01)	Acc 0.759766 (0.748429)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.752374)
Training Loss of Epoch 28: 0.2383144765123119
Training Acc of Epoch 28: 0.7484073297764228
Testing Acc of Epoch 28: 0.7523739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.3742e-01 (2.3742e-01)	Acc 0.751953 (0.751953)
Epoch: [29][300/616]	Loss 2.4987e-01 (2.3766e-01)	Acc 0.739258 (0.749085)
Epoch: [29][600/616]	Loss 2.4565e-01 (2.3797e-01)	Acc 0.752930 (0.748830)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750270)
Training Loss of Epoch 29: 0.23794711518093822
Training Acc of Epoch 29: 0.7488376524390243
Testing Acc of Epoch 29: 0.7502695652173913
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.2807e-01 (2.2807e-01)	Acc 0.758789 (0.758789)
Epoch: [30][300/616]	Loss 2.2720e-01 (2.3867e-01)	Acc 0.764648 (0.748053)
Epoch: [30][600/616]	Loss 2.2919e-01 (2.3916e-01)	Acc 0.764648 (0.747628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.748939)
Training Loss of Epoch 30: 0.2392017149343723
Training Acc of Epoch 30: 0.747583206300813
Testing Acc of Epoch 30: 0.7489391304347827
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3372e-01 (2.3372e-01)	Acc 0.754883 (0.754883)
Epoch: [31][300/616]	Loss 2.3958e-01 (2.3817e-01)	Acc 0.739258 (0.748293)
Epoch: [31][600/616]	Loss 2.3306e-01 (2.3871e-01)	Acc 0.746094 (0.748096)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.746561)
Training Loss of Epoch 31: 0.23882947726462914
Training Acc of Epoch 31: 0.7478626778455284
Testing Acc of Epoch 31: 0.7465608695652174
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.6003e-01 (2.6003e-01)	Acc 0.731445 (0.731445)
Epoch: [32][300/616]	Loss 2.2716e-01 (2.3903e-01)	Acc 0.753906 (0.747051)
Epoch: [32][600/616]	Loss 2.4641e-01 (2.3869e-01)	Acc 0.734375 (0.747867)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747257)
Training Loss of Epoch 32: 0.2386106065133723
Training Acc of Epoch 32: 0.7479865345528456
Testing Acc of Epoch 32: 0.7472565217391305
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4232e-01 (2.4232e-01)	Acc 0.738281 (0.738281)
Epoch: [33][300/616]	Loss 2.2714e-01 (2.3944e-01)	Acc 0.766602 (0.747090)
Epoch: [33][600/616]	Loss 2.4196e-01 (2.3895e-01)	Acc 0.748047 (0.747816)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.723183)
Training Loss of Epoch 33: 0.23910932763805234
Training Acc of Epoch 33: 0.7476165523373983
Testing Acc of Epoch 33: 0.7231826086956522
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.6655e-01 (2.6655e-01)	Acc 0.713867 (0.713867)
Epoch: [34][300/616]	Loss 2.3463e-01 (2.3903e-01)	Acc 0.761719 (0.747671)
Epoch: [34][600/616]	Loss 2.4163e-01 (2.3925e-01)	Acc 0.733398 (0.747639)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.746809)
Training Loss of Epoch 34: 0.23925505326530797
Training Acc of Epoch 34: 0.747730881605691
Testing Acc of Epoch 34: 0.7468086956521739
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.3215e-01 (2.3215e-01)	Acc 0.753906 (0.753906)
Epoch: [35][300/616]	Loss 2.4513e-01 (2.3943e-01)	Acc 0.738281 (0.746928)
Epoch: [35][600/616]	Loss 2.4458e-01 (2.3926e-01)	Acc 0.736328 (0.747377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750643)
Training Loss of Epoch 35: 0.2391961763302485
Training Acc of Epoch 35: 0.7474831681910569
Testing Acc of Epoch 35: 0.7506434782608695
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3709e-01 (2.3709e-01)	Acc 0.750977 (0.750977)
Epoch: [36][300/616]	Loss 2.3182e-01 (2.3881e-01)	Acc 0.745117 (0.747907)
Epoch: [36][600/616]	Loss 2.1878e-01 (2.3870e-01)	Acc 0.781250 (0.748070)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.742987)
Training Loss of Epoch 36: 0.2387040561534525
Training Acc of Epoch 36: 0.7480548145325203
Testing Acc of Epoch 36: 0.7429869565217392
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.6641e-01 (2.6641e-01)	Acc 0.724609 (0.724609)
Epoch: [37][300/616]	Loss 2.6048e-01 (2.4019e-01)	Acc 0.721680 (0.746483)
Epoch: [37][600/616]	Loss 2.3608e-01 (2.4024e-01)	Acc 0.753906 (0.746331)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744630)
Training Loss of Epoch 37: 0.24023796494414168
Training Acc of Epoch 37: 0.7464176829268293
Testing Acc of Epoch 37: 0.7446304347826087
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.5988e-01 (2.5988e-01)	Acc 0.741211 (0.741211)
Epoch: [38][300/616]	Loss 2.2700e-01 (2.3918e-01)	Acc 0.754883 (0.747732)
Epoch: [38][600/616]	Loss 2.4032e-01 (2.3909e-01)	Acc 0.744141 (0.747628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745752)
Training Loss of Epoch 38: 0.23900193608388667
Training Acc of Epoch 38: 0.7477134146341463
Testing Acc of Epoch 38: 0.7457521739130435
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.3810e-01 (2.3810e-01)	Acc 0.745117 (0.745117)
Epoch: [39][300/616]	Loss 2.5611e-01 (2.3874e-01)	Acc 0.719727 (0.747648)
Epoch: [39][600/616]	Loss 2.3855e-01 (2.3875e-01)	Acc 0.760742 (0.748001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.747357)
Training Loss of Epoch 39: 0.23876388884656796
Training Acc of Epoch 39: 0.747975419207317
Testing Acc of Epoch 39: 0.7473565217391305
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.3129e-01 (2.3129e-01)	Acc 0.753906 (0.753906)
Epoch: [40][300/616]	Loss 2.3499e-01 (2.3920e-01)	Acc 0.750977 (0.747216)
Epoch: [40][600/616]	Loss 2.6022e-01 (2.3887e-01)	Acc 0.721680 (0.747477)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.748835)
Training Loss of Epoch 40: 0.23899913955994737
Training Acc of Epoch 40: 0.747314850101626
Testing Acc of Epoch 40: 0.7488347826086956
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.3521e-01 (2.3521e-01)	Acc 0.750000 (0.750000)
Epoch: [41][300/616]	Loss 2.3615e-01 (2.3844e-01)	Acc 0.751953 (0.748235)
Epoch: [41][600/616]	Loss 2.4694e-01 (2.3950e-01)	Acc 0.734375 (0.747254)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750752)
Training Loss of Epoch 41: 0.2394504593639839
Training Acc of Epoch 41: 0.7473196138211382
Testing Acc of Epoch 41: 0.7507521739130435
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.3379e-01 (2.3379e-01)	Acc 0.742188 (0.742188)
Epoch: [42][300/616]	Loss 2.4179e-01 (2.4057e-01)	Acc 0.745117 (0.745730)
Epoch: [42][600/616]	Loss 2.3166e-01 (2.3923e-01)	Acc 0.764648 (0.747290)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748078)
Training Loss of Epoch 42: 0.2391691520931275
Training Acc of Epoch 42: 0.7473577235772357
Testing Acc of Epoch 42: 0.7480782608695652
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3163e-01 (2.3163e-01)	Acc 0.763672 (0.763672)
Epoch: [43][300/616]	Loss 2.3918e-01 (2.3941e-01)	Acc 0.740234 (0.746707)
Epoch: [43][600/616]	Loss 2.3520e-01 (2.3898e-01)	Acc 0.741211 (0.747286)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.750317)
Training Loss of Epoch 43: 0.23901190750482607
Training Acc of Epoch 43: 0.7472926194105691
Testing Acc of Epoch 43: 0.7503173913043478
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.1970e-01 (2.1970e-01)	Acc 0.780273 (0.780273)
Epoch: [44][300/616]	Loss 2.3895e-01 (2.3786e-01)	Acc 0.749023 (0.748868)
Epoch: [44][600/616]	Loss 2.7972e-01 (2.3892e-01)	Acc 0.705078 (0.747685)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747230)
Training Loss of Epoch 44: 0.23896594454602496
Training Acc of Epoch 44: 0.7475816183943089
Testing Acc of Epoch 44: 0.7472304347826086
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3929e-01 (2.3929e-01)	Acc 0.746094 (0.746094)
Epoch: [45][300/616]	Loss 2.5310e-01 (2.3909e-01)	Acc 0.714844 (0.747684)
Epoch: [45][600/616]	Loss 2.2545e-01 (2.4038e-01)	Acc 0.768555 (0.746227)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.745674)
Training Loss of Epoch 45: 0.24024520402516775
Training Acc of Epoch 45: 0.7463779852642276
Testing Acc of Epoch 45: 0.7456739130434783
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4041e-01 (2.4041e-01)	Acc 0.740234 (0.740234)
Epoch: [46][300/616]	Loss 2.4406e-01 (2.3966e-01)	Acc 0.739258 (0.746441)
Epoch: [46][600/616]	Loss 2.5695e-01 (2.3947e-01)	Acc 0.724609 (0.746957)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748239)
Training Loss of Epoch 46: 0.23953933635862862
Training Acc of Epoch 46: 0.7468543572154471
Testing Acc of Epoch 46: 0.7482391304347826
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4428e-01 (2.4428e-01)	Acc 0.733398 (0.733398)
Epoch: [47][300/616]	Loss 2.4651e-01 (2.3972e-01)	Acc 0.745117 (0.747317)
Epoch: [47][600/616]	Loss 2.3576e-01 (2.3956e-01)	Acc 0.774414 (0.747054)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.743109)
Training Loss of Epoch 47: 0.23960881538507414
Training Acc of Epoch 47: 0.7469194613821138
Testing Acc of Epoch 47: 0.743108695652174
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4484e-01 (2.4484e-01)	Acc 0.738281 (0.738281)
Epoch: [48][300/616]	Loss 2.3133e-01 (2.4035e-01)	Acc 0.752930 (0.746279)
Epoch: [48][600/616]	Loss 2.3550e-01 (2.4094e-01)	Acc 0.750977 (0.745556)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747122)
Training Loss of Epoch 48: 0.240935650758627
Training Acc of Epoch 48: 0.7456030868902439
Testing Acc of Epoch 48: 0.7471217391304348
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3181e-01 (2.3181e-01)	Acc 0.749023 (0.749023)
Epoch: [49][300/616]	Loss 2.5246e-01 (2.4072e-01)	Acc 0.734375 (0.745996)
Epoch: [49][600/616]	Loss 2.6231e-01 (2.3970e-01)	Acc 0.712891 (0.746976)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.743000)
Training Loss of Epoch 49: 0.2396432159877405
Training Acc of Epoch 49: 0.7471068343495935
Testing Acc of Epoch 49: 0.743
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4380e-01 (2.4380e-01)	Acc 0.744141 (0.744141)
Epoch: [50][300/616]	Loss 2.5004e-01 (2.4012e-01)	Acc 0.736328 (0.746314)
Epoch: [50][600/616]	Loss 2.4161e-01 (2.4059e-01)	Acc 0.746094 (0.745863)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747309)
Training Loss of Epoch 50: 0.24064021752617223
Training Acc of Epoch 50: 0.7457539380081301
Testing Acc of Epoch 50: 0.747308695652174
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3686e-01 (2.3686e-01)	Acc 0.757812 (0.757812)
Epoch: [51][300/616]	Loss 2.3494e-01 (2.3878e-01)	Acc 0.767578 (0.747855)
Epoch: [51][600/616]	Loss 2.1938e-01 (2.3949e-01)	Acc 0.772461 (0.747246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.746322)
Training Loss of Epoch 51: 0.23949072392490822
Training Acc of Epoch 51: 0.7472608612804879
Testing Acc of Epoch 51: 0.7463217391304348
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.3819e-01 (2.3819e-01)	Acc 0.750000 (0.750000)
Epoch: [52][300/616]	Loss 2.3080e-01 (2.3834e-01)	Acc 0.749023 (0.747693)
Epoch: [52][600/616]	Loss 2.3092e-01 (2.3919e-01)	Acc 0.751953 (0.747420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.740383)
Training Loss of Epoch 52: 0.2392336444156926
Training Acc of Epoch 52: 0.7473926575203252
Testing Acc of Epoch 52: 0.7403826086956522
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.4890e-01 (2.4890e-01)	Acc 0.743164 (0.743164)
Epoch: [53][300/616]	Loss 2.5284e-01 (2.3886e-01)	Acc 0.727539 (0.747258)
Epoch: [53][600/616]	Loss 2.3808e-01 (2.3893e-01)	Acc 0.750000 (0.747620)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748048)
Training Loss of Epoch 53: 0.2390545612185951
Training Acc of Epoch 53: 0.7474148882113821
Testing Acc of Epoch 53: 0.7480478260869565
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.4206e-01 (2.4206e-01)	Acc 0.756836 (0.756836)
Epoch: [54][300/616]	Loss 2.5750e-01 (2.3877e-01)	Acc 0.728516 (0.747706)
Epoch: [54][600/616]	Loss 2.4059e-01 (2.3875e-01)	Acc 0.744141 (0.747594)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.748261)
Training Loss of Epoch 54: 0.2387490467085102
Training Acc of Epoch 54: 0.747584794207317
Testing Acc of Epoch 54: 0.7482608695652174
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.2551e-01 (2.2551e-01)	Acc 0.766602 (0.766602)
Epoch: [55][300/616]	Loss 2.3319e-01 (2.4013e-01)	Acc 0.762695 (0.746700)
Epoch: [55][600/616]	Loss 2.3679e-01 (2.3900e-01)	Acc 0.744141 (0.747532)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750083)
Training Loss of Epoch 55: 0.2390828670282674
Training Acc of Epoch 55: 0.7475038109756098
Testing Acc of Epoch 55: 0.7500826086956521
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.2367e-01 (2.2367e-01)	Acc 0.757812 (0.757812)
Epoch: [56][300/616]	Loss 2.4215e-01 (2.4029e-01)	Acc 0.750000 (0.746824)
Epoch: [56][600/616]	Loss 2.4561e-01 (2.3946e-01)	Acc 0.740234 (0.747199)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747309)
Training Loss of Epoch 56: 0.23938261248716494
Training Acc of Epoch 56: 0.7473227896341463
Testing Acc of Epoch 56: 0.747308695652174
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5302e-01 (2.5302e-01)	Acc 0.733398 (0.733398)
Epoch: [57][300/616]	Loss 2.4641e-01 (2.3957e-01)	Acc 0.736328 (0.747271)
Epoch: [57][600/616]	Loss 2.3202e-01 (2.3971e-01)	Acc 0.759766 (0.746961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747504)
Training Loss of Epoch 57: 0.2397183188093387
Training Acc of Epoch 57: 0.7469464557926829
Testing Acc of Epoch 57: 0.747504347826087
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.4174e-01 (2.4174e-01)	Acc 0.731445 (0.731445)
Epoch: [58][300/616]	Loss 2.3492e-01 (2.3850e-01)	Acc 0.749023 (0.748216)
Epoch: [58][600/616]	Loss 2.4455e-01 (2.3929e-01)	Acc 0.744141 (0.747304)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.750570)
Training Loss of Epoch 58: 0.2393554241918936
Training Acc of Epoch 58: 0.7472021087398374
Testing Acc of Epoch 58: 0.7505695652173913
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.3613e-01 (2.3613e-01)	Acc 0.753906 (0.753906)
Epoch: [59][300/616]	Loss 2.3342e-01 (2.4003e-01)	Acc 0.753906 (0.746655)
Epoch: [59][600/616]	Loss 2.5011e-01 (2.3978e-01)	Acc 0.730469 (0.747148)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750913)
Training Loss of Epoch 59: 0.23979495884926338
Training Acc of Epoch 59: 0.7470671366869919
Testing Acc of Epoch 59: 0.7509130434782608
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3579e-01 (2.3579e-01)	Acc 0.760742 (0.760742)
Epoch: [60][300/616]	Loss 2.5289e-01 (2.3997e-01)	Acc 0.713867 (0.746256)
Epoch: [60][600/616]	Loss 2.3779e-01 (2.3972e-01)	Acc 0.762695 (0.746763)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.750970)
Training Loss of Epoch 60: 0.23964402246765973
Training Acc of Epoch 60: 0.746801956300813
Testing Acc of Epoch 60: 0.7509695652173913
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.2241e-01 (2.2241e-01)	Acc 0.766602 (0.766602)
Epoch: [61][300/616]	Loss 2.3957e-01 (2.4014e-01)	Acc 0.742188 (0.746434)
Epoch: [61][600/616]	Loss 2.2131e-01 (2.3929e-01)	Acc 0.769531 (0.747566)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.750630)
Training Loss of Epoch 61: 0.23922738457113746
Training Acc of Epoch 61: 0.7475959095528455
Testing Acc of Epoch 61: 0.7506304347826087
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4186e-01 (2.4186e-01)	Acc 0.743164 (0.743164)
Epoch: [62][300/616]	Loss 2.2937e-01 (2.3815e-01)	Acc 0.753906 (0.748157)
Epoch: [62][600/616]	Loss 2.5333e-01 (2.3883e-01)	Acc 0.723633 (0.747595)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.745052)
Training Loss of Epoch 62: 0.23884912506351627
Training Acc of Epoch 62: 0.7475974974593496
Testing Acc of Epoch 62: 0.7450521739130435
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4585e-01 (2.4585e-01)	Acc 0.745117 (0.745117)
Epoch: [63][300/616]	Loss 2.4497e-01 (2.4086e-01)	Acc 0.748047 (0.745623)
Epoch: [63][600/616]	Loss 2.4748e-01 (2.3947e-01)	Acc 0.725586 (0.747017)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752217)
Training Loss of Epoch 63: 0.23946073316946262
Training Acc of Epoch 63: 0.7470115599593496
Testing Acc of Epoch 63: 0.7522173913043478
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.2268e-01 (2.2268e-01)	Acc 0.772461 (0.772461)
Epoch: [64][300/616]	Loss 2.4614e-01 (2.3917e-01)	Acc 0.735352 (0.746885)
Epoch: [64][600/616]	Loss 2.3009e-01 (2.3924e-01)	Acc 0.750000 (0.747160)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.745143)
Training Loss of Epoch 64: 0.2393084394737957
Training Acc of Epoch 64: 0.7471322408536586
Testing Acc of Epoch 64: 0.7451434782608696
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.5003e-01 (2.5003e-01)	Acc 0.731445 (0.731445)
Epoch: [65][300/616]	Loss 2.2192e-01 (2.3978e-01)	Acc 0.775391 (0.747563)
Epoch: [65][600/616]	Loss 2.3854e-01 (2.4064e-01)	Acc 0.732422 (0.746454)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750804)
Training Loss of Epoch 65: 0.24060159216566784
Training Acc of Epoch 65: 0.7464430894308943
Testing Acc of Epoch 65: 0.750804347826087
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4654e-01 (2.4654e-01)	Acc 0.736328 (0.736328)
Epoch: [66][300/616]	Loss 2.3717e-01 (2.4073e-01)	Acc 0.739258 (0.745967)
Epoch: [66][600/616]	Loss 2.3705e-01 (2.4000e-01)	Acc 0.747070 (0.746685)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749465)
Training Loss of Epoch 66: 0.23994161467242048
Training Acc of Epoch 66: 0.7467908409552846
Testing Acc of Epoch 66: 0.7494652173913043
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.4987e-01 (2.4987e-01)	Acc 0.720703 (0.720703)
Epoch: [67][300/616]	Loss 2.3786e-01 (2.4098e-01)	Acc 0.745117 (0.745211)
Epoch: [67][600/616]	Loss 2.6430e-01 (2.4078e-01)	Acc 0.713867 (0.745753)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.749352)
Training Loss of Epoch 67: 0.24075054288879644
Training Acc of Epoch 67: 0.745776168699187
Testing Acc of Epoch 67: 0.7493521739130434
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3079e-01 (2.3079e-01)	Acc 0.763672 (0.763672)
Epoch: [68][300/616]	Loss 2.3851e-01 (2.4032e-01)	Acc 0.745117 (0.746882)
Epoch: [68][600/616]	Loss 2.3573e-01 (2.4082e-01)	Acc 0.750000 (0.745948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.747526)
Training Loss of Epoch 68: 0.24091415141171557
Training Acc of Epoch 68: 0.7458126905487805
Testing Acc of Epoch 68: 0.7475260869565218
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4824e-01 (2.4824e-01)	Acc 0.746094 (0.746094)
Epoch: [69][300/616]	Loss 2.3651e-01 (2.4125e-01)	Acc 0.745117 (0.745490)
Epoch: [69][600/616]	Loss 2.4566e-01 (2.4050e-01)	Acc 0.742188 (0.746341)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.746352)
Training Loss of Epoch 69: 0.24045323012320977
Training Acc of Epoch 69: 0.7464319740853659
Testing Acc of Epoch 69: 0.7463521739130434
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4322e-01 (2.4322e-01)	Acc 0.750000 (0.750000)
Epoch: [70][300/616]	Loss 2.4518e-01 (2.3994e-01)	Acc 0.744141 (0.746782)
Epoch: [70][600/616]	Loss 2.5020e-01 (2.3988e-01)	Acc 0.734375 (0.746687)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745026)
Training Loss of Epoch 70: 0.2397812332564253
Training Acc of Epoch 70: 0.7468130716463415
Testing Acc of Epoch 70: 0.7450260869565217
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5481e-01 (2.5481e-01)	Acc 0.719727 (0.719727)
Epoch: [71][300/616]	Loss 2.2481e-01 (2.4177e-01)	Acc 0.769531 (0.744864)
Epoch: [71][600/616]	Loss 2.4765e-01 (2.4081e-01)	Acc 0.748047 (0.745819)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751065)
Training Loss of Epoch 71: 0.24076871874371195
Training Acc of Epoch 71: 0.7459317835365854
Testing Acc of Epoch 71: 0.7510652173913044
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4097e-01 (2.4097e-01)	Acc 0.737305 (0.737305)
Epoch: [72][300/616]	Loss 2.4301e-01 (2.4084e-01)	Acc 0.732422 (0.746295)
Epoch: [72][600/616]	Loss 2.5844e-01 (2.4041e-01)	Acc 0.715820 (0.746029)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748200)
Training Loss of Epoch 72: 0.24041727856407322
Training Acc of Epoch 72: 0.7460842225609756
Testing Acc of Epoch 72: 0.7482
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.5842e-01 (2.5842e-01)	Acc 0.732422 (0.732422)
Epoch: [73][300/616]	Loss 2.2801e-01 (2.4104e-01)	Acc 0.760742 (0.745575)
Epoch: [73][600/616]	Loss 2.5196e-01 (2.4114e-01)	Acc 0.728516 (0.745561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.746678)
Training Loss of Epoch 73: 0.24107996479767124
Training Acc of Epoch 73: 0.7455713287601626
Testing Acc of Epoch 73: 0.7466782608695652
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3415e-01 (2.3415e-01)	Acc 0.760742 (0.760742)
Epoch: [74][300/616]	Loss 2.4497e-01 (2.3987e-01)	Acc 0.735352 (0.746279)
Epoch: [74][600/616]	Loss 2.7493e-01 (2.4108e-01)	Acc 0.700195 (0.745301)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741196)
Training Loss of Epoch 74: 0.24106866840909166
Training Acc of Epoch 74: 0.7453267911585366
Testing Acc of Epoch 74: 0.741195652173913
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.3992e-01 (2.3992e-01)	Acc 0.749023 (0.749023)
Epoch: [75][300/616]	Loss 2.1844e-01 (2.3516e-01)	Acc 0.786133 (0.750629)
Epoch: [75][600/616]	Loss 2.3956e-01 (2.3525e-01)	Acc 0.744141 (0.750630)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.751648)
Training Loss of Epoch 75: 0.23514663305709033
Training Acc of Epoch 75: 0.7507764862804878
Testing Acc of Epoch 75: 0.7516478260869566
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.1994e-01 (2.1994e-01)	Acc 0.772461 (0.772461)
Epoch: [76][300/616]	Loss 2.4965e-01 (2.3588e-01)	Acc 0.729492 (0.750438)
Epoch: [76][600/616]	Loss 2.2880e-01 (2.3563e-01)	Acc 0.763672 (0.750786)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753309)
Training Loss of Epoch 76: 0.23573496942597677
Training Acc of Epoch 76: 0.750584349593496
Testing Acc of Epoch 76: 0.753308695652174
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.3187e-01 (2.3187e-01)	Acc 0.744141 (0.744141)
Epoch: [77][300/616]	Loss 2.4367e-01 (2.3649e-01)	Acc 0.747070 (0.750078)
Epoch: [77][600/616]	Loss 2.2513e-01 (2.3570e-01)	Acc 0.762695 (0.750863)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.752826)
Training Loss of Epoch 77: 0.23572927337836444
Training Acc of Epoch 77: 0.750830475101626
Testing Acc of Epoch 77: 0.7528260869565218
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.2999e-01 (2.2999e-01)	Acc 0.762695 (0.762695)
Epoch: [78][300/616]	Loss 2.2759e-01 (2.3543e-01)	Acc 0.766602 (0.751093)
Epoch: [78][600/616]	Loss 2.3068e-01 (2.3600e-01)	Acc 0.752930 (0.750292)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751613)
Training Loss of Epoch 78: 0.2360597533423726
Training Acc of Epoch 78: 0.7502127794715447
Testing Acc of Epoch 78: 0.7516130434782609
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.4440e-01 (2.4440e-01)	Acc 0.747070 (0.747070)
Epoch: [79][300/616]	Loss 2.2801e-01 (2.3564e-01)	Acc 0.766602 (0.750678)
Epoch: [79][600/616]	Loss 2.3858e-01 (2.3604e-01)	Acc 0.741211 (0.750252)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.750730)
Training Loss of Epoch 79: 0.23604816617519875
Training Acc of Epoch 79: 0.750242949695122
Testing Acc of Epoch 79: 0.7507304347826087
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2226e-01 (2.2226e-01)	Acc 0.773438 (0.773438)
Epoch: [80][300/616]	Loss 2.4377e-01 (2.3651e-01)	Acc 0.753906 (0.749857)
Epoch: [80][600/616]	Loss 2.3815e-01 (2.3643e-01)	Acc 0.758789 (0.750133)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747370)
Training Loss of Epoch 80: 0.23657065995825016
Training Acc of Epoch 80: 0.7499364837398373
Testing Acc of Epoch 80: 0.7473695652173913
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.3461e-01 (2.3461e-01)	Acc 0.750000 (0.750000)
Epoch: [81][300/616]	Loss 2.2268e-01 (2.3662e-01)	Acc 0.754883 (0.749270)
Epoch: [81][600/616]	Loss 2.4342e-01 (2.3676e-01)	Acc 0.730469 (0.749480)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.748704)
Training Loss of Epoch 81: 0.23671481093255484
Training Acc of Epoch 81: 0.7496030233739838
Testing Acc of Epoch 81: 0.748704347826087
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.4042e-01 (2.4042e-01)	Acc 0.734375 (0.734375)
Epoch: [82][300/616]	Loss 2.2591e-01 (2.3785e-01)	Acc 0.768555 (0.748258)
Epoch: [82][600/616]	Loss 2.5043e-01 (2.3676e-01)	Acc 0.735352 (0.749659)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745474)
Training Loss of Epoch 82: 0.23671292003577318
Training Acc of Epoch 82: 0.749658600101626
Testing Acc of Epoch 82: 0.7454739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4731e-01 (2.4731e-01)	Acc 0.750977 (0.750977)
Epoch: [83][300/616]	Loss 2.4094e-01 (2.3686e-01)	Acc 0.732422 (0.749273)
Epoch: [83][600/616]	Loss 2.2049e-01 (2.3689e-01)	Acc 0.776367 (0.749318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.746448)
Training Loss of Epoch 83: 0.23695832127478064
Training Acc of Epoch 83: 0.7492854420731707
Testing Acc of Epoch 83: 0.7464478260869565
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3910e-01 (2.3910e-01)	Acc 0.741211 (0.741211)
Epoch: [84][300/616]	Loss 2.3982e-01 (2.3657e-01)	Acc 0.756836 (0.749267)
Epoch: [84][600/616]	Loss 2.4096e-01 (2.3671e-01)	Acc 0.738281 (0.749430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.752291)
Training Loss of Epoch 84: 0.2367265880350175
Training Acc of Epoch 84: 0.7494013592479675
Testing Acc of Epoch 84: 0.7522913043478261
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1927e-01 (2.1927e-01)	Acc 0.775391 (0.775391)
Epoch: [85][300/616]	Loss 2.4305e-01 (2.3711e-01)	Acc 0.747070 (0.749630)
Epoch: [85][600/616]	Loss 2.2943e-01 (2.3736e-01)	Acc 0.763672 (0.749192)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.749661)
Training Loss of Epoch 85: 0.23741101920604707
Training Acc of Epoch 85: 0.7491615853658536
Testing Acc of Epoch 85: 0.7496608695652174
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.4627e-01 (2.4627e-01)	Acc 0.744141 (0.744141)
Epoch: [86][300/616]	Loss 2.3471e-01 (2.3716e-01)	Acc 0.757812 (0.748780)
Epoch: [86][600/616]	Loss 2.3902e-01 (2.3715e-01)	Acc 0.726562 (0.749300)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749326)
Training Loss of Epoch 86: 0.23714310781257908
Training Acc of Epoch 86: 0.749217162093496
Testing Acc of Epoch 86: 0.7493260869565217
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.3885e-01 (2.3885e-01)	Acc 0.736328 (0.736328)
Epoch: [87][300/616]	Loss 2.2401e-01 (2.3732e-01)	Acc 0.766602 (0.748800)
Epoch: [87][600/616]	Loss 2.3863e-01 (2.3712e-01)	Acc 0.760742 (0.749374)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.750852)
Training Loss of Epoch 87: 0.23717292176514138
Training Acc of Epoch 87: 0.7492902057926829
Testing Acc of Epoch 87: 0.7508521739130435
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3666e-01 (2.3666e-01)	Acc 0.749023 (0.749023)
Epoch: [88][300/616]	Loss 2.4235e-01 (2.3720e-01)	Acc 0.737305 (0.749471)
Epoch: [88][600/616]	Loss 2.4678e-01 (2.3679e-01)	Acc 0.736328 (0.749646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751391)
Training Loss of Epoch 88: 0.2368069292568579
Training Acc of Epoch 88: 0.7495982596544716
Testing Acc of Epoch 88: 0.7513913043478261
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.3669e-01 (2.3669e-01)	Acc 0.757812 (0.757812)
Epoch: [89][300/616]	Loss 2.1738e-01 (2.3775e-01)	Acc 0.784180 (0.748637)
Epoch: [89][600/616]	Loss 2.1702e-01 (2.3725e-01)	Acc 0.780273 (0.748986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.751709)
Training Loss of Epoch 89: 0.2371687067960336
Training Acc of Epoch 89: 0.7491790523373983
Testing Acc of Epoch 89: 0.7517086956521739
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2852e-01 (2.2852e-01)	Acc 0.760742 (0.760742)
Epoch: [90][300/616]	Loss 2.4415e-01 (2.3667e-01)	Acc 0.738281 (0.749770)
Epoch: [90][600/616]	Loss 2.3634e-01 (2.3683e-01)	Acc 0.756836 (0.749448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751513)
Training Loss of Epoch 90: 0.23673193704306594
Training Acc of Epoch 90: 0.7496173145325203
Testing Acc of Epoch 90: 0.7515130434782609
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.4409e-01 (2.4409e-01)	Acc 0.745117 (0.745117)
Epoch: [91][300/616]	Loss 2.3877e-01 (2.3682e-01)	Acc 0.745117 (0.749990)
Epoch: [91][600/616]	Loss 2.4525e-01 (2.3653e-01)	Acc 0.740234 (0.750063)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750422)
Training Loss of Epoch 91: 0.23652239095389357
Training Acc of Epoch 91: 0.7500285823170731
Testing Acc of Epoch 91: 0.7504217391304348
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3177e-01 (2.3177e-01)	Acc 0.757812 (0.757812)
Epoch: [92][300/616]	Loss 2.4106e-01 (2.3628e-01)	Acc 0.756836 (0.749731)
Epoch: [92][600/616]	Loss 2.4673e-01 (2.3643e-01)	Acc 0.737305 (0.749617)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753617)
Training Loss of Epoch 92: 0.23638567299377627
Training Acc of Epoch 92: 0.7497014735772358
Testing Acc of Epoch 92: 0.7536173913043478
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.3305e-01 (2.3305e-01)	Acc 0.741211 (0.741211)
Epoch: [93][300/616]	Loss 2.3591e-01 (2.3601e-01)	Acc 0.752930 (0.750305)
Epoch: [93][600/616]	Loss 2.3121e-01 (2.3613e-01)	Acc 0.756836 (0.750123)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.749722)
Training Loss of Epoch 93: 0.23612358943233644
Training Acc of Epoch 93: 0.7501127413617886
Testing Acc of Epoch 93: 0.7497217391304348
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3646e-01 (2.3646e-01)	Acc 0.748047 (0.748047)
Epoch: [94][300/616]	Loss 2.4637e-01 (2.3656e-01)	Acc 0.738281 (0.749666)
Epoch: [94][600/616]	Loss 2.4613e-01 (2.3669e-01)	Acc 0.734375 (0.749766)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747813)
Training Loss of Epoch 94: 0.23670064121242462
Training Acc of Epoch 94: 0.7496776549796748
Testing Acc of Epoch 94: 0.7478130434782608
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.3586e-01 (2.3586e-01)	Acc 0.749023 (0.749023)
Epoch: [95][300/616]	Loss 2.3294e-01 (2.3635e-01)	Acc 0.751953 (0.749556)
Epoch: [95][600/616]	Loss 2.4935e-01 (2.3657e-01)	Acc 0.741211 (0.749526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742700)
Training Loss of Epoch 95: 0.23657514624963932
Training Acc of Epoch 95: 0.7495665015243902
Testing Acc of Epoch 95: 0.7427
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.5733e-01 (2.5733e-01)	Acc 0.724609 (0.724609)
Epoch: [96][300/616]	Loss 2.3294e-01 (2.3586e-01)	Acc 0.762695 (0.750993)
Epoch: [96][600/616]	Loss 2.2950e-01 (2.3618e-01)	Acc 0.773438 (0.750309)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754017)
Training Loss of Epoch 96: 0.23627773511215924
Training Acc of Epoch 96: 0.7501540269308943
Testing Acc of Epoch 96: 0.7540173913043479
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.1489e-01 (2.1489e-01)	Acc 0.776367 (0.776367)
Epoch: [97][300/616]	Loss 2.3886e-01 (2.3525e-01)	Acc 0.747070 (0.751187)
Epoch: [97][600/616]	Loss 2.1226e-01 (2.3600e-01)	Acc 0.771484 (0.750525)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.751222)
Training Loss of Epoch 97: 0.23594298900627508
Training Acc of Epoch 97: 0.7505208333333333
Testing Acc of Epoch 97: 0.7512217391304348
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.4204e-01 (2.4204e-01)	Acc 0.741211 (0.741211)
Epoch: [98][300/616]	Loss 2.2734e-01 (2.3664e-01)	Acc 0.756836 (0.749685)
Epoch: [98][600/616]	Loss 2.3093e-01 (2.3613e-01)	Acc 0.746094 (0.750201)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.750665)
Training Loss of Epoch 98: 0.23615924512467734
Training Acc of Epoch 98: 0.7502286585365854
Testing Acc of Epoch 98: 0.7506652173913043
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.4998e-01 (2.4998e-01)	Acc 0.732422 (0.732422)
Epoch: [99][300/616]	Loss 2.4033e-01 (2.3616e-01)	Acc 0.750977 (0.750169)
Epoch: [99][600/616]	Loss 2.1389e-01 (2.3646e-01)	Acc 0.788086 (0.749669)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747670)
Training Loss of Epoch 99: 0.23638999207717618
Training Acc of Epoch 99: 0.7498030995934959
Testing Acc of Epoch 99: 0.7476695652173913
Early stopping not satisfied.
