train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_3
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0155e-01 (5.0155e-01)	Acc 0.232422 (0.232422)
Epoch: [0][300/616]	Loss 3.0085e-01 (2.9509e-01)	Acc 0.697266 (0.681384)
Epoch: [0][600/616]	Loss 2.9625e-01 (2.8406e-01)	Acc 0.713867 (0.699311)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.726396)
Training Loss of Epoch 0: 0.28396108673840037
Training Acc of Epoch 0: 0.6997252921747967
Testing Acc of Epoch 0: 0.726395652173913
Model with the best training loss saved! The loss is 0.28396108673840037
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.7276e-01 (2.7276e-01)	Acc 0.714844 (0.714844)
Epoch: [1][300/616]	Loss 3.3480e-01 (2.7071e-01)	Acc 0.636719 (0.719276)
Epoch: [1][600/616]	Loss 2.5663e-01 (2.7113e-01)	Acc 0.732422 (0.718656)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.725630)
Training Loss of Epoch 1: 0.2708811196854444
Training Acc of Epoch 1: 0.7188802083333333
Testing Acc of Epoch 1: 0.7256304347826087
Model with the best training loss saved! The loss is 0.2708811196854444
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.7175e-01 (2.7175e-01)	Acc 0.716797 (0.716797)
Epoch: [2][300/616]	Loss 2.7328e-01 (2.7224e-01)	Acc 0.704102 (0.717271)
Epoch: [2][600/616]	Loss 2.8209e-01 (2.7172e-01)	Acc 0.697266 (0.716872)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.718296)
Training Loss of Epoch 2: 0.2716794575132975
Training Acc of Epoch 2: 0.7169874237804879
Testing Acc of Epoch 2: 0.718295652173913
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.8349e-01 (2.8349e-01)	Acc 0.709961 (0.709961)
Epoch: [3][300/616]	Loss 2.6588e-01 (2.6961e-01)	Acc 0.722656 (0.718812)
Epoch: [3][600/616]	Loss 2.5586e-01 (2.6908e-01)	Acc 0.728516 (0.719270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.729996)
Training Loss of Epoch 3: 0.26906837493908115
Training Acc of Epoch 3: 0.7192581300813008
Testing Acc of Epoch 3: 0.729995652173913
Model with the best training loss saved! The loss is 0.26906837493908115
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5874e-01 (2.5874e-01)	Acc 0.731445 (0.731445)
Epoch: [4][300/616]	Loss 2.9187e-01 (2.6849e-01)	Acc 0.673828 (0.719467)
Epoch: [4][600/616]	Loss 2.6931e-01 (2.6829e-01)	Acc 0.716797 (0.720089)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.727643)
Training Loss of Epoch 4: 0.268537398082454
Training Acc of Epoch 4: 0.7198218368902439
Testing Acc of Epoch 4: 0.7276434782608696
Model with the best training loss saved! The loss is 0.268537398082454
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.6835e-01 (2.6835e-01)	Acc 0.708984 (0.708984)
Epoch: [5][300/616]	Loss 2.5239e-01 (2.6814e-01)	Acc 0.745117 (0.720392)
Epoch: [5][600/616]	Loss 2.7060e-01 (2.6903e-01)	Acc 0.707031 (0.719164)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.714261)
Training Loss of Epoch 5: 0.2689984925878726
Training Acc of Epoch 5: 0.7192549542682927
Testing Acc of Epoch 5: 0.7142608695652174
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.8373e-01 (2.8373e-01)	Acc 0.711914 (0.711914)
Epoch: [6][300/616]	Loss 2.7774e-01 (2.7206e-01)	Acc 0.701172 (0.716080)
Epoch: [6][600/616]	Loss 2.5822e-01 (2.7104e-01)	Acc 0.732422 (0.717405)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.728626)
Training Loss of Epoch 6: 0.27071826952259714
Training Acc of Epoch 6: 0.7177162728658537
Testing Acc of Epoch 6: 0.7286260869565218
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.7333e-01 (2.7333e-01)	Acc 0.710938 (0.710938)
Epoch: [7][300/616]	Loss 2.6333e-01 (2.6975e-01)	Acc 0.715820 (0.718682)
Epoch: [7][600/616]	Loss 2.6040e-01 (2.7105e-01)	Acc 0.735352 (0.717570)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.720152)
Training Loss of Epoch 7: 0.2708761603609333
Training Acc of Epoch 7: 0.7177258003048781
Testing Acc of Epoch 7: 0.7201521739130434
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.8368e-01 (2.8368e-01)	Acc 0.700195 (0.700195)
Epoch: [8][300/616]	Loss 2.6019e-01 (2.7115e-01)	Acc 0.710938 (0.717264)
Epoch: [8][600/616]	Loss 2.6098e-01 (2.7080e-01)	Acc 0.724609 (0.717608)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.725161)
Training Loss of Epoch 8: 0.2706792281895149
Training Acc of Epoch 8: 0.7176781631097561
Testing Acc of Epoch 8: 0.7251608695652174
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4480e-01 (2.4480e-01)	Acc 0.742188 (0.742188)
Epoch: [9][300/616]	Loss 2.6885e-01 (2.6969e-01)	Acc 0.719727 (0.718004)
Epoch: [9][600/616]	Loss 2.7493e-01 (2.6979e-01)	Acc 0.708984 (0.718159)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.726622)
Training Loss of Epoch 9: 0.27000270929278397
Training Acc of Epoch 9: 0.7180878429878049
Testing Acc of Epoch 9: 0.7266217391304348
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.8852e-01 (2.8852e-01)	Acc 0.712891 (0.712891)
Epoch: [10][300/616]	Loss 2.7672e-01 (2.7060e-01)	Acc 0.710938 (0.717348)
Epoch: [10][600/616]	Loss 2.5734e-01 (2.7068e-01)	Acc 0.736328 (0.717575)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.725557)
Training Loss of Epoch 10: 0.27062207557321566
Training Acc of Epoch 10: 0.7177162728658537
Testing Acc of Epoch 10: 0.7255565217391304
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5958e-01 (2.5958e-01)	Acc 0.725586 (0.725586)
Epoch: [11][300/616]	Loss 2.6221e-01 (2.6950e-01)	Acc 0.723633 (0.719250)
Epoch: [11][600/616]	Loss 2.5516e-01 (2.6978e-01)	Acc 0.742188 (0.718646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.726100)
Training Loss of Epoch 11: 0.269656242539243
Training Acc of Epoch 11: 0.7187230055894309
Testing Acc of Epoch 11: 0.7261
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.7424e-01 (2.7424e-01)	Acc 0.707031 (0.707031)
Epoch: [12][300/616]	Loss 2.8036e-01 (2.7367e-01)	Acc 0.698242 (0.714879)
Epoch: [12][600/616]	Loss 2.7222e-01 (2.7231e-01)	Acc 0.723633 (0.715993)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.712596)
Training Loss of Epoch 12: 0.27229525231248963
Training Acc of Epoch 12: 0.7160172129065041
Testing Acc of Epoch 12: 0.7125956521739131
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.8470e-01 (2.8470e-01)	Acc 0.720703 (0.720703)
Epoch: [13][300/616]	Loss 2.6856e-01 (2.6743e-01)	Acc 0.715820 (0.721634)
Epoch: [13][600/616]	Loss 2.5915e-01 (2.6920e-01)	Acc 0.737305 (0.719484)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.715026)
Training Loss of Epoch 13: 0.26927787341722625
Training Acc of Epoch 13: 0.7193899263211382
Testing Acc of Epoch 13: 0.7150260869565217
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.5862e-01 (2.5862e-01)	Acc 0.729492 (0.729492)
Epoch: [14][300/616]	Loss 2.8910e-01 (2.7134e-01)	Acc 0.683594 (0.718121)
Epoch: [14][600/616]	Loss 2.9928e-01 (2.7184e-01)	Acc 0.693359 (0.717169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.728387)
Training Loss of Epoch 14: 0.27171328118661553
Training Acc of Epoch 14: 0.7174225101626016
Testing Acc of Epoch 14: 0.7283869565217391
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.5515e-01 (2.5515e-01)	Acc 0.725586 (0.725586)
Epoch: [15][300/616]	Loss 2.6296e-01 (2.7171e-01)	Acc 0.735352 (0.715999)
Epoch: [15][600/616]	Loss 2.7645e-01 (2.7217e-01)	Acc 0.710938 (0.716036)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.725943)
Training Loss of Epoch 15: 0.27230858921520107
Training Acc of Epoch 15: 0.7159346417682927
Testing Acc of Epoch 15: 0.7259434782608696
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.7941e-01 (2.7941e-01)	Acc 0.727539 (0.727539)
Epoch: [16][300/616]	Loss 2.6465e-01 (2.7156e-01)	Acc 0.721680 (0.717715)
Epoch: [16][600/616]	Loss 2.7718e-01 (2.7173e-01)	Acc 0.711914 (0.717271)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.730278)
Training Loss of Epoch 16: 0.2714142632435977
Training Acc of Epoch 16: 0.717579712906504
Testing Acc of Epoch 16: 0.7302782608695653
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.3641e-01 (2.3641e-01)	Acc 0.747070 (0.747070)
Epoch: [17][300/616]	Loss 2.5438e-01 (2.7240e-01)	Acc 0.738281 (0.716265)
Epoch: [17][600/616]	Loss 2.5870e-01 (2.7264e-01)	Acc 0.738281 (0.716157)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.721613)
Training Loss of Epoch 17: 0.272502501926771
Training Acc of Epoch 17: 0.7162077616869919
Testing Acc of Epoch 17: 0.7216130434782608
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6342e-01 (2.6342e-01)	Acc 0.711914 (0.711914)
Epoch: [18][300/616]	Loss 3.5230e-01 (2.7297e-01)	Acc 0.660156 (0.715168)
Epoch: [18][600/616]	Loss 2.6205e-01 (2.7290e-01)	Acc 0.727539 (0.715399)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.724091)
Training Loss of Epoch 18: 0.2728154787445456
Training Acc of Epoch 18: 0.7154328633130081
Testing Acc of Epoch 18: 0.7240913043478261
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.6044e-01 (2.6044e-01)	Acc 0.731445 (0.731445)
Epoch: [19][300/616]	Loss 2.6106e-01 (2.8129e-01)	Acc 0.723633 (0.704043)
Epoch: [19][600/616]	Loss 2.6456e-01 (2.7879e-01)	Acc 0.721680 (0.708406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.705509)
Training Loss of Epoch 19: 0.27870478705177465
Training Acc of Epoch 19: 0.7085492886178861
Testing Acc of Epoch 19: 0.7055086956521739
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.7064e-01 (2.7064e-01)	Acc 0.720703 (0.720703)
Epoch: [20][300/616]	Loss 2.6353e-01 (2.7365e-01)	Acc 0.720703 (0.713848)
Epoch: [20][600/616]	Loss 2.6654e-01 (2.7711e-01)	Acc 0.730469 (0.710336)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.719630)
Training Loss of Epoch 20: 0.2769688272621573
Training Acc of Epoch 20: 0.710643737296748
Testing Acc of Epoch 20: 0.7196304347826087
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.7811e-01 (2.7811e-01)	Acc 0.712891 (0.712891)
Epoch: [21][300/616]	Loss 3.0973e-01 (2.7802e-01)	Acc 0.666016 (0.709536)
Epoch: [21][600/616]	Loss 2.8312e-01 (2.8098e-01)	Acc 0.700195 (0.706828)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.722426)
Training Loss of Epoch 21: 0.2812352249777414
Training Acc of Epoch 21: 0.7065389989837398
Testing Acc of Epoch 21: 0.7224260869565218
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.7877e-01 (2.7877e-01)	Acc 0.719727 (0.719727)
Epoch: [22][300/616]	Loss 3.0613e-01 (2.7889e-01)	Acc 0.673828 (0.709296)
Epoch: [22][600/616]	Loss 4.2486e-01 (3.3734e-01)	Acc 0.436523 (0.596949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.328526 (0.325948)
Training Loss of Epoch 22: 0.3403941517438346
Training Acc of Epoch 22: 0.5909648119918699
Testing Acc of Epoch 22: 0.32594782608695655
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 4.8304e-01 (4.8304e-01)	Acc 0.333008 (0.333008)
Epoch: [23][300/616]	Loss 5.0041e-01 (4.5336e-01)	Acc 0.213867 (0.364598)
Epoch: [23][600/616]	Loss 5.0044e-01 (4.7656e-01)	Acc 0.197266 (0.287568)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 23: 0.4771014058977608
Training Acc of Epoch 23: 0.2856548526422764
Testing Acc of Epoch 23: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.197266)
Epoch: [24][300/616]	Loss 5.0040e-01 (5.0066e-01)	Acc 0.211914 (0.202703)
Epoch: [24][600/616]	Loss 5.0040e-01 (5.0053e-01)	Acc 0.190430 (0.202079)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 24: 0.5005306980474209
Training Acc of Epoch 24: 0.2021119156504065
Testing Acc of Epoch 24: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.199219)
Epoch: [25][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.201107)
Epoch: [25][600/616]	Loss 5.0040e-01 (5.0235e-01)	Acc 0.205078 (0.200936)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 25: 0.5023039406392632
Training Acc of Epoch 25: 0.20106389735772356
Testing Acc of Epoch 25: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.176758 (0.176758)
Epoch: [26][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.219727 (0.201331)
Epoch: [26][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.201352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 26: 0.500402450173851
Training Acc of Epoch 26: 0.2014529344512195
Testing Acc of Epoch 26: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.172852 (0.172852)
Epoch: [27][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.201821)
Epoch: [27][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201411)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 27: 0.5004024505615234
Training Acc of Epoch 27: 0.20144975863821138
Testing Acc of Epoch 27: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.226562 (0.226562)
Epoch: [28][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.202015)
Epoch: [28][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.185547 (0.201442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 28: 0.5004024502707691
Training Acc of Epoch 28: 0.20146404979674798
Testing Acc of Epoch 28: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.213867)
Epoch: [29][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.178711 (0.201104)
Epoch: [29][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.219727 (0.201310)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 29: 0.5004024502707691
Training Acc of Epoch 29: 0.2014529344512195
Testing Acc of Epoch 29: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.193359)
Epoch: [30][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.184570 (0.200993)
Epoch: [30][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 30: 0.5004024504646053
Training Acc of Epoch 30: 0.20144658282520325
Testing Acc of Epoch 30: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.197266)
Epoch: [31][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.212891 (0.202132)
Epoch: [31][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 31: 0.5004024502707691
Training Acc of Epoch 31: 0.20147357723577236
Testing Acc of Epoch 31: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [32][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.211914 (0.202324)
Epoch: [32][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201365)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 32: 0.5004024504646053
Training Acc of Epoch 32: 0.20146246189024392
Testing Acc of Epoch 32: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.195312)
Epoch: [33][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.202606)
Epoch: [33][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.211914 (0.201459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 33: 0.5004024502707691
Training Acc of Epoch 33: 0.2014084730691057
Testing Acc of Epoch 33: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [34][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.201350)
Epoch: [34][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.201443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 34: 0.5004024503676872
Training Acc of Epoch 34: 0.2014449949186992
Testing Acc of Epoch 34: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [35][300/616]	Loss 4.5887e-01 (5.0402e-01)	Acc 0.309570 (0.221949)
Epoch: [35][600/616]	Loss 4.5036e-01 (4.8098e-01)	Acc 0.347656 (0.271402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.322115 (0.326770)
Training Loss of Epoch 35: 0.48031475350139585
Training Acc of Epoch 35: 0.2729722433943089
Testing Acc of Epoch 35: 0.3267695652173913
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 4.5041e-01 (4.5041e-01)	Acc 0.318359 (0.318359)
Epoch: [36][300/616]	Loss 4.5159e-01 (4.5589e-01)	Acc 0.339844 (0.320488)
Epoch: [36][600/616]	Loss 4.5596e-01 (4.5618e-01)	Acc 0.322266 (0.320867)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.309295 (0.327165)
Training Loss of Epoch 36: 0.4560906011399215
Training Acc of Epoch 36: 0.3211128048780488
Testing Acc of Epoch 36: 0.32716521739130433
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 4.6316e-01 (4.6316e-01)	Acc 0.313477 (0.313477)
Epoch: [37][300/616]	Loss 4.5539e-01 (4.5677e-01)	Acc 0.333984 (0.320650)
Epoch: [37][600/616]	Loss 4.2853e-01 (4.5549e-01)	Acc 0.317383 (0.324198)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.451923 (0.462926)
Training Loss of Epoch 37: 0.4545797870411136
Training Acc of Epoch 37: 0.32602102388211385
Testing Acc of Epoch 37: 0.46292608695652177
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 4.1295e-01 (4.1295e-01)	Acc 0.468750 (0.468750)
Epoch: [38][300/616]	Loss 4.0095e-01 (4.0175e-01)	Acc 0.450195 (0.465278)
Epoch: [38][600/616]	Loss 4.0574e-01 (4.0184e-01)	Acc 0.463867 (0.466197)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.467949 (0.471017)
Training Loss of Epoch 38: 0.4017324633230039
Training Acc of Epoch 38: 0.46638084349593495
Testing Acc of Epoch 38: 0.47101739130434783
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.9577e-01 (3.9577e-01)	Acc 0.456055 (0.456055)
Epoch: [39][300/616]	Loss 3.9553e-01 (4.0389e-01)	Acc 0.475586 (0.466884)
Epoch: [39][600/616]	Loss 3.9277e-01 (4.0190e-01)	Acc 0.470703 (0.467886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.482372 (0.476096)
Training Loss of Epoch 39: 0.4019330543231189
Training Acc of Epoch 39: 0.46796875
Testing Acc of Epoch 39: 0.47609565217391303
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 4.0441e-01 (4.0441e-01)	Acc 0.484375 (0.484375)
Epoch: [40][300/616]	Loss 4.7761e-01 (4.9138e-01)	Acc 0.282227 (0.225249)
Epoch: [40][600/616]	Loss 3.8879e-01 (4.6679e-01)	Acc 0.468750 (0.295317)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.450321 (0.455322)
Training Loss of Epoch 40: 0.4652545615424955
Training Acc of Epoch 40: 0.29904408028455287
Testing Acc of Epoch 40: 0.4553217391304348
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 4.0654e-01 (4.0654e-01)	Acc 0.458984 (0.458984)
Epoch: [41][300/616]	Loss 3.9240e-01 (3.9950e-01)	Acc 0.457031 (0.454387)
Epoch: [41][600/616]	Loss 3.9634e-01 (4.0108e-01)	Acc 0.458984 (0.451212)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.458333 (0.454939)
Training Loss of Epoch 41: 0.4011121644237177
Training Acc of Epoch 41: 0.4512274517276423
Testing Acc of Epoch 41: 0.4549391304347826
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.9007e-01 (3.9007e-01)	Acc 0.482422 (0.482422)
Epoch: [42][300/616]	Loss 4.5496e-01 (4.0753e-01)	Acc 0.442383 (0.448683)
Epoch: [42][600/616]	Loss 4.2426e-01 (4.1471e-01)	Acc 0.362305 (0.431356)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.375000 (0.370765)
Training Loss of Epoch 42: 0.41500045850994144
Training Acc of Epoch 42: 0.42990821900406506
Testing Acc of Epoch 42: 0.37076521739130436
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 4.2847e-01 (4.2847e-01)	Acc 0.375000 (0.375000)
Epoch: [43][300/616]	Loss 4.6800e-01 (4.8600e-01)	Acc 0.265625 (0.258909)
Epoch: [43][600/616]	Loss 4.7972e-01 (4.8414e-01)	Acc 0.182617 (0.241993)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.224359 (0.249674)
Training Loss of Epoch 43: 0.48378941358589544
Training Acc of Epoch 43: 0.242065231199187
Testing Acc of Epoch 43: 0.24967391304347827
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 4.6600e-01 (4.6600e-01)	Acc 0.261719 (0.261719)
Epoch: [44][300/616]	Loss 4.5477e-01 (4.6731e-01)	Acc 0.271484 (0.281075)
Epoch: [44][600/616]	Loss 5.0094e-01 (4.7527e-01)	Acc 0.192383 (0.262066)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201200)
Training Loss of Epoch 44: 0.47584170001309095
Training Acc of Epoch 44: 0.2606246824186992
Testing Acc of Epoch 44: 0.2012
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 5.0097e-01 (5.0097e-01)	Acc 0.189453 (0.189453)
Epoch: [45][300/616]	Loss 5.0022e-01 (5.0061e-01)	Acc 0.185547 (0.201814)
Epoch: [45][600/616]	Loss 5.0048e-01 (5.0059e-01)	Acc 0.194336 (0.201534)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201183)
Training Loss of Epoch 45: 0.5005838191121574
Training Acc of Epoch 45: 0.20144975863821138
Testing Acc of Epoch 45: 0.2011826086956522
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 5.0043e-01 (5.0043e-01)	Acc 0.182617 (0.182617)
Epoch: [46][300/616]	Loss 5.0039e-01 (5.0041e-01)	Acc 0.183594 (0.200987)
Epoch: [46][600/616]	Loss 5.0037e-01 (5.0040e-01)	Acc 0.181641 (0.201349)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201213)
Training Loss of Epoch 46: 0.5004048628051107
Training Acc of Epoch 46: 0.20142276422764227
Testing Acc of Epoch 46: 0.20121304347826088
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 5.0042e-01 (5.0042e-01)	Acc 0.194336 (0.194336)
Epoch: [47][300/616]	Loss 5.0061e-01 (5.0045e-01)	Acc 0.193359 (0.201710)
Epoch: [47][600/616]	Loss 5.0045e-01 (5.0047e-01)	Acc 0.174805 (0.201468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201230)
Training Loss of Epoch 47: 0.5004708744646088
Training Acc of Epoch 47: 0.20143229166666668
Testing Acc of Epoch 47: 0.20123043478260869
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 5.0037e-01 (5.0037e-01)	Acc 0.211914 (0.211914)
Epoch: [48][300/616]	Loss 5.0056e-01 (5.0044e-01)	Acc 0.187500 (0.201337)
Epoch: [48][600/616]	Loss 5.0060e-01 (5.0043e-01)	Acc 0.199219 (0.201409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.190705 (0.197761)
Training Loss of Epoch 48: 0.5004655487169095
Training Acc of Epoch 48: 0.20140529725609757
Testing Acc of Epoch 48: 0.19776086956521738
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 5.1487e-01 (5.1487e-01)	Acc 0.167969 (0.167969)
Epoch: [49][300/616]	Loss 5.0077e-01 (5.0252e-01)	Acc 0.206055 (0.201006)
Epoch: [49][600/616]	Loss 5.0045e-01 (5.0192e-01)	Acc 0.208008 (0.200446)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201213)
Training Loss of Epoch 49: 0.5018862324516947
Training Acc of Epoch 49: 0.2005081300813008
Testing Acc of Epoch 49: 0.20121304347826088
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 5.0042e-01 (5.0042e-01)	Acc 0.206055 (0.206055)
Epoch: [50][300/616]	Loss 5.0053e-01 (5.0099e-01)	Acc 0.205078 (0.199861)
Epoch: [50][600/616]	Loss 4.7574e-01 (4.9759e-01)	Acc 0.276367 (0.208461)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.302885 (0.297870)
Training Loss of Epoch 50: 0.4970968336593814
Training Acc of Epoch 50: 0.21024358485772357
Testing Acc of Epoch 50: 0.29786956521739133
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 4.7761e-01 (4.7761e-01)	Acc 0.320312 (0.320312)
Epoch: [51][300/616]	Loss 4.7301e-01 (4.7476e-01)	Acc 0.317383 (0.299610)
Epoch: [51][600/616]	Loss 4.7280e-01 (4.7375e-01)	Acc 0.303711 (0.304371)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.314103 (0.312783)
Training Loss of Epoch 51: 0.47372771341626235
Training Acc of Epoch 51: 0.30439056148373983
Testing Acc of Epoch 51: 0.31278260869565216
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 4.7570e-01 (4.7570e-01)	Acc 0.285156 (0.285156)
Epoch: [52][300/616]	Loss 4.7795e-01 (4.7229e-01)	Acc 0.290039 (0.307880)
Epoch: [52][600/616]	Loss 5.0040e-01 (4.8350e-01)	Acc 0.200195 (0.265531)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 52: 0.48388217847521714
Training Acc of Epoch 52: 0.2641164888211382
Testing Acc of Epoch 52: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [53][300/616]	Loss 5.0040e-01 (5.0056e-01)	Acc 0.218750 (0.202389)
Epoch: [53][600/616]	Loss 5.0040e-01 (5.0056e-01)	Acc 0.208984 (0.202499)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 53: 0.5005582371862923
Training Acc of Epoch 53: 0.20245807926829268
Testing Acc of Epoch 53: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.202148)
Epoch: [54][300/616]	Loss 4.2849e-01 (4.6866e-01)	Acc 0.334961 (0.282820)
Epoch: [54][600/616]	Loss 3.9866e-01 (4.3868e-01)	Acc 0.453125 (0.346142)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.405449 (0.414800)
Training Loss of Epoch 54: 0.43782920032981937
Training Acc of Epoch 54: 0.3478388592479675
Testing Acc of Epoch 54: 0.4148
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 4.0526e-01 (4.0526e-01)	Acc 0.402344 (0.402344)
Epoch: [55][300/616]	Loss 3.9412e-01 (4.0040e-01)	Acc 0.435547 (0.433701)
Epoch: [55][600/616]	Loss 4.0418e-01 (4.0085e-01)	Acc 0.460938 (0.435895)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.448718 (0.445896)
Training Loss of Epoch 55: 0.40067288042083987
Training Acc of Epoch 55: 0.43593432418699185
Testing Acc of Epoch 55: 0.445895652173913
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 3.8685e-01 (3.8685e-01)	Acc 0.470703 (0.470703)
Epoch: [56][300/616]	Loss 4.2700e-01 (4.0945e-01)	Acc 0.464844 (0.441500)
Epoch: [56][600/616]	Loss 3.8376e-01 (4.0575e-01)	Acc 0.493164 (0.456295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.495192 (0.502117)
Training Loss of Epoch 56: 0.4049012030527844
Training Acc of Epoch 56: 0.4571122332317073
Testing Acc of Epoch 56: 0.5021173913043478
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 3.6222e-01 (3.6222e-01)	Acc 0.508789 (0.508789)
Epoch: [57][300/616]	Loss 3.7120e-01 (3.8025e-01)	Acc 0.493164 (0.495627)
Epoch: [57][600/616]	Loss 3.3058e-01 (3.7167e-01)	Acc 0.628906 (0.522646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.605769 (0.590309)
Training Loss of Epoch 57: 0.3707283885498357
Training Acc of Epoch 57: 0.5254779598577236
Testing Acc of Epoch 57: 0.5903086956521739
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 3.5374e-01 (3.5374e-01)	Acc 0.600586 (0.600586)
Epoch: [58][300/616]	Loss 2.8769e-01 (2.9690e-01)	Acc 0.719727 (0.698038)
Epoch: [58][600/616]	Loss 2.8966e-01 (2.9471e-01)	Acc 0.697266 (0.701654)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.718852)
Training Loss of Epoch 58: 0.2943625647362655
Training Acc of Epoch 58: 0.7017863948170732
Testing Acc of Epoch 58: 0.7188521739130435
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.6992e-01 (2.6992e-01)	Acc 0.716797 (0.716797)
Epoch: [59][300/616]	Loss 2.8762e-01 (2.8396e-01)	Acc 0.708008 (0.708530)
Epoch: [59][600/616]	Loss 2.6299e-01 (2.8289e-01)	Acc 0.733398 (0.709998)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.723391)
Training Loss of Epoch 59: 0.2826789717848708
Training Acc of Epoch 59: 0.7102419969512195
Testing Acc of Epoch 59: 0.7233913043478261
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.8992e-01 (2.8992e-01)	Acc 0.704102 (0.704102)
Epoch: [60][300/616]	Loss 2.6901e-01 (2.8077e-01)	Acc 0.731445 (0.711479)
Epoch: [60][600/616]	Loss 2.7803e-01 (2.7970e-01)	Acc 0.724609 (0.713414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.724783)
Training Loss of Epoch 60: 0.27955562096785724
Training Acc of Epoch 60: 0.7135877159552846
Testing Acc of Epoch 60: 0.7247826086956521
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.7090e-01 (2.7090e-01)	Acc 0.716797 (0.716797)
Epoch: [61][300/616]	Loss 3.0505e-01 (2.7976e-01)	Acc 0.680664 (0.713400)
Epoch: [61][600/616]	Loss 2.7983e-01 (2.7915e-01)	Acc 0.708008 (0.713783)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.641026 (0.677330)
Training Loss of Epoch 61: 0.2789985418077407
Training Acc of Epoch 61: 0.7139910442073171
Testing Acc of Epoch 61: 0.6773304347826087
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.1093e-01 (3.1093e-01)	Acc 0.685547 (0.685547)
Epoch: [62][300/616]	Loss 2.9439e-01 (2.7736e-01)	Acc 0.704102 (0.714996)
Epoch: [62][600/616]	Loss 5.0043e-01 (3.2634e-01)	Acc 0.192383 (0.611604)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201557)
Training Loss of Epoch 62: 0.3302997571423771
Training Acc of Epoch 62: 0.6022802337398374
Testing Acc of Epoch 62: 0.20155652173913044
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.190430)
Epoch: [63][300/616]	Loss 4.8194e-01 (4.9404e-01)	Acc 0.352539 (0.237840)
Epoch: [63][600/616]	Loss 4.9985e-01 (4.7811e-01)	Acc 0.218750 (0.286888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.213141 (0.214004)
Training Loss of Epoch 63: 0.47860796684172097
Training Acc of Epoch 63: 0.2849736407520325
Testing Acc of Epoch 63: 0.21400434782608696
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 4.9860e-01 (4.9860e-01)	Acc 0.215820 (0.215820)
Epoch: [64][300/616]	Loss 5.0243e-01 (4.8021e-01)	Acc 0.198242 (0.282755)
Epoch: [64][600/616]	Loss 4.5513e-01 (4.6876e-01)	Acc 0.325195 (0.318540)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.338141 (0.340243)
Training Loss of Epoch 64: 0.46850524733706217
Training Acc of Epoch 64: 0.3191962017276423
Testing Acc of Epoch 64: 0.3402434782608696
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 4.6040e-01 (4.6040e-01)	Acc 0.329102 (0.329102)
Epoch: [65][300/616]	Loss 4.8626e-01 (4.6997e-01)	Acc 0.327148 (0.318168)
Epoch: [65][600/616]	Loss 4.6183e-01 (4.6849e-01)	Acc 0.332031 (0.326248)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.339744 (0.339352)
Training Loss of Epoch 65: 0.4682109085040364
Training Acc of Epoch 65: 0.3265577362804878
Testing Acc of Epoch 65: 0.33935217391304345
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 4.6136e-01 (4.6136e-01)	Acc 0.340820 (0.340820)
Epoch: [66][300/616]	Loss 4.3217e-01 (4.4369e-01)	Acc 0.347656 (0.371924)
Epoch: [66][600/616]	Loss 4.7273e-01 (4.4310e-01)	Acc 0.292969 (0.385789)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.282051 (0.301522)
Training Loss of Epoch 66: 0.44341702422475426
Training Acc of Epoch 66: 0.3838652820121951
Testing Acc of Epoch 66: 0.3015217391304348
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 4.5642e-01 (4.5642e-01)	Acc 0.298828 (0.298828)
Epoch: [67][300/616]	Loss 3.2734e+01 (2.0965e+00)	Acc 0.181641 (0.373105)
Epoch: [67][600/616]	Loss 3.2852e+01 (1.7155e+01)	Acc 0.178711 (0.283416)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 67: 17.497550608035993
Training Acc of Epoch 67: 0.28139132367886177
Testing Acc of Epoch 67: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [68][300/616]	Loss 3.2266e+01 (3.2277e+01)	Acc 0.193359 (0.193071)
Epoch: [68][600/616]	Loss 3.2500e+01 (3.2253e+01)	Acc 0.187500 (0.193668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 68: 32.24822154471545
Training Acc of Epoch 68: 0.19379446138211381
Testing Acc of Epoch 68: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [69][300/616]	Loss 3.2695e+01 (3.2273e+01)	Acc 0.182617 (0.193184)
Epoch: [69][600/616]	Loss 3.2500e+01 (3.2245e+01)	Acc 0.187500 (0.193879)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 69: 32.24733231707317
Training Acc of Epoch 69: 0.19381669207317073
Testing Acc of Epoch 69: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [70][300/616]	Loss 3.2031e+01 (3.2290e+01)	Acc 0.199219 (0.192759)
Epoch: [70][600/616]	Loss 3.2383e+01 (3.2253e+01)	Acc 0.190430 (0.193665)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 70: 32.247649898373986
Training Acc of Epoch 70: 0.19380875254065041
Testing Acc of Epoch 70: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [71][300/616]	Loss 3.1836e+01 (3.2267e+01)	Acc 0.204102 (0.193324)
Epoch: [71][600/616]	Loss 3.2734e+01 (3.2249e+01)	Acc 0.181641 (0.193783)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 71: 32.247903963414636
Training Acc of Epoch 71: 0.19380240091463416
Testing Acc of Epoch 71: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [72][300/616]	Loss 3.2461e+01 (3.2252e+01)	Acc 0.188477 (0.193700)
Epoch: [72][600/616]	Loss 3.2539e+01 (3.2245e+01)	Acc 0.186523 (0.193863)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 72: 32.24707825203252
Training Acc of Epoch 72: 0.193823043699187
Testing Acc of Epoch 72: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [73][300/616]	Loss 3.1367e+01 (3.2239e+01)	Acc 0.215820 (0.194028)
Epoch: [73][600/616]	Loss 3.2422e+01 (3.2241e+01)	Acc 0.189453 (0.193972)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 73: 32.24719324809749
Training Acc of Epoch 73: 0.19381986788617886
Testing Acc of Epoch 73: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.2695e+01 (3.2695e+01)	Acc 0.182617 (0.182617)
Epoch: [74][300/616]	Loss 3.2773e+01 (3.2220e+01)	Acc 0.180664 (0.194511)
Epoch: [74][600/616]	Loss 3.1758e+01 (3.2251e+01)	Acc 0.206055 (0.193735)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 74: 32.24820670151129
Training Acc of Epoch 74: 0.19379446138211381
Testing Acc of Epoch 74: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [75][300/616]	Loss 3.2266e+01 (3.2236e+01)	Acc 0.193359 (0.194109)
Epoch: [75][600/616]	Loss 3.2656e+01 (3.2245e+01)	Acc 0.183594 (0.193873)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 75: 32.24756905470437
Training Acc of Epoch 75: 0.19381034044715448
Testing Acc of Epoch 75: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.3320e+01 (3.3320e+01)	Acc 0.166992 (0.166992)
Epoch: [76][300/616]	Loss 3.2617e+01 (3.2252e+01)	Acc 0.184570 (0.193700)
Epoch: [76][600/616]	Loss 3.3086e+01 (3.2244e+01)	Acc 0.172852 (0.193896)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 76: 32.24756778003724
Training Acc of Epoch 76: 0.19381034044715448
Testing Acc of Epoch 76: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [77][300/616]	Loss 3.1875e+01 (3.2259e+01)	Acc 0.203125 (0.193525)
Epoch: [77][600/616]	Loss 3.2148e+01 (3.2250e+01)	Acc 0.196289 (0.193756)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 77: 32.248264461610376
Training Acc of Epoch 77: 0.19379287347560975
Testing Acc of Epoch 77: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [78][300/616]	Loss 3.2422e+01 (3.2225e+01)	Acc 0.189453 (0.194372)
Epoch: [78][600/616]	Loss 3.2031e+01 (3.2242e+01)	Acc 0.199219 (0.193944)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 78: 32.247248288286414
Training Acc of Epoch 78: 0.1938182799796748
Testing Acc of Epoch 78: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [79][300/616]	Loss 3.2070e+01 (3.2233e+01)	Acc 0.198242 (0.194183)
Epoch: [79][600/616]	Loss 3.2109e+01 (3.2250e+01)	Acc 0.197266 (0.193757)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 79: 32.24754295659259
Training Acc of Epoch 79: 0.19381034044715448
Testing Acc of Epoch 79: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.2695e+01 (3.2695e+01)	Acc 0.182617 (0.182617)
Epoch: [80][300/616]	Loss 3.2656e+01 (3.2269e+01)	Acc 0.183594 (0.193285)
Epoch: [80][600/616]	Loss 3.2109e+01 (3.2247e+01)	Acc 0.197266 (0.193808)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 80: 32.24737065943276
Training Acc of Epoch 80: 0.19381034044715448
Testing Acc of Epoch 80: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [81][300/616]	Loss 3.2383e+01 (3.2251e+01)	Acc 0.190430 (0.193681)
Epoch: [81][600/616]	Loss 3.2100e+01 (3.2248e+01)	Acc 0.197266 (0.193728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194643)
Training Loss of Epoch 81: 32.24478703940787
Training Acc of Epoch 81: 0.19381510416666667
Testing Acc of Epoch 81: 0.19464347826086956
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [82][300/616]	Loss 3.3164e+01 (3.2200e+01)	Acc 0.170898 (0.194900)
Epoch: [82][600/616]	Loss 3.1914e+01 (3.2237e+01)	Acc 0.202148 (0.193969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 82: 32.24308429500921
Training Acc of Epoch 82: 0.19380875254065041
Testing Acc of Epoch 82: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [83][300/616]	Loss 3.1992e+01 (3.2268e+01)	Acc 0.200195 (0.193307)
Epoch: [83][600/616]	Loss 3.2535e+01 (3.2251e+01)	Acc 0.186523 (0.193660)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 83: 32.24437876755629
Training Acc of Epoch 83: 0.19382463160569105
Testing Acc of Epoch 83: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 3.2637e+01 (3.2637e+01)	Acc 0.183594 (0.183594)
Epoch: [84][300/616]	Loss 3.1546e+01 (3.1901e+01)	Acc 0.210938 (0.196510)
Epoch: [84][600/616]	Loss 3.2278e+01 (3.1868e+01)	Acc 0.191406 (0.199555)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201630)
Training Loss of Epoch 84: 31.87087888950255
Training Acc of Epoch 84: 0.19956173780487804
Testing Acc of Epoch 84: 0.2016304347826087
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 3.2172e+01 (3.2172e+01)	Acc 0.195312 (0.195312)
Epoch: [85][300/616]	Loss 3.2221e+01 (3.1836e+01)	Acc 0.194336 (0.202960)
Epoch: [85][600/616]	Loss 3.2309e+01 (3.1860e+01)	Acc 0.191406 (0.202451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201648)
Training Loss of Epoch 85: 31.863916232721593
Training Acc of Epoch 85: 0.20236439278455284
Testing Acc of Epoch 85: 0.20164782608695653
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 3.1353e+01 (3.1353e+01)	Acc 0.215820 (0.215820)
Epoch: [86][300/616]	Loss 3.1814e+01 (3.1885e+01)	Acc 0.204102 (0.202006)
Epoch: [86][600/616]	Loss 3.1664e+01 (3.1807e+01)	Acc 0.202148 (0.202196)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201457)
Training Loss of Epoch 86: 31.805188965215915
Training Acc of Epoch 86: 0.20220877794715447
Testing Acc of Epoch 86: 0.20145652173913042
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 3.2080e+01 (3.2080e+01)	Acc 0.194336 (0.194336)
Epoch: [87][300/616]	Loss 3.2659e+01 (3.1745e+01)	Acc 0.181641 (0.201720)
Epoch: [87][600/616]	Loss 3.1348e+01 (3.1722e+01)	Acc 0.210938 (0.201970)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201270)
Training Loss of Epoch 87: 31.72099438488968
Training Acc of Epoch 87: 0.202002350101626
Testing Acc of Epoch 87: 0.2012695652173913
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 3.2100e+01 (3.2100e+01)	Acc 0.192383 (0.192383)
Epoch: [88][300/616]	Loss 3.1629e+01 (3.1754e+01)	Acc 0.204102 (0.201036)
Epoch: [88][600/616]	Loss 3.1930e+01 (3.1731e+01)	Acc 0.199219 (0.201947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201248)
Training Loss of Epoch 88: 31.72933802100701
Training Acc of Epoch 88: 0.20200870172764226
Testing Acc of Epoch 88: 0.20124782608695652
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.1755e+01 (3.1755e+01)	Acc 0.204102 (0.204102)
Epoch: [89][300/616]	Loss 3.0714e+01 (3.1754e+01)	Acc 0.227539 (0.201581)
Epoch: [89][600/616]	Loss 3.0463e+01 (3.1179e+01)	Acc 0.199219 (0.201589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.189103 (0.199409)
Training Loss of Epoch 89: 31.167515849291792
Training Acc of Epoch 89: 0.2015355055894309
Testing Acc of Epoch 89: 0.19940869565217392
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.9867e+01 (2.9867e+01)	Acc 0.225586 (0.225586)
Epoch: [90][300/616]	Loss 3.1836e+01 (3.1630e+01)	Acc 0.204102 (0.201444)
Epoch: [90][600/616]	Loss 3.2031e+01 (3.1763e+01)	Acc 0.199219 (0.202012)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 90: 31.76396370554358
Training Acc of Epoch 90: 0.20208650914634146
Testing Acc of Epoch 90: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [91][300/616]	Loss 3.1328e+01 (3.1940e+01)	Acc 0.216797 (0.201509)
Epoch: [91][600/616]	Loss 3.2148e+01 (3.1904e+01)	Acc 0.196289 (0.202408)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 91: 31.903963414634145
Training Acc of Epoch 91: 0.20240091463414633
Testing Acc of Epoch 91: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 3.1602e+01 (3.1602e+01)	Acc 0.209961 (0.209961)
Epoch: [92][300/616]	Loss 3.1367e+01 (3.1913e+01)	Acc 0.215820 (0.202181)
Epoch: [92][600/616]	Loss 3.2656e+01 (3.1902e+01)	Acc 0.183594 (0.202444)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 92: 31.903264735772357
Training Acc of Epoch 92: 0.20241838160569106
Testing Acc of Epoch 92: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [93][300/616]	Loss 3.3008e+01 (3.1916e+01)	Acc 0.174805 (0.202090)
Epoch: [93][600/616]	Loss 3.1523e+01 (3.1908e+01)	Acc 0.211914 (0.202295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 93: 31.90383638211382
Training Acc of Epoch 93: 0.20240409044715446
Testing Acc of Epoch 93: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [94][300/616]	Loss 3.2227e+01 (3.1915e+01)	Acc 0.194336 (0.202135)
Epoch: [94][600/616]	Loss 3.2266e+01 (3.1905e+01)	Acc 0.193359 (0.202363)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 94: 31.903455284552845
Training Acc of Epoch 94: 0.20241361788617887
Testing Acc of Epoch 94: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [95][300/616]	Loss 3.2344e+01 (3.1926e+01)	Acc 0.191406 (0.201860)
Epoch: [95][600/616]	Loss 3.2500e+01 (3.1904e+01)	Acc 0.187500 (0.202392)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 95: 31.903518800813007
Training Acc of Epoch 95: 0.2024120299796748
Testing Acc of Epoch 95: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [96][300/616]	Loss 3.1523e+01 (3.1892e+01)	Acc 0.211914 (0.202706)
Epoch: [96][600/616]	Loss 3.1016e+01 (3.1901e+01)	Acc 0.224609 (0.202464)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 96: 31.904026930894307
Training Acc of Epoch 96: 0.20239932672764227
Testing Acc of Epoch 96: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [97][300/616]	Loss 3.2227e+01 (3.1905e+01)	Acc 0.194336 (0.202372)
Epoch: [97][600/616]	Loss 3.1484e+01 (3.1904e+01)	Acc 0.212891 (0.202405)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 97: 31.903455284552845
Training Acc of Epoch 97: 0.20241361788617887
Testing Acc of Epoch 97: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [98][300/616]	Loss 3.2070e+01 (3.1879e+01)	Acc 0.198242 (0.203018)
Epoch: [98][600/616]	Loss 3.1641e+01 (3.1906e+01)	Acc 0.208984 (0.202347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 98: 31.903709349593495
Training Acc of Epoch 98: 0.2024072662601626
Testing Acc of Epoch 98: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.0781e+01 (3.0781e+01)	Acc 0.230469 (0.230469)
Epoch: [99][300/616]	Loss 3.2344e+01 (3.1924e+01)	Acc 0.191406 (0.201912)
Epoch: [99][600/616]	Loss 3.1250e+01 (3.1909e+01)	Acc 0.218750 (0.202270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 99: 31.903899898373982
Training Acc of Epoch 99: 0.2024025025406504
Testing Acc of Epoch 99: 0.20167826086956522
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_3
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0182e-01 (5.0182e-01)	Acc 0.060547 (0.060547)
Epoch: [0][300/616]	Loss 2.6788e-01 (3.0576e-01)	Acc 0.730469 (0.666888)
Epoch: [0][600/616]	Loss 2.5554e-01 (2.8901e-01)	Acc 0.729492 (0.691689)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.666667 (0.666665)
Training Loss of Epoch 0: 0.28852887764209656
Training Acc of Epoch 0: 0.6923891641260163
Testing Acc of Epoch 0: 0.6666652173913044
Model with the best training loss saved! The loss is 0.28852887764209656
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 3.2012e-01 (3.2012e-01)	Acc 0.665039 (0.665039)
Epoch: [1][300/616]	Loss 2.6732e-01 (2.7297e-01)	Acc 0.726562 (0.715869)
Epoch: [1][600/616]	Loss 2.7291e-01 (2.7095e-01)	Acc 0.726562 (0.718882)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.653846 (0.663196)
Training Loss of Epoch 1: 0.27085802060801806
Training Acc of Epoch 1: 0.7188944994918699
Testing Acc of Epoch 1: 0.6631956521739131
Model with the best training loss saved! The loss is 0.27085802060801806
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 3.0128e-01 (3.0128e-01)	Acc 0.678711 (0.678711)
Epoch: [2][300/616]	Loss 2.7101e-01 (2.7127e-01)	Acc 0.708984 (0.719136)
Epoch: [2][600/616]	Loss 2.6778e-01 (2.7403e-01)	Acc 0.724609 (0.716374)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.712883)
Training Loss of Epoch 2: 0.27401904829633916
Training Acc of Epoch 2: 0.7164507113821138
Testing Acc of Epoch 2: 0.7128826086956521
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.6797e-01 (2.6797e-01)	Acc 0.716797 (0.716797)
Epoch: [3][300/616]	Loss 2.7238e-01 (2.7230e-01)	Acc 0.714844 (0.717228)
Epoch: [3][600/616]	Loss 2.8429e-01 (2.7277e-01)	Acc 0.706055 (0.717097)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.729139)
Training Loss of Epoch 3: 0.27257803329607333
Training Acc of Epoch 3: 0.7174113948170732
Testing Acc of Epoch 3: 0.7291391304347826
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4354e-01 (2.4354e-01)	Acc 0.751953 (0.751953)
Epoch: [4][300/616]	Loss 2.5414e-01 (2.7369e-01)	Acc 0.733398 (0.716265)
Epoch: [4][600/616]	Loss 2.6137e-01 (2.7316e-01)	Acc 0.726562 (0.716829)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.722865)
Training Loss of Epoch 4: 0.27308319819167376
Training Acc of Epoch 4: 0.7168969131097561
Testing Acc of Epoch 4: 0.7228652173913044
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.7859e-01 (2.7859e-01)	Acc 0.706055 (0.706055)
Epoch: [5][300/616]	Loss 3.1470e-01 (2.7358e-01)	Acc 0.672852 (0.717102)
Epoch: [5][600/616]	Loss 2.8539e-01 (2.7466e-01)	Acc 0.704102 (0.715957)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.714913)
Training Loss of Epoch 5: 0.2751510506722985
Training Acc of Epoch 5: 0.7153868140243902
Testing Acc of Epoch 5: 0.7149130434782609
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 3.0112e-01 (3.0112e-01)	Acc 0.688477 (0.688477)
Epoch: [6][300/616]	Loss 2.6871e-01 (2.7359e-01)	Acc 0.733398 (0.717929)
Epoch: [6][600/616]	Loss 2.5928e-01 (2.7572e-01)	Acc 0.731445 (0.714969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.718917)
Training Loss of Epoch 6: 0.2755842907399666
Training Acc of Epoch 6: 0.7150485899390244
Testing Acc of Epoch 6: 0.7189173913043478
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4834e-01 (2.4834e-01)	Acc 0.731445 (0.731445)
Epoch: [7][300/616]	Loss 2.7548e-01 (2.7644e-01)	Acc 0.708984 (0.713416)
Epoch: [7][600/616]	Loss 2.6819e-01 (2.7807e-01)	Acc 0.724609 (0.712306)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.722904)
Training Loss of Epoch 7: 0.2780708446735289
Training Acc of Epoch 7: 0.7122364075203252
Testing Acc of Epoch 7: 0.7229043478260869
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.6948e-01 (2.6948e-01)	Acc 0.730469 (0.730469)
Epoch: [8][300/616]	Loss 2.7536e-01 (2.7798e-01)	Acc 0.725586 (0.712537)
Epoch: [8][600/616]	Loss 2.7251e-01 (2.7866e-01)	Acc 0.727539 (0.711402)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.718391)
Training Loss of Epoch 8: 0.27837720888901535
Training Acc of Epoch 8: 0.7118203760162601
Testing Acc of Epoch 8: 0.718391304347826
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.6916e-01 (2.6916e-01)	Acc 0.717773 (0.717773)
Epoch: [9][300/616]	Loss 2.6853e-01 (2.7826e-01)	Acc 0.726562 (0.711288)
Epoch: [9][600/616]	Loss 2.6503e-01 (2.7875e-01)	Acc 0.748047 (0.710963)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.714209)
Training Loss of Epoch 9: 0.2789041485001401
Training Acc of Epoch 9: 0.7108565167682926
Testing Acc of Epoch 9: 0.7142086956521739
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.8602e-01 (2.8602e-01)	Acc 0.697266 (0.697266)
Epoch: [10][300/616]	Loss 2.8122e-01 (2.7733e-01)	Acc 0.721680 (0.713763)
Epoch: [10][600/616]	Loss 3.0656e-01 (2.8181e-01)	Acc 0.671875 (0.709543)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.711335)
Training Loss of Epoch 10: 0.28155830400746046
Training Acc of Epoch 10: 0.7097227515243902
Testing Acc of Epoch 10: 0.7113347826086956
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.6013e-01 (2.6013e-01)	Acc 0.737305 (0.737305)
Epoch: [11][300/616]	Loss 3.2158e-01 (2.7671e-01)	Acc 0.665039 (0.713721)
Epoch: [11][600/616]	Loss 2.6143e-01 (2.7884e-01)	Acc 0.737305 (0.711360)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.663462 (0.676513)
Training Loss of Epoch 11: 0.2790982043840052
Training Acc of Epoch 11: 0.7111486915650407
Testing Acc of Epoch 11: 0.6765130434782609
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 3.2509e-01 (3.2509e-01)	Acc 0.660156 (0.660156)
Epoch: [12][300/616]	Loss 2.7818e-01 (2.7755e-01)	Acc 0.714844 (0.712939)
Epoch: [12][600/616]	Loss 2.7302e-01 (2.7726e-01)	Acc 0.708984 (0.712627)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.679487 (0.687657)
Training Loss of Epoch 12: 0.2774309824394986
Training Acc of Epoch 12: 0.7124333079268292
Testing Acc of Epoch 12: 0.6876565217391304
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.8364e-01 (2.8364e-01)	Acc 0.703125 (0.703125)
Epoch: [13][300/616]	Loss 2.5789e-01 (2.7567e-01)	Acc 0.729492 (0.714607)
Epoch: [13][600/616]	Loss 2.5745e-01 (2.7697e-01)	Acc 0.722656 (0.712772)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.720787)
Training Loss of Epoch 13: 0.276935097817483
Training Acc of Epoch 13: 0.7126603785569106
Testing Acc of Epoch 13: 0.7207869565217391
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.8312e-01 (2.8312e-01)	Acc 0.704102 (0.704102)
Epoch: [14][300/616]	Loss 2.8606e-01 (2.7699e-01)	Acc 0.700195 (0.713491)
Epoch: [14][600/616]	Loss 2.6996e-01 (2.7677e-01)	Acc 0.720703 (0.713242)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.704239)
Training Loss of Epoch 14: 0.2765985948283498
Training Acc of Epoch 14: 0.7133257113821139
Testing Acc of Epoch 14: 0.7042391304347826
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.7770e-01 (2.7770e-01)	Acc 0.723633 (0.723633)
Epoch: [15][300/616]	Loss 2.5459e-01 (2.7659e-01)	Acc 0.742188 (0.714701)
Epoch: [15][600/616]	Loss 2.7050e-01 (2.7659e-01)	Acc 0.701172 (0.713734)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.724196)
Training Loss of Epoch 15: 0.27663992762081024
Training Acc of Epoch 15: 0.7137576219512195
Testing Acc of Epoch 15: 0.724195652173913
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.6844e-01 (2.6844e-01)	Acc 0.723633 (0.723633)
Epoch: [16][300/616]	Loss 2.6831e-01 (2.7924e-01)	Acc 0.717773 (0.711966)
Epoch: [16][600/616]	Loss 2.7685e-01 (2.7659e-01)	Acc 0.704102 (0.714137)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.715939)
Training Loss of Epoch 16: 0.2765451605000147
Training Acc of Epoch 16: 0.7141434832317073
Testing Acc of Epoch 16: 0.7159391304347826
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.8264e-01 (2.8264e-01)	Acc 0.698242 (0.698242)
Epoch: [17][300/616]	Loss 2.8546e-01 (2.8158e-01)	Acc 0.710938 (0.709455)
Epoch: [17][600/616]	Loss 2.8010e-01 (2.8774e-01)	Acc 0.685547 (0.699947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.689103 (0.675374)
Training Loss of Epoch 17: 0.2888967998144103
Training Acc of Epoch 17: 0.6978515625
Testing Acc of Epoch 17: 0.6753739130434783
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 3.1468e-01 (3.1468e-01)	Acc 0.677734 (0.677734)
Epoch: [18][300/616]	Loss 2.8762e-01 (2.9556e-01)	Acc 0.699219 (0.696789)
Epoch: [18][600/616]	Loss 2.6713e-01 (2.8966e-01)	Acc 0.713867 (0.701586)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.722435)
Training Loss of Epoch 18: 0.28912106175732805
Training Acc of Epoch 18: 0.7020214049796748
Testing Acc of Epoch 18: 0.7224347826086956
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4784e-01 (2.4784e-01)	Acc 0.743164 (0.743164)
Epoch: [19][300/616]	Loss 2.5898e-01 (2.7809e-01)	Acc 0.723633 (0.711693)
Epoch: [19][600/616]	Loss 2.5393e-01 (2.7590e-01)	Acc 0.735352 (0.713716)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.722261)
Training Loss of Epoch 19: 0.27572987947037547
Training Acc of Epoch 19: 0.7139021214430894
Testing Acc of Epoch 19: 0.7222608695652174
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6926e-01 (2.6926e-01)	Acc 0.705078 (0.705078)
Epoch: [20][300/616]	Loss 2.9729e-01 (2.8389e-01)	Acc 0.691406 (0.706609)
Epoch: [20][600/616]	Loss 3.0761e-01 (2.8780e-01)	Acc 0.718750 (0.699986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.690065)
Training Loss of Epoch 20: 0.28809542721364556
Training Acc of Epoch 20: 0.6995395071138212
Testing Acc of Epoch 20: 0.6900652173913043
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.8552e-01 (2.8552e-01)	Acc 0.693359 (0.693359)
Epoch: [21][300/616]	Loss 2.6992e-01 (2.8253e-01)	Acc 0.712891 (0.704958)
Epoch: [21][600/616]	Loss 4.1710e-01 (3.0839e-01)	Acc 0.441406 (0.650631)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.445513 (0.445552)
Training Loss of Epoch 21: 0.310664914293987
Training Acc of Epoch 21: 0.6461128048780488
Testing Acc of Epoch 21: 0.44555217391304347
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 4.1436e-01 (4.1436e-01)	Acc 0.431641 (0.431641)
Epoch: [22][300/616]	Loss 4.5329e-01 (4.0591e-01)	Acc 0.323242 (0.462226)
Epoch: [22][600/616]	Loss 4.4164e-01 (4.2379e-01)	Acc 0.426758 (0.398844)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.344551 (0.353348)
Training Loss of Epoch 22: 0.4241635070099094
Training Acc of Epoch 22: 0.39793254573170733
Testing Acc of Epoch 22: 0.35334782608695653
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 4.3832e-01 (4.3832e-01)	Acc 0.356445 (0.356445)
Epoch: [23][300/616]	Loss 4.7910e-01 (4.9024e-01)	Acc 0.319336 (0.272630)
Epoch: [23][600/616]	Loss 4.3305e-01 (4.7728e-01)	Acc 0.352539 (0.295443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.293269 (0.316774)
Training Loss of Epoch 23: 0.47629703370536247
Training Acc of Epoch 23: 0.29613185975609757
Testing Acc of Epoch 23: 0.31677391304347824
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 4.3733e-01 (4.3733e-01)	Acc 0.307617 (0.307617)
Epoch: [24][300/616]	Loss 5.0039e-01 (4.5549e-01)	Acc 0.213867 (0.368534)
Epoch: [24][600/616]	Loss 4.4817e-01 (4.6770e-01)	Acc 0.384766 (0.320707)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.408654 (0.397017)
Training Loss of Epoch 24: 0.4671253801361332
Training Acc of Epoch 24: 0.32255462398373985
Testing Acc of Epoch 24: 0.3970173913043478
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 4.3919e-01 (4.3919e-01)	Acc 0.398438 (0.398438)
Epoch: [25][300/616]	Loss 5.0044e-01 (4.5814e-01)	Acc 0.209961 (0.325513)
Epoch: [25][600/616]	Loss 4.3943e-01 (4.5529e-01)	Acc 0.389648 (0.351938)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.375000 (0.393922)
Training Loss of Epoch 25: 0.4550538014105665
Training Acc of Epoch 25: 0.3527518419715447
Testing Acc of Epoch 25: 0.39392173913043477
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 4.5615e-01 (4.5615e-01)	Acc 0.369141 (0.369141)
Epoch: [26][300/616]	Loss 3.8882e-01 (4.3251e-01)	Acc 0.460938 (0.393156)
Epoch: [26][600/616]	Loss 4.6002e-01 (4.1972e-01)	Acc 0.380859 (0.436237)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.330128 (0.341378)
Training Loss of Epoch 26: 0.42082350132911184
Training Acc of Epoch 26: 0.4341812754065041
Testing Acc of Epoch 26: 0.34137826086956524
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 4.6955e-01 (4.6955e-01)	Acc 0.329102 (0.329102)
Epoch: [27][300/616]	Loss 3.6081e-01 (3.9025e-01)	Acc 0.695312 (0.601640)
Epoch: [27][600/616]	Loss 3.6218e-01 (3.7467e-01)	Acc 0.586914 (0.606553)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.578526 (0.581613)
Training Loss of Epoch 27: 0.37458555068426985
Training Acc of Epoch 27: 0.6052575584349593
Testing Acc of Epoch 27: 0.5816130434782608
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 3.5367e-01 (3.5367e-01)	Acc 0.612305 (0.612305)
Epoch: [28][300/616]	Loss 3.6678e-01 (3.5174e-01)	Acc 0.552734 (0.575374)
Epoch: [28][600/616]	Loss 4.3999e-01 (3.7195e-01)	Acc 0.452148 (0.530511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.379808 (0.407391)
Training Loss of Epoch 28: 0.37367977315817424
Training Acc of Epoch 28: 0.5279090447154472
Testing Acc of Epoch 28: 0.4073913043478261
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 4.4605e-01 (4.4605e-01)	Acc 0.421875 (0.421875)
Epoch: [29][300/616]	Loss 4.6163e-01 (4.5224e-01)	Acc 0.292969 (0.390181)
Epoch: [29][600/616]	Loss 4.1453e-01 (4.3336e-01)	Acc 0.431641 (0.412936)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.560897 (0.573265)
Training Loss of Epoch 29: 0.43270081397963733
Training Acc of Epoch 29: 0.4139576981707317
Testing Acc of Epoch 29: 0.5732652173913043
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.8261e-01 (3.8261e-01)	Acc 0.564453 (0.564453)
Epoch: [30][300/616]	Loss 5.0040e-01 (4.0900e-01)	Acc 0.206055 (0.469337)
Epoch: [30][600/616]	Loss 4.8123e-01 (4.5238e-01)	Acc 0.258789 (0.344824)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.238782 (0.256696)
Training Loss of Epoch 30: 0.4530211512150803
Training Acc of Epoch 30: 0.34285600863821136
Testing Acc of Epoch 30: 0.25669565217391305
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 4.8257e-01 (4.8257e-01)	Acc 0.253906 (0.253906)
Epoch: [31][300/616]	Loss 4.7540e-01 (4.7716e-01)	Acc 0.276367 (0.268023)
Epoch: [31][600/616]	Loss 4.6435e-01 (4.7346e-01)	Acc 0.288086 (0.278470)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.294872 (0.299526)
Training Loss of Epoch 31: 0.4733804475485794
Training Acc of Epoch 31: 0.278783981199187
Testing Acc of Epoch 31: 0.2995260869565217
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 4.6593e-01 (4.6593e-01)	Acc 0.312500 (0.312500)
Epoch: [32][300/616]	Loss 4.7195e-01 (4.7114e-01)	Acc 0.309570 (0.292375)
Epoch: [32][600/616]	Loss 4.6817e-01 (4.7055e-01)	Acc 0.281250 (0.294841)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.290064 (0.294330)
Training Loss of Epoch 32: 0.47055932734070754
Training Acc of Epoch 32: 0.2949933307926829
Testing Acc of Epoch 32: 0.2943304347826087
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 4.7426e-01 (4.7426e-01)	Acc 0.293945 (0.293945)
Epoch: [33][300/616]	Loss 4.6690e-01 (4.7096e-01)	Acc 0.296875 (0.294896)
Epoch: [33][600/616]	Loss 4.6101e-01 (4.6911e-01)	Acc 0.343750 (0.299519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.312500 (0.313291)
Training Loss of Epoch 33: 0.469074991272717
Training Acc of Epoch 33: 0.2997713414634146
Testing Acc of Epoch 33: 0.3132913043478261
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 4.6090e-01 (4.6090e-01)	Acc 0.315430 (0.315430)
Epoch: [34][300/616]	Loss 4.6836e-01 (4.6450e-01)	Acc 0.293945 (0.312143)
Epoch: [34][600/616]	Loss 4.6059e-01 (4.6380e-01)	Acc 0.298828 (0.313152)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.296474 (0.303370)
Training Loss of Epoch 34: 0.46383589157244054
Training Acc of Epoch 34: 0.31300971798780486
Testing Acc of Epoch 34: 0.3033695652173913
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 4.6936e-01 (4.6936e-01)	Acc 0.298828 (0.298828)
Epoch: [35][300/616]	Loss 4.6621e-01 (4.6332e-01)	Acc 0.319336 (0.314025)
Epoch: [35][600/616]	Loss 4.6299e-01 (4.6268e-01)	Acc 0.314453 (0.314785)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.320513 (0.314178)
Training Loss of Epoch 35: 0.462634571490249
Training Acc of Epoch 35: 0.31472306910569103
Testing Acc of Epoch 35: 0.31417826086956524
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 4.6357e-01 (4.6357e-01)	Acc 0.308594 (0.308594)
Epoch: [36][300/616]	Loss 4.5634e-01 (4.6171e-01)	Acc 0.309570 (0.317831)
Epoch: [36][600/616]	Loss 4.6606e-01 (4.6561e-01)	Acc 0.328125 (0.309944)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.333333 (0.331357)
Training Loss of Epoch 36: 0.46555339379039234
Training Acc of Epoch 36: 0.31023088160569107
Testing Acc of Epoch 36: 0.33135652173913044
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 4.6294e-01 (4.6294e-01)	Acc 0.337891 (0.337891)
Epoch: [37][300/616]	Loss 4.6509e-01 (4.6080e-01)	Acc 0.321289 (0.326636)
Epoch: [37][600/616]	Loss 4.6870e-01 (4.6116e-01)	Acc 0.342773 (0.325697)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.333333 (0.328674)
Training Loss of Epoch 37: 0.4611759587032039
Training Acc of Epoch 37: 0.32551448170731706
Testing Acc of Epoch 37: 0.32867391304347826
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 4.5669e-01 (4.5669e-01)	Acc 0.309570 (0.309570)
Epoch: [38][300/616]	Loss 4.5001e-01 (4.5128e-01)	Acc 0.355469 (0.339860)
Epoch: [38][600/616]	Loss 3.9493e-01 (4.2906e-01)	Acc 0.472656 (0.380492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.501603 (0.507213)
Training Loss of Epoch 38: 0.42833051347151035
Training Acc of Epoch 38: 0.382421875
Testing Acc of Epoch 38: 0.5072130434782609
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.9578e-01 (3.9578e-01)	Acc 0.490234 (0.490234)
Epoch: [39][300/616]	Loss 3.9170e-01 (3.9387e-01)	Acc 0.468750 (0.483308)
Epoch: [39][600/616]	Loss 3.8634e-01 (3.9149e-01)	Acc 0.442383 (0.482880)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.493590 (0.491809)
Training Loss of Epoch 39: 0.39114807107584265
Training Acc of Epoch 39: 0.4829792301829268
Testing Acc of Epoch 39: 0.4918086956521739
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.6920e-01 (3.6920e-01)	Acc 0.501953 (0.501953)
Epoch: [40][300/616]	Loss 4.4946e-01 (3.7893e-01)	Acc 0.556641 (0.516806)
Epoch: [40][600/616]	Loss 3.4718e-01 (3.5753e-01)	Acc 0.617188 (0.579310)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.623397 (0.610917)
Training Loss of Epoch 40: 0.3566325235172985
Training Acc of Epoch 40: 0.5818629319105691
Testing Acc of Epoch 40: 0.6109173913043479
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.7353e-01 (3.7353e-01)	Acc 0.595703 (0.595703)
Epoch: [41][300/616]	Loss 3.1583e-01 (2.9984e-01)	Acc 0.689453 (0.704059)
Epoch: [41][600/616]	Loss 2.9496e-01 (2.9917e-01)	Acc 0.702148 (0.704201)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.649038 (0.671187)
Training Loss of Epoch 41: 0.2989402010188839
Training Acc of Epoch 41: 0.7042778201219512
Testing Acc of Epoch 41: 0.6711869565217391
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.5639e-01 (3.5639e-01)	Acc 0.657227 (0.657227)
Epoch: [42][300/616]	Loss 2.8083e-01 (2.9834e-01)	Acc 0.718750 (0.702447)
Epoch: [42][600/616]	Loss 2.8841e-01 (2.9531e-01)	Acc 0.724609 (0.705957)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.722552)
Training Loss of Epoch 42: 0.29517072225973856
Training Acc of Epoch 42: 0.7061340828252033
Testing Acc of Epoch 42: 0.7225521739130435
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.9284e-01 (2.9284e-01)	Acc 0.710938 (0.710938)
Epoch: [43][300/616]	Loss 2.5552e-01 (2.8888e-01)	Acc 0.744141 (0.710431)
Epoch: [43][600/616]	Loss 3.2657e+01 (7.1275e+00)	Acc 0.181641 (0.597019)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201674)
Training Loss of Epoch 43: 7.693105274245021
Training Acc of Epoch 43: 0.5879795477642277
Testing Acc of Epoch 43: 0.20167391304347826
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.0782e+01 (3.0782e+01)	Acc 0.229492 (0.229492)
Epoch: [44][300/616]	Loss 3.1797e+01 (3.1909e+01)	Acc 0.204102 (0.201603)
Epoch: [44][600/616]	Loss 3.2227e+01 (3.1876e+01)	Acc 0.194336 (0.202417)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201674)
Training Loss of Epoch 44: 31.87791381463772
Training Acc of Epoch 44: 0.20237233231707316
Testing Acc of Epoch 44: 0.20167391304347826
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.191406 (0.191406)
Epoch: [45][300/616]	Loss 3.1367e+01 (3.1834e+01)	Acc 0.215820 (0.203440)
Epoch: [45][600/616]	Loss 3.1720e+01 (3.1878e+01)	Acc 0.205078 (0.202347)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201670)
Training Loss of Epoch 45: 31.876782431253574
Training Acc of Epoch 45: 0.20237709603658535
Testing Acc of Epoch 45: 0.2016695652173913
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [46][300/616]	Loss 3.0352e+01 (3.1855e+01)	Acc 0.241211 (0.202891)
Epoch: [46][600/616]	Loss 3.1797e+01 (3.1877e+01)	Acc 0.204102 (0.202384)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201670)
Training Loss of Epoch 46: 31.87627350101626
Training Acc of Epoch 46: 0.20239297510162602
Testing Acc of Epoch 46: 0.2016695652173913
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.191406 (0.191406)
Epoch: [47][300/616]	Loss 3.2109e+01 (3.1877e+01)	Acc 0.197266 (0.202337)
Epoch: [47][600/616]	Loss 3.1797e+01 (3.1873e+01)	Acc 0.204102 (0.202446)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 47: 31.873122306761704
Training Acc of Epoch 47: 0.20242790904471544
Testing Acc of Epoch 47: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.1133e+01 (3.1133e+01)	Acc 0.220703 (0.220703)
Epoch: [48][300/616]	Loss 3.1251e+01 (3.1819e+01)	Acc 0.217773 (0.203602)
Epoch: [48][600/616]	Loss 3.2033e+01 (3.1857e+01)	Acc 0.196289 (0.202449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201683)
Training Loss of Epoch 48: 31.8585953689203
Training Acc of Epoch 48: 0.20241361788617887
Testing Acc of Epoch 48: 0.20168260869565216
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.2345e+01 (3.2345e+01)	Acc 0.188477 (0.188477)
Epoch: [49][300/616]	Loss 3.1761e+01 (3.1882e+01)	Acc 0.201172 (0.201315)
Epoch: [49][600/616]	Loss 3.0587e+01 (3.1786e+01)	Acc 0.232422 (0.202483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.202078)
Training Loss of Epoch 49: 31.781216405853023
Training Acc of Epoch 49: 0.2025073043699187
Testing Acc of Epoch 49: 0.2020782608695652
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.1412e+01 (3.1412e+01)	Acc 0.204102 (0.204102)
Epoch: [50][300/616]	Loss 3.1449e+01 (3.1582e+01)	Acc 0.207031 (0.202048)
Epoch: [50][600/616]	Loss 3.1880e+01 (3.1461e+01)	Acc 0.193359 (0.203062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.202726)
Training Loss of Epoch 50: 31.46114305015502
Training Acc of Epoch 50: 0.20297573678861788
Testing Acc of Epoch 50: 0.20272608695652175
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.1961e+01 (3.1961e+01)	Acc 0.187500 (0.187500)
Epoch: [51][300/616]	Loss 3.0678e+01 (3.1277e+01)	Acc 0.208008 (0.202148)
Epoch: [51][600/616]	Loss 3.2082e+01 (3.1168e+01)	Acc 0.175781 (0.203404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.203291)
Training Loss of Epoch 51: 31.15951268265887
Training Acc of Epoch 51: 0.20353785569105692
Testing Acc of Epoch 51: 0.20329130434782608
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.1142e+01 (3.1142e+01)	Acc 0.205078 (0.205078)
Epoch: [52][300/616]	Loss 3.1181e+01 (3.0945e+01)	Acc 0.204102 (0.203297)
Epoch: [52][600/616]	Loss 3.0836e+01 (3.0844e+01)	Acc 0.199219 (0.204014)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.197115 (0.203952)
Training Loss of Epoch 52: 30.840336407297027
Training Acc of Epoch 52: 0.20407774390243902
Testing Acc of Epoch 52: 0.2039521739130435
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.1067e+01 (3.1067e+01)	Acc 0.202148 (0.202148)
Epoch: [53][300/616]	Loss 3.0445e+01 (3.0528e+01)	Acc 0.211914 (0.204381)
Epoch: [53][600/616]	Loss 3.0138e+01 (3.0439e+01)	Acc 0.207031 (0.204886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.198718 (0.204717)
Training Loss of Epoch 53: 30.434433327651604
Training Acc of Epoch 53: 0.20483993902439024
Testing Acc of Epoch 53: 0.20471739130434782
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 3.0099e+01 (3.0099e+01)	Acc 0.209961 (0.209961)
Epoch: [54][300/616]	Loss 2.9594e+01 (3.0081e+01)	Acc 0.217773 (0.205714)
Epoch: [54][600/616]	Loss 2.9363e+01 (2.9938e+01)	Acc 0.219727 (0.205832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.198718 (0.205709)
Training Loss of Epoch 54: 29.93328150927536
Training Acc of Epoch 54: 0.20578474339430894
Testing Acc of Epoch 54: 0.20570869565217392
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.9836e+01 (2.9836e+01)	Acc 0.200195 (0.200195)
Epoch: [55][300/616]	Loss 2.9250e+01 (2.9501e+01)	Acc 0.211914 (0.206532)
Epoch: [55][600/616]	Loss 2.9140e+01 (2.9373e+01)	Acc 0.201172 (0.206789)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.198718 (0.206643)
Training Loss of Epoch 55: 29.363894222228506
Training Acc of Epoch 55: 0.20682482215447154
Testing Acc of Epoch 55: 0.20664347826086957
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.8782e+01 (2.8782e+01)	Acc 0.219727 (0.219727)
Epoch: [56][300/616]	Loss 2.9525e+01 (2.8812e+01)	Acc 0.202148 (0.207401)
Epoch: [56][600/616]	Loss 2.8943e+01 (2.8630e+01)	Acc 0.208008 (0.208091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.200321 (0.207987)
Training Loss of Epoch 56: 28.618740770293446
Training Acc of Epoch 56: 0.2080760924796748
Testing Acc of Epoch 56: 0.20798695652173912
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.7818e+01 (2.7818e+01)	Acc 0.221680 (0.221680)
Epoch: [57][300/616]	Loss 2.8604e+01 (2.8035e+01)	Acc 0.197266 (0.208602)
Epoch: [57][600/616]	Loss 2.7634e+01 (2.7779e+01)	Acc 0.217773 (0.209469)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.200321 (0.209948)
Training Loss of Epoch 57: 27.766784996715018
Training Acc of Epoch 57: 0.20951314786585365
Testing Acc of Epoch 57: 0.20994782608695653
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.7837e+01 (2.7837e+01)	Acc 0.188477 (0.188477)
Epoch: [58][300/616]	Loss 2.6632e+01 (2.7030e+01)	Acc 0.209961 (0.210175)
Epoch: [58][600/616]	Loss 2.6128e+01 (2.6818e+01)	Acc 0.216797 (0.211084)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.205128 (0.211730)
Training Loss of Epoch 58: 26.805094720483794
Training Acc of Epoch 58: 0.21119950457317074
Testing Acc of Epoch 58: 0.2117304347826087
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.6714e+01 (2.6714e+01)	Acc 0.203125 (0.203125)
Epoch: [59][300/616]	Loss 2.5234e+01 (2.6056e+01)	Acc 0.235352 (0.212063)
Epoch: [59][600/616]	Loss 2.6210e+01 (2.5881e+01)	Acc 0.202148 (0.213303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.206731 (0.214087)
Training Loss of Epoch 59: 25.871902847290038
Training Acc of Epoch 59: 0.21327807418699188
Testing Acc of Epoch 59: 0.21408695652173912
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5202e+01 (2.5202e+01)	Acc 0.218750 (0.218750)
Epoch: [60][300/616]	Loss 2.5713e+01 (2.5348e+01)	Acc 0.199219 (0.215862)
Epoch: [60][600/616]	Loss 2.4621e+01 (2.5304e+01)	Acc 0.219727 (0.215466)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.208333 (0.216217)
Training Loss of Epoch 60: 25.301713115413015
Training Acc of Epoch 60: 0.21537569867886178
Testing Acc of Epoch 60: 0.21621739130434783
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.4465e+01 (2.4465e+01)	Acc 0.226562 (0.226562)
Epoch: [61][300/616]	Loss 2.3779e+01 (2.4788e+01)	Acc 0.215820 (0.218150)
Epoch: [61][600/616]	Loss 2.3865e+01 (2.4579e+01)	Acc 0.212891 (0.218740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.209936 (0.220270)
Training Loss of Epoch 61: 24.5665487553046
Training Acc of Epoch 61: 0.2188008130081301
Testing Acc of Epoch 61: 0.2202695652173913
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.3576e+01 (2.3576e+01)	Acc 0.241211 (0.241211)
Epoch: [62][300/616]	Loss 2.4160e+01 (2.4156e+01)	Acc 0.224609 (0.213627)
Epoch: [62][600/616]	Loss 2.4064e+01 (2.4201e+01)	Acc 0.199219 (0.207454)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 62: 24.204743588455326
Training Acc of Epoch 62: 0.20722021087398373
Testing Acc of Epoch 62: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3360e+01 (2.3360e+01)	Acc 0.216797 (0.216797)
Epoch: [63][300/616]	Loss 2.4526e+01 (2.4027e+01)	Acc 0.194336 (0.200980)
Epoch: [63][600/616]	Loss 2.2985e+01 (2.4008e+01)	Acc 0.205078 (0.201505)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 63: 24.00893921580741
Training Acc of Epoch 63: 0.20146087398373982
Testing Acc of Epoch 63: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3834e+01 (2.3834e+01)	Acc 0.196289 (0.196289)
Epoch: [64][300/616]	Loss 2.3751e+01 (2.4143e+01)	Acc 0.206055 (0.201435)
Epoch: [64][600/616]	Loss 2.3911e+01 (2.4073e+01)	Acc 0.195312 (0.201500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 64: 24.07134072528622
Training Acc of Epoch 64: 0.20146087398373982
Testing Acc of Epoch 64: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4535e+01 (2.4535e+01)	Acc 0.185547 (0.185547)
Epoch: [65][300/616]	Loss 2.3087e+01 (2.3821e+01)	Acc 0.216797 (0.201263)
Epoch: [65][600/616]	Loss 2.4017e+01 (2.3805e+01)	Acc 0.217773 (0.201377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 65: 23.796909775772715
Training Acc of Epoch 65: 0.20145452235772357
Testing Acc of Epoch 65: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3986e+01 (2.3986e+01)	Acc 0.208984 (0.208984)
Epoch: [66][300/616]	Loss 2.2600e+01 (2.3634e+01)	Acc 0.184570 (0.201045)
Epoch: [66][600/616]	Loss 2.2234e+01 (2.3273e+01)	Acc 0.224609 (0.201484)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 66: 23.27864656836037
Training Acc of Epoch 66: 0.20144975863821138
Testing Acc of Epoch 66: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.2706e+01 (2.2706e+01)	Acc 0.205078 (0.205078)
Epoch: [67][300/616]	Loss 2.4208e+01 (2.3196e+01)	Acc 0.212891 (0.203930)
Epoch: [67][600/616]	Loss 2.2779e+01 (2.3485e+01)	Acc 0.209961 (0.206736)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 67: 23.461802992781973
Training Acc of Epoch 67: 0.2065501143292683
Testing Acc of Epoch 67: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.3441e+01 (2.3441e+01)	Acc 0.188477 (0.188477)
Epoch: [68][300/616]	Loss 2.2242e+01 (2.2638e+01)	Acc 0.208984 (0.212303)
Epoch: [68][600/616]	Loss 2.2047e+01 (2.2931e+01)	Acc 0.204102 (0.215828)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 68: 22.910964081927045
Training Acc of Epoch 68: 0.21559165396341465
Testing Acc of Epoch 68: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.2014e+01 (2.2014e+01)	Acc 0.200195 (0.200195)
Epoch: [69][300/616]	Loss 2.0697e+01 (2.2311e+01)	Acc 0.210938 (0.221105)
Epoch: [69][600/616]	Loss 2.5276e+01 (2.2731e+01)	Acc 0.332031 (0.223903)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 69: 22.76884395010103
Training Acc of Epoch 69: 0.2247506986788618
Testing Acc of Epoch 69: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.2164e+01 (2.2164e+01)	Acc 0.208984 (0.208984)
Epoch: [70][300/616]	Loss 2.6805e+01 (2.3215e+01)	Acc 0.288086 (0.236033)
Epoch: [70][600/616]	Loss 2.5543e+01 (2.3441e+01)	Acc 0.311523 (0.239396)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.268165)
Training Loss of Epoch 70: 23.499188579776423
Training Acc of Epoch 70: 0.24066310975609756
Testing Acc of Epoch 70: 0.26816521739130433
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5552e+01 (2.5552e+01)	Acc 0.318359 (0.318359)
Epoch: [71][300/616]	Loss 2.2663e+01 (2.4374e+01)	Acc 0.217773 (0.250668)
Epoch: [71][600/616]	Loss 1.5609e+01 (2.3360e+01)	Acc 0.193359 (0.252926)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.338141 (0.303970)
Training Loss of Epoch 71: 23.23684447218732
Training Acc of Epoch 71: 0.2537474593495935
Testing Acc of Epoch 71: 0.3039695652173913
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 1.6315e+01 (1.6315e+01)	Acc 0.350586 (0.350586)
Epoch: [72][300/616]	Loss 2.7301e+01 (2.2271e+01)	Acc 0.262695 (0.270264)
Epoch: [72][600/616]	Loss 2.9941e+01 (2.5191e+01)	Acc 0.236328 (0.262705)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.208333 (0.213422)
Training Loss of Epoch 72: 25.316012739166013
Training Acc of Epoch 72: 0.2617409806910569
Testing Acc of Epoch 72: 0.21342173913043477
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.1580e+01 (3.1580e+01)	Acc 0.199219 (0.199219)
Epoch: [73][300/616]	Loss 3.1744e+01 (3.1632e+01)	Acc 0.202148 (0.203589)
Epoch: [73][600/616]	Loss 3.1993e+01 (3.1105e+01)	Acc 0.199219 (0.203757)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 73: 31.119314826407084
Training Acc of Epoch 73: 0.2037522230691057
Testing Acc of Epoch 73: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.2306e+01 (3.2306e+01)	Acc 0.189453 (0.189453)
Epoch: [74][300/616]	Loss 2.1545e+01 (2.5559e+01)	Acc 0.285156 (0.214685)
Epoch: [74][600/616]	Loss 2.5031e+01 (2.4187e+01)	Acc 0.288086 (0.241211)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.272436 (0.280291)
Training Loss of Epoch 74: 24.198055258029846
Training Acc of Epoch 74: 0.2421366869918699
Testing Acc of Epoch 74: 0.28029130434782606
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4883e+01 (2.4883e+01)	Acc 0.280273 (0.280273)
Epoch: [75][300/616]	Loss 2.5224e+01 (2.4879e+01)	Acc 0.268555 (0.262261)
Epoch: [75][600/616]	Loss 2.4792e+01 (2.4916e+01)	Acc 0.276367 (0.266954)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.272436 (0.259752)
Training Loss of Epoch 75: 24.907623793439168
Training Acc of Epoch 75: 0.26694931402439026
Testing Acc of Epoch 75: 0.2597521739130435
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.4764e+01 (2.4764e+01)	Acc 0.288086 (0.288086)
Epoch: [76][300/616]	Loss 2.5657e+01 (2.4990e+01)	Acc 0.249023 (0.271355)
Epoch: [76][600/616]	Loss 2.4404e+01 (2.5115e+01)	Acc 0.287109 (0.269539)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.261218 (0.273070)
Training Loss of Epoch 76: 25.11990230839427
Training Acc of Epoch 76: 0.2697090955284553
Testing Acc of Epoch 76: 0.2730695652173913
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.6071e+01 (2.6071e+01)	Acc 0.263672 (0.263672)
Epoch: [77][300/616]	Loss 2.7244e+01 (2.6328e+01)	Acc 0.259766 (0.255558)
Epoch: [77][600/616]	Loss 2.6531e+01 (2.6440e+01)	Acc 0.270508 (0.254272)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.261218 (0.260817)
Training Loss of Epoch 77: 26.450707018472315
Training Acc of Epoch 77: 0.2543588033536585
Testing Acc of Epoch 77: 0.26081739130434783
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.5877e+01 (2.5877e+01)	Acc 0.270508 (0.270508)
Epoch: [78][300/616]	Loss 2.1599e+01 (2.5846e+01)	Acc 0.318359 (0.254010)
Epoch: [78][600/616]	Loss 2.2090e+01 (2.4038e+01)	Acc 0.294922 (0.276489)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.304487 (0.308204)
Training Loss of Epoch 78: 24.006933848063152
Training Acc of Epoch 78: 0.2768784933943089
Testing Acc of Epoch 78: 0.30820434782608697
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2907e+01 (2.2907e+01)	Acc 0.292969 (0.292969)
Epoch: [79][300/616]	Loss 2.1418e+01 (2.2389e+01)	Acc 0.325195 (0.298410)
Epoch: [79][600/616]	Loss 2.5964e+01 (2.3477e+01)	Acc 0.251953 (0.288611)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.267628 (0.274430)
Training Loss of Epoch 79: 23.505822992712503
Training Acc of Epoch 79: 0.28843845274390245
Testing Acc of Epoch 79: 0.2744304347826087
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4874e+01 (2.4874e+01)	Acc 0.274414 (0.274414)
Epoch: [80][300/616]	Loss 2.3373e+01 (2.3358e+01)	Acc 0.294922 (0.296443)
Epoch: [80][600/616]	Loss 2.3171e+01 (2.3438e+01)	Acc 0.302734 (0.301781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.309295 (0.311117)
Training Loss of Epoch 80: 23.434529650308253
Training Acc of Epoch 80: 0.30167524136178864
Testing Acc of Epoch 80: 0.31111739130434785
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.1545e+01 (2.1545e+01)	Acc 0.329102 (0.329102)
Epoch: [81][300/616]	Loss 2.0527e+01 (2.2755e+01)	Acc 0.338867 (0.308250)
Epoch: [81][600/616]	Loss 2.1073e+01 (2.2149e+01)	Acc 0.332031 (0.313455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.322115 (0.322135)
Training Loss of Epoch 81: 22.126596404284967
Training Acc of Epoch 81: 0.31360041920731707
Testing Acc of Epoch 81: 0.32213478260869566
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.1778e+01 (2.1778e+01)	Acc 0.303711 (0.303711)
Epoch: [82][300/616]	Loss 3.1836e+01 (3.0555e+01)	Acc 0.204102 (0.216933)
Epoch: [82][600/616]	Loss 3.1599e+01 (3.1261e+01)	Acc 0.209961 (0.208846)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 82: 31.27310932438548
Training Acc of Epoch 82: 0.20877159552845528
Testing Acc of Epoch 82: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [83][300/616]	Loss 3.2104e+01 (3.1972e+01)	Acc 0.197266 (0.200624)
Epoch: [83][600/616]	Loss 3.1515e+01 (3.1773e+01)	Acc 0.209961 (0.201807)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 83: 31.777040388525986
Training Acc of Epoch 83: 0.20175146087398374
Testing Acc of Epoch 83: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 3.1617e+01 (3.1617e+01)	Acc 0.206055 (0.206055)
Epoch: [84][300/616]	Loss 3.2070e+01 (3.1909e+01)	Acc 0.195312 (0.200212)
Epoch: [84][600/616]	Loss 3.2425e+01 (3.1828e+01)	Acc 0.179688 (0.201136)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 84: 31.819670781081285
Training Acc of Epoch 84: 0.2011448805894309
Testing Acc of Epoch 84: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 3.1674e+01 (3.1674e+01)	Acc 0.198242 (0.198242)
Epoch: [85][300/616]	Loss 3.1184e+01 (3.1873e+01)	Acc 0.216797 (0.200250)
Epoch: [85][600/616]	Loss 3.1147e+01 (3.1854e+01)	Acc 0.218750 (0.200927)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 85: 31.852900249202076
Training Acc of Epoch 85: 0.20096068343495935
Testing Acc of Epoch 85: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 3.1777e+01 (3.1777e+01)	Acc 0.202148 (0.202148)
Epoch: [86][300/616]	Loss 3.2036e+01 (3.1897e+01)	Acc 0.196289 (0.199897)
Epoch: [86][600/616]	Loss 3.1521e+01 (3.1814e+01)	Acc 0.207031 (0.200793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 86: 31.81237569669398
Training Acc of Epoch 86: 0.20080189278455285
Testing Acc of Epoch 86: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 3.1162e+01 (3.1162e+01)	Acc 0.216797 (0.216797)
Epoch: [87][300/616]	Loss 3.1370e+01 (3.1755e+01)	Acc 0.210938 (0.201282)
Epoch: [87][600/616]	Loss 3.1975e+01 (3.1750e+01)	Acc 0.194336 (0.201394)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 87: 31.74815220096247
Training Acc of Epoch 87: 0.20145452235772357
Testing Acc of Epoch 87: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 3.1423e+01 (3.1423e+01)	Acc 0.208984 (0.208984)
Epoch: [88][300/616]	Loss 3.1908e+01 (3.1744e+01)	Acc 0.198242 (0.201886)
Epoch: [88][600/616]	Loss 3.1460e+01 (3.1757e+01)	Acc 0.208984 (0.201573)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 88: 31.762986550680022
Training Acc of Epoch 88: 0.201440231199187
Testing Acc of Epoch 88: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.2430e+01 (3.2430e+01)	Acc 0.183594 (0.183594)
Epoch: [89][300/616]	Loss 3.1661e+01 (3.1770e+01)	Acc 0.203125 (0.201269)
Epoch: [89][600/616]	Loss 3.1091e+01 (3.1770e+01)	Acc 0.218750 (0.201440)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 89: 31.770748141529115
Training Acc of Epoch 89: 0.20142752794715446
Testing Acc of Epoch 89: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 3.2513e+01 (3.2513e+01)	Acc 0.183594 (0.183594)
Epoch: [90][300/616]	Loss 3.1779e+01 (3.1804e+01)	Acc 0.200195 (0.200896)
Epoch: [90][600/616]	Loss 3.0737e+01 (3.1776e+01)	Acc 0.228516 (0.201575)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 90: 31.780820976815573
Training Acc of Epoch 90: 0.2014449949186992
Testing Acc of Epoch 90: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 3.2132e+01 (3.2132e+01)	Acc 0.193359 (0.193359)
Epoch: [91][300/616]	Loss 3.1327e+01 (3.1753e+01)	Acc 0.212891 (0.202265)
Epoch: [91][600/616]	Loss 3.1746e+01 (3.1777e+01)	Acc 0.202148 (0.201507)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 91: 31.779065456235312
Training Acc of Epoch 91: 0.20146087398373982
Testing Acc of Epoch 91: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 3.2079e+01 (3.2079e+01)	Acc 0.192383 (0.192383)
Epoch: [92][300/616]	Loss 3.2449e+01 (3.1765e+01)	Acc 0.184570 (0.201827)
Epoch: [92][600/616]	Loss 3.1553e+01 (3.1784e+01)	Acc 0.208008 (0.201432)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 92: 31.78259674320376
Training Acc of Epoch 92: 0.20145134654471544
Testing Acc of Epoch 92: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 3.2759e+01 (3.2759e+01)	Acc 0.176758 (0.176758)
Epoch: [93][300/616]	Loss 3.1617e+01 (3.1785e+01)	Acc 0.207031 (0.201616)
Epoch: [93][600/616]	Loss 3.0824e+01 (3.1786e+01)	Acc 0.224609 (0.201370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 93: 31.783457394731723
Training Acc of Epoch 93: 0.2014449949186992
Testing Acc of Epoch 93: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 3.2257e+01 (3.2257e+01)	Acc 0.189453 (0.189453)
Epoch: [94][300/616]	Loss 3.1698e+01 (3.1781e+01)	Acc 0.203125 (0.201597)
Epoch: [94][600/616]	Loss 3.1745e+01 (3.1781e+01)	Acc 0.201172 (0.201528)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 94: 31.783840610535165
Training Acc of Epoch 94: 0.20144658282520325
Testing Acc of Epoch 94: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 3.1824e+01 (3.1824e+01)	Acc 0.199219 (0.199219)
Epoch: [95][300/616]	Loss 3.1919e+01 (3.1815e+01)	Acc 0.198242 (0.200740)
Epoch: [95][600/616]	Loss 3.2449e+01 (3.1788e+01)	Acc 0.184570 (0.201524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 95: 31.790622525098847
Training Acc of Epoch 95: 0.2014576981707317
Testing Acc of Epoch 95: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.2116e+01 (3.2116e+01)	Acc 0.192383 (0.192383)
Epoch: [96][300/616]	Loss 3.1997e+01 (3.1805e+01)	Acc 0.196289 (0.201198)
Epoch: [96][600/616]	Loss 3.1841e+01 (3.1795e+01)	Acc 0.200195 (0.201477)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 96: 31.796032044945694
Training Acc of Epoch 96: 0.20144658282520325
Testing Acc of Epoch 96: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.1465e+01 (3.1465e+01)	Acc 0.208984 (0.208984)
Epoch: [97][300/616]	Loss 3.1983e+01 (3.1810e+01)	Acc 0.197266 (0.201188)
Epoch: [97][600/616]	Loss 3.1552e+01 (3.1802e+01)	Acc 0.207031 (0.201362)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 97: 31.798117642286346
Training Acc of Epoch 97: 0.20145611026422763
Testing Acc of Epoch 97: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.1580e+01 (3.1580e+01)	Acc 0.207031 (0.207031)
Epoch: [98][300/616]	Loss 3.2578e+01 (3.1868e+01)	Acc 0.185547 (0.201659)
Epoch: [98][600/616]	Loss 3.1797e+01 (3.1904e+01)	Acc 0.205078 (0.201589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 98: 31.908264851763967
Training Acc of Epoch 98: 0.20149422002032522
Testing Acc of Epoch 98: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.191406 (0.191406)
Epoch: [99][300/616]	Loss 3.1719e+01 (3.1935e+01)	Acc 0.207031 (0.201633)
Epoch: [99][600/616]	Loss 3.1914e+01 (3.1944e+01)	Acc 0.202148 (0.201388)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 99: 31.942771849593495
Training Acc of Epoch 99: 0.2014307037601626
Testing Acc of Epoch 99: 0.20121739130434782
Early stopping not satisfied.
