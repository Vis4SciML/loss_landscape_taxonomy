train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9927e-01 (4.9927e-01)	Acc 0.192383 (0.192383)
Epoch: [0][300/616]	Loss 2.7780e-01 (3.0650e-01)	Acc 0.718750 (0.667304)
Epoch: [0][600/616]	Loss 2.6539e-01 (2.8976e-01)	Acc 0.697266 (0.691650)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.726378)
Training Loss of Epoch 0: 0.28919229456564277
Training Acc of Epoch 0: 0.6923447027439025
Testing Acc of Epoch 0: 0.7263782608695653
Model with the best training loss saved! The loss is 0.28919229456564277
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6572e-01 (2.6572e-01)	Acc 0.713867 (0.713867)
Epoch: [1][300/616]	Loss 2.5005e-01 (2.7240e-01)	Acc 0.750977 (0.717816)
Epoch: [1][600/616]	Loss 2.7061e-01 (2.7001e-01)	Acc 0.732422 (0.720498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.719774)
Training Loss of Epoch 1: 0.27028656524371325
Training Acc of Epoch 1: 0.7202632748983739
Testing Acc of Epoch 1: 0.7197739130434783
Model with the best training loss saved! The loss is 0.27028656524371325
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.6087e-01 (2.6087e-01)	Acc 0.736328 (0.736328)
Epoch: [2][300/616]	Loss 2.6987e-01 (2.7548e-01)	Acc 0.721680 (0.715937)
Epoch: [2][600/616]	Loss 2.6772e-01 (2.7489e-01)	Acc 0.725586 (0.715824)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.716213)
Training Loss of Epoch 2: 0.27474548225480366
Training Acc of Epoch 2: 0.7159584603658536
Testing Acc of Epoch 2: 0.7162130434782609
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.7108e-01 (2.7108e-01)	Acc 0.713867 (0.713867)
Epoch: [3][300/616]	Loss 2.9825e-01 (2.7429e-01)	Acc 0.671875 (0.716602)
Epoch: [3][600/616]	Loss 2.7998e-01 (2.7513e-01)	Acc 0.702148 (0.715785)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.725696)
Training Loss of Epoch 3: 0.27522865453871287
Training Acc of Epoch 3: 0.7156853404471545
Testing Acc of Epoch 3: 0.7256956521739131
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6184e-01 (2.6184e-01)	Acc 0.722656 (0.722656)
Epoch: [4][300/616]	Loss 2.6534e-01 (2.7714e-01)	Acc 0.725586 (0.713403)
Epoch: [4][600/616]	Loss 2.9091e-01 (2.7571e-01)	Acc 0.691406 (0.715295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.723130)
Training Loss of Epoch 4: 0.2756689221151476
Training Acc of Epoch 4: 0.715332825203252
Testing Acc of Epoch 4: 0.7231304347826087
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.6467e-01 (2.6467e-01)	Acc 0.722656 (0.722656)
Epoch: [5][300/616]	Loss 2.6256e-01 (2.7436e-01)	Acc 0.747070 (0.716667)
Epoch: [5][600/616]	Loss 2.5905e-01 (2.7509e-01)	Acc 0.724609 (0.715617)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.716113)
Training Loss of Epoch 5: 0.27501639113193604
Training Acc of Epoch 5: 0.7156551702235773
Testing Acc of Epoch 5: 0.7161130434782609
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.8075e-01 (2.8075e-01)	Acc 0.706055 (0.706055)
Epoch: [6][300/616]	Loss 2.6779e-01 (2.7458e-01)	Acc 0.713867 (0.716401)
Epoch: [6][600/616]	Loss 2.7798e-01 (2.7384e-01)	Acc 0.722656 (0.716740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.710578)
Training Loss of Epoch 6: 0.2740649122290495
Training Acc of Epoch 6: 0.7165586890243902
Testing Acc of Epoch 6: 0.7105782608695652
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.7480e-01 (2.7480e-01)	Acc 0.714844 (0.714844)
Epoch: [7][300/616]	Loss 2.9183e-01 (2.7751e-01)	Acc 0.706055 (0.713565)
Epoch: [7][600/616]	Loss 2.5903e-01 (2.7560e-01)	Acc 0.740234 (0.715295)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.719170)
Training Loss of Epoch 7: 0.27534167313478825
Training Acc of Epoch 7: 0.7155837144308943
Testing Acc of Epoch 7: 0.7191695652173913
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.8321e-01 (2.8321e-01)	Acc 0.713867 (0.713867)
Epoch: [8][300/616]	Loss 2.7303e-01 (2.7612e-01)	Acc 0.711914 (0.713848)
Epoch: [8][600/616]	Loss 2.7109e-01 (2.7443e-01)	Acc 0.715820 (0.715921)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.729022)
Training Loss of Epoch 8: 0.2744474802075363
Training Acc of Epoch 8: 0.7159298780487805
Testing Acc of Epoch 8: 0.7290217391304348
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.6349e-01 (2.6349e-01)	Acc 0.734375 (0.734375)
Epoch: [9][300/616]	Loss 2.6725e-01 (2.7455e-01)	Acc 0.719727 (0.716047)
Epoch: [9][600/616]	Loss 2.8776e-01 (2.7491e-01)	Acc 0.704102 (0.716046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.679487 (0.689735)
Training Loss of Epoch 9: 0.27496696422255135
Training Acc of Epoch 9: 0.7159505208333333
Testing Acc of Epoch 9: 0.6897347826086957
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.9492e-01 (2.9492e-01)	Acc 0.692383 (0.692383)
Epoch: [10][300/616]	Loss 2.7381e-01 (2.7328e-01)	Acc 0.720703 (0.717287)
Epoch: [10][600/616]	Loss 2.7640e-01 (2.7355e-01)	Acc 0.726562 (0.716875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.731365)
Training Loss of Epoch 10: 0.2734005532129024
Training Acc of Epoch 10: 0.7170128302845529
Testing Acc of Epoch 10: 0.7313652173913043
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.6032e-01 (2.6032e-01)	Acc 0.728516 (0.728516)
Epoch: [11][300/616]	Loss 2.7655e-01 (2.7256e-01)	Acc 0.714844 (0.718403)
Epoch: [11][600/616]	Loss 2.7815e-01 (2.7331e-01)	Acc 0.715820 (0.717549)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.699483)
Training Loss of Epoch 11: 0.27321237955636124
Training Acc of Epoch 11: 0.7176670477642276
Testing Acc of Epoch 11: 0.6994826086956522
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6548e-01 (2.6548e-01)	Acc 0.721680 (0.721680)
Epoch: [12][300/616]	Loss 2.6409e-01 (2.7347e-01)	Acc 0.716797 (0.717381)
Epoch: [12][600/616]	Loss 2.8442e-01 (2.7525e-01)	Acc 0.695312 (0.715523)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.700900)
Training Loss of Epoch 12: 0.27541011697877715
Training Acc of Epoch 12: 0.715382050304878
Testing Acc of Epoch 12: 0.7009
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 3.0290e-01 (3.0290e-01)	Acc 0.685547 (0.685547)
Epoch: [13][300/616]	Loss 2.7341e-01 (2.7406e-01)	Acc 0.737305 (0.716498)
Epoch: [13][600/616]	Loss 2.7648e-01 (2.7464e-01)	Acc 0.706055 (0.716166)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.728783)
Training Loss of Epoch 13: 0.2744508008646771
Training Acc of Epoch 13: 0.7163665523373983
Testing Acc of Epoch 13: 0.7287826086956521
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6774e-01 (2.6774e-01)	Acc 0.732422 (0.732422)
Epoch: [14][300/616]	Loss 3.0400e-01 (2.7359e-01)	Acc 0.695312 (0.717209)
Epoch: [14][600/616]	Loss 2.6814e-01 (2.7451e-01)	Acc 0.735352 (0.716383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.720174)
Training Loss of Epoch 14: 0.27435120007371516
Training Acc of Epoch 14: 0.7166190294715448
Testing Acc of Epoch 14: 0.7201739130434782
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.6828e-01 (2.6828e-01)	Acc 0.740234 (0.740234)
Epoch: [15][300/616]	Loss 2.6675e-01 (2.7415e-01)	Acc 0.717773 (0.716829)
Epoch: [15][600/616]	Loss 2.8991e-01 (2.7386e-01)	Acc 0.693359 (0.717128)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.725435)
Training Loss of Epoch 15: 0.2740452872301505
Training Acc of Epoch 15: 0.7170287093495935
Testing Acc of Epoch 15: 0.7254347826086956
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.5711e-01 (2.5711e-01)	Acc 0.733398 (0.733398)
Epoch: [16][300/616]	Loss 2.4516e-01 (2.7605e-01)	Acc 0.753906 (0.714477)
Epoch: [16][600/616]	Loss 2.9917e-01 (2.7626e-01)	Acc 0.681641 (0.714774)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.714278)
Training Loss of Epoch 16: 0.2762429127121359
Training Acc of Epoch 16: 0.7147818216463414
Testing Acc of Epoch 16: 0.7142782608695653
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.8370e-01 (2.8370e-01)	Acc 0.708984 (0.708984)
Epoch: [17][300/616]	Loss 2.7435e-01 (2.7878e-01)	Acc 0.701172 (0.712939)
Epoch: [17][600/616]	Loss 2.7573e-01 (2.7912e-01)	Acc 0.721680 (0.712530)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.726787)
Training Loss of Epoch 17: 0.27920514429972426
Training Acc of Epoch 17: 0.7124348958333333
Testing Acc of Epoch 17: 0.7267869565217391
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4942e-01 (2.4942e-01)	Acc 0.741211 (0.741211)
Epoch: [18][300/616]	Loss 2.9556e-01 (2.7730e-01)	Acc 0.690430 (0.713961)
Epoch: [18][600/616]	Loss 2.7596e-01 (2.7659e-01)	Acc 0.717773 (0.714457)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.708583)
Training Loss of Epoch 18: 0.27700278383444965
Training Acc of Epoch 18: 0.7142006478658537
Testing Acc of Epoch 18: 0.7085826086956521
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.8846e-01 (2.8846e-01)	Acc 0.706055 (0.706055)
Epoch: [19][300/616]	Loss 2.6352e-01 (2.7741e-01)	Acc 0.722656 (0.713481)
Epoch: [19][600/616]	Loss 2.8566e-01 (2.8061e-01)	Acc 0.697266 (0.710106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.727483)
Training Loss of Epoch 19: 0.28024162587111556
Training Acc of Epoch 19: 0.7105230564024391
Testing Acc of Epoch 19: 0.7274826086956522
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6485e-01 (2.6485e-01)	Acc 0.732422 (0.732422)
Epoch: [20][300/616]	Loss 2.7587e-01 (2.7621e-01)	Acc 0.710938 (0.713653)
Epoch: [20][600/616]	Loss 2.7045e-01 (2.7796e-01)	Acc 0.722656 (0.711349)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.698104)
Training Loss of Epoch 20: 0.2777445437704645
Training Acc of Epoch 20: 0.7115917174796748
Testing Acc of Epoch 20: 0.698104347826087
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.9387e-01 (2.9387e-01)	Acc 0.696289 (0.696289)
Epoch: [21][300/616]	Loss 4.3655e-01 (2.8548e-01)	Acc 0.334961 (0.702583)
Epoch: [21][600/616]	Loss 4.7347e-01 (3.7819e-01)	Acc 0.290039 (0.503349)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.323718 (0.321952)
Training Loss of Epoch 21: 0.3803021317332741
Training Acc of Epoch 21: 0.4986010543699187
Testing Acc of Epoch 21: 0.3219521739130435
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 4.6056e-01 (4.6056e-01)	Acc 0.331055 (0.331055)
Epoch: [22][300/616]	Loss 4.5644e-01 (4.6976e-01)	Acc 0.322266 (0.297586)
Epoch: [22][600/616]	Loss 4.4841e-01 (4.6225e-01)	Acc 0.337891 (0.316489)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.312500 (0.320861)
Training Loss of Epoch 22: 0.46201423750660287
Training Acc of Epoch 22: 0.3169064405487805
Testing Acc of Epoch 22: 0.32086086956521737
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 4.6838e-01 (4.6838e-01)	Acc 0.292969 (0.292969)
Epoch: [23][300/616]	Loss 4.3137e-01 (4.4822e-01)	Acc 0.374023 (0.352756)
Epoch: [23][600/616]	Loss 5.0041e-01 (4.6102e-01)	Acc 0.191406 (0.312664)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 23: 0.4619143734617931
Training Acc of Epoch 23: 0.31008479420731705
Testing Acc of Epoch 23: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.191406)
Epoch: [24][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.215820 (0.201126)
Epoch: [24][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.201412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 24: 0.500402429433373
Training Acc of Epoch 24: 0.2013941819105691
Testing Acc of Epoch 24: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.213867)
Epoch: [25][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.218750 (0.201055)
Epoch: [25][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 25: 0.5004024505615234
Training Acc of Epoch 25: 0.20146246189024392
Testing Acc of Epoch 25: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.202148)
Epoch: [26][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.220703 (0.202437)
Epoch: [26][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.201461)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 26: 0.5004024503676872
Training Acc of Epoch 26: 0.20144975863821138
Testing Acc of Epoch 26: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.203125)
Epoch: [27][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.201616)
Epoch: [27][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.225586 (0.201518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 27: 0.5004024502707691
Training Acc of Epoch 27: 0.20144658282520325
Testing Acc of Epoch 27: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.193359)
Epoch: [28][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.210938 (0.201107)
Epoch: [28][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201425)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 28: 0.500402450173851
Training Acc of Epoch 28: 0.2014529344512195
Testing Acc of Epoch 28: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.208008)
Epoch: [29][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.186523 (0.200792)
Epoch: [29][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.201331)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 29: 0.5004024502707691
Training Acc of Epoch 29: 0.20144658282520325
Testing Acc of Epoch 29: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.189453)
Epoch: [30][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.201013)
Epoch: [30][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 30: 0.5004024504646053
Training Acc of Epoch 30: 0.2014449949186992
Testing Acc of Epoch 30: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [31][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.201318)
Epoch: [31][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.201492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 31: 0.500402450173851
Training Acc of Epoch 31: 0.2014529344512195
Testing Acc of Epoch 31: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.212891 (0.212891)
Epoch: [32][300/616]	Loss 3.1875e+01 (3.6652e+00)	Acc 0.203125 (0.203190)
Epoch: [32][600/616]	Loss 3.1562e+01 (1.7771e+01)	Acc 0.210938 (0.202555)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 32: 18.09562013745308
Training Acc of Epoch 32: 0.20247237042682928
Testing Acc of Epoch 32: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [33][300/616]	Loss 3.2148e+01 (3.1930e+01)	Acc 0.196289 (0.201756)
Epoch: [33][600/616]	Loss 3.1523e+01 (3.1941e+01)	Acc 0.211914 (0.201481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 33: 31.94169207317073
Training Acc of Epoch 33: 0.2014576981707317
Testing Acc of Epoch 33: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [34][300/616]	Loss 3.2422e+01 (3.1915e+01)	Acc 0.189453 (0.202113)
Epoch: [34][600/616]	Loss 3.0859e+01 (3.1946e+01)	Acc 0.228516 (0.201346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 34: 31.94213668699187
Training Acc of Epoch 34: 0.20144658282520325
Testing Acc of Epoch 34: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [35][300/616]	Loss 3.1992e+01 (3.1930e+01)	Acc 0.200195 (0.201753)
Epoch: [35][600/616]	Loss 3.1562e+01 (3.1944e+01)	Acc 0.210938 (0.201404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 35: 31.94239075203252
Training Acc of Epoch 35: 0.201440231199187
Testing Acc of Epoch 35: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [36][300/616]	Loss 3.2031e+01 (3.1960e+01)	Acc 0.199219 (0.200997)
Epoch: [36][600/616]	Loss 3.2148e+01 (3.1946e+01)	Acc 0.196289 (0.201346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 36: 31.941755589430894
Training Acc of Epoch 36: 0.20145611026422763
Testing Acc of Epoch 36: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [37][300/616]	Loss 3.1719e+01 (3.1945e+01)	Acc 0.207031 (0.201380)
Epoch: [37][600/616]	Loss 3.2266e+01 (3.1941e+01)	Acc 0.193359 (0.201463)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 37: 31.941565040650406
Training Acc of Epoch 37: 0.20146087398373982
Testing Acc of Epoch 37: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.3203e+01 (3.3203e+01)	Acc 0.169922 (0.169922)
Epoch: [38][300/616]	Loss 3.2500e+01 (3.1969e+01)	Acc 0.187500 (0.200773)
Epoch: [38][600/616]	Loss 3.1484e+01 (3.1950e+01)	Acc 0.212891 (0.201245)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 38: 31.941501524390244
Training Acc of Epoch 38: 0.20146246189024392
Testing Acc of Epoch 38: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [39][300/616]	Loss 3.1445e+01 (3.1943e+01)	Acc 0.213867 (0.201415)
Epoch: [39][600/616]	Loss 3.2188e+01 (3.1945e+01)	Acc 0.195312 (0.201375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 39: 31.94169207317073
Training Acc of Epoch 39: 0.2014576981707317
Testing Acc of Epoch 39: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.1016e+01 (3.1016e+01)	Acc 0.224609 (0.224609)
Epoch: [40][300/616]	Loss 3.2773e+01 (3.1938e+01)	Acc 0.180664 (0.201551)
Epoch: [40][600/616]	Loss 3.2070e+01 (3.1945e+01)	Acc 0.198242 (0.201386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 40: 31.94188262195122
Training Acc of Epoch 40: 0.2014529344512195
Testing Acc of Epoch 40: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [41][300/616]	Loss 3.2695e+01 (3.1946e+01)	Acc 0.182617 (0.201360)
Epoch: [41][600/616]	Loss 3.1641e+01 (3.1942e+01)	Acc 0.208984 (0.201442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 41: 31.941565040650406
Training Acc of Epoch 41: 0.20146087398373982
Testing Acc of Epoch 41: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [42][300/616]	Loss 3.0898e+01 (3.1926e+01)	Acc 0.227539 (0.201860)
Epoch: [42][600/616]	Loss 3.1875e+01 (3.1938e+01)	Acc 0.203125 (0.201541)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 42: 31.941565040650406
Training Acc of Epoch 42: 0.20146087398373982
Testing Acc of Epoch 42: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.2266e+01 (3.2266e+01)	Acc 0.193359 (0.193359)
Epoch: [43][300/616]	Loss 3.1328e+01 (3.1968e+01)	Acc 0.216797 (0.200802)
Epoch: [43][600/616]	Loss 3.2188e+01 (3.1946e+01)	Acc 0.195312 (0.201362)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 43: 31.94188262195122
Training Acc of Epoch 43: 0.2014529344512195
Testing Acc of Epoch 43: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [44][300/616]	Loss 3.2031e+01 (3.1928e+01)	Acc 0.199219 (0.201788)
Epoch: [44][600/616]	Loss 3.1914e+01 (3.1940e+01)	Acc 0.202148 (0.201490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 44: 31.941819105691057
Training Acc of Epoch 44: 0.20145452235772357
Testing Acc of Epoch 44: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [45][300/616]	Loss 3.2188e+01 (3.1945e+01)	Acc 0.195312 (0.201373)
Epoch: [45][600/616]	Loss 3.0977e+01 (3.1942e+01)	Acc 0.225586 (0.201445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 45: 31.94169207317073
Training Acc of Epoch 45: 0.2014576981707317
Testing Acc of Epoch 45: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [46][300/616]	Loss 3.2188e+01 (3.1945e+01)	Acc 0.195312 (0.201370)
Epoch: [46][600/616]	Loss 3.2109e+01 (3.1940e+01)	Acc 0.197266 (0.201503)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 46: 31.94169207317073
Training Acc of Epoch 46: 0.2014576981707317
Testing Acc of Epoch 46: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [47][300/616]	Loss 3.1562e+01 (3.1896e+01)	Acc 0.210938 (0.202612)
Epoch: [47][600/616]	Loss 3.2617e+01 (3.1942e+01)	Acc 0.184570 (0.201461)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 47: 31.94188262195122
Training Acc of Epoch 47: 0.2014529344512195
Testing Acc of Epoch 47: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [48][300/616]	Loss 3.2500e+01 (3.1982e+01)	Acc 0.187500 (0.200439)
Epoch: [48][600/616]	Loss 3.1992e+01 (3.1947e+01)	Acc 0.200195 (0.201328)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 48: 31.943089430894307
Training Acc of Epoch 48: 0.20142276422764227
Testing Acc of Epoch 48: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.0938e+01 (3.0938e+01)	Acc 0.226562 (0.226562)
Epoch: [49][300/616]	Loss 3.1211e+01 (3.1914e+01)	Acc 0.219727 (0.202158)
Epoch: [49][600/616]	Loss 3.2734e+01 (3.1939e+01)	Acc 0.181641 (0.201518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 49: 31.941565040650406
Training Acc of Epoch 49: 0.20146087398373982
Testing Acc of Epoch 49: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [50][300/616]	Loss 3.1875e+01 (3.1968e+01)	Acc 0.203125 (0.200812)
Epoch: [50][600/616]	Loss 3.2500e+01 (3.1940e+01)	Acc 0.187500 (0.201490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 50: 31.94173448454074
Training Acc of Epoch 50: 0.20145611026422763
Testing Acc of Epoch 50: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [51][300/616]	Loss 3.2266e+01 (3.1926e+01)	Acc 0.193359 (0.201853)
Epoch: [51][600/616]	Loss 3.2070e+01 (3.1920e+01)	Acc 0.198242 (0.201832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 51: 31.910499278122817
Training Acc of Epoch 51: 0.2020801575203252
Testing Acc of Epoch 51: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [52][300/616]	Loss 3.1719e+01 (3.1931e+01)	Acc 0.207031 (0.201723)
Epoch: [52][600/616]	Loss 3.2344e+01 (3.1905e+01)	Acc 0.191406 (0.202365)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 52: 31.904344512195124
Training Acc of Epoch 52: 0.20239138719512195
Testing Acc of Epoch 52: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [53][300/616]	Loss 3.1250e+01 (3.1926e+01)	Acc 0.218750 (0.201853)
Epoch: [53][600/616]	Loss 3.1250e+01 (3.1904e+01)	Acc 0.218750 (0.202395)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 53: 31.904408028455286
Training Acc of Epoch 53: 0.2023897992886179
Testing Acc of Epoch 53: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [54][300/616]	Loss 3.1797e+01 (3.1863e+01)	Acc 0.205078 (0.203430)
Epoch: [54][600/616]	Loss 3.1797e+01 (3.1903e+01)	Acc 0.205078 (0.202420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 54: 31.90358231707317
Training Acc of Epoch 54: 0.20241044207317074
Testing Acc of Epoch 54: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [55][300/616]	Loss 3.1367e+01 (3.1890e+01)	Acc 0.215820 (0.202739)
Epoch: [55][600/616]	Loss 3.1914e+01 (3.1904e+01)	Acc 0.202148 (0.202399)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 55: 31.904344512195124
Training Acc of Epoch 55: 0.20239138719512195
Testing Acc of Epoch 55: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [56][300/616]	Loss 3.1797e+01 (3.1907e+01)	Acc 0.205078 (0.202324)
Epoch: [56][600/616]	Loss 3.1055e+01 (3.1903e+01)	Acc 0.223633 (0.202433)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 56: 31.90383638211382
Training Acc of Epoch 56: 0.20240409044715446
Testing Acc of Epoch 56: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 3.1328e+01 (3.1328e+01)	Acc 0.216797 (0.216797)
Epoch: [57][300/616]	Loss 3.2422e+01 (3.1865e+01)	Acc 0.189453 (0.203368)
Epoch: [57][600/616]	Loss 3.2305e+01 (3.1902e+01)	Acc 0.192383 (0.202451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 57: 31.90409044715447
Training Acc of Epoch 57: 0.2023977388211382
Testing Acc of Epoch 57: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [58][300/616]	Loss 3.1328e+01 (3.1921e+01)	Acc 0.216797 (0.201980)
Epoch: [58][600/616]	Loss 3.2500e+01 (3.1899e+01)	Acc 0.187500 (0.202517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 58: 31.90358231707317
Training Acc of Epoch 58: 0.20241044207317074
Testing Acc of Epoch 58: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 3.1367e+01 (3.1367e+01)	Acc 0.215820 (0.215820)
Epoch: [59][300/616]	Loss 3.1328e+01 (3.1916e+01)	Acc 0.216797 (0.202097)
Epoch: [59][600/616]	Loss 3.2188e+01 (3.1905e+01)	Acc 0.195312 (0.202365)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 59: 31.9047256097561
Training Acc of Epoch 59: 0.20238185975609757
Testing Acc of Epoch 59: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [60][300/616]	Loss 3.2578e+01 (3.1901e+01)	Acc 0.185547 (0.202470)
Epoch: [60][600/616]	Loss 3.1094e+01 (3.1908e+01)	Acc 0.222656 (0.202303)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 60: 31.904026930894307
Training Acc of Epoch 60: 0.20239932672764227
Testing Acc of Epoch 60: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 3.1172e+01 (3.1172e+01)	Acc 0.220703 (0.220703)
Epoch: [61][300/616]	Loss 3.1445e+01 (3.1906e+01)	Acc 0.213867 (0.202359)
Epoch: [61][600/616]	Loss 3.2031e+01 (3.1897e+01)	Acc 0.199219 (0.202571)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 61: 31.903645833333332
Training Acc of Epoch 61: 0.20240885416666668
Testing Acc of Epoch 61: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [62][300/616]	Loss 3.1797e+01 (3.1918e+01)	Acc 0.205078 (0.202048)
Epoch: [62][600/616]	Loss 3.2422e+01 (3.1910e+01)	Acc 0.189453 (0.202241)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 62: 31.904153963414632
Training Acc of Epoch 62: 0.20239615091463414
Testing Acc of Epoch 62: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [63][300/616]	Loss 3.2031e+01 (3.1909e+01)	Acc 0.199219 (0.202259)
Epoch: [63][600/616]	Loss 3.1016e+01 (3.1904e+01)	Acc 0.224609 (0.202352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201674)
Training Loss of Epoch 63: 31.902208777947155
Training Acc of Epoch 63: 0.20240091463414633
Testing Acc of Epoch 63: 0.20167391304347826
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [64][300/616]	Loss 3.2227e+01 (3.1879e+01)	Acc 0.194336 (0.202898)
Epoch: [64][600/616]	Loss 3.1719e+01 (3.1897e+01)	Acc 0.206055 (0.202441)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 64: 31.899153645833334
Training Acc of Epoch 64: 0.20238185975609757
Testing Acc of Epoch 64: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.1484e+01 (3.1484e+01)	Acc 0.212891 (0.212891)
Epoch: [65][300/616]	Loss 3.0781e+01 (3.1917e+01)	Acc 0.230469 (0.201925)
Epoch: [65][600/616]	Loss 3.2031e+01 (3.1898e+01)	Acc 0.199219 (0.202378)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201665)
Training Loss of Epoch 65: 31.898787633384146
Training Acc of Epoch 65: 0.20236756859756097
Testing Acc of Epoch 65: 0.20166521739130436
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.190430 (0.190430)
Epoch: [66][300/616]	Loss 3.0977e+01 (3.1897e+01)	Acc 0.225586 (0.202379)
Epoch: [66][600/616]	Loss 3.2070e+01 (3.1898e+01)	Acc 0.198242 (0.202345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201661)
Training Loss of Epoch 66: 31.896967892530487
Training Acc of Epoch 66: 0.2023707444105691
Testing Acc of Epoch 66: 0.2016608695652174
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [67][300/616]	Loss 3.0704e+01 (3.1884e+01)	Acc 0.231445 (0.202648)
Epoch: [67][600/616]	Loss 3.2422e+01 (3.1890e+01)	Acc 0.189453 (0.202496)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201657)
Training Loss of Epoch 67: 31.89401518038618
Training Acc of Epoch 67: 0.20239138719512195
Testing Acc of Epoch 67: 0.20165652173913043
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [68][300/616]	Loss 3.2188e+01 (3.1913e+01)	Acc 0.194336 (0.201860)
Epoch: [68][600/616]	Loss 3.1680e+01 (3.1893e+01)	Acc 0.208008 (0.202379)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201665)
Training Loss of Epoch 68: 31.89378017022358
Training Acc of Epoch 68: 0.20236280487804878
Testing Acc of Epoch 68: 0.20166521739130436
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.202148 (0.202148)
Epoch: [69][300/616]	Loss 3.1446e+01 (3.1887e+01)	Acc 0.212891 (0.202460)
Epoch: [69][600/616]	Loss 3.1875e+01 (3.1887e+01)	Acc 0.202148 (0.202483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201670)
Training Loss of Epoch 69: 31.892338351117886
Training Acc of Epoch 69: 0.20236280487804878
Testing Acc of Epoch 69: 0.2016695652173913
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [70][300/616]	Loss 3.2032e+01 (3.1870e+01)	Acc 0.198242 (0.202801)
Epoch: [70][600/616]	Loss 3.2032e+01 (3.1884e+01)	Acc 0.198242 (0.202423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201661)
Training Loss of Epoch 70: 31.886825139735773
Training Acc of Epoch 70: 0.20236439278455284
Testing Acc of Epoch 70: 0.2016608695652174
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.1562e+01 (3.1562e+01)	Acc 0.210938 (0.210938)
Epoch: [71][300/616]	Loss 3.2228e+01 (3.1866e+01)	Acc 0.192383 (0.202752)
Epoch: [71][600/616]	Loss 3.1329e+01 (3.1886e+01)	Acc 0.214844 (0.202213)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201596)
Training Loss of Epoch 71: 31.88177321519309
Training Acc of Epoch 71: 0.20232469512195123
Testing Acc of Epoch 71: 0.20159565217391304
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.1916e+01 (3.1916e+01)	Acc 0.199219 (0.199219)
Epoch: [72][300/616]	Loss 3.2735e+01 (3.1862e+01)	Acc 0.180664 (0.202629)
Epoch: [72][600/616]	Loss 3.2384e+01 (3.1870e+01)	Acc 0.188477 (0.202311)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201430)
Training Loss of Epoch 72: 31.869517758997475
Training Acc of Epoch 72: 0.2023024644308943
Testing Acc of Epoch 72: 0.2014304347826087
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [73][300/616]	Loss 3.1721e+01 (3.1882e+01)	Acc 0.203125 (0.201487)
Epoch: [73][600/616]	Loss 3.1642e+01 (3.1854e+01)	Acc 0.207031 (0.202033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201196)
Training Loss of Epoch 73: 31.850778911559562
Training Acc of Epoch 73: 0.20211032774390245
Testing Acc of Epoch 73: 0.20119565217391305
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.2502e+01 (3.2502e+01)	Acc 0.183594 (0.183594)
Epoch: [74][300/616]	Loss 3.2541e+01 (3.1796e+01)	Acc 0.182617 (0.202317)
Epoch: [74][600/616]	Loss 3.1606e+01 (3.1814e+01)	Acc 0.201172 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.190705 (0.199665)
Training Loss of Epoch 74: 31.81264417694836
Training Acc of Epoch 74: 0.20146881351626017
Testing Acc of Epoch 74: 0.19966521739130436
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 3.1216e+01 (3.1216e+01)	Acc 0.210938 (0.210938)
Epoch: [75][300/616]	Loss 3.1212e+01 (3.1745e+01)	Acc 0.216797 (0.200815)
Epoch: [75][600/616]	Loss 3.1799e+01 (3.1772e+01)	Acc 0.201172 (0.200420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.190705 (0.199774)
Training Loss of Epoch 75: 31.76917725849927
Training Acc of Epoch 75: 0.20050971798780487
Testing Acc of Epoch 75: 0.19977391304347827
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.1487e+01 (3.1487e+01)	Acc 0.207031 (0.207031)
Epoch: [76][300/616]	Loss 3.2072e+01 (3.1778e+01)	Acc 0.195312 (0.200776)
Epoch: [76][600/616]	Loss 3.2111e+01 (3.1788e+01)	Acc 0.194336 (0.200584)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.190705 (0.200274)
Training Loss of Epoch 76: 31.785664153680568
Training Acc of Epoch 76: 0.20065580538617886
Testing Acc of Epoch 76: 0.20027391304347827
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 3.2111e+01 (3.2111e+01)	Acc 0.193359 (0.193359)
Epoch: [77][300/616]	Loss 3.2502e+01 (3.1777e+01)	Acc 0.184570 (0.201347)
Epoch: [77][600/616]	Loss 3.1918e+01 (3.1784e+01)	Acc 0.195312 (0.201143)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.189103 (0.199983)
Training Loss of Epoch 77: 31.790113883289866
Training Acc of Epoch 77: 0.20097973831300814
Testing Acc of Epoch 77: 0.19998260869565218
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 3.1840e+01 (3.1840e+01)	Acc 0.198242 (0.198242)
Epoch: [78][300/616]	Loss 3.1216e+01 (3.1754e+01)	Acc 0.210938 (0.200114)
Epoch: [78][600/616]	Loss 3.1801e+01 (3.1728e+01)	Acc 0.196289 (0.199519)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.189103 (0.198513)
Training Loss of Epoch 78: 31.729604345802368
Training Acc of Epoch 78: 0.1994728150406504
Testing Acc of Epoch 78: 0.19851304347826088
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.2346e+01 (3.2346e+01)	Acc 0.190430 (0.190430)
Epoch: [79][300/616]	Loss 3.1956e+01 (3.1705e+01)	Acc 0.197266 (0.200095)
Epoch: [79][600/616]	Loss 3.1221e+01 (3.1701e+01)	Acc 0.202148 (0.198483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.190705 (0.195243)
Training Loss of Epoch 79: 31.695113679839345
Training Acc of Epoch 79: 0.1985550050813008
Testing Acc of Epoch 79: 0.19524347826086957
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.1179e+01 (3.1179e+01)	Acc 0.207031 (0.207031)
Epoch: [80][300/616]	Loss 3.1063e+01 (3.1644e+01)	Acc 0.208984 (0.195617)
Epoch: [80][600/616]	Loss 3.0986e+01 (3.1564e+01)	Acc 0.208008 (0.194898)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.189103 (0.191752)
Training Loss of Epoch 80: 31.559617211566707
Training Acc of Epoch 80: 0.19485200711382114
Testing Acc of Epoch 80: 0.19175217391304347
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.1693e+01 (3.1693e+01)	Acc 0.187500 (0.187500)
Epoch: [81][300/616]	Loss 3.0793e+01 (3.1323e+01)	Acc 0.211914 (0.189920)
Epoch: [81][600/616]	Loss 3.1125e+01 (3.1248e+01)	Acc 0.169922 (0.189484)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.169872 (0.178996)
Training Loss of Epoch 81: 31.240952415776448
Training Acc of Epoch 81: 0.18931021341463414
Testing Acc of Epoch 81: 0.17899565217391306
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 3.1829e+01 (3.1829e+01)	Acc 0.156250 (0.156250)
Epoch: [82][300/616]	Loss 3.0701e+01 (3.0847e+01)	Acc 0.171875 (0.183289)
Epoch: [82][600/616]	Loss 3.1525e+01 (3.0770e+01)	Acc 0.141602 (0.183707)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.199000)
Training Loss of Epoch 82: 30.771903945178522
Training Acc of Epoch 82: 0.1839065675813008
Testing Acc of Epoch 82: 0.199
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 3.1207e+01 (3.1207e+01)	Acc 0.167969 (0.167969)
Epoch: [83][300/616]	Loss 3.0197e+01 (3.0499e+01)	Acc 0.175781 (0.181300)
Epoch: [83][600/616]	Loss 2.9776e+01 (3.0468e+01)	Acc 0.179688 (0.181775)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.199583)
Training Loss of Epoch 83: 30.46653422844119
Training Acc of Epoch 83: 0.18166920731707317
Testing Acc of Epoch 83: 0.19958260869565217
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 3.0095e+01 (3.0095e+01)	Acc 0.206055 (0.206055)
Epoch: [84][300/616]	Loss 3.0509e+01 (3.0426e+01)	Acc 0.178711 (0.181907)
Epoch: [84][600/616]	Loss 3.0801e+01 (3.0282e+01)	Acc 0.158203 (0.181321)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.157051 (0.164243)
Training Loss of Epoch 84: 30.27293697140081
Training Acc of Epoch 84: 0.1814040269308943
Testing Acc of Epoch 84: 0.16424347826086957
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.9231e+01 (2.9231e+01)	Acc 0.180664 (0.180664)
Epoch: [85][300/616]	Loss 2.8512e+01 (2.9927e+01)	Acc 0.161133 (0.179879)
Epoch: [85][600/616]	Loss 3.0292e+01 (2.9891e+01)	Acc 0.149414 (0.182508)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.163462 (0.160178)
Training Loss of Epoch 85: 29.88971085587168
Training Acc of Epoch 85: 0.18269817073170733
Testing Acc of Epoch 85: 0.16017826086956521
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.9398e+01 (2.9398e+01)	Acc 0.162109 (0.162109)
Epoch: [86][300/616]	Loss 2.7502e+01 (2.7188e+01)	Acc 0.174805 (0.180667)
Epoch: [86][600/616]	Loss 2.1100e+01 (2.4435e+01)	Acc 0.248047 (0.207486)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.241987 (0.238517)
Training Loss of Epoch 86: 24.3575047717831
Training Acc of Epoch 86: 0.2083349212398374
Testing Acc of Epoch 86: 0.23851739130434782
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.0761e+01 (2.0761e+01)	Acc 0.245117 (0.245117)
Epoch: [87][300/616]	Loss 3.1317e+01 (2.1362e+01)	Acc 0.208008 (0.239018)
Epoch: [87][600/616]	Loss 3.2500e+01 (2.6635e+01)	Acc 0.187500 (0.220417)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 87: 26.756897757305364
Training Acc of Epoch 87: 0.2199631605691057
Testing Acc of Epoch 87: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [88][300/616]	Loss 3.1794e+01 (3.1918e+01)	Acc 0.205078 (0.201918)
Epoch: [88][600/616]	Loss 3.1593e+01 (3.1890e+01)	Acc 0.208008 (0.202417)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 88: 31.890148745901218
Training Acc of Epoch 88: 0.20239297510162602
Testing Acc of Epoch 88: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.1498e+01 (3.1498e+01)	Acc 0.210938 (0.210938)
Epoch: [89][300/616]	Loss 3.1137e+01 (3.1834e+01)	Acc 0.216797 (0.201830)
Epoch: [89][600/616]	Loss 3.1521e+01 (3.1682e+01)	Acc 0.198242 (0.202353)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 89: 31.671918772875777
Training Acc of Epoch 89: 0.20239297510162602
Testing Acc of Epoch 89: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 3.0356e+01 (3.0356e+01)	Acc 0.227539 (0.227539)
Epoch: [90][300/616]	Loss 3.1170e+01 (3.1389e+01)	Acc 0.207031 (0.203203)
Epoch: [90][600/616]	Loss 3.1982e+01 (3.1257e+01)	Acc 0.174805 (0.202400)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 90: 31.2502578053048
Training Acc of Epoch 90: 0.2024025025406504
Testing Acc of Epoch 90: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.9838e+01 (2.9838e+01)	Acc 0.233398 (0.233398)
Epoch: [91][300/616]	Loss 1.2617e+01 (2.7077e+01)	Acc 0.186523 (0.201237)
Epoch: [91][600/616]	Loss 1.2115e+01 (1.9688e+01)	Acc 0.148438 (0.189117)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.182692 (0.179817)
Training Loss of Epoch 91: 19.520870999592105
Training Acc of Epoch 91: 0.1887766768292683
Testing Acc of Epoch 91: 0.17981739130434782
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 1.2753e+01 (1.2753e+01)	Acc 0.172852 (0.172852)
Epoch: [92][300/616]	Loss 1.2103e+01 (1.3634e+01)	Acc 0.190430 (0.180028)
Epoch: [92][600/616]	Loss 1.2521e+01 (1.3965e+01)	Acc 0.178711 (0.179488)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.185952)
Training Loss of Epoch 92: 13.927014958761571
Training Acc of Epoch 92: 0.17977165904471545
Testing Acc of Epoch 92: 0.18595217391304347
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 1.2330e+01 (1.2330e+01)	Acc 0.175781 (0.175781)
Epoch: [93][300/616]	Loss 1.2381e+01 (1.5473e+01)	Acc 0.178711 (0.180116)
Epoch: [93][600/616]	Loss 3.1505e+01 (1.5214e+01)	Acc 0.189453 (0.182526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.171474 (0.178983)
Training Loss of Epoch 93: 15.179297681358772
Training Acc of Epoch 93: 0.18222021087398374
Testing Acc of Epoch 93: 0.17898260869565216
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 1.1676e+01 (1.1676e+01)	Acc 0.174805 (0.174805)
Epoch: [94][300/616]	Loss 3.1911e+01 (1.6145e+01)	Acc 0.176758 (0.191984)
Epoch: [94][600/616]	Loss 3.1219e+01 (1.6381e+01)	Acc 0.197266 (0.192194)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.190283)
Training Loss of Epoch 94: 16.40772003545994
Training Acc of Epoch 94: 0.1922923018292683
Testing Acc of Epoch 94: 0.19028260869565217
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 1.2397e+01 (1.2397e+01)	Acc 0.184570 (0.184570)
Epoch: [95][300/616]	Loss 3.0819e+01 (2.0765e+01)	Acc 0.210938 (0.198099)
Epoch: [95][600/616]	Loss 1.2081e+01 (2.0838e+01)	Acc 0.206055 (0.200553)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.203378)
Training Loss of Epoch 95: 20.832405315182072
Training Acc of Epoch 95: 0.20036204268292682
Testing Acc of Epoch 95: 0.20337826086956523
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 1.2313e+01 (1.2313e+01)	Acc 0.184570 (0.184570)
Epoch: [96][300/616]	Loss 1.2170e+01 (2.4097e+01)	Acc 0.212891 (0.202651)
Epoch: [96][600/616]	Loss 3.1663e+01 (2.5159e+01)	Acc 0.196289 (0.203744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 96: 25.119143127813572
Training Acc of Epoch 96: 0.20364900914634146
Testing Acc of Epoch 96: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.1118e+01 (3.1118e+01)	Acc 0.209961 (0.209961)
Epoch: [97][300/616]	Loss 3.1709e+01 (2.8592e+01)	Acc 0.195312 (0.201925)
Epoch: [97][600/616]	Loss 3.2183e+01 (2.9175e+01)	Acc 0.181641 (0.202064)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 97: 29.167212677001952
Training Acc of Epoch 97: 0.20203728404471544
Testing Acc of Epoch 97: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.1090e+01 (3.1090e+01)	Acc 0.208984 (0.208984)
Epoch: [98][300/616]	Loss 3.1556e+01 (2.9806e+01)	Acc 0.199219 (0.201772)
Epoch: [98][600/616]	Loss 3.1458e+01 (3.0104e+01)	Acc 0.201172 (0.202218)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 98: 30.137419289689724
Training Acc of Epoch 98: 0.2021357342479675
Testing Acc of Epoch 98: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.1409e+01 (3.1409e+01)	Acc 0.203125 (0.203125)
Epoch: [99][300/616]	Loss 3.1796e+01 (3.0677e+01)	Acc 0.193359 (0.202463)
Epoch: [99][600/616]	Loss 3.1390e+01 (3.0735e+01)	Acc 0.204102 (0.201908)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 99: 30.75338960042814
Training Acc of Epoch 99: 0.2018530868902439
Testing Acc of Epoch 99: 0.2011695652173913
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0285e-01 (5.0285e-01)	Acc 0.197266 (0.197266)
Epoch: [0][300/616]	Loss 2.8380e-01 (3.1974e-01)	Acc 0.709961 (0.647257)
Epoch: [0][600/616]	Loss 2.6818e-01 (2.9766e-01)	Acc 0.711914 (0.680505)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.723300)
Training Loss of Epoch 0: 0.29711723332482626
Training Acc of Epoch 0: 0.6813151041666666
Testing Acc of Epoch 0: 0.7233
Model with the best training loss saved! The loss is 0.29711723332482626
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6111e-01 (2.6111e-01)	Acc 0.722656 (0.722656)
Epoch: [1][300/616]	Loss 2.5751e-01 (2.7662e-01)	Acc 0.737305 (0.711804)
Epoch: [1][600/616]	Loss 2.7498e-01 (2.7599e-01)	Acc 0.719727 (0.713125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.724104)
Training Loss of Epoch 1: 0.2759567722799332
Training Acc of Epoch 1: 0.7131383384146341
Testing Acc of Epoch 1: 0.7241043478260869
Model with the best training loss saved! The loss is 0.2759567722799332
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.5286e-01 (2.5286e-01)	Acc 0.735352 (0.735352)
Epoch: [2][300/616]	Loss 2.7867e-01 (2.7097e-01)	Acc 0.698242 (0.718237)
Epoch: [2][600/616]	Loss 2.7554e-01 (2.7257e-01)	Acc 0.701172 (0.716709)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.723226)
Training Loss of Epoch 2: 0.2725335190208947
Training Acc of Epoch 2: 0.716820693597561
Testing Acc of Epoch 2: 0.7232260869565217
Model with the best training loss saved! The loss is 0.2725335190208947
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.6185e-01 (2.6185e-01)	Acc 0.728516 (0.728516)
Epoch: [3][300/616]	Loss 2.5216e-01 (2.7186e-01)	Acc 0.751953 (0.718448)
Epoch: [3][600/616]	Loss 2.6142e-01 (2.7346e-01)	Acc 0.722656 (0.716292)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.666667 (0.676617)
Training Loss of Epoch 3: 0.2733634263519349
Training Acc of Epoch 3: 0.7164078379065041
Testing Acc of Epoch 3: 0.6766173913043478
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 3.3314e-01 (3.3314e-01)	Acc 0.650391 (0.650391)
Epoch: [4][300/616]	Loss 3.1332e-01 (2.7707e-01)	Acc 0.688477 (0.712028)
Epoch: [4][600/616]	Loss 2.7911e-01 (2.7674e-01)	Acc 0.702148 (0.712866)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.715283)
Training Loss of Epoch 4: 0.2767218456035707
Training Acc of Epoch 4: 0.712817581300813
Testing Acc of Epoch 4: 0.7152826086956522
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.7409e-01 (2.7409e-01)	Acc 0.711914 (0.711914)
Epoch: [5][300/616]	Loss 2.8827e-01 (2.7670e-01)	Acc 0.704102 (0.714370)
Epoch: [5][600/616]	Loss 2.6958e-01 (2.7603e-01)	Acc 0.729492 (0.714270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.718622)
Training Loss of Epoch 5: 0.27652486603919085
Training Acc of Epoch 5: 0.713719512195122
Testing Acc of Epoch 5: 0.7186217391304348
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.7619e-01 (2.7619e-01)	Acc 0.721680 (0.721680)
Epoch: [6][300/616]	Loss 2.7045e-01 (2.7737e-01)	Acc 0.701172 (0.713102)
Epoch: [6][600/616]	Loss 2.7592e-01 (2.7699e-01)	Acc 0.699219 (0.713389)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.721613)
Training Loss of Epoch 6: 0.27687640163472027
Training Acc of Epoch 6: 0.7134908536585366
Testing Acc of Epoch 6: 0.7216130434782608
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.6660e-01 (2.6660e-01)	Acc 0.722656 (0.722656)
Epoch: [7][300/616]	Loss 2.8807e-01 (2.8008e-01)	Acc 0.696289 (0.710204)
Epoch: [7][600/616]	Loss 2.8313e-01 (2.7801e-01)	Acc 0.718750 (0.712099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.692308 (0.700596)
Training Loss of Epoch 7: 0.27830278439735007
Training Acc of Epoch 7: 0.7118140243902439
Testing Acc of Epoch 7: 0.7005956521739131
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 3.1037e-01 (3.1037e-01)	Acc 0.678711 (0.678711)
Epoch: [8][300/616]	Loss 2.8323e-01 (2.7720e-01)	Acc 0.698242 (0.713676)
Epoch: [8][600/616]	Loss 2.4990e-01 (2.7667e-01)	Acc 0.740234 (0.714317)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.727043)
Training Loss of Epoch 8: 0.27658339658888376
Training Acc of Epoch 8: 0.7142959222560976
Testing Acc of Epoch 8: 0.7270434782608696
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5967e-01 (2.5967e-01)	Acc 0.720703 (0.720703)
Epoch: [9][300/616]	Loss 2.6455e-01 (2.7882e-01)	Acc 0.716797 (0.711181)
Epoch: [9][600/616]	Loss 2.8933e-01 (2.7795e-01)	Acc 0.707031 (0.711825)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.722661)
Training Loss of Epoch 9: 0.27804145609460224
Training Acc of Epoch 9: 0.71186324949187
Testing Acc of Epoch 9: 0.7226608695652174
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5708e-01 (2.5708e-01)	Acc 0.748047 (0.748047)
Epoch: [10][300/616]	Loss 2.8587e-01 (2.7790e-01)	Acc 0.708008 (0.711392)
Epoch: [10][600/616]	Loss 2.8104e-01 (2.7807e-01)	Acc 0.705078 (0.712169)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.663462 (0.671135)
Training Loss of Epoch 10: 0.2781909266623055
Training Acc of Epoch 10: 0.7119744029471544
Testing Acc of Epoch 10: 0.6711347826086956
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 3.1481e-01 (3.1481e-01)	Acc 0.670898 (0.670898)
Epoch: [11][300/616]	Loss 2.7209e-01 (2.7811e-01)	Acc 0.720703 (0.711398)
Epoch: [11][600/616]	Loss 2.5888e-01 (2.7759e-01)	Acc 0.741211 (0.712046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.720013)
Training Loss of Epoch 11: 0.27761453394967367
Training Acc of Epoch 11: 0.712107787093496
Testing Acc of Epoch 11: 0.7200130434782609
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6951e-01 (2.6951e-01)	Acc 0.727539 (0.727539)
Epoch: [12][300/616]	Loss 2.7943e-01 (2.7942e-01)	Acc 0.709961 (0.710308)
Epoch: [12][600/616]	Loss 3.0164e-01 (2.7798e-01)	Acc 0.676758 (0.712280)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.668269 (0.678874)
Training Loss of Epoch 12: 0.2780363737325358
Training Acc of Epoch 12: 0.7122538744918699
Testing Acc of Epoch 12: 0.6788739130434782
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 3.0877e-01 (3.0877e-01)	Acc 0.662109 (0.662109)
Epoch: [13][300/616]	Loss 2.6899e-01 (2.7876e-01)	Acc 0.728516 (0.711343)
Epoch: [13][600/616]	Loss 2.8459e-01 (2.7774e-01)	Acc 0.698242 (0.712299)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.724570)
Training Loss of Epoch 13: 0.2778077145902122
Training Acc of Epoch 13: 0.7121379573170732
Testing Acc of Epoch 13: 0.7245695652173914
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6276e-01 (2.6276e-01)	Acc 0.723633 (0.723633)
Epoch: [14][300/616]	Loss 2.6555e-01 (2.7841e-01)	Acc 0.722656 (0.711982)
Epoch: [14][600/616]	Loss 2.8207e-01 (2.7843e-01)	Acc 0.685547 (0.711817)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.680030)
Training Loss of Epoch 14: 0.27850341397087747
Training Acc of Epoch 14: 0.7117632113821138
Testing Acc of Epoch 14: 0.6800304347826087
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 3.0961e-01 (3.0961e-01)	Acc 0.684570 (0.684570)
Epoch: [15][300/616]	Loss 3.0936e-01 (2.7749e-01)	Acc 0.687500 (0.713066)
Epoch: [15][600/616]	Loss 3.2478e-01 (2.7714e-01)	Acc 0.680664 (0.712936)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.715513)
Training Loss of Epoch 15: 0.2772083836357768
Training Acc of Epoch 15: 0.7129858993902439
Testing Acc of Epoch 15: 0.7155130434782608
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.7309e-01 (2.7309e-01)	Acc 0.731445 (0.731445)
Epoch: [16][300/616]	Loss 2.8949e-01 (2.7831e-01)	Acc 0.695312 (0.711648)
Epoch: [16][600/616]	Loss 2.5814e-01 (2.7825e-01)	Acc 0.732422 (0.712343)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.726674)
Training Loss of Epoch 16: 0.2780122568209966
Training Acc of Epoch 16: 0.7125079395325203
Testing Acc of Epoch 16: 0.7266739130434783
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.5248e-01 (2.5248e-01)	Acc 0.739258 (0.739258)
Epoch: [17][300/616]	Loss 2.8446e-01 (2.7684e-01)	Acc 0.715820 (0.713007)
Epoch: [17][600/616]	Loss 2.7762e-01 (2.7743e-01)	Acc 0.716797 (0.712845)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.726065)
Training Loss of Epoch 17: 0.2773740052449994
Training Acc of Epoch 17: 0.7128922129065041
Testing Acc of Epoch 17: 0.7260652173913044
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6684e-01 (2.6684e-01)	Acc 0.723633 (0.723633)
Epoch: [18][300/616]	Loss 3.1294e-01 (2.8341e-01)	Acc 0.675781 (0.706674)
Epoch: [18][600/616]	Loss 2.7979e-01 (2.7844e-01)	Acc 0.722656 (0.711908)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.703230)
Training Loss of Epoch 18: 0.2785064179965151
Training Acc of Epoch 18: 0.7118981834349594
Testing Acc of Epoch 18: 0.7032304347826087
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 3.0431e-01 (3.0431e-01)	Acc 0.680664 (0.680664)
Epoch: [19][300/616]	Loss 2.8014e-01 (2.7699e-01)	Acc 0.706055 (0.713647)
Epoch: [19][600/616]	Loss 2.7661e-01 (2.7648e-01)	Acc 0.717773 (0.713937)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.725761)
Training Loss of Epoch 19: 0.2765273716633882
Training Acc of Epoch 19: 0.713916412601626
Testing Acc of Epoch 19: 0.7257608695652173
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6624e-01 (2.6624e-01)	Acc 0.727539 (0.727539)
Epoch: [20][300/616]	Loss 2.6945e-01 (2.7750e-01)	Acc 0.726562 (0.713160)
Epoch: [20][600/616]	Loss 3.6580e-01 (2.8672e-01)	Acc 0.539062 (0.701242)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.684295 (0.694830)
Training Loss of Epoch 20: 0.2876642085672394
Training Acc of Epoch 20: 0.6997411712398374
Testing Acc of Epoch 20: 0.6948304347826086
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 3.1570e-01 (3.1570e-01)	Acc 0.690430 (0.690430)
Epoch: [21][300/616]	Loss 5.0156e-01 (4.4011e-01)	Acc 0.181641 (0.378439)
Epoch: [21][600/616]	Loss 4.9061e-01 (4.7031e-01)	Acc 0.251953 (0.291094)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.306090 (0.306891)
Training Loss of Epoch 21: 0.47066969706760187
Training Acc of Epoch 21: 0.2910712017276423
Testing Acc of Epoch 21: 0.3068913043478261
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 4.8112e-01 (4.8112e-01)	Acc 0.316406 (0.316406)
Epoch: [22][300/616]	Loss 5.0040e-01 (4.9589e-01)	Acc 0.222656 (0.230394)
Epoch: [22][600/616]	Loss 5.0040e-01 (4.9814e-01)	Acc 0.220703 (0.215633)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 22: 0.49819609687580324
Training Acc of Epoch 22: 0.21525978150406505
Testing Acc of Epoch 22: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [23][300/616]	Loss 5.0040e-01 (5.0043e-01)	Acc 0.194336 (0.201915)
Epoch: [23][600/616]	Loss 5.0040e-01 (5.0042e-01)	Acc 0.201172 (0.201589)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 23: 0.5004153558393803
Training Acc of Epoch 23: 0.201684768800813
Testing Acc of Epoch 23: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.178711 (0.178711)
Epoch: [24][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.188477 (0.202256)
Epoch: [24][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.172852 (0.201728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 24: 0.5004024485262429
Training Acc of Epoch 24: 0.20174352134146342
Testing Acc of Epoch 24: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.182617 (0.182617)
Epoch: [25][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.233398 (0.201100)
Epoch: [25][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.201424)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 25: 0.500402450173851
Training Acc of Epoch 25: 0.2014529344512195
Testing Acc of Epoch 25: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201172)
Epoch: [26][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.201159)
Epoch: [26][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.201440)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 26: 0.5004024504646053
Training Acc of Epoch 26: 0.20145928607723576
Testing Acc of Epoch 26: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [27][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201607)
Epoch: [27][600/616]	Loss 3.2267e+01 (1.4146e+00)	Acc 0.190430 (0.202309)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 27: 2.107659319939652
Training Acc of Epoch 27: 0.20227229420731707
Testing Acc of Epoch 27: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 3.1485e+01 (3.1485e+01)	Acc 0.211914 (0.211914)
Epoch: [28][300/616]	Loss 3.2227e+01 (3.1946e+01)	Acc 0.193359 (0.200919)
Epoch: [28][600/616]	Loss 3.2148e+01 (3.1937e+01)	Acc 0.196289 (0.201351)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 28: 31.934080474357295
Training Acc of Epoch 28: 0.20143705538617887
Testing Acc of Epoch 28: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 3.2578e+01 (3.2578e+01)	Acc 0.185547 (0.185547)
Epoch: [29][300/616]	Loss 3.2383e+01 (3.1919e+01)	Acc 0.190430 (0.202015)
Epoch: [29][600/616]	Loss 3.1758e+01 (3.1940e+01)	Acc 0.206055 (0.201505)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 29: 31.942265307418698
Training Acc of Epoch 29: 0.201440231199187
Testing Acc of Epoch 29: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [30][300/616]	Loss 3.2344e+01 (3.1940e+01)	Acc 0.191406 (0.201509)
Epoch: [30][600/616]	Loss 3.1094e+01 (3.1940e+01)	Acc 0.222656 (0.201490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 30: 31.94169207317073
Training Acc of Epoch 30: 0.2014576981707317
Testing Acc of Epoch 30: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [31][300/616]	Loss 3.2266e+01 (3.1953e+01)	Acc 0.193359 (0.201169)
Epoch: [31][600/616]	Loss 3.1484e+01 (3.1946e+01)	Acc 0.212891 (0.201357)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 31: 31.941819105691057
Training Acc of Epoch 31: 0.20145452235772357
Testing Acc of Epoch 31: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [32][300/616]	Loss 3.2266e+01 (3.1956e+01)	Acc 0.193359 (0.201100)
Epoch: [32][600/616]	Loss 3.1680e+01 (3.1944e+01)	Acc 0.208008 (0.201394)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 32: 31.94289888211382
Training Acc of Epoch 32: 0.20142752794715446
Testing Acc of Epoch 32: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [33][300/616]	Loss 3.2227e+01 (3.1950e+01)	Acc 0.194336 (0.201256)
Epoch: [33][600/616]	Loss 3.2461e+01 (3.1938e+01)	Acc 0.188477 (0.201560)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 33: 31.941819105691057
Training Acc of Epoch 33: 0.20145452235772357
Testing Acc of Epoch 33: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [34][300/616]	Loss 3.1562e+01 (3.1959e+01)	Acc 0.210938 (0.201013)
Epoch: [34][600/616]	Loss 3.2109e+01 (3.1938e+01)	Acc 0.197266 (0.201546)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 34: 31.942073170731707
Training Acc of Epoch 34: 0.20144817073170732
Testing Acc of Epoch 34: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [35][300/616]	Loss 3.1523e+01 (3.1945e+01)	Acc 0.211914 (0.201386)
Epoch: [35][600/616]	Loss 3.2148e+01 (3.1939e+01)	Acc 0.196289 (0.201537)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 35: 31.942073170731707
Training Acc of Epoch 35: 0.20144817073170732
Testing Acc of Epoch 35: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [36][300/616]	Loss 3.2773e+01 (3.1945e+01)	Acc 0.180664 (0.201363)
Epoch: [36][600/616]	Loss 3.1719e+01 (3.1939e+01)	Acc 0.207031 (0.201520)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 36: 31.942263719512194
Training Acc of Epoch 36: 0.20144340701219512
Testing Acc of Epoch 36: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [37][300/616]	Loss 3.2891e+01 (3.1946e+01)	Acc 0.177734 (0.201344)
Epoch: [37][600/616]	Loss 3.2031e+01 (3.1939e+01)	Acc 0.199219 (0.201515)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 37: 31.942327235772357
Training Acc of Epoch 37: 0.20144181910569106
Testing Acc of Epoch 37: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [38][300/616]	Loss 3.1797e+01 (3.1916e+01)	Acc 0.205078 (0.202110)
Epoch: [38][600/616]	Loss 3.1328e+01 (3.1948e+01)	Acc 0.216797 (0.201289)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 38: 31.941501524390244
Training Acc of Epoch 38: 0.20146246189024392
Testing Acc of Epoch 38: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [39][300/616]	Loss 3.1953e+01 (3.1919e+01)	Acc 0.201172 (0.202035)
Epoch: [39][600/616]	Loss 3.1406e+01 (3.1940e+01)	Acc 0.214844 (0.201492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 39: 31.94264481707317
Training Acc of Epoch 39: 0.20143387957317074
Testing Acc of Epoch 39: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [40][300/616]	Loss 3.2383e+01 (3.1971e+01)	Acc 0.190430 (0.200718)
Epoch: [40][600/616]	Loss 3.1602e+01 (3.1941e+01)	Acc 0.209961 (0.201481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 40: 31.941819105691057
Training Acc of Epoch 40: 0.20145452235772357
Testing Acc of Epoch 40: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.1562e+01 (3.1562e+01)	Acc 0.210938 (0.210938)
Epoch: [41][300/616]	Loss 3.2188e+01 (3.1658e+01)	Acc 0.195312 (0.208446)
Epoch: [41][600/616]	Loss 3.1328e+01 (3.1666e+01)	Acc 0.216797 (0.208222)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 41: 31.676555819627715
Training Acc of Epoch 41: 0.2079633511178862
Testing Acc of Epoch 41: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [42][300/616]	Loss 3.1211e+01 (3.1915e+01)	Acc 0.219727 (0.202126)
Epoch: [42][600/616]	Loss 3.1758e+01 (3.1942e+01)	Acc 0.206055 (0.201445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 42: 31.942009654471544
Training Acc of Epoch 42: 0.20144975863821138
Testing Acc of Epoch 42: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [43][300/616]	Loss 3.1875e+01 (3.1932e+01)	Acc 0.203125 (0.201704)
Epoch: [43][600/616]	Loss 3.1797e+01 (3.1939e+01)	Acc 0.205078 (0.201521)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 43: 31.942263719512194
Training Acc of Epoch 43: 0.20144340701219512
Testing Acc of Epoch 43: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [44][300/616]	Loss 3.2578e+01 (3.1939e+01)	Acc 0.185547 (0.201513)
Epoch: [44][600/616]	Loss 3.1250e+01 (3.1944e+01)	Acc 0.218750 (0.201404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 44: 31.942708333333332
Training Acc of Epoch 44: 0.20143229166666668
Testing Acc of Epoch 44: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [45][300/616]	Loss 3.1055e+01 (3.1850e+01)	Acc 0.223633 (0.203057)
Epoch: [45][600/616]	Loss 3.1797e+01 (3.1928e+01)	Acc 0.205078 (0.201446)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 45: 31.927530391041824
Training Acc of Epoch 45: 0.2014767530487805
Testing Acc of Epoch 45: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.1523e+01 (3.1523e+01)	Acc 0.211914 (0.211914)
Epoch: [46][300/616]	Loss 3.1094e+01 (3.1948e+01)	Acc 0.222656 (0.201311)
Epoch: [46][600/616]	Loss 3.1953e+01 (3.1918e+01)	Acc 0.201172 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 46: 31.919776835868028
Training Acc of Epoch 46: 0.20146246189024392
Testing Acc of Epoch 46: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [47][300/616]	Loss 2.9827e+01 (3.1926e+01)	Acc 0.206055 (0.201081)
Epoch: [47][600/616]	Loss 3.2344e+01 (3.1861e+01)	Acc 0.191406 (0.202056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 47: 31.859519093211105
Training Acc of Epoch 47: 0.20213732215447155
Testing Acc of Epoch 47: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [48][300/616]	Loss 3.2383e+01 (3.1943e+01)	Acc 0.190430 (0.201422)
Epoch: [48][600/616]	Loss 3.2070e+01 (3.1945e+01)	Acc 0.198242 (0.201370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 48: 31.941755589430894
Training Acc of Epoch 48: 0.20145611026422763
Testing Acc of Epoch 48: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [49][300/616]	Loss 3.1562e+01 (3.1919e+01)	Acc 0.210938 (0.201989)
Epoch: [49][600/616]	Loss 3.2461e+01 (3.1943e+01)	Acc 0.188477 (0.201393)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 49: 31.940818724593495
Training Acc of Epoch 49: 0.20144817073170732
Testing Acc of Epoch 49: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.1289e+01 (3.1289e+01)	Acc 0.217773 (0.217773)
Epoch: [50][300/616]	Loss 3.1680e+01 (3.1966e+01)	Acc 0.208008 (0.200847)
Epoch: [50][600/616]	Loss 3.2227e+01 (3.1945e+01)	Acc 0.194336 (0.201357)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 50: 31.941828633130083
Training Acc of Epoch 50: 0.2014354674796748
Testing Acc of Epoch 50: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [51][300/616]	Loss 3.2656e+01 (3.1945e+01)	Acc 0.183594 (0.201331)
Epoch: [51][600/616]	Loss 3.0859e+01 (3.1940e+01)	Acc 0.228516 (0.201477)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 51: 31.940752826473577
Training Acc of Epoch 51: 0.20145452235772357
Testing Acc of Epoch 51: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.1172e+01 (3.1172e+01)	Acc 0.220703 (0.220703)
Epoch: [52][300/616]	Loss 3.0820e+01 (3.1939e+01)	Acc 0.229492 (0.201496)
Epoch: [52][600/616]	Loss 3.1875e+01 (3.1941e+01)	Acc 0.203125 (0.201455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 52: 31.94069248602642
Training Acc of Epoch 52: 0.20145134654471544
Testing Acc of Epoch 52: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [53][300/616]	Loss 3.1875e+01 (3.1924e+01)	Acc 0.203125 (0.201866)
Epoch: [53][600/616]	Loss 3.2812e+01 (3.1941e+01)	Acc 0.179688 (0.201442)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 53: 31.941006097560976
Training Acc of Epoch 53: 0.20144975863821138
Testing Acc of Epoch 53: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 3.2032e+01 (3.2032e+01)	Acc 0.198242 (0.198242)
Epoch: [54][300/616]	Loss 3.2773e+01 (3.1966e+01)	Acc 0.180664 (0.200796)
Epoch: [54][600/616]	Loss 3.2148e+01 (3.1943e+01)	Acc 0.196289 (0.201393)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 54: 31.94094893292683
Training Acc of Epoch 54: 0.201440231199187
Testing Acc of Epoch 54: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 3.3008e+01 (3.3008e+01)	Acc 0.174805 (0.174805)
Epoch: [55][300/616]	Loss 3.1992e+01 (3.1964e+01)	Acc 0.200195 (0.200812)
Epoch: [55][600/616]	Loss 3.2266e+01 (3.1944e+01)	Acc 0.193359 (0.201326)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 55: 31.940204998729676
Training Acc of Epoch 55: 0.2014259400406504
Testing Acc of Epoch 55: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 3.3164e+01 (3.3164e+01)	Acc 0.170898 (0.170898)
Epoch: [56][300/616]	Loss 3.2305e+01 (3.1956e+01)	Acc 0.192383 (0.201049)
Epoch: [56][600/616]	Loss 3.2227e+01 (3.1944e+01)	Acc 0.194336 (0.201346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 56: 31.94038760797764
Training Acc of Epoch 56: 0.201440231199187
Testing Acc of Epoch 56: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [57][300/616]	Loss 3.1914e+01 (3.1903e+01)	Acc 0.202148 (0.202346)
Epoch: [57][600/616]	Loss 3.2734e+01 (3.1936e+01)	Acc 0.181641 (0.201526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 57: 31.939318152947155
Training Acc of Epoch 57: 0.20144817073170732
Testing Acc of Epoch 57: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [58][300/616]	Loss 3.1992e+01 (3.1917e+01)	Acc 0.200195 (0.201733)
Epoch: [58][600/616]	Loss 3.0938e+01 (3.1927e+01)	Acc 0.226562 (0.201641)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 58: 31.935185685196544
Training Acc of Epoch 58: 0.20144975863821138
Testing Acc of Epoch 58: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [59][300/616]	Loss 3.1641e+01 (3.1964e+01)	Acc 0.208984 (0.200893)
Epoch: [59][600/616]	Loss 3.2109e+01 (3.1946e+01)	Acc 0.197266 (0.201346)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 59: 31.941885003810974
Training Acc of Epoch 59: 0.20144817073170732
Testing Acc of Epoch 59: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [60][300/616]	Loss 3.1562e+01 (3.1923e+01)	Acc 0.210938 (0.201918)
Epoch: [60][600/616]	Loss 3.1953e+01 (3.1942e+01)	Acc 0.201172 (0.201453)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 60: 31.942073170731707
Training Acc of Epoch 60: 0.20144817073170732
Testing Acc of Epoch 60: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 3.1523e+01 (3.1523e+01)	Acc 0.211914 (0.211914)
Epoch: [61][300/616]	Loss 3.2852e+01 (3.1968e+01)	Acc 0.178711 (0.200812)
Epoch: [61][600/616]	Loss 3.2891e+01 (3.1946e+01)	Acc 0.177734 (0.201359)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 61: 31.942200203252032
Training Acc of Epoch 61: 0.2014449949186992
Testing Acc of Epoch 61: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.2695e+01 (3.2695e+01)	Acc 0.182617 (0.182617)
Epoch: [62][300/616]	Loss 3.0977e+01 (3.1941e+01)	Acc 0.225586 (0.201470)
Epoch: [62][600/616]	Loss 3.2754e+01 (3.1941e+01)	Acc 0.179688 (0.201360)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 62: 31.93638546021004
Training Acc of Epoch 62: 0.2014449949186992
Testing Acc of Epoch 62: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 3.2717e+01 (3.2717e+01)	Acc 0.176758 (0.176758)
Epoch: [63][300/616]	Loss 3.2051e+01 (3.1244e+01)	Acc 0.197266 (0.200922)
Epoch: [63][600/616]	Loss 3.2290e+01 (3.1552e+01)	Acc 0.191406 (0.201458)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 63: 31.5599195433826
Training Acc of Epoch 63: 0.20144817073170732
Testing Acc of Epoch 63: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 3.2812e+01 (3.2812e+01)	Acc 0.179688 (0.179688)
Epoch: [64][300/616]	Loss 3.1797e+01 (3.1897e+01)	Acc 0.205078 (0.201603)
Epoch: [64][600/616]	Loss 3.2043e+01 (3.1917e+01)	Acc 0.196289 (0.201422)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 64: 31.914673552474355
Training Acc of Epoch 64: 0.2014576981707317
Testing Acc of Epoch 64: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.2243e+01 (3.2243e+01)	Acc 0.191406 (0.191406)
Epoch: [65][300/616]	Loss 3.1797e+01 (3.1914e+01)	Acc 0.204102 (0.201230)
Epoch: [65][600/616]	Loss 3.1055e+01 (3.1932e+01)	Acc 0.223633 (0.201084)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 65: 31.931173349396
Training Acc of Epoch 65: 0.2011131224593496
Testing Acc of Epoch 65: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 3.3242e+01 (3.3242e+01)	Acc 0.168945 (0.168945)
Epoch: [66][300/616]	Loss 3.1758e+01 (3.1894e+01)	Acc 0.206055 (0.202658)
Epoch: [66][600/616]	Loss 3.2109e+01 (3.1904e+01)	Acc 0.197266 (0.202412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 66: 31.904598577235774
Training Acc of Epoch 66: 0.2023850355691057
Testing Acc of Epoch 66: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [67][300/616]	Loss 3.1367e+01 (3.1805e+01)	Acc 0.215820 (0.202901)
Epoch: [67][600/616]	Loss 3.1758e+01 (3.1869e+01)	Acc 0.206055 (0.202298)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 67: 31.868973507144588
Training Acc of Epoch 67: 0.20230881605691056
Testing Acc of Epoch 67: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.1562e+01 (3.1562e+01)	Acc 0.210938 (0.210938)
Epoch: [68][300/616]	Loss 3.2422e+01 (3.1906e+01)	Acc 0.189453 (0.202350)
Epoch: [68][600/616]	Loss 3.2212e+01 (3.1836e+01)	Acc 0.180664 (0.202384)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.193910 (0.201678)
Training Loss of Epoch 68: 31.822863372554625
Training Acc of Epoch 68: 0.20240567835365852
Testing Acc of Epoch 68: 0.20167826086956522
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.1385e+01 (3.1385e+01)	Acc 0.199219 (0.199219)
Epoch: [69][300/616]	Loss 3.2536e+01 (3.1691e+01)	Acc 0.186523 (0.202256)
Epoch: [69][600/616]	Loss 3.2148e+01 (3.1771e+01)	Acc 0.196289 (0.202698)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 69: 31.779385006912356
Training Acc of Epoch 69: 0.20256446900406505
Testing Acc of Epoch 69: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.1523e+01 (3.1523e+01)	Acc 0.211914 (0.211914)
Epoch: [70][300/616]	Loss 3.1133e+01 (3.1921e+01)	Acc 0.221680 (0.201983)
Epoch: [70][600/616]	Loss 3.1367e+01 (3.1945e+01)	Acc 0.215820 (0.201364)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 70: 31.94162855691057
Training Acc of Epoch 70: 0.20145928607723576
Testing Acc of Epoch 70: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.0859e+01 (3.0859e+01)	Acc 0.228516 (0.228516)
Epoch: [71][300/616]	Loss 3.2617e+01 (3.1942e+01)	Acc 0.184570 (0.201448)
Epoch: [71][600/616]	Loss 3.1836e+01 (3.1947e+01)	Acc 0.204102 (0.201320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 71: 31.94162855691057
Training Acc of Epoch 71: 0.20145928607723576
Testing Acc of Epoch 71: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.2266e+01 (3.2266e+01)	Acc 0.193359 (0.193359)
Epoch: [72][300/616]	Loss 3.2461e+01 (3.1938e+01)	Acc 0.188477 (0.201542)
Epoch: [72][600/616]	Loss 3.1875e+01 (3.1945e+01)	Acc 0.203125 (0.201378)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 72: 31.942009654471544
Training Acc of Epoch 72: 0.20144975863821138
Testing Acc of Epoch 72: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [73][300/616]	Loss 3.2070e+01 (3.1950e+01)	Acc 0.198242 (0.201253)
Epoch: [73][600/616]	Loss 3.1602e+01 (3.1941e+01)	Acc 0.209961 (0.201468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 73: 31.942454268292682
Training Acc of Epoch 73: 0.20143864329268293
Testing Acc of Epoch 73: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [74][300/616]	Loss 3.1406e+01 (3.1935e+01)	Acc 0.214844 (0.201623)
Epoch: [74][600/616]	Loss 3.1719e+01 (3.1942e+01)	Acc 0.207031 (0.201451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 74: 31.94143800813008
Training Acc of Epoch 74: 0.20146404979674798
Testing Acc of Epoch 74: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 3.1289e+01 (3.1289e+01)	Acc 0.217773 (0.217773)
Epoch: [75][300/616]	Loss 3.2617e+01 (3.1973e+01)	Acc 0.184570 (0.200685)
Epoch: [75][600/616]	Loss 3.1758e+01 (3.1944e+01)	Acc 0.206055 (0.201391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 75: 31.941565040650406
Training Acc of Epoch 75: 0.20146087398373982
Testing Acc of Epoch 75: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [76][300/616]	Loss 3.1641e+01 (3.1918e+01)	Acc 0.208984 (0.202054)
Epoch: [76][600/616]	Loss 3.2188e+01 (3.1934e+01)	Acc 0.195312 (0.201641)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 76: 31.94213668699187
Training Acc of Epoch 76: 0.20144658282520325
Testing Acc of Epoch 76: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 3.0820e+01 (3.0820e+01)	Acc 0.229492 (0.229492)
Epoch: [77][300/616]	Loss 3.2070e+01 (3.1961e+01)	Acc 0.198242 (0.200964)
Epoch: [77][600/616]	Loss 3.1523e+01 (3.1950e+01)	Acc 0.211914 (0.201247)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 77: 31.942200203252032
Training Acc of Epoch 77: 0.2014449949186992
Testing Acc of Epoch 77: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [78][300/616]	Loss 3.1367e+01 (3.1978e+01)	Acc 0.215820 (0.200542)
Epoch: [78][600/616]	Loss 3.1484e+01 (3.1944e+01)	Acc 0.212891 (0.201412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 78: 31.942708333333332
Training Acc of Epoch 78: 0.20143229166666668
Testing Acc of Epoch 78: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.191406 (0.191406)
Epoch: [79][300/616]	Loss 3.1289e+01 (3.1932e+01)	Acc 0.217773 (0.201688)
Epoch: [79][600/616]	Loss 3.1484e+01 (3.1942e+01)	Acc 0.212891 (0.201448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 79: 31.941819105691057
Training Acc of Epoch 79: 0.20145452235772357
Testing Acc of Epoch 79: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [80][300/616]	Loss 3.2891e+01 (3.1973e+01)	Acc 0.177734 (0.200672)
Epoch: [80][600/616]	Loss 3.0767e+01 (3.1924e+01)	Acc 0.222656 (0.201411)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 80: 31.915374932638027
Training Acc of Epoch 80: 0.20144181910569106
Testing Acc of Epoch 80: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.1465e+01 (3.1465e+01)	Acc 0.199219 (0.199219)
Epoch: [81][300/616]	Loss 3.0485e+01 (3.0209e+01)	Acc 0.187500 (0.201714)
Epoch: [81][600/616]	Loss 2.6971e+01 (2.9557e+01)	Acc 0.203125 (0.201476)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 81: 29.499295028438414
Training Acc of Epoch 81: 0.20146087398373982
Testing Acc of Epoch 81: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.6850e+01 (2.6850e+01)	Acc 0.203125 (0.203125)
Epoch: [82][300/616]	Loss 2.6498e+01 (2.7511e+01)	Acc 0.193359 (0.202213)
Epoch: [82][600/616]	Loss 2.6603e+01 (2.7065e+01)	Acc 0.185547 (0.201534)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 82: 27.04798210578236
Training Acc of Epoch 82: 0.2014529344512195
Testing Acc of Epoch 82: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.6226e+01 (2.6226e+01)	Acc 0.192383 (0.192383)
Epoch: [83][300/616]	Loss 1.8106e+01 (2.6080e+01)	Acc 0.238281 (0.202827)
Epoch: [83][600/616]	Loss 2.3060e+01 (2.4224e+01)	Acc 0.200195 (0.202217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 83: 24.195205409352372
Training Acc of Epoch 83: 0.2022071900406504
Testing Acc of Epoch 83: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3022e+01 (2.3022e+01)	Acc 0.203125 (0.203125)
Epoch: [84][300/616]	Loss 2.1827e+01 (2.2629e+01)	Acc 0.226562 (0.201263)
Epoch: [84][600/616]	Loss 1.2262e+01 (2.2578e+01)	Acc 0.188477 (0.201399)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 84: 22.587534217524336
Training Acc of Epoch 84: 0.20149263211382115
Testing Acc of Epoch 84: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1721e+01 (2.1721e+01)	Acc 0.213867 (0.213867)
Epoch: [85][300/616]	Loss 2.3446e+01 (2.3177e+01)	Acc 0.208984 (0.201863)
Epoch: [85][600/616]	Loss 2.1836e+01 (2.3309e+01)	Acc 0.208984 (0.201268)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 85: 23.29231949472815
Training Acc of Epoch 85: 0.20136559959349593
Testing Acc of Epoch 85: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3556e+01 (2.3556e+01)	Acc 0.192383 (0.192383)
Epoch: [86][300/616]	Loss 2.3170e+01 (2.3324e+01)	Acc 0.203125 (0.200987)
Epoch: [86][600/616]	Loss 2.3805e+01 (2.3333e+01)	Acc 0.201172 (0.201468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 86: 23.33743384756693
Training Acc of Epoch 86: 0.20143387957317074
Testing Acc of Epoch 86: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.1917e+01 (2.1917e+01)	Acc 0.226562 (0.226562)
Epoch: [87][300/616]	Loss 2.3635e+01 (2.3348e+01)	Acc 0.202148 (0.201892)
Epoch: [87][600/616]	Loss 2.5713e+01 (2.4205e+01)	Acc 0.180664 (0.198605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 87: 24.260625443807463
Training Acc of Epoch 87: 0.1984152693089431
Testing Acc of Epoch 87: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.6291e+01 (2.6291e+01)	Acc 0.199219 (0.199219)
Epoch: [88][300/616]	Loss 1.2312e+01 (2.5606e+01)	Acc 0.202148 (0.194141)
Epoch: [88][600/616]	Loss 2.7113e+01 (2.5668e+01)	Acc 0.182617 (0.194121)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 88: 25.662061004328535
Training Acc of Epoch 88: 0.1940310594512195
Testing Acc of Epoch 88: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.6401e+01 (2.6401e+01)	Acc 0.201172 (0.201172)
Epoch: [89][300/616]	Loss 2.6059e+01 (2.5465e+01)	Acc 0.197266 (0.193810)
Epoch: [89][600/616]	Loss 2.6238e+01 (2.5602e+01)	Acc 0.208984 (0.194355)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 89: 25.623631530079415
Training Acc of Epoch 89: 0.19434228912601625
Testing Acc of Epoch 89: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.7121e+01 (2.7121e+01)	Acc 0.179688 (0.179688)
Epoch: [90][300/616]	Loss 2.6422e+01 (2.5532e+01)	Acc 0.198242 (0.193103)
Epoch: [90][600/616]	Loss 2.6336e+01 (2.5714e+01)	Acc 0.181641 (0.193187)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 90: 25.721956332136944
Training Acc of Epoch 90: 0.19323551829268293
Testing Acc of Epoch 90: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.6145e+01 (2.6145e+01)	Acc 0.185547 (0.185547)
Epoch: [91][300/616]	Loss 2.6390e+01 (2.6010e+01)	Acc 0.204102 (0.191543)
Epoch: [91][600/616]	Loss 2.6131e+01 (2.6080e+01)	Acc 0.186523 (0.192246)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.188726)
Training Loss of Epoch 91: 26.083998765402693
Training Acc of Epoch 91: 0.19234311483739838
Testing Acc of Epoch 91: 0.18872608695652174
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.5777e+01 (2.5777e+01)	Acc 0.191406 (0.191406)
Epoch: [92][300/616]	Loss 2.6953e+01 (2.6045e+01)	Acc 0.143555 (0.161746)
Epoch: [92][600/616]	Loss 2.5430e+01 (2.5987e+01)	Acc 0.161133 (0.156112)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.190417)
Training Loss of Epoch 92: 25.98512919046045
Training Acc of Epoch 92: 0.15605786331300814
Testing Acc of Epoch 92: 0.19041739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.6670e+01 (2.6670e+01)	Acc 0.189453 (0.189453)
Epoch: [93][300/616]	Loss 2.5559e+01 (2.5961e+01)	Acc 0.149414 (0.161736)
Epoch: [93][600/616]	Loss 2.4669e+01 (2.5998e+01)	Acc 0.166016 (0.159441)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.149038 (0.159870)
Training Loss of Epoch 93: 25.995490607595055
Training Acc of Epoch 93: 0.15920509400406505
Testing Acc of Epoch 93: 0.1598695652173913
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.6013e+01 (2.6013e+01)	Acc 0.151367 (0.151367)
Epoch: [94][300/616]	Loss 2.6510e+01 (2.5965e+01)	Acc 0.183594 (0.159348)
Epoch: [94][600/616]	Loss 2.5260e+01 (2.5970e+01)	Acc 0.159180 (0.158356)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 94: 25.969004714779736
Training Acc of Epoch 94: 0.1582618775406504
Testing Acc of Epoch 94: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.6392e+01 (2.6392e+01)	Acc 0.185547 (0.185547)
Epoch: [95][300/616]	Loss 2.6911e+01 (2.5990e+01)	Acc 0.175781 (0.155439)
Epoch: [95][600/616]	Loss 2.6689e+01 (2.6027e+01)	Acc 0.148438 (0.157558)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.149038 (0.149865)
Training Loss of Epoch 95: 26.025143730349658
Training Acc of Epoch 95: 0.15767117632113822
Testing Acc of Epoch 95: 0.14986521739130435
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.6307e+01 (2.6307e+01)	Acc 0.144531 (0.144531)
Epoch: [96][300/616]	Loss 2.6479e+01 (2.6245e+01)	Acc 0.189453 (0.181972)
Epoch: [96][600/616]	Loss 2.6753e+01 (2.6249e+01)	Acc 0.196289 (0.187711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 96: 26.229118494483515
Training Acc of Epoch 96: 0.18754763719512196
Testing Acc of Epoch 96: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.6566e+01 (2.6566e+01)	Acc 0.193359 (0.193359)
Epoch: [97][300/616]	Loss 2.6599e+01 (2.6279e+01)	Acc 0.174805 (0.192970)
Epoch: [97][600/616]	Loss 2.6256e+01 (2.6254e+01)	Acc 0.205078 (0.192854)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 97: 26.23596057116501
Training Acc of Epoch 97: 0.1925606580284553
Testing Acc of Epoch 97: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.6001e+01 (2.6001e+01)	Acc 0.205078 (0.205078)
Epoch: [98][300/616]	Loss 2.6880e+01 (2.6253e+01)	Acc 0.184570 (0.192889)
Epoch: [98][600/616]	Loss 2.6704e+01 (2.6240e+01)	Acc 0.199219 (0.193057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 98: 26.23765216300158
Training Acc of Epoch 98: 0.193091018800813
Testing Acc of Epoch 98: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.7166e+01 (2.7166e+01)	Acc 0.169922 (0.169922)
Epoch: [99][300/616]	Loss 2.5892e+01 (2.6197e+01)	Acc 0.212891 (0.193564)
Epoch: [99][600/616]	Loss 2.7015e+01 (2.6226e+01)	Acc 0.159180 (0.192947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.155449 (0.163235)
Training Loss of Epoch 99: 26.23120491524053
Training Acc of Epoch 99: 0.19285918445121952
Testing Acc of Epoch 99: 0.16323478260869564
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0085e-01 (5.0085e-01)	Acc 0.199219 (0.199219)
Epoch: [0][300/616]	Loss 2.8559e-01 (3.2628e-01)	Acc 0.694336 (0.646196)
Epoch: [0][600/616]	Loss 3.1174e-01 (3.0018e-01)	Acc 0.669922 (0.681082)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.719309)
Training Loss of Epoch 0: 0.30003354028473056
Training Acc of Epoch 0: 0.6813929115853659
Testing Acc of Epoch 0: 0.7193086956521739
Model with the best training loss saved! The loss is 0.30003354028473056
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6247e-01 (2.6247e-01)	Acc 0.720703 (0.720703)
Epoch: [1][300/616]	Loss 2.7062e-01 (2.7657e-01)	Acc 0.725586 (0.714075)
Epoch: [1][600/616]	Loss 2.6120e-01 (2.7627e-01)	Acc 0.726562 (0.714455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.722657)
Training Loss of Epoch 1: 0.27634149850384004
Training Acc of Epoch 1: 0.7142705157520325
Testing Acc of Epoch 1: 0.7226565217391304
Model with the best training loss saved! The loss is 0.27634149850384004
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.7497e-01 (2.7497e-01)	Acc 0.719727 (0.719727)
Epoch: [2][300/616]	Loss 2.9270e-01 (2.7628e-01)	Acc 0.699219 (0.714432)
Epoch: [2][600/616]	Loss 2.9165e-01 (2.7623e-01)	Acc 0.688477 (0.714568)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.714161)
Training Loss of Epoch 2: 0.2762512112778377
Training Acc of Epoch 2: 0.7145229928861788
Testing Acc of Epoch 2: 0.7141608695652174
Model with the best training loss saved! The loss is 0.2762512112778377
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.8545e-01 (2.8545e-01)	Acc 0.713867 (0.713867)
Epoch: [3][300/616]	Loss 2.8766e-01 (2.7716e-01)	Acc 0.701172 (0.713997)
Epoch: [3][600/616]	Loss 2.8166e-01 (2.7544e-01)	Acc 0.707031 (0.714933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.665064 (0.680296)
Training Loss of Epoch 3: 0.2755872921003559
Training Acc of Epoch 3: 0.71479293699187
Testing Acc of Epoch 3: 0.6802956521739131
Model with the best training loss saved! The loss is 0.2755872921003559
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 3.0077e-01 (3.0077e-01)	Acc 0.693359 (0.693359)
Epoch: [4][300/616]	Loss 2.5334e-01 (2.7441e-01)	Acc 0.735352 (0.716722)
Epoch: [4][600/616]	Loss 2.7026e-01 (2.7583e-01)	Acc 0.752930 (0.714949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.687500 (0.707430)
Training Loss of Epoch 4: 0.27590458301024706
Training Acc of Epoch 4: 0.7149644308943089
Testing Acc of Epoch 4: 0.7074304347826087
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.9535e-01 (2.9535e-01)	Acc 0.702148 (0.702148)
Epoch: [5][300/616]	Loss 2.6371e-01 (2.7873e-01)	Acc 0.722656 (0.712427)
Epoch: [5][600/616]	Loss 2.6012e-01 (2.7634e-01)	Acc 0.725586 (0.714546)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.712735)
Training Loss of Epoch 5: 0.2763916955246189
Training Acc of Epoch 5: 0.7144848831300813
Testing Acc of Epoch 5: 0.7127347826086956
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.8481e-01 (2.8481e-01)	Acc 0.702148 (0.702148)
Epoch: [6][300/616]	Loss 2.5954e-01 (2.8864e-01)	Acc 0.741211 (0.704410)
Epoch: [6][600/616]	Loss 2.6729e-01 (2.8167e-01)	Acc 0.729492 (0.710106)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.644231 (0.672270)
Training Loss of Epoch 6: 0.281280259291331
Training Acc of Epoch 6: 0.7104563643292683
Testing Acc of Epoch 6: 0.6722695652173913
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 3.0908e-01 (3.0908e-01)	Acc 0.686523 (0.686523)
Epoch: [7][300/616]	Loss 2.7105e-01 (2.8095e-01)	Acc 0.721680 (0.708838)
Epoch: [7][600/616]	Loss 2.7478e-01 (2.8026e-01)	Acc 0.711914 (0.710099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.715674)
Training Loss of Epoch 7: 0.28027480818876405
Training Acc of Epoch 7: 0.7100895579268293
Testing Acc of Epoch 7: 0.7156739130434783
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.7448e-01 (2.7448e-01)	Acc 0.723633 (0.723633)
Epoch: [8][300/616]	Loss 2.9709e-01 (2.7924e-01)	Acc 0.676758 (0.710639)
Epoch: [8][600/616]	Loss 2.7349e-01 (2.7862e-01)	Acc 0.719727 (0.711449)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.727687)
Training Loss of Epoch 8: 0.27845822553324506
Training Acc of Epoch 8: 0.7116568216463415
Testing Acc of Epoch 8: 0.7276869565217391
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.7422e-01 (2.7422e-01)	Acc 0.717773 (0.717773)
Epoch: [9][300/616]	Loss 2.8452e-01 (2.7830e-01)	Acc 0.708984 (0.711080)
Epoch: [9][600/616]	Loss 2.5969e-01 (2.7718e-01)	Acc 0.744141 (0.712616)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.726470)
Training Loss of Epoch 9: 0.27726077556125517
Training Acc of Epoch 9: 0.7125031758130081
Testing Acc of Epoch 9: 0.7264695652173913
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.6227e-01 (2.6227e-01)	Acc 0.727539 (0.727539)
Epoch: [10][300/616]	Loss 2.6148e-01 (2.7907e-01)	Acc 0.734375 (0.710743)
Epoch: [10][600/616]	Loss 2.6368e-01 (2.7897e-01)	Acc 0.724609 (0.711605)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.687500 (0.705252)
Training Loss of Epoch 10: 0.2789761061833157
Training Acc of Epoch 10: 0.711644118394309
Testing Acc of Epoch 10: 0.7052521739130435
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.8119e-01 (2.8119e-01)	Acc 0.716797 (0.716797)
Epoch: [11][300/616]	Loss 2.5296e-01 (2.7727e-01)	Acc 0.742188 (0.713244)
Epoch: [11][600/616]	Loss 2.7178e-01 (2.7874e-01)	Acc 0.712891 (0.711659)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.722848)
Training Loss of Epoch 11: 0.2784919532091637
Training Acc of Epoch 11: 0.7119410569105691
Testing Acc of Epoch 11: 0.7228478260869565
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6125e-01 (2.6125e-01)	Acc 0.726562 (0.726562)
Epoch: [12][300/616]	Loss 2.6907e-01 (2.7498e-01)	Acc 0.725586 (0.715386)
Epoch: [12][600/616]	Loss 2.6873e-01 (2.7625e-01)	Acc 0.719727 (0.714122)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.671474 (0.692309)
Training Loss of Epoch 12: 0.27651165360357705
Training Acc of Epoch 12: 0.7138894181910569
Testing Acc of Epoch 12: 0.6923086956521739
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 3.0147e-01 (3.0147e-01)	Acc 0.695312 (0.695312)
Epoch: [13][300/616]	Loss 3.1184e-01 (2.7512e-01)	Acc 0.680664 (0.715496)
Epoch: [13][600/616]	Loss 2.7441e-01 (2.7514e-01)	Acc 0.712891 (0.715196)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.717478)
Training Loss of Epoch 13: 0.27514065678526717
Training Acc of Epoch 13: 0.7151533917682927
Testing Acc of Epoch 13: 0.7174782608695652
Model with the best training loss saved! The loss is 0.27514065678526717
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.7475e-01 (2.7475e-01)	Acc 0.724609 (0.724609)
Epoch: [14][300/616]	Loss 2.9396e-01 (2.7825e-01)	Acc 0.693359 (0.712248)
Epoch: [14][600/616]	Loss 2.7236e-01 (2.7849e-01)	Acc 0.710938 (0.711942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.711122)
Training Loss of Epoch 14: 0.27876926871334634
Training Acc of Epoch 14: 0.7117743267276423
Testing Acc of Epoch 14: 0.7111217391304347
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 3.0629e-01 (3.0629e-01)	Acc 0.701172 (0.701172)
Epoch: [15][300/616]	Loss 2.5834e-01 (2.7782e-01)	Acc 0.724609 (0.713251)
Epoch: [15][600/616]	Loss 2.9957e-01 (2.9553e-01)	Acc 0.703125 (0.687004)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.702913)
Training Loss of Epoch 15: 0.29576836761420333
Training Acc of Epoch 15: 0.6870172764227642
Testing Acc of Epoch 15: 0.7029130434782609
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 3.0154e-01 (3.0154e-01)	Acc 0.700195 (0.700195)
Epoch: [16][300/616]	Loss 2.8282e-01 (2.8234e-01)	Acc 0.699219 (0.712777)
Epoch: [16][600/616]	Loss 2.7194e-01 (2.8365e-01)	Acc 0.709961 (0.709339)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.725796)
Training Loss of Epoch 16: 0.283149787977459
Training Acc of Epoch 16: 0.7097354547764227
Testing Acc of Epoch 16: 0.7257956521739131
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.8390e-01 (2.8390e-01)	Acc 0.704102 (0.704102)
Epoch: [17][300/616]	Loss 3.4242e-01 (2.9467e-01)	Acc 0.662109 (0.685102)
Epoch: [17][600/616]	Loss 2.7328e-01 (2.8786e-01)	Acc 0.709961 (0.697656)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.708570)
Training Loss of Epoch 17: 0.28759220809471314
Training Acc of Epoch 17: 0.6980722815040651
Testing Acc of Epoch 17: 0.7085695652173913
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6839e-01 (2.6839e-01)	Acc 0.734375 (0.734375)
Epoch: [18][300/616]	Loss 2.8258e-01 (2.8054e-01)	Acc 0.704102 (0.709215)
Epoch: [18][600/616]	Loss 2.5264e-01 (2.7898e-01)	Acc 0.738281 (0.710676)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.726474)
Training Loss of Epoch 18: 0.27873164005880435
Training Acc of Epoch 18: 0.7108803353658537
Testing Acc of Epoch 18: 0.7264739130434783
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.6654e-01 (2.6654e-01)	Acc 0.724609 (0.724609)
Epoch: [19][300/616]	Loss 2.5613e-01 (2.8273e-01)	Acc 0.732422 (0.706613)
Epoch: [19][600/616]	Loss 2.7552e-01 (2.8063e-01)	Acc 0.708984 (0.708869)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.726574)
Training Loss of Epoch 19: 0.28046060056705785
Training Acc of Epoch 19: 0.7090685340447155
Testing Acc of Epoch 19: 0.7265739130434783
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.6758e-01 (2.6758e-01)	Acc 0.720703 (0.720703)
Epoch: [20][300/616]	Loss 2.6361e-01 (2.7521e-01)	Acc 0.721680 (0.715626)
Epoch: [20][600/616]	Loss 2.6031e-01 (2.7597e-01)	Acc 0.730469 (0.713635)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.727191)
Training Loss of Epoch 20: 0.2759786184241132
Training Acc of Epoch 20: 0.713648056402439
Testing Acc of Epoch 20: 0.7271913043478261
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.5328e-01 (2.5328e-01)	Acc 0.737305 (0.737305)
Epoch: [21][300/616]	Loss 2.8085e-01 (2.7764e-01)	Acc 0.701172 (0.712884)
Epoch: [21][600/616]	Loss 2.7614e-01 (2.7941e-01)	Acc 0.718750 (0.711375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.715039)
Training Loss of Epoch 21: 0.2792715105826293
Training Acc of Epoch 21: 0.7115361407520325
Testing Acc of Epoch 21: 0.7150391304347826
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.8180e-01 (2.8180e-01)	Acc 0.698242 (0.698242)
Epoch: [22][300/616]	Loss 2.6918e-01 (3.1608e-01)	Acc 0.730469 (0.623952)
Epoch: [22][600/616]	Loss 3.4496e-01 (3.0355e-01)	Acc 0.574219 (0.662143)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.687135)
Training Loss of Epoch 22: 0.3040077928847414
Training Acc of Epoch 22: 0.6623951981707317
Testing Acc of Epoch 22: 0.6871347826086956
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 3.0473e-01 (3.0473e-01)	Acc 0.695312 (0.695312)
Epoch: [23][300/616]	Loss 2.8677e-01 (2.8775e-01)	Acc 0.699219 (0.703472)
Epoch: [23][600/616]	Loss 2.8798e-01 (2.8450e-01)	Acc 0.731445 (0.705692)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.625000 (0.633543)
Training Loss of Epoch 23: 0.28503410278297053
Training Acc of Epoch 23: 0.7055068597560976
Testing Acc of Epoch 23: 0.6335434782608695
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 4.3244e-01 (4.3244e-01)	Acc 0.642578 (0.642578)
Epoch: [24][300/616]	Loss 3.2751e-01 (3.3338e-01)	Acc 0.589844 (0.590181)
Epoch: [24][600/616]	Loss 2.8395e-01 (3.1044e-01)	Acc 0.700195 (0.643397)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.721983)
Training Loss of Epoch 24: 0.3096333717669898
Training Acc of Epoch 24: 0.6450584349593496
Testing Acc of Epoch 24: 0.7219826086956522
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.6895e-01 (2.6895e-01)	Acc 0.726562 (0.726562)
Epoch: [25][300/616]	Loss 2.7763e-01 (2.8272e-01)	Acc 0.709961 (0.708861)
Epoch: [25][600/616]	Loss 3.5425e-01 (2.9195e-01)	Acc 0.623047 (0.698153)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.717174)
Training Loss of Epoch 25: 0.29241756241980604
Training Acc of Epoch 25: 0.697729293699187
Testing Acc of Epoch 25: 0.7171739130434782
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.8307e-01 (2.8307e-01)	Acc 0.714844 (0.714844)
Epoch: [26][300/616]	Loss 2.6818e-01 (2.8708e-01)	Acc 0.718750 (0.703819)
Epoch: [26][600/616]	Loss 2.7303e-01 (2.8229e-01)	Acc 0.726562 (0.708373)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.708465)
Training Loss of Epoch 26: 0.28220203671513533
Training Acc of Epoch 26: 0.7084047891260162
Testing Acc of Epoch 26: 0.7084652173913043
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.8919e-01 (2.8919e-01)	Acc 0.689453 (0.689453)
Epoch: [27][300/616]	Loss 2.6329e-01 (2.7659e-01)	Acc 0.724609 (0.713520)
Epoch: [27][600/616]	Loss 2.6391e-01 (2.7837e-01)	Acc 0.734375 (0.712314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.719813)
Training Loss of Epoch 27: 0.2784704399545018
Training Acc of Epoch 27: 0.7121125508130082
Testing Acc of Epoch 27: 0.7198130434782609
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.7009e-01 (2.7009e-01)	Acc 0.725586 (0.725586)
Epoch: [28][300/616]	Loss 2.9045e-01 (2.7767e-01)	Acc 0.706055 (0.713750)
Epoch: [28][600/616]	Loss 2.7309e-01 (2.7778e-01)	Acc 0.708008 (0.713419)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.679487 (0.690709)
Training Loss of Epoch 28: 0.27805366121171937
Training Acc of Epoch 28: 0.7131653328252032
Testing Acc of Epoch 28: 0.690708695652174
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.9995e-01 (2.9995e-01)	Acc 0.682617 (0.682617)
Epoch: [29][300/616]	Loss 2.6083e-01 (2.7811e-01)	Acc 0.728516 (0.711927)
Epoch: [29][600/616]	Loss 2.6564e-01 (2.7667e-01)	Acc 0.721680 (0.713578)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.644231 (0.650917)
Training Loss of Epoch 29: 0.276516403197273
Training Acc of Epoch 29: 0.7137068089430895
Testing Acc of Epoch 29: 0.6509173913043478
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.0881e-01 (3.0881e-01)	Acc 0.674805 (0.674805)
Epoch: [30][300/616]	Loss 2.8057e-01 (2.7548e-01)	Acc 0.722656 (0.715223)
Epoch: [30][600/616]	Loss 2.4964e-01 (2.7546e-01)	Acc 0.746094 (0.715089)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.728339)
Training Loss of Epoch 30: 0.27568215308635213
Training Acc of Epoch 30: 0.7148755081300813
Testing Acc of Epoch 30: 0.7283391304347826
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.6517e-01 (2.6517e-01)	Acc 0.723633 (0.723633)
Epoch: [31][300/616]	Loss 2.6623e-01 (2.7717e-01)	Acc 0.722656 (0.712865)
Epoch: [31][600/616]	Loss 2.7389e-01 (2.7654e-01)	Acc 0.709961 (0.713277)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.723452)
Training Loss of Epoch 31: 0.2763359246699791
Training Acc of Epoch 31: 0.7134479801829269
Testing Acc of Epoch 31: 0.7234521739130435
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.7047e-01 (2.7047e-01)	Acc 0.708008 (0.708008)
Epoch: [32][300/616]	Loss 2.7898e-01 (2.7681e-01)	Acc 0.714844 (0.714279)
Epoch: [32][600/616]	Loss 2.7565e-01 (2.7802e-01)	Acc 0.710938 (0.711548)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.726491)
Training Loss of Epoch 32: 0.2776478836206886
Training Acc of Epoch 32: 0.7119410569105691
Testing Acc of Epoch 32: 0.726491304347826
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.5126e-01 (2.5126e-01)	Acc 0.739258 (0.739258)
Epoch: [33][300/616]	Loss 2.5783e-01 (2.7398e-01)	Acc 0.739258 (0.715691)
Epoch: [33][600/616]	Loss 3.1665e-01 (2.7346e-01)	Acc 0.682617 (0.716691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.728117)
Training Loss of Epoch 33: 0.27350588348822863
Training Acc of Epoch 33: 0.7167079522357723
Testing Acc of Epoch 33: 0.7281173913043478
Model with the best training loss saved! The loss is 0.27350588348822863
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.7236e-01 (2.7236e-01)	Acc 0.721680 (0.721680)
Epoch: [34][300/616]	Loss 2.6888e-01 (2.7257e-01)	Acc 0.715820 (0.717232)
Epoch: [34][600/616]	Loss 2.8113e-01 (2.7496e-01)	Acc 0.707031 (0.713815)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.713883)
Training Loss of Epoch 34: 0.2751734096102598
Training Acc of Epoch 34: 0.7139211763211382
Testing Acc of Epoch 34: 0.7138826086956521
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.8465e-01 (2.8465e-01)	Acc 0.702148 (0.702148)
Epoch: [35][300/616]	Loss 2.9602e-01 (2.7032e-01)	Acc 0.687500 (0.720109)
Epoch: [35][600/616]	Loss 2.8071e-01 (2.7367e-01)	Acc 0.698242 (0.716459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.721743)
Training Loss of Epoch 35: 0.27350906749081805
Training Acc of Epoch 35: 0.7166095020325203
Testing Acc of Epoch 35: 0.7217434782608696
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.6841e-01 (2.6841e-01)	Acc 0.726562 (0.726562)
Epoch: [36][300/616]	Loss 2.7266e-01 (2.7255e-01)	Acc 0.706055 (0.717212)
Epoch: [36][600/616]	Loss 2.7130e-01 (2.7353e-01)	Acc 0.736328 (0.715988)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.684295 (0.698604)
Training Loss of Epoch 36: 0.27367169113178563
Training Acc of Epoch 36: 0.7157901422764228
Testing Acc of Epoch 36: 0.6986043478260869
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.8638e-01 (2.8638e-01)	Acc 0.709961 (0.709961)
Epoch: [37][300/616]	Loss 4.1003e-01 (3.6537e-01)	Acc 0.435547 (0.529505)
Epoch: [37][600/616]	Loss 3.7760e-01 (3.8489e-01)	Acc 0.590820 (0.487639)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.237179 (0.217509)
Training Loss of Epoch 37: 0.38682433623608536
Training Acc of Epoch 37: 0.4865599593495935
Testing Acc of Epoch 37: 0.2175086956521739
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 4.9643e-01 (4.9643e-01)	Acc 0.215820 (0.215820)
Epoch: [38][300/616]	Loss 4.6354e-01 (4.9114e-01)	Acc 0.310547 (0.230595)
Epoch: [38][600/616]	Loss 4.5981e-01 (4.7726e-01)	Acc 0.308594 (0.271967)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.315705 (0.323300)
Training Loss of Epoch 38: 0.4768847616707406
Training Acc of Epoch 38: 0.27312627032520326
Testing Acc of Epoch 38: 0.3233
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 4.6555e-01 (4.6555e-01)	Acc 0.333008 (0.333008)
Epoch: [39][300/616]	Loss 5.0040e-01 (4.7738e-01)	Acc 0.197266 (0.291097)
Epoch: [39][600/616]	Loss 4.6870e-01 (4.8489e-01)	Acc 0.310547 (0.265250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.326923 (0.322613)
Training Loss of Epoch 39: 0.4846444405675904
Training Acc of Epoch 39: 0.26605849847560975
Testing Acc of Epoch 39: 0.32261304347826086
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 4.7294e-01 (4.7294e-01)	Acc 0.313477 (0.313477)
Epoch: [40][300/616]	Loss 5.0045e-01 (4.6964e-01)	Acc 0.194336 (0.318161)
Epoch: [40][600/616]	Loss 5.0474e-01 (4.7662e-01)	Acc 0.144531 (0.293872)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.237179 (0.248513)
Training Loss of Epoch 40: 0.4771031033217422
Training Acc of Epoch 40: 0.2922589557926829
Testing Acc of Epoch 40: 0.24851304347826086
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 4.8543e-01 (4.8543e-01)	Acc 0.246094 (0.246094)
Epoch: [41][300/616]	Loss 4.9715e-01 (4.7178e-01)	Acc 0.236328 (0.303821)
Epoch: [41][600/616]	Loss 4.5760e-01 (4.8267e-01)	Acc 0.317383 (0.271138)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.341346 (0.338530)
Training Loss of Epoch 41: 0.4820143568806532
Training Acc of Epoch 41: 0.27280392530487807
Testing Acc of Epoch 41: 0.3385304347826087
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 4.5851e-01 (4.5851e-01)	Acc 0.352539 (0.352539)
Epoch: [42][300/616]	Loss 4.6373e-01 (4.8368e-01)	Acc 0.347656 (0.277139)
Epoch: [42][600/616]	Loss 4.1526e-01 (4.5487e-01)	Acc 0.480469 (0.348305)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.405449 (0.404317)
Training Loss of Epoch 42: 0.4541546151890018
Training Acc of Epoch 42: 0.3501762576219512
Testing Acc of Epoch 42: 0.40431739130434785
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 4.3296e-01 (4.3296e-01)	Acc 0.413086 (0.413086)
Epoch: [43][300/616]	Loss 4.6609e-01 (4.8486e-01)	Acc 0.307617 (0.258659)
Epoch: [43][600/616]	Loss 4.8783e-01 (4.7829e-01)	Acc 0.251953 (0.284859)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.341346 (0.345522)
Training Loss of Epoch 43: 0.4780507324187736
Training Acc of Epoch 43: 0.28573742378048783
Testing Acc of Epoch 43: 0.34552173913043477
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 4.6887e-01 (4.6887e-01)	Acc 0.342773 (0.342773)
Epoch: [44][300/616]	Loss 4.5801e-01 (4.6589e-01)	Acc 0.352539 (0.336220)
Epoch: [44][600/616]	Loss 4.2061e-01 (4.4833e-01)	Acc 0.431641 (0.371087)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.456731 (0.464235)
Training Loss of Epoch 44: 0.44723371083174296
Training Acc of Epoch 44: 0.3730087652439024
Testing Acc of Epoch 44: 0.46423478260869566
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 4.0500e-01 (4.0500e-01)	Acc 0.474609 (0.474609)
Epoch: [45][300/616]	Loss 4.1183e-01 (4.0188e-01)	Acc 0.455078 (0.452801)
Epoch: [45][600/616]	Loss 4.1315e-01 (4.1656e-01)	Acc 0.434570 (0.406952)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.418269 (0.442987)
Training Loss of Epoch 45: 0.41636690284178507
Training Acc of Epoch 45: 0.4076314786585366
Testing Acc of Epoch 45: 0.44298695652173914
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 4.1506e-01 (4.1506e-01)	Acc 0.429688 (0.429688)
Epoch: [46][300/616]	Loss 4.5182e-01 (4.5328e-01)	Acc 0.357422 (0.356565)
Epoch: [46][600/616]	Loss 5.0965e-01 (4.6585e-01)	Acc 0.184570 (0.337675)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194617)
Training Loss of Epoch 46: 0.46667531956502095
Training Acc of Epoch 46: 0.33445121951219514
Testing Acc of Epoch 46: 0.19461739130434783
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 5.0097e-01 (5.0097e-01)	Acc 0.195312 (0.195312)
Epoch: [47][300/616]	Loss 4.4171e-01 (4.8608e-01)	Acc 0.478516 (0.258705)
Epoch: [47][600/616]	Loss 3.2322e-01 (4.4052e-01)	Acc 0.599609 (0.387760)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.605769 (0.612126)
Training Loss of Epoch 47: 0.4380319148544374
Training Acc of Epoch 47: 0.39276549796747967
Testing Acc of Epoch 47: 0.6121260869565217
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.3187e-01 (3.3187e-01)	Acc 0.601562 (0.601562)
Epoch: [48][300/616]	Loss 3.9828e-01 (4.0360e-01)	Acc 0.482422 (0.437182)
Epoch: [48][600/616]	Loss 4.8085e-01 (4.0467e-01)	Acc 0.312500 (0.454479)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.304487 (0.317000)
Training Loss of Epoch 48: 0.4061394436572625
Training Acc of Epoch 48: 0.4516625381097561
Testing Acc of Epoch 48: 0.317
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 4.6424e-01 (4.6424e-01)	Acc 0.298828 (0.298828)
Epoch: [49][300/616]	Loss 4.5203e-01 (4.5970e-01)	Acc 0.393555 (0.348802)
Epoch: [49][600/616]	Loss 4.0867e-01 (4.4155e-01)	Acc 0.447266 (0.393777)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.487179 (0.482048)
Training Loss of Epoch 49: 0.44064646280877956
Training Acc of Epoch 49: 0.3955967352642276
Testing Acc of Epoch 49: 0.4820478260869565
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.9324e-01 (3.9324e-01)	Acc 0.489258 (0.489258)
Epoch: [50][300/616]	Loss 3.9984e-01 (4.1112e-01)	Acc 0.481445 (0.457787)
Epoch: [50][600/616]	Loss 4.0448e-01 (4.0755e-01)	Acc 0.473633 (0.465190)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.488782 (0.483239)
Training Loss of Epoch 50: 0.4072978372980909
Training Acc of Epoch 50: 0.4656646976626016
Testing Acc of Epoch 50: 0.4832391304347826
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 4.0603e-01 (4.0603e-01)	Acc 0.458008 (0.458008)
Epoch: [51][300/616]	Loss 3.9100e-01 (4.0305e-01)	Acc 0.483398 (0.476066)
Epoch: [51][600/616]	Loss 4.6563e-01 (4.1725e-01)	Acc 0.291992 (0.436788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.315705 (0.307422)
Training Loss of Epoch 51: 0.4182975390577704
Training Acc of Epoch 51: 0.4339907266260163
Testing Acc of Epoch 51: 0.3074217391304348
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 4.5699e-01 (4.5699e-01)	Acc 0.307617 (0.307617)
Epoch: [52][300/616]	Loss 4.5548e-01 (4.5452e-01)	Acc 0.333984 (0.340168)
Epoch: [52][600/616]	Loss 4.9991e-01 (4.6880e-01)	Acc 0.214844 (0.299507)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.209936 (0.212713)
Training Loss of Epoch 52: 0.46950156131410986
Training Acc of Epoch 52: 0.29751810213414637
Testing Acc of Epoch 52: 0.21271304347826087
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 5.0014e-01 (5.0014e-01)	Acc 0.221680 (0.221680)
Epoch: [53][300/616]	Loss 3.0790e-01 (4.3335e-01)	Acc 0.687500 (0.367058)
Epoch: [53][600/616]	Loss 2.8125e-01 (3.6184e-01)	Acc 0.695312 (0.533619)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.711643)
Training Loss of Epoch 53: 0.3599232464059582
Training Acc of Epoch 53: 0.5376857850609756
Testing Acc of Epoch 53: 0.7116434782608696
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.8734e-01 (2.8734e-01)	Acc 0.701172 (0.701172)
Epoch: [54][300/616]	Loss 2.5803e-01 (2.7926e-01)	Acc 0.730469 (0.711716)
Epoch: [54][600/616]	Loss 2.9366e-01 (2.7903e-01)	Acc 0.685547 (0.711405)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.718191)
Training Loss of Epoch 54: 0.2792609539458422
Training Acc of Epoch 54: 0.711205856199187
Testing Acc of Epoch 54: 0.7181913043478261
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.7099e-01 (2.7099e-01)	Acc 0.743164 (0.743164)
Epoch: [55][300/616]	Loss 2.8025e-01 (2.7644e-01)	Acc 0.708008 (0.714617)
Epoch: [55][600/616]	Loss 2.7058e-01 (2.7618e-01)	Acc 0.717773 (0.714070)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.724400)
Training Loss of Epoch 55: 0.27592685447960363
Training Acc of Epoch 55: 0.7142260543699187
Testing Acc of Epoch 55: 0.7244
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.9176e-01 (2.9176e-01)	Acc 0.685547 (0.685547)
Epoch: [56][300/616]	Loss 2.9316e-01 (2.7431e-01)	Acc 0.689453 (0.714753)
Epoch: [56][600/616]	Loss 2.6962e-01 (2.7625e-01)	Acc 0.723633 (0.713893)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.722691)
Training Loss of Epoch 56: 0.2762493148809526
Training Acc of Epoch 56: 0.714040269308943
Testing Acc of Epoch 56: 0.7226913043478261
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.7071e-01 (2.7071e-01)	Acc 0.727539 (0.727539)
Epoch: [57][300/616]	Loss 2.7338e-01 (2.7657e-01)	Acc 0.719727 (0.713955)
Epoch: [57][600/616]	Loss 2.6147e-01 (2.7530e-01)	Acc 0.731445 (0.714946)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.724683)
Training Loss of Epoch 57: 0.27521999139126724
Training Acc of Epoch 57: 0.7150454141260163
Testing Acc of Epoch 57: 0.7246826086956522
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.8122e-01 (2.8122e-01)	Acc 0.701172 (0.701172)
Epoch: [58][300/616]	Loss 2.8619e-01 (2.7438e-01)	Acc 0.695312 (0.715671)
Epoch: [58][600/616]	Loss 2.5832e-01 (2.7584e-01)	Acc 0.735352 (0.713950)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.710170)
Training Loss of Epoch 58: 0.27588100229821555
Training Acc of Epoch 58: 0.7138592479674797
Testing Acc of Epoch 58: 0.7101695652173913
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.8640e-01 (2.8640e-01)	Acc 0.709961 (0.709961)
Epoch: [59][300/616]	Loss 2.7605e-01 (2.7559e-01)	Acc 0.702148 (0.713828)
Epoch: [59][600/616]	Loss 2.7003e-01 (2.8707e-01)	Acc 0.713867 (0.692532)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.717035)
Training Loss of Epoch 59: 0.28680744013650633
Training Acc of Epoch 59: 0.6930560848577236
Testing Acc of Epoch 59: 0.7170347826086957
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.7982e-01 (2.7982e-01)	Acc 0.710938 (0.710938)
Epoch: [60][300/616]	Loss 2.8603e-01 (2.8233e-01)	Acc 0.703125 (0.707787)
Epoch: [60][600/616]	Loss 2.8808e-01 (2.7997e-01)	Acc 0.693359 (0.711243)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.715770)
Training Loss of Epoch 60: 0.27992461377043065
Training Acc of Epoch 60: 0.7113709984756098
Testing Acc of Epoch 60: 0.7157695652173913
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.8108e-01 (2.8108e-01)	Acc 0.710938 (0.710938)
Epoch: [61][300/616]	Loss 3.3180e-01 (3.0204e-01)	Acc 0.651367 (0.668365)
Epoch: [61][600/616]	Loss 3.0016e-01 (3.0358e-01)	Acc 0.604492 (0.643602)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.618590 (0.617496)
Training Loss of Epoch 61: 0.3035145498388182
Training Acc of Epoch 61: 0.6429369918699187
Testing Acc of Epoch 61: 0.617495652173913
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.0702e-01 (3.0702e-01)	Acc 0.610352 (0.610352)
Epoch: [62][300/616]	Loss 3.3225e-01 (2.9826e-01)	Acc 0.671875 (0.629088)
Epoch: [62][600/616]	Loss 2.7885e-01 (2.8962e-01)	Acc 0.714844 (0.669599)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.711652)
Training Loss of Epoch 62: 0.2896420815126683
Training Acc of Epoch 62: 0.6703299669715447
Testing Acc of Epoch 62: 0.7116521739130435
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.8376e-01 (2.8376e-01)	Acc 0.699219 (0.699219)
Epoch: [63][300/616]	Loss 2.6178e-01 (2.7841e-01)	Acc 0.734375 (0.713183)
Epoch: [63][600/616]	Loss 2.8125e-01 (2.7752e-01)	Acc 0.715820 (0.713934)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.728717)
Training Loss of Epoch 63: 0.2772961804537269
Training Acc of Epoch 63: 0.714135543699187
Testing Acc of Epoch 63: 0.7287173913043479
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.6189e-01 (2.6189e-01)	Acc 0.742188 (0.742188)
Epoch: [64][300/616]	Loss 2.7021e-01 (2.7801e-01)	Acc 0.708984 (0.711599)
Epoch: [64][600/616]	Loss 2.8314e-01 (2.7711e-01)	Acc 0.718750 (0.712883)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.726165)
Training Loss of Epoch 64: 0.2768903306102365
Training Acc of Epoch 64: 0.713257431402439
Testing Acc of Epoch 64: 0.7261652173913044
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.7820e-01 (2.7820e-01)	Acc 0.707031 (0.707031)
Epoch: [65][300/616]	Loss 2.6029e-01 (2.8237e-01)	Acc 0.739258 (0.707155)
Epoch: [65][600/616]	Loss 2.7584e-01 (2.7952e-01)	Acc 0.713867 (0.710268)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.727278)
Training Loss of Epoch 65: 0.27941584456257706
Training Acc of Epoch 65: 0.7103388592479675
Testing Acc of Epoch 65: 0.7272782608695653
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.7693e-01 (2.7693e-01)	Acc 0.722656 (0.722656)
Epoch: [66][300/616]	Loss 2.6307e-01 (2.7343e-01)	Acc 0.728516 (0.715570)
Epoch: [66][600/616]	Loss 3.0588e-01 (2.7272e-01)	Acc 0.687500 (0.716709)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.720287)
Training Loss of Epoch 66: 0.27267579223082317
Training Acc of Epoch 66: 0.7167444740853659
Testing Acc of Epoch 66: 0.7202869565217391
Model with the best training loss saved! The loss is 0.27267579223082317
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.6706e-01 (2.6706e-01)	Acc 0.723633 (0.723633)
Epoch: [67][300/616]	Loss 2.8179e-01 (2.9160e-01)	Acc 0.707031 (0.685265)
Epoch: [67][600/616]	Loss 2.7440e-01 (2.8788e-01)	Acc 0.717773 (0.695969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.725143)
Training Loss of Epoch 67: 0.2874879251892974
Training Acc of Epoch 67: 0.6965923526422764
Testing Acc of Epoch 67: 0.7251434782608696
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.4127e-01 (2.4127e-01)	Acc 0.757812 (0.757812)
Epoch: [68][300/616]	Loss 2.5371e-01 (2.7610e-01)	Acc 0.728516 (0.713394)
Epoch: [68][600/616]	Loss 2.6532e-01 (2.7724e-01)	Acc 0.729492 (0.711903)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.723730)
Training Loss of Epoch 68: 0.2771016835439496
Training Acc of Epoch 68: 0.7119664634146341
Testing Acc of Epoch 68: 0.7237304347826087
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.9137e-01 (2.9137e-01)	Acc 0.694336 (0.694336)
Epoch: [69][300/616]	Loss 4.4136e-01 (2.8494e-01)	Acc 0.361328 (0.703300)
Epoch: [69][600/616]	Loss 4.0174e-01 (3.4937e-01)	Acc 0.414062 (0.558202)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.394231 (0.427683)
Training Loss of Epoch 69: 0.35082741292511543
Training Acc of Epoch 69: 0.554858993902439
Testing Acc of Epoch 69: 0.42768260869565217
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 4.0452e-01 (4.0452e-01)	Acc 0.441406 (0.441406)
Epoch: [70][300/616]	Loss 5.0040e-01 (4.7125e-01)	Acc 0.206055 (0.273538)
Epoch: [70][600/616]	Loss 5.0059e-01 (4.8590e-01)	Acc 0.183594 (0.239576)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 70: 0.48622992721999564
Training Acc of Epoch 70: 0.2388116107723577
Testing Acc of Epoch 70: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.203125)
Epoch: [71][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.188477 (0.201818)
Epoch: [71][600/616]	Loss 4.6878e-01 (4.9589e-01)	Acc 0.291016 (0.215703)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.280449 (0.294326)
Training Loss of Epoch 71: 0.49540889917350395
Training Acc of Epoch 71: 0.21734311483739838
Testing Acc of Epoch 71: 0.29432608695652174
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 4.7792e-01 (4.7792e-01)	Acc 0.285156 (0.285156)
Epoch: [72][300/616]	Loss 4.6654e-01 (4.6980e-01)	Acc 0.316406 (0.287989)
Epoch: [72][600/616]	Loss 5.0009e-01 (4.6825e-01)	Acc 0.218750 (0.299723)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 72: 0.4689861650389384
Training Acc of Epoch 72: 0.2975133384146341
Testing Acc of Epoch 72: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.203125)
Epoch: [73][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.225586 (0.201415)
Epoch: [73][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.222656 (0.201576)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 73: 0.5004024502707691
Training Acc of Epoch 73: 0.2013814786585366
Testing Acc of Epoch 73: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.200195)
Epoch: [74][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201587)
Epoch: [74][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.201529)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 74: 0.500402450173851
Training Acc of Epoch 74: 0.20144658282520325
Testing Acc of Epoch 74: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.189453)
Epoch: [75][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.201788)
Epoch: [75][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201393)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 75: 0.5004024502707691
Training Acc of Epoch 75: 0.20144975863821138
Testing Acc of Epoch 75: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.209961)
Epoch: [76][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.201568)
Epoch: [76][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.201365)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 76: 0.5004024505615234
Training Acc of Epoch 76: 0.20145928607723576
Testing Acc of Epoch 76: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [77][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.201584)
Epoch: [77][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201354)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 77: 0.5004024503676872
Training Acc of Epoch 77: 0.2014449949186992
Testing Acc of Epoch 77: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.223633 (0.223633)
Epoch: [78][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.201243)
Epoch: [78][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201435)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 78: 0.5004024503676872
Training Acc of Epoch 78: 0.20143864329268293
Testing Acc of Epoch 78: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.215820 (0.215820)
Epoch: [79][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.176758 (0.200650)
Epoch: [79][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 79: 0.5004024503676872
Training Acc of Epoch 79: 0.2014576981707317
Testing Acc of Epoch 79: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.203125)
Epoch: [80][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.202197)
Epoch: [80][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201557)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 80: 0.5004024504646053
Training Acc of Epoch 80: 0.20143705538617887
Testing Acc of Epoch 80: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.211914 (0.211914)
Epoch: [81][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201266)
Epoch: [81][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.223633 (0.201422)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 81: 0.500402450173851
Training Acc of Epoch 81: 0.20144975863821138
Testing Acc of Epoch 81: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.192383)
Epoch: [82][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.225586 (0.201957)
Epoch: [82][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201419)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 82: 0.5004024504646053
Training Acc of Epoch 82: 0.20143229166666668
Testing Acc of Epoch 82: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [83][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201084)
Epoch: [83][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.168945 (0.201344)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 83: 0.5004024502707691
Training Acc of Epoch 83: 0.20145452235772357
Testing Acc of Epoch 83: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.209961)
Epoch: [84][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201075)
Epoch: [84][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 84: 0.5004024500769328
Training Acc of Epoch 84: 0.20144340701219512
Testing Acc of Epoch 84: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.192383)
Epoch: [85][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.219727 (0.201873)
Epoch: [85][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 85: 0.500402450173851
Training Acc of Epoch 85: 0.20144817073170732
Testing Acc of Epoch 85: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.186523 (0.186523)
Epoch: [86][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.201156)
Epoch: [86][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.201406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 86: 0.5004024502707691
Training Acc of Epoch 86: 0.20145928607723576
Testing Acc of Epoch 86: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.179688 (0.179688)
Epoch: [87][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201337)
Epoch: [87][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.188477 (0.201474)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 87: 0.5004024499800147
Training Acc of Epoch 87: 0.20145134654471544
Testing Acc of Epoch 87: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [88][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.223633 (0.201110)
Epoch: [88][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.217773 (0.201500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 88: 0.5004024503676872
Training Acc of Epoch 88: 0.20146563770325204
Testing Acc of Epoch 88: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.232422 (0.232422)
Epoch: [89][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.183594 (0.201143)
Epoch: [89][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.188477 (0.201541)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 89: 0.5004024502707691
Training Acc of Epoch 89: 0.20144817073170732
Testing Acc of Epoch 89: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.186523 (0.186523)
Epoch: [90][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.220703 (0.201032)
Epoch: [90][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.201489)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 90: 0.5004024500769328
Training Acc of Epoch 90: 0.2014529344512195
Testing Acc of Epoch 90: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.181641 (0.181641)
Epoch: [91][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.173828 (0.201243)
Epoch: [91][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.201406)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 91: 0.5004024502707691
Training Acc of Epoch 91: 0.20144340701219512
Testing Acc of Epoch 91: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.208008)
Epoch: [92][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.175781 (0.201441)
Epoch: [92][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.201464)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 92: 0.5004024500769328
Training Acc of Epoch 92: 0.20144340701219512
Testing Acc of Epoch 92: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [93][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.201769)
Epoch: [93][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.210938 (0.201446)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 93: 0.5004024503676872
Training Acc of Epoch 93: 0.20143864329268293
Testing Acc of Epoch 93: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.212891 (0.212891)
Epoch: [94][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201146)
Epoch: [94][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.211914 (0.201451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 94: 0.5004024503676872
Training Acc of Epoch 94: 0.20145134654471544
Testing Acc of Epoch 94: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.213867)
Epoch: [95][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.201610)
Epoch: [95][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.201352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 95: 0.5004024502707691
Training Acc of Epoch 95: 0.20143864329268293
Testing Acc of Epoch 95: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.216797 (0.216797)
Epoch: [96][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.202028)
Epoch: [96][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.183594 (0.201414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 96: 0.5004024504646053
Training Acc of Epoch 96: 0.20144658282520325
Testing Acc of Epoch 96: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.202148)
Epoch: [97][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.201094)
Epoch: [97][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.201352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 97: 0.5004024503676872
Training Acc of Epoch 97: 0.2014576981707317
Testing Acc of Epoch 97: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.197266)
Epoch: [98][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.207031 (0.201470)
Epoch: [98][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 98: 0.5004024504646053
Training Acc of Epoch 98: 0.2014449949186992
Testing Acc of Epoch 98: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.223633 (0.223633)
Epoch: [99][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.202496)
Epoch: [99][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.180664 (0.201471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 99: 0.5004024504646053
Training Acc of Epoch 99: 0.20146404979674798
Testing Acc of Epoch 99: 0.20121739130434782
Early stopping not satisfied.
