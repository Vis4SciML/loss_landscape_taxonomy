train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9871e-01 (4.9871e-01)	Acc 0.230469 (0.230469)
Epoch: [0][300/616]	Loss 2.6568e-01 (3.2292e-01)	Acc 0.726562 (0.648428)
Epoch: [0][600/616]	Loss 2.8341e-01 (3.0095e-01)	Acc 0.711914 (0.680393)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.718722)
Training Loss of Epoch 0: 0.30040853142253754
Training Acc of Epoch 0: 0.6810483358739837
Testing Acc of Epoch 0: 0.7187217391304348
Model with the best training loss saved! The loss is 0.30040853142253754
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.7519e-01 (2.7519e-01)	Acc 0.714844 (0.714844)
Epoch: [1][300/616]	Loss 2.8165e-01 (2.7656e-01)	Acc 0.716797 (0.713319)
Epoch: [1][600/616]	Loss 2.8750e-01 (2.7616e-01)	Acc 0.715820 (0.713053)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.722087)
Training Loss of Epoch 1: 0.27635079518081695
Training Acc of Epoch 1: 0.712865218495935
Testing Acc of Epoch 1: 0.7220869565217392
Model with the best training loss saved! The loss is 0.27635079518081695
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.8156e-01 (2.8156e-01)	Acc 0.710938 (0.710938)
Epoch: [2][300/616]	Loss 2.7560e-01 (2.7521e-01)	Acc 0.708008 (0.714331)
Epoch: [2][600/616]	Loss 2.7247e-01 (2.7671e-01)	Acc 0.709961 (0.712330)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.665064 (0.689300)
Training Loss of Epoch 2: 0.27679259079258617
Training Acc of Epoch 2: 0.7122681656504065
Testing Acc of Epoch 2: 0.6893
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 3.0448e-01 (3.0448e-01)	Acc 0.696289 (0.696289)
Epoch: [3][300/616]	Loss 3.0233e-01 (2.7728e-01)	Acc 0.692383 (0.711408)
Epoch: [3][600/616]	Loss 2.6355e-01 (2.7741e-01)	Acc 0.728516 (0.711375)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.722987)
Training Loss of Epoch 3: 0.2773135195660397
Training Acc of Epoch 3: 0.711424987296748
Testing Acc of Epoch 3: 0.7229869565217392
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6595e-01 (2.6595e-01)	Acc 0.718750 (0.718750)
Epoch: [4][300/616]	Loss 2.8728e-01 (2.7547e-01)	Acc 0.698242 (0.713189)
Epoch: [4][600/616]	Loss 2.8021e-01 (2.7656e-01)	Acc 0.739258 (0.712517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.663462 (0.679496)
Training Loss of Epoch 4: 0.2771179767401238
Training Acc of Epoch 4: 0.7120522103658536
Testing Acc of Epoch 4: 0.6794956521739131
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 3.1966e-01 (3.1966e-01)	Acc 0.665039 (0.665039)
Epoch: [5][300/616]	Loss 2.9951e-01 (2.7769e-01)	Acc 0.683594 (0.711483)
Epoch: [5][600/616]	Loss 2.6656e-01 (2.7751e-01)	Acc 0.728516 (0.711171)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.719396)
Training Loss of Epoch 5: 0.27753427433289163
Training Acc of Epoch 5: 0.7110788236788618
Testing Acc of Epoch 5: 0.719395652173913
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.7324e-01 (2.7324e-01)	Acc 0.733398 (0.733398)
Epoch: [6][300/616]	Loss 2.5706e-01 (2.7710e-01)	Acc 0.738281 (0.711901)
Epoch: [6][600/616]	Loss 2.7815e-01 (2.7843e-01)	Acc 0.714844 (0.710600)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.726439)
Training Loss of Epoch 6: 0.2782816700334471
Training Acc of Epoch 6: 0.7107802972560976
Testing Acc of Epoch 6: 0.7264391304347826
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.6529e-01 (2.6529e-01)	Acc 0.732422 (0.732422)
Epoch: [7][300/616]	Loss 2.6889e-01 (2.7886e-01)	Acc 0.724609 (0.710224)
Epoch: [7][600/616]	Loss 2.7565e-01 (2.7944e-01)	Acc 0.727539 (0.709798)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.716774)
Training Loss of Epoch 7: 0.27939822918515866
Training Acc of Epoch 7: 0.7099942835365853
Testing Acc of Epoch 7: 0.7167739130434783
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.6396e-01 (2.6396e-01)	Acc 0.702148 (0.702148)
Epoch: [8][300/616]	Loss 2.6013e-01 (2.7894e-01)	Acc 0.730469 (0.710354)
Epoch: [8][600/616]	Loss 2.8629e-01 (2.7886e-01)	Acc 0.711914 (0.711093)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.725517)
Training Loss of Epoch 8: 0.2788538582683579
Training Acc of Epoch 8: 0.7111169334349593
Testing Acc of Epoch 8: 0.7255173913043478
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.6888e-01 (2.6888e-01)	Acc 0.717773 (0.717773)
Epoch: [9][300/616]	Loss 2.6001e-01 (2.7850e-01)	Acc 0.736328 (0.711872)
Epoch: [9][600/616]	Loss 2.7178e-01 (2.7763e-01)	Acc 0.737305 (0.711968)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.712043)
Training Loss of Epoch 9: 0.27750154547090455
Training Acc of Epoch 9: 0.7120633257113821
Testing Acc of Epoch 9: 0.7120434782608696
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.7752e-01 (2.7752e-01)	Acc 0.721680 (0.721680)
Epoch: [10][300/616]	Loss 2.8586e-01 (2.7909e-01)	Acc 0.703125 (0.711087)
Epoch: [10][600/616]	Loss 2.7254e-01 (2.8075e-01)	Acc 0.713867 (0.708853)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.714665)
Training Loss of Epoch 10: 0.28070199833652837
Training Acc of Epoch 10: 0.7088700457317073
Testing Acc of Epoch 10: 0.7146652173913044
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.8195e-01 (2.8195e-01)	Acc 0.688477 (0.688477)
Epoch: [11][300/616]	Loss 2.8664e-01 (2.7798e-01)	Acc 0.705078 (0.713199)
Epoch: [11][600/616]	Loss 3.3779e-01 (2.7907e-01)	Acc 0.627930 (0.711895)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.721904)
Training Loss of Epoch 11: 0.2790100770510309
Training Acc of Epoch 11: 0.7119188262195122
Testing Acc of Epoch 11: 0.7219043478260869
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.6413e-01 (2.6413e-01)	Acc 0.720703 (0.720703)
Epoch: [12][300/616]	Loss 2.7687e-01 (2.7702e-01)	Acc 0.707031 (0.712748)
Epoch: [12][600/616]	Loss 2.7702e-01 (2.7714e-01)	Acc 0.713867 (0.712840)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.716900)
Training Loss of Epoch 12: 0.277256020612833
Training Acc of Epoch 12: 0.7127207190040651
Testing Acc of Epoch 12: 0.7169
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.5892e-01 (2.5892e-01)	Acc 0.740234 (0.740234)
Epoch: [13][300/616]	Loss 2.5863e-01 (2.8013e-01)	Acc 0.733398 (0.710363)
Epoch: [13][600/616]	Loss 2.8893e-01 (2.7897e-01)	Acc 0.725586 (0.711587)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.722404)
Training Loss of Epoch 13: 0.2790901482832141
Training Acc of Epoch 13: 0.7113852896341464
Testing Acc of Epoch 13: 0.722404347826087
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.7798e-01 (2.7798e-01)	Acc 0.713867 (0.713867)
Epoch: [14][300/616]	Loss 2.6650e-01 (2.8342e-01)	Acc 0.731445 (0.708258)
Epoch: [14][600/616]	Loss 2.4960e-01 (2.8132e-01)	Acc 0.758789 (0.708835)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.718835)
Training Loss of Epoch 14: 0.2811963697274526
Training Acc of Epoch 14: 0.7089510289634147
Testing Acc of Epoch 14: 0.7188347826086956
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4858e-01 (2.4858e-01)	Acc 0.740234 (0.740234)
Epoch: [15][300/616]	Loss 2.7457e-01 (2.7911e-01)	Acc 0.716797 (0.711405)
Epoch: [15][600/616]	Loss 2.7624e-01 (2.8148e-01)	Acc 0.728516 (0.708265)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.719526)
Training Loss of Epoch 15: 0.281670597728675
Training Acc of Epoch 15: 0.7081761305894309
Testing Acc of Epoch 15: 0.7195260869565218
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.7893e-01 (2.7893e-01)	Acc 0.700195 (0.700195)
Epoch: [16][300/616]	Loss 3.1610e-01 (2.8153e-01)	Acc 0.665039 (0.709659)
Epoch: [16][600/616]	Loss 2.9221e-01 (2.8171e-01)	Acc 0.706055 (0.709857)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.720722)
Training Loss of Epoch 16: 0.281603355451328
Training Acc of Epoch 16: 0.7099005970528456
Testing Acc of Epoch 16: 0.7207217391304348
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.7185e-01 (2.7185e-01)	Acc 0.726562 (0.726562)
Epoch: [17][300/616]	Loss 2.6758e-01 (2.7700e-01)	Acc 0.737305 (0.713760)
Epoch: [17][600/616]	Loss 2.5270e-01 (2.7736e-01)	Acc 0.746094 (0.713573)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.729335)
Training Loss of Epoch 17: 0.277242050398656
Training Acc of Epoch 17: 0.7137115726626017
Testing Acc of Epoch 17: 0.7293347826086957
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6459e-01 (2.6459e-01)	Acc 0.716797 (0.716797)
Epoch: [18][300/616]	Loss 2.6053e-01 (2.7839e-01)	Acc 0.735352 (0.711337)
Epoch: [18][600/616]	Loss 3.0625e-01 (2.7829e-01)	Acc 0.682617 (0.711885)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.714617)
Training Loss of Epoch 18: 0.27826486985857896
Training Acc of Epoch 18: 0.7119601117886178
Testing Acc of Epoch 18: 0.7146173913043479
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.9090e-01 (2.9090e-01)	Acc 0.701172 (0.701172)
Epoch: [19][300/616]	Loss 2.7131e-01 (2.8159e-01)	Acc 0.715820 (0.710208)
Epoch: [19][600/616]	Loss 2.7362e-01 (2.8091e-01)	Acc 0.707031 (0.709756)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.716726)
Training Loss of Epoch 19: 0.2807520115520896
Training Acc of Epoch 19: 0.7099053607723578
Testing Acc of Epoch 19: 0.7167260869565217
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 3.0178e-01 (3.0178e-01)	Acc 0.703125 (0.703125)
Epoch: [20][300/616]	Loss 2.6697e-01 (2.7889e-01)	Acc 0.731445 (0.710899)
Epoch: [20][600/616]	Loss 2.6648e-01 (2.7692e-01)	Acc 0.733398 (0.713118)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.720191)
Training Loss of Epoch 20: 0.2770658975694238
Training Acc of Epoch 20: 0.7131161077235773
Testing Acc of Epoch 20: 0.7201913043478261
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.9258e-01 (2.9258e-01)	Acc 0.703125 (0.703125)
Epoch: [21][300/616]	Loss 2.6296e-01 (2.7907e-01)	Acc 0.731445 (0.710970)
Epoch: [21][600/616]	Loss 3.1071e-01 (2.8288e-01)	Acc 0.668945 (0.704154)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.724961)
Training Loss of Epoch 21: 0.2828624702323743
Training Acc of Epoch 21: 0.704344512195122
Testing Acc of Epoch 21: 0.7249608695652174
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.8560e-01 (2.8560e-01)	Acc 0.708984 (0.708984)
Epoch: [22][300/616]	Loss 2.6507e-01 (2.7861e-01)	Acc 0.730469 (0.710266)
Epoch: [22][600/616]	Loss 3.0915e-01 (2.9642e-01)	Acc 0.658203 (0.683353)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.719539)
Training Loss of Epoch 22: 0.2966621259363686
Training Acc of Epoch 22: 0.6839176829268293
Testing Acc of Epoch 22: 0.7195391304347826
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.9906e-01 (2.9906e-01)	Acc 0.735352 (0.735352)
Epoch: [23][300/616]	Loss 3.4889e-01 (3.5335e-01)	Acc 0.579102 (0.590833)
Epoch: [23][600/616]	Loss 3.3049e-01 (3.5134e-01)	Acc 0.614258 (0.585000)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.594551 (0.598391)
Training Loss of Epoch 23: 0.350843075911204
Training Acc of Epoch 23: 0.5857787093495935
Testing Acc of Epoch 23: 0.5983913043478261
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 3.5051e-01 (3.5051e-01)	Acc 0.583984 (0.583984)
Epoch: [24][300/616]	Loss 4.1498e-01 (3.8582e-01)	Acc 0.328125 (0.485858)
Epoch: [24][600/616]	Loss 5.0041e-01 (4.1373e-01)	Acc 0.210938 (0.403419)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.208333 (0.201626)
Training Loss of Epoch 24: 0.4156866745735572
Training Acc of Epoch 24: 0.3986613948170732
Testing Acc of Epoch 24: 0.20162608695652173
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 4.9699e-01 (4.9699e-01)	Acc 0.214844 (0.214844)
Epoch: [25][300/616]	Loss 5.0041e-01 (4.9038e-01)	Acc 0.198242 (0.244718)
Epoch: [25][600/616]	Loss 5.0040e-01 (4.9538e-01)	Acc 0.209961 (0.223366)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201409)
Training Loss of Epoch 25: 0.49549619865611316
Training Acc of Epoch 25: 0.22285315040650405
Testing Acc of Epoch 25: 0.20140869565217392
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.224609 (0.224609)
Epoch: [26][300/616]	Loss 4.6137e-01 (4.8805e-01)	Acc 0.334961 (0.253455)
Epoch: [26][600/616]	Loss 4.9897e-01 (4.9403e-01)	Acc 0.229492 (0.230160)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 26: 0.49413688735264105
Training Acc of Epoch 26: 0.22947472052845527
Testing Acc of Epoch 26: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 4.9449e-01 (4.9449e-01)	Acc 0.200195 (0.200195)
Epoch: [27][300/616]	Loss 5.0040e-01 (5.0024e-01)	Acc 0.208984 (0.202398)
Epoch: [27][600/616]	Loss 5.0040e-01 (5.0032e-01)	Acc 0.202148 (0.201744)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 27: 0.5003242184476154
Training Acc of Epoch 27: 0.2017212906504065
Testing Acc of Epoch 27: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [28][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.200724)
Epoch: [28][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.201266)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 28: 0.5004024500769328
Training Acc of Epoch 28: 0.20124333079268292
Testing Acc of Epoch 28: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.192383)
Epoch: [29][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.200763)
Epoch: [29][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.183594 (0.201440)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 29: 0.5004024502707691
Training Acc of Epoch 29: 0.2014529344512195
Testing Acc of Epoch 29: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.193359)
Epoch: [30][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.185547 (0.201856)
Epoch: [30][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.201424)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 30: 0.5004024504646053
Training Acc of Epoch 30: 0.201440231199187
Testing Acc of Epoch 30: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.191406)
Epoch: [31][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.201772)
Epoch: [31][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.220703 (0.201485)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 31: 0.500402450173851
Training Acc of Epoch 31: 0.2014449949186992
Testing Acc of Epoch 31: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [32][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.186523 (0.201915)
Epoch: [32][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.201235)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 32: 0.5004024504646053
Training Acc of Epoch 32: 0.20134813262195123
Testing Acc of Epoch 32: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.195312 (0.195312)
Epoch: [33][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.199014)
Epoch: [33][600/616]	Loss 4.7138e-01 (4.9671e-01)	Acc 0.346680 (0.227297)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.370192 (0.375578)
Training Loss of Epoch 33: 0.49611529121553993
Training Acc of Epoch 33: 0.2299463287601626
Testing Acc of Epoch 33: 0.3755782608695652
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 4.6316e-01 (4.6316e-01)	Acc 0.387695 (0.387695)
Epoch: [34][300/616]	Loss 4.3505e-01 (4.5034e-01)	Acc 0.443359 (0.428052)
Epoch: [34][600/616]	Loss 4.8035e-01 (4.6065e-01)	Acc 0.300781 (0.393041)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.322115 (0.314739)
Training Loss of Epoch 34: 0.4612119980943881
Training Acc of Epoch 34: 0.3912712779471545
Testing Acc of Epoch 34: 0.31473913043478263
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 4.8944e-01 (4.8944e-01)	Acc 0.314453 (0.314453)
Epoch: [35][300/616]	Loss 4.7997e-01 (4.8630e-01)	Acc 0.331055 (0.297307)
Epoch: [35][600/616]	Loss 4.4998e-01 (4.7621e-01)	Acc 0.365234 (0.316668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.354167 (0.348365)
Training Loss of Epoch 35: 0.4757056135956834
Training Acc of Epoch 35: 0.3174050431910569
Testing Acc of Epoch 35: 0.3483652173913043
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 4.5842e-01 (4.5842e-01)	Acc 0.333984 (0.333984)
Epoch: [36][300/616]	Loss 4.5965e-01 (4.5544e-01)	Acc 0.331055 (0.344360)
Epoch: [36][600/616]	Loss 4.5599e-01 (4.5600e-01)	Acc 0.338867 (0.341919)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.330128 (0.330557)
Training Loss of Epoch 36: 0.45597500345571257
Training Acc of Epoch 36: 0.3419270833333333
Testing Acc of Epoch 36: 0.3305565217391304
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 4.6404e-01 (4.6404e-01)	Acc 0.299805 (0.299805)
Epoch: [37][300/616]	Loss 4.5410e-01 (4.5793e-01)	Acc 0.359375 (0.337920)
Epoch: [37][600/616]	Loss 4.5440e-01 (4.5850e-01)	Acc 0.333008 (0.338791)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.347756 (0.345548)
Training Loss of Epoch 37: 0.4585333253309979
Training Acc of Epoch 37: 0.3389180005081301
Testing Acc of Epoch 37: 0.3455478260869565
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 4.6309e-01 (4.6309e-01)	Acc 0.317383 (0.317383)
Epoch: [38][300/616]	Loss 4.5410e-01 (4.5891e-01)	Acc 0.350586 (0.334516)
Epoch: [38][600/616]	Loss 5.0040e-01 (4.6800e-01)	Acc 0.174805 (0.304601)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201274)
Training Loss of Epoch 38: 0.46873984850519074
Training Acc of Epoch 38: 0.30240250254065043
Testing Acc of Epoch 38: 0.20127391304347825
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.199219)
Epoch: [39][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201392)
Epoch: [39][600/616]	Loss 4.9894e-01 (5.0161e-01)	Acc 0.225586 (0.201386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.259615 (0.280804)
Training Loss of Epoch 39: 0.5015535461224192
Training Acc of Epoch 39: 0.20238344766260163
Testing Acc of Epoch 39: 0.28080434782608693
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 4.9994e-01 (4.9994e-01)	Acc 0.282227 (0.282227)
Epoch: [40][300/616]	Loss 5.0037e-01 (5.0028e-01)	Acc 0.303711 (0.326830)
Epoch: [40][600/616]	Loss 4.9944e-01 (4.9884e-01)	Acc 0.304688 (0.334774)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.371795 (0.366935)
Training Loss of Epoch 40: 0.49886087440862886
Training Acc of Epoch 40: 0.33423526422764227
Testing Acc of Epoch 40: 0.36693478260869566
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 4.9993e-01 (4.9993e-01)	Acc 0.366211 (0.366211)
Epoch: [41][300/616]	Loss 5.0040e-01 (5.0406e-01)	Acc 0.198242 (0.268250)
Epoch: [41][600/616]	Loss 4.9501e-01 (5.0195e-01)	Acc 0.318359 (0.237781)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.323718 (0.317287)
Training Loss of Epoch 41: 0.5018001574810927
Training Acc of Epoch 41: 0.23940548780487805
Testing Acc of Epoch 41: 0.3172869565217391
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 4.9657e-01 (4.9657e-01)	Acc 0.298828 (0.298828)
Epoch: [42][300/616]	Loss 4.9834e-01 (4.9706e-01)	Acc 0.321289 (0.331473)
Epoch: [42][600/616]	Loss 5.0040e-01 (4.9280e-01)	Acc 0.208984 (0.301677)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 42: 0.4929743647090788
Training Acc of Epoch 42: 0.2992219258130081
Testing Acc of Epoch 42: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.189453)
Epoch: [43][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.215820 (0.201331)
Epoch: [43][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.201542)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 43: 0.5004024453279449
Training Acc of Epoch 43: 0.20145928607723576
Testing Acc of Epoch 43: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.191406)
Epoch: [44][300/616]	Loss 4.9751e-01 (4.9771e-01)	Acc 0.218750 (0.214360)
Epoch: [44][600/616]	Loss 4.9931e-01 (4.9529e-01)	Acc 0.219727 (0.229882)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.245192 (0.237787)
Training Loss of Epoch 44: 0.4951119218899952
Training Acc of Epoch 44: 0.23043381605691057
Testing Acc of Epoch 44: 0.23778695652173912
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 4.9484e-01 (4.9484e-01)	Acc 0.235352 (0.235352)
Epoch: [45][300/616]	Loss 5.0040e-01 (4.9925e-01)	Acc 0.190430 (0.208348)
Epoch: [45][600/616]	Loss 4.9894e-01 (4.9962e-01)	Acc 0.205078 (0.223513)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.243590 (0.242513)
Training Loss of Epoch 45: 0.4995953002111699
Training Acc of Epoch 45: 0.2239646849593496
Testing Acc of Epoch 45: 0.24251304347826086
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 5.0007e-01 (5.0007e-01)	Acc 0.234375 (0.234375)
Epoch: [46][300/616]	Loss 5.0040e-01 (5.0033e-01)	Acc 0.192383 (0.220106)
Epoch: [46][600/616]	Loss 5.0040e-01 (5.0036e-01)	Acc 0.219727 (0.210653)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 46: 0.5003651038417971
Training Acc of Epoch 46: 0.21036426575203251
Testing Acc of Epoch 46: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.207031 (0.207031)
Epoch: [47][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.200779)
Epoch: [47][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.201537)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 47: 0.5004024504646053
Training Acc of Epoch 47: 0.20146404979674798
Testing Acc of Epoch 47: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.193359)
Epoch: [48][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201876)
Epoch: [48][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.215820 (0.201381)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 48: 0.5004024503676872
Training Acc of Epoch 48: 0.20145134654471544
Testing Acc of Epoch 48: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.208984)
Epoch: [49][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.223633 (0.201058)
Epoch: [49][600/616]	Loss 5.0064e-01 (5.0420e-01)	Acc 0.177734 (0.201937)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 49: 0.5041176722786291
Training Acc of Epoch 49: 0.20190548780487805
Testing Acc of Epoch 49: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [50][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.202194)
Epoch: [50][600/616]	Loss 5.0023e-01 (5.0037e-01)	Acc 0.157227 (0.214483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.235577 (0.254457)
Training Loss of Epoch 50: 0.5003641479383639
Training Acc of Epoch 50: 0.21524707825203251
Testing Acc of Epoch 50: 0.2544565217391304
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 5.0006e-01 (5.0006e-01)	Acc 0.254883 (0.254883)
Epoch: [51][300/616]	Loss 5.0040e-01 (5.0030e-01)	Acc 0.216797 (0.270352)
Epoch: [51][600/616]	Loss 5.0040e-01 (5.0035e-01)	Acc 0.211914 (0.237160)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 51: 0.5003539718263518
Training Acc of Epoch 51: 0.23639799288617885
Testing Acc of Epoch 51: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [52][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.201860)
Epoch: [52][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 52: 0.500402450173851
Training Acc of Epoch 52: 0.20144658282520325
Testing Acc of Epoch 52: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.196289)
Epoch: [53][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.206055 (0.201263)
Epoch: [53][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 53: 0.5004024502707691
Training Acc of Epoch 53: 0.20144975863821138
Testing Acc of Epoch 53: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.215820 (0.215820)
Epoch: [54][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.201590)
Epoch: [54][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.201515)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 54: 0.5004024502707691
Training Acc of Epoch 54: 0.2014449949186992
Testing Acc of Epoch 54: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.226562 (0.226562)
Epoch: [55][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.201133)
Epoch: [55][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.206055 (0.201440)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 55: 0.5004024505615234
Training Acc of Epoch 55: 0.20145452235772357
Testing Acc of Epoch 55: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [56][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.201986)
Epoch: [56][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208008 (0.201429)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 56: 0.5004024502707691
Training Acc of Epoch 56: 0.20145452235772357
Testing Acc of Epoch 56: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.197266)
Epoch: [57][300/616]	Loss 5.0041e-01 (5.0040e-01)	Acc 0.089844 (0.198242)
Epoch: [57][600/616]	Loss 4.8594e-01 (4.9923e-01)	Acc 0.280273 (0.222679)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.290064 (0.283078)
Training Loss of Epoch 57: 0.4987391666668217
Training Acc of Epoch 57: 0.22430132113821138
Testing Acc of Epoch 57: 0.2830782608695652
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 4.7540e-01 (4.7540e-01)	Acc 0.264648 (0.264648)
Epoch: [58][300/616]	Loss 5.0036e-01 (4.9158e-01)	Acc 0.322266 (0.317269)
Epoch: [58][600/616]	Loss 5.0040e-01 (4.9598e-01)	Acc 0.197266 (0.278155)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 58: 0.4960841059200163
Training Acc of Epoch 58: 0.27646087398373986
Testing Acc of Epoch 58: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.206055 (0.206055)
Epoch: [59][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.200929)
Epoch: [59][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.217773 (0.201481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 59: 0.5004024505615234
Training Acc of Epoch 59: 0.20145928607723576
Testing Acc of Epoch 59: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.181641 (0.181641)
Epoch: [60][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.230469 (0.201623)
Epoch: [60][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.201502)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 60: 0.5004024503676872
Training Acc of Epoch 60: 0.20146404979674798
Testing Acc of Epoch 60: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.214844)
Epoch: [61][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.207031 (0.201733)
Epoch: [61][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.206055 (0.201404)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 61: 0.5004024503676872
Training Acc of Epoch 61: 0.20144817073170732
Testing Acc of Epoch 61: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.218750 (0.218750)
Epoch: [62][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.202710)
Epoch: [62][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.202148 (0.201367)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 62: 0.5004024502707691
Training Acc of Epoch 62: 0.20142752794715446
Testing Acc of Epoch 62: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [63][300/616]	Loss 5.0040e-01 (5.0349e-01)	Acc 0.200195 (0.200455)
Epoch: [63][600/616]	Loss 5.0040e-01 (5.0195e-01)	Acc 0.216797 (0.200949)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 63: 0.501913595296503
Training Acc of Epoch 63: 0.20083841463414634
Testing Acc of Epoch 63: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.199219)
Epoch: [64][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201976)
Epoch: [64][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.201459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 64: 0.5004024503676872
Training Acc of Epoch 64: 0.2014529344512195
Testing Acc of Epoch 64: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [65][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.176758 (0.201344)
Epoch: [65][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.201386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 65: 0.500402450173851
Training Acc of Epoch 65: 0.20145611026422763
Testing Acc of Epoch 65: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.183594 (0.183594)
Epoch: [66][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.207031 (0.201188)
Epoch: [66][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.201354)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 66: 0.5004024503676872
Training Acc of Epoch 66: 0.20145611026422763
Testing Acc of Epoch 66: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.189453)
Epoch: [67][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201568)
Epoch: [67][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.201555)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 67: 0.500402450173851
Training Acc of Epoch 67: 0.20143387957317074
Testing Acc of Epoch 67: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.205078)
Epoch: [68][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.201457)
Epoch: [68][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.180664 (0.201498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 68: 0.500402450173851
Training Acc of Epoch 68: 0.2014529344512195
Testing Acc of Epoch 68: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.196289 (0.196289)
Epoch: [69][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201016)
Epoch: [69][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.186523 (0.201368)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 69: 0.5004024500769328
Training Acc of Epoch 69: 0.20146246189024392
Testing Acc of Epoch 69: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.207031 (0.207031)
Epoch: [70][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.202090)
Epoch: [70][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.201680)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 70: 0.5004024503676872
Training Acc of Epoch 70: 0.20145134654471544
Testing Acc of Epoch 70: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.187500)
Epoch: [71][300/616]	Loss 4.5400e-01 (4.9880e-01)	Acc 0.316406 (0.240595)
Epoch: [71][600/616]	Loss 4.4342e-01 (4.7282e-01)	Acc 0.293945 (0.280007)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.165064 (0.183122)
Training Loss of Epoch 71: 0.47229687765361816
Training Acc of Epoch 71: 0.27961286839430893
Testing Acc of Epoch 71: 0.18312173913043478
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 4.7319e-01 (4.7319e-01)	Acc 0.184570 (0.184570)
Epoch: [72][300/616]	Loss 4.4134e-01 (4.5217e-01)	Acc 0.231445 (0.288154)
Epoch: [72][600/616]	Loss 4.6149e-01 (4.4887e-01)	Acc 0.273438 (0.299502)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.266026 (0.289478)
Training Loss of Epoch 72: 0.44901372993864663
Training Acc of Epoch 72: 0.29938071646341463
Testing Acc of Epoch 72: 0.28947826086956524
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 4.6748e-01 (4.6748e-01)	Acc 0.293945 (0.293945)
Epoch: [73][300/616]	Loss 4.4518e-01 (4.4892e-01)	Acc 0.308594 (0.296135)
Epoch: [73][600/616]	Loss 4.8932e-01 (4.5892e-01)	Acc 0.280273 (0.293931)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.245192 (0.235552)
Training Loss of Epoch 73: 0.45956532053831145
Training Acc of Epoch 73: 0.29270356961382116
Testing Acc of Epoch 73: 0.23555217391304348
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 4.9085e-01 (4.9085e-01)	Acc 0.216797 (0.216797)
Epoch: [74][300/616]	Loss 4.9236e-01 (4.9493e-01)	Acc 0.251953 (0.278732)
Epoch: [74][600/616]	Loss 4.8710e-01 (4.9023e-01)	Acc 0.280273 (0.289571)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.280449 (0.278096)
Training Loss of Epoch 74: 0.4901326887491273
Training Acc of Epoch 74: 0.28910219766260165
Testing Acc of Epoch 74: 0.278095652173913
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 4.8515e-01 (4.8515e-01)	Acc 0.266602 (0.266602)
Epoch: [75][300/616]	Loss 4.9639e-01 (4.8965e-01)	Acc 0.315430 (0.296693)
Epoch: [75][600/616]	Loss 4.9981e-01 (4.9405e-01)	Acc 0.293945 (0.303745)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.333333 (0.318843)
Training Loss of Epoch 75: 0.4941848327473896
Training Acc of Epoch 75: 0.3039475355691057
Testing Acc of Epoch 75: 0.31884347826086956
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 4.9987e-01 (4.9987e-01)	Acc 0.350586 (0.350586)
Epoch: [76][300/616]	Loss 5.0031e-01 (5.0014e-01)	Acc 0.337891 (0.310057)
Epoch: [76][600/616]	Loss 5.0039e-01 (5.0025e-01)	Acc 0.329102 (0.308407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.336538 (0.325270)
Training Loss of Epoch 76: 0.5002564877029357
Training Acc of Epoch 76: 0.3087430132113821
Testing Acc of Epoch 76: 0.3252695652173913
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 5.0039e-01 (5.0039e-01)	Acc 0.338867 (0.338867)
Epoch: [77][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.296875 (0.313700)
Epoch: [77][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.293945 (0.310758)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.312500 (0.301978)
Training Loss of Epoch 77: 0.5003995800405983
Training Acc of Epoch 77: 0.310572281504065
Testing Acc of Epoch 77: 0.3019782608695652
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.285156 (0.285156)
Epoch: [78][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.183594 (0.227854)
Epoch: [78][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.205078 (0.214938)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 78: 0.5004023295107896
Training Acc of Epoch 78: 0.2145388719512195
Testing Acc of Epoch 78: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.206055 (0.206055)
Epoch: [79][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.201133)
Epoch: [79][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201468)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 79: 0.5004024503676872
Training Acc of Epoch 79: 0.2014307037601626
Testing Acc of Epoch 79: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.192383)
Epoch: [80][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.177734 (0.201931)
Epoch: [80][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.200195 (0.201474)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 80: 0.5004024503676872
Training Acc of Epoch 80: 0.2014576981707317
Testing Acc of Epoch 80: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.214844)
Epoch: [81][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.214844 (0.201328)
Epoch: [81][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.212891 (0.201537)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 81: 0.500402450173851
Training Acc of Epoch 81: 0.20144817073170732
Testing Acc of Epoch 81: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.204102)
Epoch: [82][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201435)
Epoch: [82][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.201500)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 82: 0.5004024500769328
Training Acc of Epoch 82: 0.201440231199187
Testing Acc of Epoch 82: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.191406)
Epoch: [83][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201389)
Epoch: [83][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.190430 (0.201536)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 83: 0.5004024503676872
Training Acc of Epoch 83: 0.20146563770325204
Testing Acc of Epoch 83: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.187500)
Epoch: [84][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.201347)
Epoch: [84][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.201443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 84: 0.5004024503676872
Training Acc of Epoch 84: 0.20146563770325204
Testing Acc of Epoch 84: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.216797 (0.216797)
Epoch: [85][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.200964)
Epoch: [85][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.222656 (0.201469)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 85: 0.5004024503676872
Training Acc of Epoch 85: 0.20146246189024392
Testing Acc of Epoch 85: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [86][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.216797 (0.202093)
Epoch: [86][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 86: 0.500402450173851
Training Acc of Epoch 86: 0.2014529344512195
Testing Acc of Epoch 86: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.203125)
Epoch: [87][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.201642)
Epoch: [87][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 87: 0.5004024498830966
Training Acc of Epoch 87: 0.20144181910569106
Testing Acc of Epoch 87: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.226562 (0.226562)
Epoch: [88][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.222656 (0.201500)
Epoch: [88][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.226562 (0.201407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 88: 0.5004024502707691
Training Acc of Epoch 88: 0.20146881351626017
Testing Acc of Epoch 88: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.199219 (0.199219)
Epoch: [89][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.173828 (0.201668)
Epoch: [89][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.193359 (0.201430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 89: 0.5004024503676872
Training Acc of Epoch 89: 0.20145928607723576
Testing Acc of Epoch 89: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.213867)
Epoch: [90][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.191406 (0.202087)
Epoch: [90][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.225586 (0.201334)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 90: 0.5004024503676872
Training Acc of Epoch 90: 0.20145611026422763
Testing Acc of Epoch 90: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [91][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.201665)
Epoch: [91][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.201430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 91: 0.500402450173851
Training Acc of Epoch 91: 0.20146404979674798
Testing Acc of Epoch 91: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.198242)
Epoch: [92][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.181641 (0.201740)
Epoch: [92][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201698)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 92: 0.5004024500769328
Training Acc of Epoch 92: 0.2014672256097561
Testing Acc of Epoch 92: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.213867 (0.213867)
Epoch: [93][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.189453 (0.201204)
Epoch: [93][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.204102 (0.201495)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 93: 0.5004024502707691
Training Acc of Epoch 93: 0.2014529344512195
Testing Acc of Epoch 93: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.178711 (0.178711)
Epoch: [94][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.203125 (0.200805)
Epoch: [94][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.217773 (0.201560)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 94: 0.500402450173851
Training Acc of Epoch 94: 0.2014576981707317
Testing Acc of Epoch 94: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.212891 (0.212891)
Epoch: [95][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.181641 (0.201973)
Epoch: [95][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.173828 (0.201396)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 95: 0.5004024505615234
Training Acc of Epoch 95: 0.20144975863821138
Testing Acc of Epoch 95: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.197266 (0.197266)
Epoch: [96][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.216797 (0.201697)
Epoch: [96][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.216797 (0.201518)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 96: 0.5004024504646053
Training Acc of Epoch 96: 0.20146563770325204
Testing Acc of Epoch 96: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.187500 (0.187500)
Epoch: [97][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.209961 (0.201788)
Epoch: [97][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.219727 (0.201542)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 97: 0.5004024505615234
Training Acc of Epoch 97: 0.2014354674796748
Testing Acc of Epoch 97: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.188477 (0.188477)
Epoch: [98][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.202392)
Epoch: [98][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.222656 (0.201461)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 98: 0.5004024504646053
Training Acc of Epoch 98: 0.2014529344512195
Testing Acc of Epoch 98: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.192383 (0.192383)
Epoch: [99][300/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.198242 (0.201467)
Epoch: [99][600/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.208984 (0.201451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 99: 0.500402450173851
Training Acc of Epoch 99: 0.20145452235772357
Testing Acc of Epoch 99: 0.20121739130434782
Early stopping not satisfied.
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_6b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0050e-01 (5.0050e-01)	Acc 0.137695 (0.137695)
Epoch: [0][300/616]	Loss 2.8252e-01 (3.0596e-01)	Acc 0.712891 (0.667310)
Epoch: [0][600/616]	Loss 2.6178e-01 (2.9201e-01)	Acc 0.738281 (0.688360)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.714339)
Training Loss of Epoch 0: 0.29156681352514563
Training Acc of Epoch 0: 0.688940231199187
Testing Acc of Epoch 0: 0.7143391304347826
Model with the best training loss saved! The loss is 0.29156681352514563
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6766e-01 (2.6766e-01)	Acc 0.714844 (0.714844)
Epoch: [1][300/616]	Loss 2.6417e-01 (2.7395e-01)	Acc 0.736328 (0.714552)
Epoch: [1][600/616]	Loss 2.8062e-01 (2.7382e-01)	Acc 0.700195 (0.714792)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.722983)
Training Loss of Epoch 1: 0.2736324026817229
Training Acc of Epoch 1: 0.7149993648373983
Testing Acc of Epoch 1: 0.7229826086956522
Model with the best training loss saved! The loss is 0.2736324026817229
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.6229e-01 (2.6229e-01)	Acc 0.732422 (0.732422)
Epoch: [2][300/616]	Loss 2.8846e-01 (2.7475e-01)	Acc 0.706055 (0.714276)
Epoch: [2][600/616]	Loss 2.6591e-01 (2.7367e-01)	Acc 0.729492 (0.714636)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.728483)
Training Loss of Epoch 2: 0.2735175448462246
Training Acc of Epoch 2: 0.7147913490853659
Testing Acc of Epoch 2: 0.7284826086956522
Model with the best training loss saved! The loss is 0.2735175448462246
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4806e-01 (2.4806e-01)	Acc 0.728516 (0.728516)
Epoch: [3][300/616]	Loss 2.8100e-01 (2.7383e-01)	Acc 0.715820 (0.714227)
Epoch: [3][600/616]	Loss 2.8486e-01 (2.7439e-01)	Acc 0.708984 (0.714096)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.724787)
Training Loss of Epoch 3: 0.2741882022803392
Training Acc of Epoch 3: 0.7142879827235772
Testing Acc of Epoch 3: 0.7247869565217391
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6945e-01 (2.6945e-01)	Acc 0.714844 (0.714844)
Epoch: [4][300/616]	Loss 2.8174e-01 (2.7360e-01)	Acc 0.706055 (0.715051)
Epoch: [4][600/616]	Loss 2.7271e-01 (2.7261e-01)	Acc 0.720703 (0.716074)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.727126)
Training Loss of Epoch 4: 0.2724440807976374
Training Acc of Epoch 4: 0.7161490091463415
Testing Acc of Epoch 4: 0.7271260869565217
Model with the best training loss saved! The loss is 0.2724440807976374
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.7448e-01 (2.7448e-01)	Acc 0.719727 (0.719727)
Epoch: [5][300/616]	Loss 3.0446e-01 (2.7152e-01)	Acc 0.668945 (0.716398)
Epoch: [5][600/616]	Loss 2.5795e-01 (2.7123e-01)	Acc 0.730469 (0.716847)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.719278)
Training Loss of Epoch 5: 0.2711716352924099
Training Acc of Epoch 5: 0.7169715447154471
Testing Acc of Epoch 5: 0.7192782608695653
Model with the best training loss saved! The loss is 0.2711716352924099
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.7799e-01 (2.7799e-01)	Acc 0.704102 (0.704102)
Epoch: [6][300/616]	Loss 2.6289e-01 (2.7285e-01)	Acc 0.726562 (0.715606)
Epoch: [6][600/616]	Loss 2.7043e-01 (2.7222e-01)	Acc 0.710938 (0.716178)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.727026)
Training Loss of Epoch 6: 0.2722296361758457
Training Acc of Epoch 6: 0.716137893800813
Testing Acc of Epoch 6: 0.7270260869565217
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.6190e-01 (2.6190e-01)	Acc 0.742188 (0.742188)
Epoch: [7][300/616]	Loss 2.8282e-01 (2.7073e-01)	Acc 0.706055 (0.717546)
Epoch: [7][600/616]	Loss 3.0852e-01 (2.7107e-01)	Acc 0.689453 (0.717494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.730413)
Training Loss of Epoch 7: 0.2710502532439503
Training Acc of Epoch 7: 0.7174733231707318
Testing Acc of Epoch 7: 0.7304130434782609
Model with the best training loss saved! The loss is 0.2710502532439503
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5841e-01 (2.5841e-01)	Acc 0.735352 (0.735352)
Epoch: [8][300/616]	Loss 3.0749e-01 (2.7264e-01)	Acc 0.692383 (0.714935)
Epoch: [8][600/616]	Loss 2.5818e-01 (2.7239e-01)	Acc 0.735352 (0.715871)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.713078)
Training Loss of Epoch 8: 0.2724726374071788
Training Acc of Epoch 8: 0.7158584222560975
Testing Acc of Epoch 8: 0.7130782608695653
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.8611e-01 (2.8611e-01)	Acc 0.709961 (0.709961)
Epoch: [9][300/616]	Loss 2.8109e-01 (2.7173e-01)	Acc 0.707031 (0.716716)
Epoch: [9][600/616]	Loss 2.6573e-01 (2.7185e-01)	Acc 0.715820 (0.716867)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.674679 (0.697013)
Training Loss of Epoch 9: 0.2717149082964998
Training Acc of Epoch 9: 0.7170477642276423
Testing Acc of Epoch 9: 0.6970130434782609
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.9777e-01 (2.9777e-01)	Acc 0.688477 (0.688477)
Epoch: [10][300/616]	Loss 2.5973e-01 (2.7324e-01)	Acc 0.726562 (0.716291)
Epoch: [10][600/616]	Loss 2.8298e-01 (2.7291e-01)	Acc 0.712891 (0.716028)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.718239)
Training Loss of Epoch 10: 0.2729486314261832
Training Acc of Epoch 10: 0.7162331681910569
Testing Acc of Epoch 10: 0.7182391304347826
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.7214e-01 (2.7214e-01)	Acc 0.717773 (0.717773)
Epoch: [11][300/616]	Loss 2.7784e-01 (2.7027e-01)	Acc 0.699219 (0.718179)
Epoch: [11][600/616]	Loss 3.0223e-01 (2.7025e-01)	Acc 0.657227 (0.718163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.700661)
Training Loss of Epoch 11: 0.27047666066545784
Training Acc of Epoch 11: 0.7178591844512195
Testing Acc of Epoch 11: 0.7006608695652173
Model with the best training loss saved! The loss is 0.27047666066545784
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.7721e-01 (2.7721e-01)	Acc 0.710938 (0.710938)
Epoch: [12][300/616]	Loss 2.6753e-01 (2.7790e-01)	Acc 0.717773 (0.712475)
Epoch: [12][600/616]	Loss 2.7342e-01 (2.7567e-01)	Acc 0.715820 (0.714390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.727452)
Training Loss of Epoch 12: 0.27546397815874923
Training Acc of Epoch 12: 0.7145356961382113
Testing Acc of Epoch 12: 0.7274521739130435
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.5407e-01 (2.5407e-01)	Acc 0.732422 (0.732422)
Epoch: [13][300/616]	Loss 2.6738e-01 (2.6853e-01)	Acc 0.724609 (0.719217)
Epoch: [13][600/616]	Loss 2.7594e-01 (2.6991e-01)	Acc 0.703125 (0.717928)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.665064 (0.687717)
Training Loss of Epoch 13: 0.26998726151338437
Training Acc of Epoch 13: 0.7178512449186992
Testing Acc of Epoch 13: 0.6877173913043478
Model with the best training loss saved! The loss is 0.26998726151338437
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 3.0140e-01 (3.0140e-01)	Acc 0.681641 (0.681641)
Epoch: [14][300/616]	Loss 2.7631e-01 (2.7038e-01)	Acc 0.713867 (0.717949)
Epoch: [14][600/616]	Loss 2.9473e-01 (2.7066e-01)	Acc 0.675781 (0.718030)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.689103 (0.709343)
Training Loss of Epoch 14: 0.27084962679603236
Training Acc of Epoch 14: 0.7178814151422764
Testing Acc of Epoch 14: 0.7093434782608695
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.9218e-01 (2.9218e-01)	Acc 0.684570 (0.684570)
Epoch: [15][300/616]	Loss 2.7897e-01 (2.7421e-01)	Acc 0.702148 (0.714944)
Epoch: [15][600/616]	Loss 2.7942e-01 (2.7359e-01)	Acc 0.705078 (0.715156)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.704409)
Training Loss of Epoch 15: 0.27346248803584555
Training Acc of Epoch 15: 0.7153026549796748
Testing Acc of Epoch 15: 0.7044086956521739
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.6675e-01 (2.6675e-01)	Acc 0.717773 (0.717773)
Epoch: [16][300/616]	Loss 2.6310e-01 (2.7065e-01)	Acc 0.732422 (0.717517)
Epoch: [16][600/616]	Loss 2.4466e-01 (2.7152e-01)	Acc 0.754883 (0.717164)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.709609)
Training Loss of Epoch 16: 0.27172874725931057
Training Acc of Epoch 16: 0.716968368902439
Testing Acc of Epoch 16: 0.7096086956521739
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.8258e-01 (2.8258e-01)	Acc 0.711914 (0.711914)
Epoch: [17][300/616]	Loss 2.6678e-01 (2.7524e-01)	Acc 0.709961 (0.713085)
Epoch: [17][600/616]	Loss 2.7605e-01 (2.7532e-01)	Acc 0.704102 (0.713570)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.696722)
Training Loss of Epoch 17: 0.27546052499030665
Training Acc of Epoch 17: 0.7134019308943089
Testing Acc of Epoch 17: 0.6967217391304348
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 3.0953e-01 (3.0953e-01)	Acc 0.696289 (0.696289)
Epoch: [18][300/616]	Loss 2.8480e-01 (2.7120e-01)	Acc 0.692383 (0.716618)
Epoch: [18][600/616]	Loss 2.7481e-01 (2.7468e-01)	Acc 0.714844 (0.714163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.701923 (0.722848)
Training Loss of Epoch 18: 0.2746264263381803
Training Acc of Epoch 18: 0.7142133511178862
Testing Acc of Epoch 18: 0.7228478260869565
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.5634e-01 (2.5634e-01)	Acc 0.713867 (0.713867)
Epoch: [19][300/616]	Loss 2.6328e-01 (2.7090e-01)	Acc 0.738281 (0.717261)
Epoch: [19][600/616]	Loss 2.5384e-01 (2.7207e-01)	Acc 0.736328 (0.716597)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.700321 (0.722913)
Training Loss of Epoch 19: 0.2720164100086786
Training Acc of Epoch 19: 0.7166555513211382
Testing Acc of Epoch 19: 0.7229130434782609
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.8028e-01 (2.8028e-01)	Acc 0.712891 (0.712891)
Epoch: [20][300/616]	Loss 3.1829e-01 (3.0750e-01)	Acc 0.599609 (0.638974)
Epoch: [20][600/616]	Loss 3.6743e-01 (3.1697e-01)	Acc 0.482422 (0.631241)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.482372 (0.470539)
Training Loss of Epoch 20: 0.3185129632067874
Training Acc of Epoch 20: 0.6270150533536586
Testing Acc of Epoch 20: 0.4705391304347826
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 3.6710e-01 (3.6710e-01)	Acc 0.454102 (0.454102)
Epoch: [21][300/616]	Loss 4.6004e-01 (3.7780e-01)	Acc 0.323242 (0.493508)
Epoch: [21][600/616]	Loss 4.4796e-01 (4.0920e-01)	Acc 0.414062 (0.433238)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.334936 (0.343639)
Training Loss of Epoch 21: 0.41079125729033616
Training Acc of Epoch 21: 0.4321058816056911
Testing Acc of Epoch 21: 0.3436391304347826
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 4.9856e-01 (4.9856e-01)	Acc 0.328125 (0.328125)
Epoch: [22][300/616]	Loss 5.0165e-01 (4.7735e-01)	Acc 0.187500 (0.308788)
Epoch: [22][600/616]	Loss 5.0038e-01 (4.8937e-01)	Acc 0.213867 (0.255140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 22: 0.48962370559452023
Training Acc of Epoch 22: 0.25386655233739835
Testing Acc of Epoch 22: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.194336 (0.194336)
Epoch: [23][300/616]	Loss 5.0027e-01 (5.0131e-01)	Acc 0.211914 (0.201454)
Epoch: [23][600/616]	Loss 4.9885e-01 (5.0075e-01)	Acc 0.216797 (0.202925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.225962 (0.227230)
Training Loss of Epoch 23: 0.5007116939963364
Training Acc of Epoch 23: 0.2033298399390244
Testing Acc of Epoch 23: 0.2272304347826087
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 4.9903e-01 (4.9903e-01)	Acc 0.238281 (0.238281)
Epoch: [24][300/616]	Loss 4.5574e-01 (4.6823e-01)	Acc 0.343750 (0.317902)
Epoch: [24][600/616]	Loss 3.7713e-01 (4.5411e-01)	Acc 0.510742 (0.340100)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.519231 (0.532239)
Training Loss of Epoch 24: 0.45230986950843316
Training Acc of Epoch 24: 0.34400565294715446
Testing Acc of Epoch 24: 0.5322391304347827
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 3.7379e-01 (3.7379e-01)	Acc 0.537109 (0.537109)
Epoch: [25][300/616]	Loss 3.8029e-01 (3.7957e-01)	Acc 0.470703 (0.502021)
Epoch: [25][600/616]	Loss 3.2266e+01 (3.5592e+00)	Acc 0.193359 (0.377184)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 25: 4.208124015631714
Training Acc of Epoch 25: 0.3731135670731707
Testing Acc of Epoch 25: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [26][300/616]	Loss 2.2369e+01 (2.8592e+01)	Acc 0.222656 (0.210944)
Epoch: [26][600/616]	Loss 3.1602e+01 (2.7035e+01)	Acc 0.209961 (0.227745)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 26: 27.143395146315658
Training Acc of Epoch 26: 0.2272341844512195
Testing Acc of Epoch 26: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [27][300/616]	Loss 2.2236e+01 (3.0918e+01)	Acc 0.257812 (0.203767)
Epoch: [27][600/616]	Loss 3.2070e+01 (2.8835e+01)	Acc 0.198242 (0.216886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 27: 28.90301507469115
Training Acc of Epoch 27: 0.21659362296747967
Testing Acc of Epoch 27: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [28][300/616]	Loss 3.1836e+01 (2.8739e+01)	Acc 0.204102 (0.210804)
Epoch: [28][600/616]	Loss 2.4029e+01 (2.8498e+01)	Acc 0.226562 (0.211511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.202143)
Training Loss of Epoch 28: 28.54818537642316
Training Acc of Epoch 28: 0.2113455919715447
Testing Acc of Epoch 28: 0.20214347826086956
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 3.2812e+01 (3.2812e+01)	Acc 0.179688 (0.179688)
Epoch: [29][300/616]	Loss 3.2227e+01 (2.9419e+01)	Acc 0.194336 (0.209510)
Epoch: [29][600/616]	Loss 3.1523e+01 (2.8919e+01)	Acc 0.211914 (0.210179)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201513)
Training Loss of Epoch 29: 28.986471104815724
Training Acc of Epoch 29: 0.21001333841463415
Testing Acc of Epoch 29: 0.20151304347826088
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [30][300/616]	Loss 3.2422e+01 (3.1957e+01)	Acc 0.189453 (0.201075)
Epoch: [30][600/616]	Loss 3.1562e+01 (3.1957e+01)	Acc 0.210938 (0.201083)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 30: 31.961509146341463
Training Acc of Epoch 30: 0.2009622713414634
Testing Acc of Epoch 30: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [31][300/616]	Loss 3.1602e+01 (3.1977e+01)	Acc 0.209961 (0.200581)
Epoch: [31][600/616]	Loss 3.2539e+01 (3.1958e+01)	Acc 0.186523 (0.201057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 31: 31.96068343495935
Training Acc of Epoch 31: 0.20098291412601627
Testing Acc of Epoch 31: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [32][300/616]	Loss 3.1758e+01 (3.1973e+01)	Acc 0.206055 (0.200675)
Epoch: [32][600/616]	Loss 3.2070e+01 (3.1963e+01)	Acc 0.198242 (0.200914)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 32: 31.96169969512195
Training Acc of Epoch 32: 0.20095750762195122
Testing Acc of Epoch 32: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [33][300/616]	Loss 3.1875e+01 (3.1966e+01)	Acc 0.203125 (0.200847)
Epoch: [33][600/616]	Loss 3.0977e+01 (3.1963e+01)	Acc 0.225586 (0.200935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 33: 31.960873983739837
Training Acc of Epoch 33: 0.20097815040650407
Testing Acc of Epoch 33: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [34][300/616]	Loss 3.1992e+01 (3.1960e+01)	Acc 0.200195 (0.201003)
Epoch: [34][600/616]	Loss 3.1953e+01 (3.1962e+01)	Acc 0.201172 (0.200941)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 34: 31.961763211382113
Training Acc of Epoch 34: 0.20095591971544716
Testing Acc of Epoch 34: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.2891e+01 (3.2891e+01)	Acc 0.177734 (0.177734)
Epoch: [35][300/616]	Loss 3.2109e+01 (3.1958e+01)	Acc 0.197266 (0.201049)
Epoch: [35][600/616]	Loss 3.1719e+01 (3.1967e+01)	Acc 0.207031 (0.200818)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 35: 31.961636178861788
Training Acc of Epoch 35: 0.20095909552845528
Testing Acc of Epoch 35: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [36][300/616]	Loss 3.1562e+01 (3.1976e+01)	Acc 0.210938 (0.200594)
Epoch: [36][600/616]	Loss 3.2656e+01 (3.1962e+01)	Acc 0.183594 (0.200961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 36: 31.961890243902438
Training Acc of Epoch 36: 0.20095274390243903
Testing Acc of Epoch 36: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [37][300/616]	Loss 3.2031e+01 (3.1914e+01)	Acc 0.199219 (0.202155)
Epoch: [37][600/616]	Loss 3.2109e+01 (3.1957e+01)	Acc 0.197266 (0.201081)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 37: 31.961636178861788
Training Acc of Epoch 37: 0.20095909552845528
Testing Acc of Epoch 37: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.2617e+01 (3.2617e+01)	Acc 0.184570 (0.184570)
Epoch: [38][300/616]	Loss 3.2734e+01 (3.1979e+01)	Acc 0.181641 (0.200529)
Epoch: [38][600/616]	Loss 3.1875e+01 (3.1959e+01)	Acc 0.203125 (0.201019)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 38: 31.961064532520325
Training Acc of Epoch 38: 0.20097338668699186
Testing Acc of Epoch 38: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [39][300/616]	Loss 3.1797e+01 (3.1979e+01)	Acc 0.205078 (0.200526)
Epoch: [39][600/616]	Loss 3.1992e+01 (3.1966e+01)	Acc 0.200195 (0.200839)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 39: 31.961572662601625
Training Acc of Epoch 39: 0.20096068343495935
Testing Acc of Epoch 39: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.1289e+01 (3.1289e+01)	Acc 0.217773 (0.217773)
Epoch: [40][300/616]	Loss 3.2930e+01 (3.1942e+01)	Acc 0.176758 (0.201451)
Epoch: [40][600/616]	Loss 3.1562e+01 (3.1959e+01)	Acc 0.210938 (0.201013)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 40: 31.960746951219512
Training Acc of Epoch 40: 0.2009813262195122
Testing Acc of Epoch 40: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.3008e+01 (3.3008e+01)	Acc 0.174805 (0.174805)
Epoch: [41][300/616]	Loss 3.2422e+01 (3.1975e+01)	Acc 0.189453 (0.200620)
Epoch: [41][600/616]	Loss 3.1523e+01 (3.1958e+01)	Acc 0.211914 (0.201045)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 41: 31.961255081300813
Training Acc of Epoch 41: 0.20096862296747967
Testing Acc of Epoch 41: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.1602e+01 (3.1602e+01)	Acc 0.209961 (0.209961)
Epoch: [42][300/616]	Loss 3.1641e+01 (3.1925e+01)	Acc 0.208984 (0.201879)
Epoch: [42][600/616]	Loss 3.2422e+01 (3.1958e+01)	Acc 0.189453 (0.201057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 42: 31.96214430894309
Training Acc of Epoch 42: 0.20094639227642278
Testing Acc of Epoch 42: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [43][300/616]	Loss 3.1328e+01 (3.1967e+01)	Acc 0.216797 (0.200828)
Epoch: [43][600/616]	Loss 3.1992e+01 (3.1962e+01)	Acc 0.200195 (0.200941)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 43: 31.9614456300813
Training Acc of Epoch 43: 0.20096385924796747
Testing Acc of Epoch 43: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [44][300/616]	Loss 3.2422e+01 (3.1947e+01)	Acc 0.189453 (0.201331)
Epoch: [44][600/616]	Loss 3.2128e+01 (3.1955e+01)	Acc 0.195312 (0.201026)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 44: 31.95617791152582
Training Acc of Epoch 44: 0.2009765625
Testing Acc of Epoch 44: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.1334e+01 (3.1334e+01)	Acc 0.214844 (0.214844)
Epoch: [45][300/616]	Loss 3.1337e+01 (3.1908e+01)	Acc 0.213867 (0.200234)
Epoch: [45][600/616]	Loss 3.1004e+01 (3.1866e+01)	Acc 0.221680 (0.200855)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 45: 31.860556445858343
Training Acc of Epoch 45: 0.20095750762195122
Testing Acc of Epoch 45: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.1869e+01 (3.1869e+01)	Acc 0.200195 (0.200195)
Epoch: [46][300/616]	Loss 3.2523e+01 (3.1803e+01)	Acc 0.183594 (0.200896)
Epoch: [46][600/616]	Loss 3.1681e+01 (3.1785e+01)	Acc 0.203125 (0.200875)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 46: 31.78052467408219
Training Acc of Epoch 46: 0.20097338668699186
Testing Acc of Epoch 46: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.1241e+01 (3.1241e+01)	Acc 0.213867 (0.213867)
Epoch: [47][300/616]	Loss 3.1795e+01 (3.1718e+01)	Acc 0.199219 (0.200783)
Epoch: [47][600/616]	Loss 3.2109e+01 (3.1767e+01)	Acc 0.197266 (0.200964)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 47: 31.767522349784045
Training Acc of Epoch 47: 0.20105436991869918
Testing Acc of Epoch 47: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [48][300/616]	Loss 3.3125e+01 (3.1957e+01)	Acc 0.171875 (0.201078)
Epoch: [48][600/616]	Loss 3.1914e+01 (3.1944e+01)	Acc 0.202148 (0.201399)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 48: 31.945551969946884
Training Acc of Epoch 48: 0.20136083587398373
Testing Acc of Epoch 48: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.1602e+01 (3.1602e+01)	Acc 0.209961 (0.209961)
Epoch: [49][300/616]	Loss 3.1445e+01 (3.1942e+01)	Acc 0.213867 (0.201444)
Epoch: [49][600/616]	Loss 3.1484e+01 (3.1944e+01)	Acc 0.212891 (0.201401)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 49: 31.945185467479675
Training Acc of Epoch 49: 0.20137036331300814
Testing Acc of Epoch 49: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [50][300/616]	Loss 3.2070e+01 (3.1923e+01)	Acc 0.198242 (0.201925)
Epoch: [50][600/616]	Loss 3.2188e+01 (3.1943e+01)	Acc 0.195312 (0.201433)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 50: 31.945503048780488
Training Acc of Epoch 50: 0.2013624237804878
Testing Acc of Epoch 50: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [51][300/616]	Loss 3.1719e+01 (3.1910e+01)	Acc 0.207031 (0.202243)
Epoch: [51][600/616]	Loss 3.1562e+01 (3.1941e+01)	Acc 0.210938 (0.201487)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 51: 31.944994918699187
Training Acc of Epoch 51: 0.20137512703252033
Testing Acc of Epoch 51: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [52][300/616]	Loss 3.2148e+01 (3.1941e+01)	Acc 0.196289 (0.201470)
Epoch: [52][600/616]	Loss 3.1875e+01 (3.1942e+01)	Acc 0.203125 (0.201453)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 52: 31.945185467479675
Training Acc of Epoch 52: 0.20137036331300814
Testing Acc of Epoch 52: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [53][300/616]	Loss 3.2656e+01 (3.1962e+01)	Acc 0.183594 (0.200938)
Epoch: [53][600/616]	Loss 3.2227e+01 (3.1945e+01)	Acc 0.194336 (0.201367)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 53: 31.945630081300813
Training Acc of Epoch 53: 0.20135924796747967
Testing Acc of Epoch 53: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 3.2344e+01 (3.2344e+01)	Acc 0.191406 (0.191406)
Epoch: [54][300/616]	Loss 3.1992e+01 (3.1957e+01)	Acc 0.200195 (0.201071)
Epoch: [54][600/616]	Loss 3.2109e+01 (3.1943e+01)	Acc 0.197266 (0.201430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 54: 31.945693597560975
Training Acc of Epoch 54: 0.2013576600609756
Testing Acc of Epoch 54: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [55][300/616]	Loss 3.1914e+01 (3.1961e+01)	Acc 0.202148 (0.200971)
Epoch: [55][600/616]	Loss 3.2811e+01 (3.1942e+01)	Acc 0.179688 (0.201437)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 55: 31.94481233426226
Training Acc of Epoch 55: 0.20137512703252033
Testing Acc of Epoch 55: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 3.2266e+01 (3.2266e+01)	Acc 0.193359 (0.193359)
Epoch: [56][300/616]	Loss 3.1796e+01 (3.1922e+01)	Acc 0.204102 (0.201441)
Epoch: [56][600/616]	Loss 3.1144e+01 (3.1895e+01)	Acc 0.218750 (0.201399)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 56: 31.89416866457559
Training Acc of Epoch 56: 0.20137353912601627
Testing Acc of Epoch 56: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 3.2444e+01 (3.2444e+01)	Acc 0.185547 (0.185547)
Epoch: [57][300/616]	Loss 3.1296e+01 (3.1652e+01)	Acc 0.200195 (0.200805)
Epoch: [57][600/616]	Loss 2.9979e+01 (3.1187e+01)	Acc 0.212891 (0.201273)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 57: 31.16327812691045
Training Acc of Epoch 57: 0.20136559959349593
Testing Acc of Epoch 57: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 3.0845e+01 (3.0845e+01)	Acc 0.187500 (0.187500)
Epoch: [58][300/616]	Loss 3.2769e+01 (3.0675e+01)	Acc 0.180664 (0.201097)
Epoch: [58][600/616]	Loss 3.2266e+01 (3.1289e+01)	Acc 0.193359 (0.201052)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 58: 31.300303395589193
Training Acc of Epoch 58: 0.20115123221544715
Testing Acc of Epoch 58: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [59][300/616]	Loss 3.1523e+01 (3.1949e+01)	Acc 0.211914 (0.201282)
Epoch: [59][600/616]	Loss 3.2031e+01 (3.1964e+01)	Acc 0.199219 (0.200905)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 59: 31.9609375
Training Acc of Epoch 59: 0.2009765625
Testing Acc of Epoch 59: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [60][300/616]	Loss 3.1758e+01 (3.1959e+01)	Acc 0.206055 (0.201023)
Epoch: [60][600/616]	Loss 3.2109e+01 (3.1961e+01)	Acc 0.197266 (0.200979)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 60: 31.961318597560975
Training Acc of Epoch 60: 0.2009670350609756
Testing Acc of Epoch 60: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [61][300/616]	Loss 3.1602e+01 (3.1965e+01)	Acc 0.209961 (0.200880)
Epoch: [61][600/616]	Loss 3.1250e+01 (3.1961e+01)	Acc 0.218750 (0.200970)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 61: 31.961763211382113
Training Acc of Epoch 61: 0.20095591971544716
Testing Acc of Epoch 61: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.2930e+01 (3.2930e+01)	Acc 0.176758 (0.176758)
Epoch: [62][300/616]	Loss 3.2109e+01 (3.1951e+01)	Acc 0.197266 (0.201234)
Epoch: [62][600/616]	Loss 3.2422e+01 (3.1963e+01)	Acc 0.189453 (0.200936)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 62: 31.961064532520325
Training Acc of Epoch 62: 0.20097338668699186
Testing Acc of Epoch 62: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 3.1797e+01 (3.1797e+01)	Acc 0.205078 (0.205078)
Epoch: [63][300/616]	Loss 3.2773e+01 (3.1973e+01)	Acc 0.180664 (0.200679)
Epoch: [63][600/616]	Loss 3.2305e+01 (3.1965e+01)	Acc 0.192383 (0.200881)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 63: 31.96169969512195
Training Acc of Epoch 63: 0.20095750762195122
Testing Acc of Epoch 63: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 3.1055e+01 (3.1055e+01)	Acc 0.223633 (0.223633)
Epoch: [64][300/616]	Loss 3.1875e+01 (3.1952e+01)	Acc 0.203125 (0.201201)
Epoch: [64][600/616]	Loss 3.2656e+01 (3.1961e+01)	Acc 0.183594 (0.200982)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 64: 31.961128048780488
Training Acc of Epoch 64: 0.2009717987804878
Testing Acc of Epoch 64: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [65][300/616]	Loss 3.2188e+01 (3.1899e+01)	Acc 0.195312 (0.202525)
Epoch: [65][600/616]	Loss 3.3086e+01 (3.1960e+01)	Acc 0.172852 (0.201011)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 65: 31.961318597560975
Training Acc of Epoch 65: 0.2009670350609756
Testing Acc of Epoch 65: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 3.3086e+01 (3.3086e+01)	Acc 0.172852 (0.172852)
Epoch: [66][300/616]	Loss 3.1523e+01 (3.1967e+01)	Acc 0.211914 (0.200818)
Epoch: [66][600/616]	Loss 3.2344e+01 (3.1964e+01)	Acc 0.191406 (0.200910)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 66: 31.96068343495935
Training Acc of Epoch 66: 0.20098291412601627
Testing Acc of Epoch 66: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [67][300/616]	Loss 3.2070e+01 (3.1986e+01)	Acc 0.198242 (0.200354)
Epoch: [67][600/616]	Loss 3.0703e+01 (3.1963e+01)	Acc 0.232422 (0.200917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 67: 31.961509146341463
Training Acc of Epoch 67: 0.2009622713414634
Testing Acc of Epoch 67: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.1562e+01 (3.1562e+01)	Acc 0.210938 (0.210938)
Epoch: [68][300/616]	Loss 3.2148e+01 (3.1996e+01)	Acc 0.196289 (0.200104)
Epoch: [68][600/616]	Loss 3.1523e+01 (3.1965e+01)	Acc 0.211914 (0.200870)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 68: 31.96169969512195
Training Acc of Epoch 68: 0.20095750762195122
Testing Acc of Epoch 68: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [69][300/616]	Loss 3.2109e+01 (3.1957e+01)	Acc 0.197266 (0.201065)
Epoch: [69][600/616]	Loss 3.2227e+01 (3.1958e+01)	Acc 0.194336 (0.201039)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201291)
Training Loss of Epoch 69: 31.9614456300813
Training Acc of Epoch 69: 0.20096385924796747
Testing Acc of Epoch 69: 0.20129130434782608
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [70][300/616]	Loss 3.2695e+01 (3.1962e+01)	Acc 0.182617 (0.200637)
Epoch: [70][600/616]	Loss 3.2109e+01 (3.1962e+01)	Acc 0.197266 (0.200784)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 70: 31.955058347500437
Training Acc of Epoch 70: 0.20097021087398373
Testing Acc of Epoch 70: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.1133e+01 (3.1133e+01)	Acc 0.221680 (0.221680)
Epoch: [71][300/616]	Loss 3.1562e+01 (3.1954e+01)	Acc 0.210938 (0.201149)
Epoch: [71][600/616]	Loss 3.1953e+01 (3.1962e+01)	Acc 0.201172 (0.200959)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 71: 31.961509146341463
Training Acc of Epoch 71: 0.2009622713414634
Testing Acc of Epoch 71: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.2812e+01 (3.2812e+01)	Acc 0.179688 (0.179688)
Epoch: [72][300/616]	Loss 3.2344e+01 (3.1969e+01)	Acc 0.191406 (0.200766)
Epoch: [72][600/616]	Loss 3.0683e+01 (3.1473e+01)	Acc 0.198242 (0.200524)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.214483)
Training Loss of Epoch 72: 31.4594943100844
Training Acc of Epoch 72: 0.20048589939024392
Testing Acc of Epoch 72: 0.21448260869565217
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.0696e+01 (3.0696e+01)	Acc 0.219727 (0.219727)
Epoch: [73][300/616]	Loss 2.4752e+01 (2.8115e+01)	Acc 0.248047 (0.222913)
Epoch: [73][600/616]	Loss 2.5173e+01 (2.6227e+01)	Acc 0.246094 (0.239596)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.266026 (0.270170)
Training Loss of Epoch 73: 26.186632528537658
Training Acc of Epoch 73: 0.2399025025406504
Testing Acc of Epoch 73: 0.2701695652173913
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4628e+01 (2.4628e+01)	Acc 0.255859 (0.255859)
Epoch: [74][300/616]	Loss 2.6432e+01 (2.5353e+01)	Acc 0.258789 (0.242457)
Epoch: [74][600/616]	Loss 2.8258e+01 (2.6276e+01)	Acc 0.205078 (0.233935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.213141 (0.215274)
Training Loss of Epoch 74: 26.320413729039632
Training Acc of Epoch 74: 0.23359375
Testing Acc of Epoch 74: 0.21527391304347826
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.8406e+01 (2.8406e+01)	Acc 0.204102 (0.204102)
Epoch: [75][300/616]	Loss 2.9034e+01 (2.8191e+01)	Acc 0.188477 (0.217452)
Epoch: [75][600/616]	Loss 2.8410e+01 (2.8241e+01)	Acc 0.201172 (0.217002)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.211538 (0.209109)
Training Loss of Epoch 75: 28.243391595235686
Training Acc of Epoch 75: 0.2171096925813008
Testing Acc of Epoch 75: 0.2091086956521739
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.0238e+01 (3.0238e+01)	Acc 0.171875 (0.171875)
Epoch: [76][300/616]	Loss 2.8206e+01 (2.8476e+01)	Acc 0.221680 (0.215424)
Epoch: [76][600/616]	Loss 2.9111e+01 (2.8551e+01)	Acc 0.189453 (0.214761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.208333 (0.208600)
Training Loss of Epoch 76: 28.55667523639958
Training Acc of Epoch 76: 0.21465637703252033
Testing Acc of Epoch 76: 0.2086
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.8755e+01 (2.8755e+01)	Acc 0.205078 (0.205078)
Epoch: [77][300/616]	Loss 2.8987e+01 (2.8763e+01)	Acc 0.208008 (0.213014)
Epoch: [77][600/616]	Loss 2.8918e+01 (2.8914e+01)	Acc 0.228516 (0.212796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.205128 (0.207752)
Training Loss of Epoch 77: 28.91617317819983
Training Acc of Epoch 77: 0.2126905487804878
Testing Acc of Epoch 77: 0.2077521739130435
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.9335e+01 (2.9335e+01)	Acc 0.198242 (0.198242)
Epoch: [78][300/616]	Loss 3.0385e+01 (2.9292e+01)	Acc 0.177734 (0.210727)
Epoch: [78][600/616]	Loss 2.8651e+01 (2.9393e+01)	Acc 0.250977 (0.209660)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.205128 (0.201296)
Training Loss of Epoch 78: 29.39574086259051
Training Acc of Epoch 78: 0.2098021468495935
Testing Acc of Epoch 78: 0.20129565217391304
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.0031e+01 (3.0031e+01)	Acc 0.194336 (0.194336)
Epoch: [79][300/616]	Loss 2.9735e+01 (2.9589e+01)	Acc 0.228516 (0.207784)
Epoch: [79][600/616]	Loss 3.0148e+01 (2.9606e+01)	Acc 0.197266 (0.208466)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.229167 (0.215043)
Training Loss of Epoch 79: 29.613113707255543
Training Acc of Epoch 79: 0.20825552591463414
Testing Acc of Epoch 79: 0.21504347826086956
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.0039e+01 (3.0039e+01)	Acc 0.224609 (0.224609)
Epoch: [80][300/616]	Loss 3.0574e+01 (3.0260e+01)	Acc 0.189453 (0.206207)
Epoch: [80][600/616]	Loss 2.8903e+01 (3.0418e+01)	Acc 0.210938 (0.205314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.200622)
Training Loss of Epoch 80: 30.427525546686436
Training Acc of Epoch 80: 0.2055100355691057
Testing Acc of Epoch 80: 0.2006217391304348
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.0781e+01 (3.0781e+01)	Acc 0.230469 (0.230469)
Epoch: [81][300/616]	Loss 3.2031e+01 (3.1904e+01)	Acc 0.199219 (0.200847)
Epoch: [81][600/616]	Loss 3.1289e+01 (3.1868e+01)	Acc 0.217773 (0.200616)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 81: 31.862625032130296
Training Acc of Epoch 81: 0.20080983231707317
Testing Acc of Epoch 81: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 3.1133e+01 (3.1133e+01)	Acc 0.221680 (0.221680)
Epoch: [82][300/616]	Loss 3.1445e+01 (3.1991e+01)	Acc 0.213867 (0.200231)
Epoch: [82][600/616]	Loss 3.1914e+01 (3.1959e+01)	Acc 0.202148 (0.201018)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 82: 31.961128048780488
Training Acc of Epoch 82: 0.2009717987804878
Testing Acc of Epoch 82: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [83][300/616]	Loss 3.1953e+01 (3.1982e+01)	Acc 0.201172 (0.200439)
Epoch: [83][600/616]	Loss 3.1211e+01 (3.1964e+01)	Acc 0.219727 (0.200905)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 83: 31.961128048780488
Training Acc of Epoch 83: 0.2009717987804878
Testing Acc of Epoch 83: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [84][300/616]	Loss 3.1680e+01 (3.1910e+01)	Acc 0.208008 (0.202243)
Epoch: [84][600/616]	Loss 3.1484e+01 (3.1962e+01)	Acc 0.212891 (0.200956)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 84: 31.961318597560975
Training Acc of Epoch 84: 0.2009670350609756
Testing Acc of Epoch 84: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [85][300/616]	Loss 3.2188e+01 (3.1931e+01)	Acc 0.195312 (0.201733)
Epoch: [85][600/616]	Loss 3.2148e+01 (3.1962e+01)	Acc 0.196289 (0.200953)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 85: 31.961128048780488
Training Acc of Epoch 85: 0.2009717987804878
Testing Acc of Epoch 85: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [86][300/616]	Loss 3.2598e+01 (3.1930e+01)	Acc 0.182617 (0.201480)
Epoch: [86][600/616]	Loss 3.2131e+01 (3.1822e+01)	Acc 0.185547 (0.200961)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 86: 31.813427759186037
Training Acc of Epoch 86: 0.20096862296747967
Testing Acc of Epoch 86: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 3.1947e+01 (3.1947e+01)	Acc 0.188477 (0.188477)
Epoch: [87][300/616]	Loss 3.1876e+01 (3.1236e+01)	Acc 0.177734 (0.201058)
Epoch: [87][600/616]	Loss 2.9449e+01 (3.0897e+01)	Acc 0.208008 (0.201013)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 87: 30.875661837570068
Training Acc of Epoch 87: 0.2009670350609756
Testing Acc of Epoch 87: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.9587e+01 (2.9587e+01)	Acc 0.212891 (0.212891)
Epoch: [88][300/616]	Loss 3.1914e+01 (3.0342e+01)	Acc 0.202148 (0.201652)
Epoch: [88][600/616]	Loss 3.2193e+01 (3.1164e+01)	Acc 0.194336 (0.200878)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 88: 31.17787545056847
Training Acc of Epoch 88: 0.20097021087398373
Testing Acc of Epoch 88: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.2936e+01 (3.2936e+01)	Acc 0.175781 (0.175781)
Epoch: [89][300/616]	Loss 3.1460e+01 (3.1920e+01)	Acc 0.210938 (0.200740)
Epoch: [89][600/616]	Loss 3.1437e+01 (3.1888e+01)	Acc 0.209961 (0.200805)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 89: 31.880018839022007
Training Acc of Epoch 89: 0.20097021087398373
Testing Acc of Epoch 89: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 3.1238e+01 (3.1238e+01)	Acc 0.214844 (0.214844)
Epoch: [90][300/616]	Loss 3.1044e+01 (3.1688e+01)	Acc 0.210938 (0.200883)
Epoch: [90][600/616]	Loss 3.3398e+01 (3.1634e+01)	Acc 0.165039 (0.197971)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 90: 31.644108970766144
Training Acc of Epoch 90: 0.19797700711382113
Testing Acc of Epoch 90: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [91][300/616]	Loss 3.2615e+01 (3.2247e+01)	Acc 0.184570 (0.193797)
Epoch: [91][600/616]	Loss 3.2383e+01 (3.2245e+01)	Acc 0.190430 (0.193847)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 91: 32.24704940609816
Training Acc of Epoch 91: 0.19380716463414635
Testing Acc of Epoch 91: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [92][300/616]	Loss 3.2383e+01 (3.2212e+01)	Acc 0.190430 (0.194677)
Epoch: [92][600/616]	Loss 3.1992e+01 (3.2252e+01)	Acc 0.200195 (0.193676)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 92: 32.24749310191085
Training Acc of Epoch 92: 0.19379604928861788
Testing Acc of Epoch 92: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [93][300/616]	Loss 3.2422e+01 (3.2201e+01)	Acc 0.189453 (0.194965)
Epoch: [93][600/616]	Loss 3.1758e+01 (3.2248e+01)	Acc 0.206055 (0.193793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 93: 32.24725144238976
Training Acc of Epoch 93: 0.19380240091463416
Testing Acc of Epoch 93: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 3.3438e+01 (3.3438e+01)	Acc 0.164062 (0.164062)
Epoch: [94][300/616]	Loss 3.2305e+01 (3.2219e+01)	Acc 0.192383 (0.194508)
Epoch: [94][600/616]	Loss 3.2461e+01 (3.2243e+01)	Acc 0.188477 (0.193902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 94: 32.24703202596525
Training Acc of Epoch 94: 0.19380716463414635
Testing Acc of Epoch 94: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [95][300/616]	Loss 3.2656e+01 (3.2251e+01)	Acc 0.183594 (0.193700)
Epoch: [95][600/616]	Loss 3.2031e+01 (3.2249e+01)	Acc 0.199219 (0.193756)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 95: 32.24616621374115
Training Acc of Epoch 95: 0.19382939532520324
Testing Acc of Epoch 95: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [96][300/616]	Loss 3.2378e+01 (3.2268e+01)	Acc 0.190430 (0.193272)
Epoch: [96][600/616]	Loss 3.2653e+01 (3.2243e+01)	Acc 0.183594 (0.193904)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 96: 32.246607757196195
Training Acc of Epoch 96: 0.1938182799796748
Testing Acc of Epoch 96: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [97][300/616]	Loss 3.1680e+01 (3.2248e+01)	Acc 0.208008 (0.193784)
Epoch: [97][600/616]	Loss 3.1875e+01 (3.2247e+01)	Acc 0.203125 (0.193806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 97: 32.24706085018995
Training Acc of Epoch 97: 0.19380716463414635
Testing Acc of Epoch 97: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.2805e+01 (3.2805e+01)	Acc 0.179688 (0.179688)
Epoch: [98][300/616]	Loss 3.2109e+01 (3.2224e+01)	Acc 0.197266 (0.194378)
Epoch: [98][600/616]	Loss 3.2344e+01 (3.2250e+01)	Acc 0.191406 (0.193736)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 98: 32.24659345363214
Training Acc of Epoch 98: 0.1938182799796748
Testing Acc of Epoch 98: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [99][300/616]	Loss 3.2266e+01 (3.2237e+01)	Acc 0.193359 (0.194063)
Epoch: [99][600/616]	Loss 3.1797e+01 (3.2248e+01)	Acc 0.205078 (0.193782)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 99: 32.2470374316704
Training Acc of Epoch 99: 0.19380716463414635
Testing Acc of Epoch 99: 0.19464782608695652
Early stopping not satisfied.
