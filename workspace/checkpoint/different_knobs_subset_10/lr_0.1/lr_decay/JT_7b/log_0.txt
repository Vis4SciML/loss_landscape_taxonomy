train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_7b
different_width False
resnet18_width 64
weight_precision 7
bias_precision 7
act_precision 10
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_7b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_7b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0116e-01 (5.0116e-01)	Acc 0.258789 (0.258789)
Epoch: [0][300/616]	Loss 2.6631e-01 (2.9619e-01)	Acc 0.718750 (0.683717)
Epoch: [0][600/616]	Loss 2.8308e-01 (2.8246e-01)	Acc 0.694336 (0.703119)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.727935)
Training Loss of Epoch 0: 0.2820102949937185
Training Acc of Epoch 0: 0.7036585365853658
Testing Acc of Epoch 0: 0.7279347826086957
Model with the best training loss saved! The loss is 0.2820102949937185
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.7712e-01 (2.7712e-01)	Acc 0.716797 (0.716797)
Epoch: [1][300/616]	Loss 2.5891e-01 (2.6709e-01)	Acc 0.736328 (0.723701)
Epoch: [1][600/616]	Loss 2.5779e-01 (2.6812e-01)	Acc 0.734375 (0.722391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.730926)
Training Loss of Epoch 1: 0.2680628418680129
Training Acc of Epoch 1: 0.7224275914634146
Testing Acc of Epoch 1: 0.7309260869565217
Model with the best training loss saved! The loss is 0.2680628418680129
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4796e-01 (2.4796e-01)	Acc 0.745117 (0.745117)
Epoch: [2][300/616]	Loss 2.6539e-01 (2.6456e-01)	Acc 0.738281 (0.725761)
Epoch: [2][600/616]	Loss 2.6846e-01 (2.6605e-01)	Acc 0.737305 (0.724380)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.730000)
Training Loss of Epoch 2: 0.2660396442665317
Training Acc of Epoch 2: 0.7243981834349593
Testing Acc of Epoch 2: 0.73
Model with the best training loss saved! The loss is 0.2660396442665317
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.5909e-01 (2.5909e-01)	Acc 0.730469 (0.730469)
Epoch: [3][300/616]	Loss 2.6892e-01 (2.6747e-01)	Acc 0.709961 (0.723169)
Epoch: [3][600/616]	Loss 2.6564e-01 (2.6600e-01)	Acc 0.733398 (0.725038)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.726870)
Training Loss of Epoch 3: 0.26589998850008334
Training Acc of Epoch 3: 0.7251159171747967
Testing Acc of Epoch 3: 0.7268695652173913
Model with the best training loss saved! The loss is 0.26589998850008334
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5362e-01 (2.5362e-01)	Acc 0.729492 (0.729492)
Epoch: [4][300/616]	Loss 2.4393e-01 (2.6433e-01)	Acc 0.744141 (0.727127)
Epoch: [4][600/616]	Loss 2.6955e-01 (2.6360e-01)	Acc 0.732422 (0.727424)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.719374)
Training Loss of Epoch 4: 0.2638122520068797
Training Acc of Epoch 4: 0.7272786458333333
Testing Acc of Epoch 4: 0.7193739130434783
Model with the best training loss saved! The loss is 0.2638122520068797
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.7215e-01 (2.7215e-01)	Acc 0.708984 (0.708984)
Epoch: [5][300/616]	Loss 2.7471e-01 (2.6339e-01)	Acc 0.708984 (0.727860)
Epoch: [5][600/616]	Loss 2.4387e-01 (2.6356e-01)	Acc 0.767578 (0.727414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.729535)
Training Loss of Epoch 5: 0.26346385617081713
Training Acc of Epoch 5: 0.7274977769308943
Testing Acc of Epoch 5: 0.7295347826086956
Model with the best training loss saved! The loss is 0.26346385617081713
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.5254e-01 (2.5254e-01)	Acc 0.728516 (0.728516)
Epoch: [6][300/616]	Loss 2.4947e-01 (2.6372e-01)	Acc 0.747070 (0.727075)
Epoch: [6][600/616]	Loss 2.6356e-01 (2.6346e-01)	Acc 0.748047 (0.727331)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.732878)
Training Loss of Epoch 6: 0.2634318656795393
Training Acc of Epoch 6: 0.727442200203252
Testing Acc of Epoch 6: 0.7328782608695652
Model with the best training loss saved! The loss is 0.2634318656795393
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4584e-01 (2.4584e-01)	Acc 0.747070 (0.747070)
Epoch: [7][300/616]	Loss 2.6862e-01 (2.6643e-01)	Acc 0.707031 (0.725326)
Epoch: [7][600/616]	Loss 2.7720e-01 (2.6441e-01)	Acc 0.719727 (0.726827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.736457)
Training Loss of Epoch 7: 0.2643330302907199
Training Acc of Epoch 7: 0.7268705538617887
Testing Acc of Epoch 7: 0.7364565217391305
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5643e-01 (2.5643e-01)	Acc 0.728516 (0.728516)
Epoch: [8][300/616]	Loss 3.0034e-01 (2.6509e-01)	Acc 0.709961 (0.726137)
Epoch: [8][600/616]	Loss 2.6055e-01 (2.6533e-01)	Acc 0.736328 (0.725942)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.732604)
Training Loss of Epoch 8: 0.26523756707102303
Training Acc of Epoch 8: 0.7259860899390244
Testing Acc of Epoch 8: 0.732604347826087
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.7151e-01 (2.7151e-01)	Acc 0.714844 (0.714844)
Epoch: [9][300/616]	Loss 2.4182e-01 (2.6294e-01)	Acc 0.757812 (0.728026)
Epoch: [9][600/616]	Loss 2.6635e-01 (2.6360e-01)	Acc 0.727539 (0.727398)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.733861)
Training Loss of Epoch 9: 0.2635754972454009
Training Acc of Epoch 9: 0.7274437881097561
Testing Acc of Epoch 9: 0.7338608695652173
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5398e-01 (2.5398e-01)	Acc 0.739258 (0.739258)
Epoch: [10][300/616]	Loss 2.5696e-01 (2.6197e-01)	Acc 0.730469 (0.728931)
Epoch: [10][600/616]	Loss 2.5914e-01 (2.6264e-01)	Acc 0.734375 (0.728109)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.735713)
Training Loss of Epoch 10: 0.26256507184447314
Training Acc of Epoch 10: 0.7282234502032521
Testing Acc of Epoch 10: 0.7357130434782608
Model with the best training loss saved! The loss is 0.26256507184447314
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.4975e-01 (2.4975e-01)	Acc 0.745117 (0.745117)
Epoch: [11][300/616]	Loss 2.7509e-01 (2.6364e-01)	Acc 0.697266 (0.726816)
Epoch: [11][600/616]	Loss 2.5634e-01 (2.6385e-01)	Acc 0.734375 (0.726813)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.714100)
Training Loss of Epoch 11: 0.26380933564368303
Training Acc of Epoch 11: 0.726856262703252
Testing Acc of Epoch 11: 0.7141
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.7357e-01 (2.7357e-01)	Acc 0.726562 (0.726562)
Epoch: [12][300/616]	Loss 2.6164e-01 (2.6239e-01)	Acc 0.741211 (0.728224)
Epoch: [12][600/616]	Loss 2.7481e-01 (2.6209e-01)	Acc 0.717773 (0.728902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.728957)
Training Loss of Epoch 12: 0.2621395915019803
Training Acc of Epoch 12: 0.7289141895325203
Testing Acc of Epoch 12: 0.7289565217391304
Model with the best training loss saved! The loss is 0.2621395915019803
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.5925e-01 (2.5925e-01)	Acc 0.730469 (0.730469)
Epoch: [13][300/616]	Loss 3.1339e-01 (4.2143e-01)	Acc 0.700195 (0.399258)
Epoch: [13][600/616]	Loss 2.8599e-01 (3.5900e-01)	Acc 0.700195 (0.549603)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.716083)
Training Loss of Epoch 13: 0.35734424920586066
Training Acc of Epoch 13: 0.5531996316056911
Testing Acc of Epoch 13: 0.7160826086956522
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.6875e-01 (2.6875e-01)	Acc 0.726562 (0.726562)
Epoch: [14][300/616]	Loss 2.8505e-01 (2.7223e-01)	Acc 0.713867 (0.719817)
Epoch: [14][600/616]	Loss 2.7194e-01 (2.7128e-01)	Acc 0.726562 (0.720799)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.734383)
Training Loss of Epoch 14: 0.27105083293546506
Training Acc of Epoch 14: 0.7208952616869919
Testing Acc of Epoch 14: 0.7343826086956522
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.5901e-01 (2.5901e-01)	Acc 0.728516 (0.728516)
Epoch: [15][300/616]	Loss 2.6935e-01 (2.6850e-01)	Acc 0.737305 (0.721427)
Epoch: [15][600/616]	Loss 2.6136e-01 (2.6755e-01)	Acc 0.718750 (0.722721)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.720513)
Training Loss of Epoch 15: 0.2675321790987883
Training Acc of Epoch 15: 0.7226975355691057
Testing Acc of Epoch 15: 0.7205130434782608
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.6026e-01 (2.6026e-01)	Acc 0.744141 (0.744141)
Epoch: [16][300/616]	Loss 2.4970e-01 (2.8810e-01)	Acc 0.741211 (0.705549)
Epoch: [16][600/616]	Loss 2.4719e-01 (2.7617e-01)	Acc 0.737305 (0.715385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.671474 (0.695213)
Training Loss of Epoch 16: 0.275809481013112
Training Acc of Epoch 16: 0.7157806148373984
Testing Acc of Epoch 16: 0.6952130434782609
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.8873e-01 (2.8873e-01)	Acc 0.698242 (0.698242)
Epoch: [17][300/616]	Loss 2.6395e-01 (2.6514e-01)	Acc 0.726562 (0.725466)
Epoch: [17][600/616]	Loss 2.5750e-01 (2.6323e-01)	Acc 0.733398 (0.726892)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.739278)
Training Loss of Epoch 17: 0.26295674969025745
Training Acc of Epoch 17: 0.7271071519308943
Testing Acc of Epoch 17: 0.7392782608695652
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.4069e-01 (2.4069e-01)	Acc 0.751953 (0.751953)
Epoch: [18][300/616]	Loss 3.0953e-01 (3.0998e-01)	Acc 0.703125 (0.650316)
Epoch: [18][600/616]	Loss 2.8766e-01 (3.4351e-01)	Acc 0.701172 (0.585005)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.722887)
Training Loss of Epoch 18: 0.34205898412359437
Training Acc of Epoch 18: 0.5880827616869919
Testing Acc of Epoch 18: 0.7228869565217392
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.6792e-01 (2.6792e-01)	Acc 0.728516 (0.728516)
Epoch: [19][300/616]	Loss 3.7028e-01 (2.6687e-01)	Acc 0.637695 (0.722095)
Epoch: [19][600/616]	Loss 4.7474e-01 (3.6128e-01)	Acc 0.284180 (0.543308)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.358974 (0.334465)
Training Loss of Epoch 19: 0.36381035073985896
Training Acc of Epoch 19: 0.5378969766260162
Testing Acc of Epoch 19: 0.33446521739130436
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 4.6576e-01 (4.6576e-01)	Acc 0.320312 (0.320312)
Epoch: [20][300/616]	Loss 2.9370e+01 (7.7193e-01)	Acc 0.204102 (0.253585)
Epoch: [20][600/616]	Loss 3.1797e+01 (1.6325e+01)	Acc 0.205078 (0.227432)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 20: 16.683172905008966
Training Acc of Epoch 20: 0.22677369156504065
Testing Acc of Epoch 20: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [21][300/616]	Loss 3.2227e+01 (3.1935e+01)	Acc 0.194336 (0.201613)
Epoch: [21][600/616]	Loss 3.1367e+01 (3.1943e+01)	Acc 0.215820 (0.201432)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 21: 31.9453125
Training Acc of Epoch 21: 0.2013671875
Testing Acc of Epoch 21: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 3.1367e+01 (3.1367e+01)	Acc 0.215820 (0.215820)
Epoch: [22][300/616]	Loss 3.0898e+01 (3.1930e+01)	Acc 0.227539 (0.201756)
Epoch: [22][600/616]	Loss 3.2461e+01 (3.1941e+01)	Acc 0.188477 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 22: 31.9453125
Training Acc of Epoch 22: 0.2013671875
Testing Acc of Epoch 22: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [23][300/616]	Loss 3.2344e+01 (3.1945e+01)	Acc 0.191406 (0.201383)
Epoch: [23][600/616]	Loss 3.2500e+01 (3.1946e+01)	Acc 0.187500 (0.201352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 23: 31.945693597560975
Training Acc of Epoch 23: 0.2013576600609756
Testing Acc of Epoch 23: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [24][300/616]	Loss 3.2344e+01 (3.1992e+01)	Acc 0.191406 (0.200205)
Epoch: [24][600/616]	Loss 3.1992e+01 (3.1947e+01)	Acc 0.200195 (0.201328)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 24: 31.944931402439025
Training Acc of Epoch 24: 0.2013767149390244
Testing Acc of Epoch 24: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [25][300/616]	Loss 3.1484e+01 (3.1949e+01)	Acc 0.212891 (0.201263)
Epoch: [25][600/616]	Loss 3.1289e+01 (3.1943e+01)	Acc 0.217773 (0.201430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 25: 31.944931402439025
Training Acc of Epoch 25: 0.2013767149390244
Testing Acc of Epoch 25: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [26][300/616]	Loss 3.2461e+01 (3.1958e+01)	Acc 0.188477 (0.201052)
Epoch: [26][600/616]	Loss 3.1641e+01 (3.1944e+01)	Acc 0.208984 (0.201391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 26: 31.94505843495935
Training Acc of Epoch 26: 0.20137353912601627
Testing Acc of Epoch 26: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 3.1602e+01 (3.1602e+01)	Acc 0.209961 (0.209961)
Epoch: [27][300/616]	Loss 3.2773e+01 (3.1939e+01)	Acc 0.180664 (0.201526)
Epoch: [27][600/616]	Loss 3.0586e+01 (3.1941e+01)	Acc 0.235352 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 27: 31.944677337398375
Training Acc of Epoch 27: 0.20138306656504065
Testing Acc of Epoch 27: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [28][300/616]	Loss 3.1797e+01 (3.1949e+01)	Acc 0.205078 (0.201269)
Epoch: [28][600/616]	Loss 3.2109e+01 (3.1942e+01)	Acc 0.197266 (0.201438)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 28: 31.94505843495935
Training Acc of Epoch 28: 0.20137353912601627
Testing Acc of Epoch 28: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 3.2852e+01 (3.2852e+01)	Acc 0.178711 (0.178711)
Epoch: [29][300/616]	Loss 3.1445e+01 (3.1941e+01)	Acc 0.213867 (0.201464)
Epoch: [29][600/616]	Loss 3.1797e+01 (3.1943e+01)	Acc 0.205078 (0.201420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 29: 31.945376016260163
Training Acc of Epoch 29: 0.20136559959349593
Testing Acc of Epoch 29: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [30][300/616]	Loss 3.1680e+01 (3.1929e+01)	Acc 0.208008 (0.201775)
Epoch: [30][600/616]	Loss 3.2539e+01 (3.1944e+01)	Acc 0.186523 (0.201403)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 30: 31.94505843495935
Training Acc of Epoch 30: 0.20137353912601627
Testing Acc of Epoch 30: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [31][300/616]	Loss 3.2109e+01 (3.1920e+01)	Acc 0.197266 (0.201993)
Epoch: [31][600/616]	Loss 3.1602e+01 (3.1946e+01)	Acc 0.209961 (0.201357)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 31: 31.9448043699187
Training Acc of Epoch 31: 0.20137989075203253
Testing Acc of Epoch 31: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [32][300/616]	Loss 3.2266e+01 (3.1938e+01)	Acc 0.193359 (0.201542)
Epoch: [32][600/616]	Loss 3.1172e+01 (3.1940e+01)	Acc 0.220703 (0.201490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 32: 31.944740853658537
Training Acc of Epoch 32: 0.2013814786585366
Testing Acc of Epoch 32: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [33][300/616]	Loss 3.1484e+01 (3.1926e+01)	Acc 0.212891 (0.201853)
Epoch: [33][600/616]	Loss 3.2500e+01 (3.1945e+01)	Acc 0.187500 (0.201377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 33: 31.944867886178862
Training Acc of Epoch 33: 0.20137830284552846
Testing Acc of Epoch 33: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [34][300/616]	Loss 3.2305e+01 (3.1948e+01)	Acc 0.192383 (0.201302)
Epoch: [34][600/616]	Loss 3.2656e+01 (3.1949e+01)	Acc 0.183594 (0.201263)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 34: 31.945947662601625
Training Acc of Epoch 34: 0.20135130843495935
Testing Acc of Epoch 34: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.1406e+01 (3.1406e+01)	Acc 0.214844 (0.214844)
Epoch: [35][300/616]	Loss 3.3125e+01 (3.1921e+01)	Acc 0.171875 (0.201986)
Epoch: [35][600/616]	Loss 3.1992e+01 (3.1952e+01)	Acc 0.200195 (0.201199)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 35: 31.944987261392235
Training Acc of Epoch 35: 0.20137512703252033
Testing Acc of Epoch 35: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [36][300/616]	Loss 3.1602e+01 (3.1912e+01)	Acc 0.209961 (0.202194)
Epoch: [36][600/616]	Loss 3.2461e+01 (3.1943e+01)	Acc 0.188477 (0.201414)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201187)
Training Loss of Epoch 36: 31.944169802782014
Training Acc of Epoch 36: 0.2013894181910569
Testing Acc of Epoch 36: 0.20118695652173912
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.1954e+01 (3.1954e+01)	Acc 0.201172 (0.201172)
Epoch: [37][300/616]	Loss 3.2109e+01 (3.1942e+01)	Acc 0.197266 (0.201341)
Epoch: [37][600/616]	Loss 3.2229e+01 (3.1930e+01)	Acc 0.192383 (0.201531)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.217949 (0.201443)
Training Loss of Epoch 37: 31.930532421329158
Training Acc of Epoch 37: 0.20152280233739836
Testing Acc of Epoch 37: 0.20144347826086956
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.1245e+01 (3.1245e+01)	Acc 0.217773 (0.217773)
Epoch: [38][300/616]	Loss 3.1321e+01 (3.1934e+01)	Acc 0.216797 (0.201201)
Epoch: [38][600/616]	Loss 3.1892e+01 (3.1894e+01)	Acc 0.201172 (0.201861)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.202309)
Training Loss of Epoch 38: 31.88950758523088
Training Acc of Epoch 38: 0.201953125
Testing Acc of Epoch 38: 0.2023086956521739
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.1369e+01 (3.1369e+01)	Acc 0.215820 (0.215820)
Epoch: [39][300/616]	Loss 2.6691e+01 (2.6436e+01)	Acc 0.214844 (0.271647)
Epoch: [39][600/616]	Loss 3.1677e+01 (2.8318e+01)	Acc 0.207031 (0.246752)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201300)
Training Loss of Epoch 39: 28.405573827852077
Training Acc of Epoch 39: 0.24557926829268292
Testing Acc of Epoch 39: 0.2013
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.1743e+01 (3.1743e+01)	Acc 0.206055 (0.206055)
Epoch: [40][300/616]	Loss 3.2582e+01 (3.1901e+01)	Acc 0.184570 (0.201853)
Epoch: [40][600/616]	Loss 3.2089e+01 (3.1916e+01)	Acc 0.197266 (0.201383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 40: 31.915994768220234
Training Acc of Epoch 40: 0.2013894181910569
Testing Acc of Epoch 40: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.1747e+01 (3.1747e+01)	Acc 0.206055 (0.206055)
Epoch: [41][300/616]	Loss 3.2852e+01 (3.1958e+01)	Acc 0.178711 (0.200844)
Epoch: [41][600/616]	Loss 3.2344e+01 (3.1941e+01)	Acc 0.191406 (0.201370)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 41: 31.94127280072468
Training Acc of Epoch 41: 0.20136401168699186
Testing Acc of Epoch 41: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [42][300/616]	Loss 3.2186e+01 (3.1948e+01)	Acc 0.195312 (0.201298)
Epoch: [42][600/616]	Loss 3.1445e+01 (3.1951e+01)	Acc 0.213867 (0.201204)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 42: 31.94560723653654
Training Acc of Epoch 42: 0.20134654471544716
Testing Acc of Epoch 42: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [43][300/616]	Loss 3.2094e+01 (3.2006e+01)	Acc 0.197266 (0.199803)
Epoch: [43][600/616]	Loss 3.2032e+01 (3.1935e+01)	Acc 0.198242 (0.201407)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.201170)
Training Loss of Epoch 43: 31.935233127004732
Training Acc of Epoch 43: 0.2013989456300813
Testing Acc of Epoch 43: 0.2011695652173913
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.1680e+01 (3.1680e+01)	Acc 0.208008 (0.208008)
Epoch: [44][300/616]	Loss 2.8108e+01 (3.1781e+01)	Acc 0.243164 (0.203826)
Epoch: [44][600/616]	Loss 2.6019e+01 (2.7275e+01)	Acc 0.219727 (0.247564)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.346154 (0.324909)
Training Loss of Epoch 44: 27.213296353720068
Training Acc of Epoch 44: 0.2487376143292683
Testing Acc of Epoch 44: 0.3249086956521739
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.4690e+01 (2.4690e+01)	Acc 0.336914 (0.336914)
Epoch: [45][300/616]	Loss 2.4663e+01 (2.4755e+01)	Acc 0.221680 (0.288530)
Epoch: [45][600/616]	Loss 2.5054e+01 (2.4437e+01)	Acc 0.329102 (0.289773)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.347756 (0.331887)
Training Loss of Epoch 45: 24.43163529993073
Training Acc of Epoch 45: 0.2896801956300813
Testing Acc of Epoch 45: 0.3318869565217391
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.5065e+01 (2.5065e+01)	Acc 0.337891 (0.337891)
Epoch: [46][300/616]	Loss 2.3735e+01 (2.4320e+01)	Acc 0.251953 (0.291353)
Epoch: [46][600/616]	Loss 2.5588e+01 (2.4248e+01)	Acc 0.310547 (0.290866)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.347756 (0.303087)
Training Loss of Epoch 46: 24.25321053915877
Training Acc of Epoch 46: 0.2905297256097561
Testing Acc of Epoch 46: 0.3030869565217391
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.4408e+01 (2.4408e+01)	Acc 0.353516 (0.353516)
Epoch: [47][300/616]	Loss 2.5149e+01 (2.3214e+01)	Acc 0.327148 (0.295087)
Epoch: [47][600/616]	Loss 2.4947e+01 (2.3493e+01)	Acc 0.325195 (0.293429)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.254808 (0.319070)
Training Loss of Epoch 47: 23.505967383656074
Training Acc of Epoch 47: 0.293432418699187
Testing Acc of Epoch 47: 0.3190695652173913
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.3564e+01 (2.3564e+01)	Acc 0.266602 (0.266602)
Epoch: [48][300/616]	Loss 2.4213e+01 (2.4351e+01)	Acc 0.340820 (0.289857)
Epoch: [48][600/616]	Loss 2.0880e+01 (2.3519e+01)	Acc 0.289062 (0.290444)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.282051 (0.307096)
Training Loss of Epoch 48: 23.50355879930946
Training Acc of Epoch 48: 0.29025978150406506
Testing Acc of Epoch 48: 0.30709565217391305
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.1906e+01 (2.1906e+01)	Acc 0.270508 (0.270508)
Epoch: [49][300/616]	Loss 2.2308e+01 (2.2608e+01)	Acc 0.349609 (0.288560)
Epoch: [49][600/616]	Loss 2.2888e+01 (2.2876e+01)	Acc 0.261719 (0.283972)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.278846 (0.262726)
Training Loss of Epoch 49: 22.89147355304501
Training Acc of Epoch 49: 0.2838494029471545
Testing Acc of Epoch 49: 0.2627260869565217
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.2719e+01 (2.2719e+01)	Acc 0.263672 (0.263672)
Epoch: [50][300/616]	Loss 2.2190e+01 (2.3333e+01)	Acc 0.285156 (0.278019)
Epoch: [50][600/616]	Loss 2.5373e+01 (2.3611e+01)	Acc 0.287109 (0.275764)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.306090 (0.283417)
Training Loss of Epoch 50: 23.62704373801627
Training Acc of Epoch 50: 0.27563357469512195
Testing Acc of Epoch 50: 0.28341739130434784
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.4812e+01 (2.4812e+01)	Acc 0.309570 (0.309570)
Epoch: [51][300/616]	Loss 2.6444e+01 (2.5162e+01)	Acc 0.244141 (0.260801)
Epoch: [51][600/616]	Loss 2.8567e+01 (2.6209e+01)	Acc 0.237305 (0.252785)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.254808 (0.241626)
Training Loss of Epoch 51: 26.25030741807891
Training Acc of Epoch 51: 0.25271214430894307
Testing Acc of Epoch 51: 0.24162608695652174
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.8808e+01 (2.8808e+01)	Acc 0.238281 (0.238281)
Epoch: [52][300/616]	Loss 2.9143e+01 (2.8833e+01)	Acc 0.240234 (0.240384)
Epoch: [52][600/616]	Loss 2.9728e+01 (2.9007e+01)	Acc 0.222656 (0.240070)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.253205 (0.239930)
Training Loss of Epoch 52: 29.008533818934993
Training Acc of Epoch 52: 0.24011686991869918
Testing Acc of Epoch 52: 0.2399304347826087
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.0259e+01 (3.0259e+01)	Acc 0.212891 (0.212891)
Epoch: [53][300/616]	Loss 2.9174e+01 (2.9252e+01)	Acc 0.242188 (0.239689)
Epoch: [53][600/616]	Loss 2.8673e+01 (2.9284e+01)	Acc 0.255859 (0.239258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.251603 (0.239291)
Training Loss of Epoch 53: 29.2838599910581
Training Acc of Epoch 53: 0.2392705157520325
Testing Acc of Epoch 53: 0.23929130434782608
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.9763e+01 (2.9763e+01)	Acc 0.228516 (0.228516)
Epoch: [54][300/616]	Loss 2.8356e+01 (2.9247e+01)	Acc 0.264648 (0.239543)
Epoch: [54][600/616]	Loss 2.9992e+01 (2.9233e+01)	Acc 0.221680 (0.239989)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.253205 (0.240065)
Training Loss of Epoch 54: 29.228822888009915
Training Acc of Epoch 54: 0.2401343368902439
Testing Acc of Epoch 54: 0.24006521739130435
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.9377e+01 (2.9377e+01)	Acc 0.230469 (0.230469)
Epoch: [55][300/616]	Loss 3.0039e+01 (2.9138e+01)	Acc 0.226562 (0.241428)
Epoch: [55][600/616]	Loss 2.9611e+01 (2.9125e+01)	Acc 0.226562 (0.241125)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.256410 (0.241991)
Training Loss of Epoch 55: 29.12946913804465
Training Acc of Epoch 55: 0.24103467987804877
Testing Acc of Epoch 55: 0.2419913043478261
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.9934e+01 (2.9934e+01)	Acc 0.225586 (0.225586)
Epoch: [56][300/616]	Loss 2.8828e+01 (2.9120e+01)	Acc 0.241211 (0.241240)
Epoch: [56][600/616]	Loss 2.8642e+01 (2.9063e+01)	Acc 0.248047 (0.241656)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.256410 (0.242343)
Training Loss of Epoch 56: 29.062891360027034
Training Acc of Epoch 56: 0.2415872713414634
Testing Acc of Epoch 56: 0.24234347826086958
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.9349e+01 (2.9349e+01)	Acc 0.246094 (0.246094)
Epoch: [57][300/616]	Loss 2.8600e+01 (2.8990e+01)	Acc 0.246094 (0.241737)
Epoch: [57][600/616]	Loss 2.9582e+01 (2.8927e+01)	Acc 0.210938 (0.242386)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.253205 (0.243157)
Training Loss of Epoch 57: 28.92479561286244
Training Acc of Epoch 57: 0.2423780487804878
Testing Acc of Epoch 57: 0.24315652173913044
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.8606e+01 (2.8606e+01)	Acc 0.242188 (0.242188)
Epoch: [58][300/616]	Loss 2.7752e+01 (2.8834e+01)	Acc 0.256836 (0.242613)
Epoch: [58][600/616]	Loss 2.9021e+01 (2.8766e+01)	Acc 0.229492 (0.243483)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.254808 (0.244917)
Training Loss of Epoch 58: 28.76059457422272
Training Acc of Epoch 58: 0.24351657774390245
Testing Acc of Epoch 58: 0.24491739130434784
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.9226e+01 (2.9226e+01)	Acc 0.232422 (0.232422)
Epoch: [59][300/616]	Loss 2.8330e+01 (2.8567e+01)	Acc 0.254883 (0.244734)
Epoch: [59][600/616]	Loss 2.9381e+01 (2.8496e+01)	Acc 0.221680 (0.245120)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.258013 (0.247143)
Training Loss of Epoch 59: 28.489358529811952
Training Acc of Epoch 59: 0.24517117632113822
Testing Acc of Epoch 59: 0.24714347826086958
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.9192e+01 (2.9192e+01)	Acc 0.237305 (0.237305)
Epoch: [60][300/616]	Loss 2.5763e+01 (2.8170e+01)	Acc 0.299805 (0.246756)
Epoch: [60][600/616]	Loss 2.7448e+01 (2.7993e+01)	Acc 0.249023 (0.247988)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.262821 (0.250022)
Training Loss of Epoch 60: 27.98156704476209
Training Acc of Epoch 60: 0.2481469131097561
Testing Acc of Epoch 60: 0.2500217391304348
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.6948e+01 (2.6948e+01)	Acc 0.273438 (0.273438)
Epoch: [61][300/616]	Loss 2.6865e+01 (2.7274e+01)	Acc 0.259766 (0.250967)
Epoch: [61][600/616]	Loss 2.4639e+01 (2.6637e+01)	Acc 0.273438 (0.252385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.270833 (0.258548)
Training Loss of Epoch 61: 26.603029570540762
Training Acc of Epoch 61: 0.2525104801829268
Testing Acc of Epoch 61: 0.25854782608695653
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.5331e+01 (2.5331e+01)	Acc 0.255859 (0.255859)
Epoch: [62][300/616]	Loss 2.4207e+01 (2.4613e+01)	Acc 0.252930 (0.261349)
Epoch: [62][600/616]	Loss 2.3711e+01 (2.4171e+01)	Acc 0.269531 (0.265805)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.280449 (0.274904)
Training Loss of Epoch 62: 24.14973885140768
Training Acc of Epoch 62: 0.26604579522357724
Testing Acc of Epoch 62: 0.27490434782608697
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.2960e+01 (2.2960e+01)	Acc 0.285156 (0.285156)
Epoch: [63][300/616]	Loss 2.3421e+01 (2.3396e+01)	Acc 0.273438 (0.272854)
Epoch: [63][600/616]	Loss 2.1864e+01 (2.3363e+01)	Acc 0.295898 (0.273561)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.283654 (0.269830)
Training Loss of Epoch 63: 23.36263151711565
Training Acc of Epoch 63: 0.2735470655487805
Testing Acc of Epoch 63: 0.2698304347826087
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.3688e+01 (2.3688e+01)	Acc 0.271484 (0.271484)
Epoch: [64][300/616]	Loss 2.2958e+01 (2.3327e+01)	Acc 0.302734 (0.274443)
Epoch: [64][600/616]	Loss 2.3066e+01 (2.3311e+01)	Acc 0.279297 (0.273788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.286859 (0.270535)
Training Loss of Epoch 64: 23.305104300646278
Training Acc of Epoch 64: 0.27382971290650404
Testing Acc of Epoch 64: 0.2705347826086957
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3417e+01 (2.3417e+01)	Acc 0.284180 (0.284180)
Epoch: [65][300/616]	Loss 2.3694e+01 (2.3033e+01)	Acc 0.269531 (0.274116)
Epoch: [65][600/616]	Loss 2.2018e+01 (2.2841e+01)	Acc 0.274414 (0.274271)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.294872 (0.289726)
Training Loss of Epoch 65: 22.818952855055894
Training Acc of Epoch 65: 0.27430290904471544
Testing Acc of Epoch 65: 0.28972608695652174
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.1668e+01 (2.1668e+01)	Acc 0.297852 (0.297852)
Epoch: [66][300/616]	Loss 2.1256e+01 (2.1616e+01)	Acc 0.282227 (0.279021)
Epoch: [66][600/616]	Loss 3.1836e+01 (2.2569e+01)	Acc 0.204102 (0.270493)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 66: 22.78141353886302
Training Acc of Epoch 66: 0.2689405487804878
Testing Acc of Epoch 66: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [67][300/616]	Loss 3.2266e+01 (3.1947e+01)	Acc 0.193359 (0.201334)
Epoch: [67][600/616]	Loss 3.1875e+01 (3.1939e+01)	Acc 0.203125 (0.201529)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 67: 31.941247459349594
Training Acc of Epoch 67: 0.20146881351626017
Testing Acc of Epoch 67: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [68][300/616]	Loss 3.2539e+01 (3.1912e+01)	Acc 0.186523 (0.202194)
Epoch: [68][600/616]	Loss 3.2617e+01 (3.1942e+01)	Acc 0.184570 (0.201459)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 68: 31.94194613821138
Training Acc of Epoch 68: 0.20145134654471544
Testing Acc of Epoch 68: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [69][300/616]	Loss 3.1562e+01 (3.1954e+01)	Acc 0.210938 (0.201149)
Epoch: [69][600/616]	Loss 3.2578e+01 (3.1948e+01)	Acc 0.185547 (0.201297)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 69: 31.94188262195122
Training Acc of Epoch 69: 0.2014529344512195
Testing Acc of Epoch 69: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [70][300/616]	Loss 3.1445e+01 (3.1938e+01)	Acc 0.213867 (0.201558)
Epoch: [70][600/616]	Loss 3.1680e+01 (3.1940e+01)	Acc 0.208008 (0.201490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 70: 31.94188262195122
Training Acc of Epoch 70: 0.2014529344512195
Testing Acc of Epoch 70: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [71][300/616]	Loss 3.1758e+01 (3.1937e+01)	Acc 0.206055 (0.201571)
Epoch: [71][600/616]	Loss 3.2422e+01 (3.1942e+01)	Acc 0.189453 (0.201445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 71: 31.94194613821138
Training Acc of Epoch 71: 0.20145134654471544
Testing Acc of Epoch 71: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [72][300/616]	Loss 3.1836e+01 (3.1937e+01)	Acc 0.204102 (0.201581)
Epoch: [72][600/616]	Loss 3.1094e+01 (3.1941e+01)	Acc 0.222656 (0.201464)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 72: 31.941755589430894
Training Acc of Epoch 72: 0.20145611026422763
Testing Acc of Epoch 72: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [73][300/616]	Loss 3.1836e+01 (3.1972e+01)	Acc 0.204102 (0.200705)
Epoch: [73][600/616]	Loss 3.1055e+01 (3.1942e+01)	Acc 0.223633 (0.201461)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 73: 31.942517784552845
Training Acc of Epoch 73: 0.20143705538617887
Testing Acc of Epoch 73: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [74][300/616]	Loss 3.1953e+01 (3.1958e+01)	Acc 0.201172 (0.201052)
Epoch: [74][600/616]	Loss 3.2148e+01 (3.1946e+01)	Acc 0.196289 (0.201354)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 74: 31.94188262195122
Training Acc of Epoch 74: 0.2014529344512195
Testing Acc of Epoch 74: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [75][300/616]	Loss 3.1602e+01 (3.1939e+01)	Acc 0.209961 (0.201513)
Epoch: [75][600/616]	Loss 3.1680e+01 (3.1943e+01)	Acc 0.208008 (0.201424)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 75: 31.94143800813008
Training Acc of Epoch 75: 0.20146404979674798
Testing Acc of Epoch 75: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [76][300/616]	Loss 3.2148e+01 (3.1951e+01)	Acc 0.196289 (0.201224)
Epoch: [76][600/616]	Loss 3.1914e+01 (3.1942e+01)	Acc 0.202148 (0.201440)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 76: 31.942454268292682
Training Acc of Epoch 76: 0.20143864329268293
Testing Acc of Epoch 76: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 3.2773e+01 (3.2773e+01)	Acc 0.180664 (0.180664)
Epoch: [77][300/616]	Loss 3.1953e+01 (3.1925e+01)	Acc 0.201172 (0.201882)
Epoch: [77][600/616]	Loss 3.1875e+01 (3.1944e+01)	Acc 0.203125 (0.201390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 77: 31.94213668699187
Training Acc of Epoch 77: 0.20144658282520325
Testing Acc of Epoch 77: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [78][300/616]	Loss 3.1367e+01 (3.1902e+01)	Acc 0.215820 (0.202447)
Epoch: [78][600/616]	Loss 3.1719e+01 (3.1938e+01)	Acc 0.207031 (0.201554)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 78: 31.942263719512194
Training Acc of Epoch 78: 0.20144340701219512
Testing Acc of Epoch 78: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 3.2695e+01 (3.2695e+01)	Acc 0.182617 (0.182617)
Epoch: [79][300/616]	Loss 3.1562e+01 (3.1928e+01)	Acc 0.210938 (0.201808)
Epoch: [79][600/616]	Loss 3.0938e+01 (3.1940e+01)	Acc 0.226562 (0.201494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 79: 31.941503112296747
Training Acc of Epoch 79: 0.20145928607723576
Testing Acc of Epoch 79: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [80][300/616]	Loss 3.1445e+01 (3.1936e+01)	Acc 0.213867 (0.201594)
Epoch: [80][600/616]	Loss 3.0318e+01 (3.1581e+01)	Acc 0.238281 (0.207647)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.223909)
Training Loss of Epoch 80: 31.56505927729413
Training Acc of Epoch 80: 0.20788236788617886
Testing Acc of Epoch 80: 0.22390869565217392
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 3.0911e+01 (3.0911e+01)	Acc 0.214844 (0.214844)
Epoch: [81][300/616]	Loss 2.9098e+01 (3.0018e+01)	Acc 0.238281 (0.228853)
Epoch: [81][600/616]	Loss 2.7457e+01 (2.9256e+01)	Acc 0.244141 (0.235220)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.232372 (0.241304)
Training Loss of Epoch 81: 29.22845401608847
Training Acc of Epoch 81: 0.23534838668699187
Testing Acc of Epoch 81: 0.24130434782608695
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.8204e+01 (2.8204e+01)	Acc 0.217773 (0.217773)
Epoch: [82][300/616]	Loss 2.7307e+01 (2.7530e+01)	Acc 0.275391 (0.250918)
Epoch: [82][600/616]	Loss 2.8202e+01 (2.7524e+01)	Acc 0.232422 (0.250476)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.235577 (0.239770)
Training Loss of Epoch 82: 27.526556393382997
Training Acc of Epoch 82: 0.2504446138211382
Testing Acc of Epoch 82: 0.23976956521739132
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.7926e+01 (2.7926e+01)	Acc 0.233398 (0.233398)
Epoch: [83][300/616]	Loss 2.7501e+01 (2.7688e+01)	Acc 0.237305 (0.249277)
Epoch: [83][600/616]	Loss 2.8047e+01 (2.7720e+01)	Acc 0.260742 (0.249062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.229167 (0.235252)
Training Loss of Epoch 83: 27.7186881398767
Training Acc of Epoch 83: 0.24907266260162603
Testing Acc of Epoch 83: 0.23525217391304348
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.8126e+01 (2.8126e+01)	Acc 0.222656 (0.222656)
Epoch: [84][300/616]	Loss 2.7381e+01 (2.7850e+01)	Acc 0.276367 (0.247330)
Epoch: [84][600/616]	Loss 2.7531e+01 (2.7900e+01)	Acc 0.256836 (0.247277)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.233974 (0.235409)
Training Loss of Epoch 84: 27.90443883136036
Training Acc of Epoch 84: 0.24726244918699186
Testing Acc of Epoch 84: 0.2354086956521739
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.8553e+01 (2.8553e+01)	Acc 0.215820 (0.215820)
Epoch: [85][300/616]	Loss 2.7725e+01 (2.7910e+01)	Acc 0.275391 (0.249695)
Epoch: [85][600/616]	Loss 2.8308e+01 (2.7899e+01)	Acc 0.256836 (0.248729)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.229167 (0.236926)
Training Loss of Epoch 85: 27.92112302082341
Training Acc of Epoch 85: 0.2484279725609756
Testing Acc of Epoch 85: 0.23692608695652173
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.9218e+01 (2.9218e+01)	Acc 0.215820 (0.215820)
Epoch: [86][300/616]	Loss 2.9597e+01 (2.8716e+01)	Acc 0.216797 (0.236182)
Epoch: [86][600/616]	Loss 2.6494e+01 (2.8735e+01)	Acc 0.283203 (0.239420)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.264423 (0.244813)
Training Loss of Epoch 86: 28.693797007615004
Training Acc of Epoch 86: 0.24029947916666666
Testing Acc of Epoch 86: 0.24481304347826088
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.6142e+01 (2.6142e+01)	Acc 0.285156 (0.285156)
Epoch: [87][300/616]	Loss 3.1554e+01 (2.8716e+01)	Acc 0.201172 (0.245302)
Epoch: [87][600/616]	Loss 3.1697e+01 (2.9985e+01)	Acc 0.180664 (0.223168)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 87: 30.002962754412394
Training Acc of Epoch 87: 0.22271659044715447
Testing Acc of Epoch 87: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 3.1451e+01 (3.1451e+01)	Acc 0.183594 (0.183594)
Epoch: [88][300/616]	Loss 3.0267e+01 (3.0499e+01)	Acc 0.194336 (0.201522)
Epoch: [88][600/616]	Loss 2.7075e+01 (2.9021e+01)	Acc 0.241211 (0.219691)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.238782 (0.246117)
Training Loss of Epoch 88: 28.974801620235286
Training Acc of Epoch 88: 0.22035696138211383
Testing Acc of Epoch 88: 0.24611739130434782
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.6333e+01 (2.6333e+01)	Acc 0.254883 (0.254883)
Epoch: [89][300/616]	Loss 3.1046e+01 (2.6004e+01)	Acc 0.215820 (0.246665)
Epoch: [89][600/616]	Loss 3.2621e+01 (2.8974e+01)	Acc 0.183594 (0.223649)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 89: 29.04077064079967
Training Acc of Epoch 89: 0.22314850101626016
Testing Acc of Epoch 89: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 3.1980e+01 (3.1980e+01)	Acc 0.199219 (0.199219)
Epoch: [90][300/616]	Loss 3.2109e+01 (3.1752e+01)	Acc 0.197266 (0.201785)
Epoch: [90][600/616]	Loss 3.1757e+01 (3.1856e+01)	Acc 0.206055 (0.201390)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 90: 31.856124304174408
Training Acc of Epoch 90: 0.20142911585365852
Testing Acc of Epoch 90: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 3.1523e+01 (3.1523e+01)	Acc 0.211914 (0.211914)
Epoch: [91][300/616]	Loss 3.3286e+01 (3.1945e+01)	Acc 0.166992 (0.201285)
Epoch: [91][600/616]	Loss 3.1232e+01 (3.1895e+01)	Acc 0.212891 (0.201482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 91: 31.890473035486732
Training Acc of Epoch 91: 0.20146404979674798
Testing Acc of Epoch 91: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 3.1590e+01 (3.1590e+01)	Acc 0.200195 (0.200195)
Epoch: [92][300/616]	Loss 3.3047e+01 (3.0623e+01)	Acc 0.173828 (0.199381)
Epoch: [92][600/616]	Loss 3.1633e+01 (3.1265e+01)	Acc 0.208984 (0.200745)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 92: 31.277979227391686
Training Acc of Epoch 92: 0.20081618394308942
Testing Acc of Epoch 92: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 3.1792e+01 (3.1792e+01)	Acc 0.205078 (0.205078)
Epoch: [93][300/616]	Loss 3.2454e+01 (3.1884e+01)	Acc 0.188477 (0.202726)
Epoch: [93][600/616]	Loss 3.1753e+01 (3.1938e+01)	Acc 0.206055 (0.201391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 93: 31.935426144483614
Training Acc of Epoch 93: 0.20145452235772357
Testing Acc of Epoch 93: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 3.1939e+01 (3.1939e+01)	Acc 0.201172 (0.201172)
Epoch: [94][300/616]	Loss 3.2305e+01 (3.1949e+01)	Acc 0.192383 (0.201126)
Epoch: [94][600/616]	Loss 3.1948e+01 (3.1936e+01)	Acc 0.201172 (0.201430)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 94: 31.93584071213637
Training Acc of Epoch 94: 0.2014449949186992
Testing Acc of Epoch 94: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 3.1211e+01 (3.1211e+01)	Acc 0.219727 (0.219727)
Epoch: [95][300/616]	Loss 3.1778e+01 (3.1970e+01)	Acc 0.205078 (0.200588)
Epoch: [95][600/616]	Loss 3.2453e+01 (3.1939e+01)	Acc 0.188477 (0.201355)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 95: 31.935476588427537
Training Acc of Epoch 95: 0.20145452235772357
Testing Acc of Epoch 95: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.2262e+01 (3.2262e+01)	Acc 0.193359 (0.193359)
Epoch: [96][300/616]	Loss 3.1273e+01 (3.1991e+01)	Acc 0.217773 (0.200066)
Epoch: [96][600/616]	Loss 3.1836e+01 (3.1940e+01)	Acc 0.204102 (0.201352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 96: 31.93512955952466
Training Acc of Epoch 96: 0.20146404979674798
Testing Acc of Epoch 96: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.0938e+01 (3.0938e+01)	Acc 0.226562 (0.226562)
Epoch: [97][300/616]	Loss 3.1363e+01 (3.1928e+01)	Acc 0.215820 (0.201668)
Epoch: [97][600/616]	Loss 3.2266e+01 (3.1935e+01)	Acc 0.193359 (0.201455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 97: 31.935789644815088
Training Acc of Epoch 97: 0.20144658282520325
Testing Acc of Epoch 97: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.2031e+01 (3.2031e+01)	Acc 0.199219 (0.199219)
Epoch: [98][300/616]	Loss 3.2342e+01 (3.1977e+01)	Acc 0.191406 (0.200416)
Epoch: [98][600/616]	Loss 3.1676e+01 (3.1941e+01)	Acc 0.208008 (0.201320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 98: 31.935649211232256
Training Acc of Epoch 98: 0.20145134654471544
Testing Acc of Epoch 98: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.1004e+01 (3.1004e+01)	Acc 0.224609 (0.224609)
Epoch: [99][300/616]	Loss 3.2300e+01 (3.1965e+01)	Acc 0.192383 (0.200714)
Epoch: [99][600/616]	Loss 3.1508e+01 (3.1933e+01)	Acc 0.211914 (0.201511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 99: 31.9360039408614
Training Acc of Epoch 99: 0.20144340701219512
Testing Acc of Epoch 99: 0.20121739130434782
Early stopping not satisfied.
