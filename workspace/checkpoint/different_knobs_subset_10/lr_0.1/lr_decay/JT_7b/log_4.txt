train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_7b
different_width False
resnet18_width 64
weight_precision 7
bias_precision 7
act_precision 10
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_7b/
file_prefix exp_4
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_7b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0017e-01 (5.0017e-01)	Acc 0.178711 (0.178711)
Epoch: [0][300/616]	Loss 2.8981e-01 (2.9107e-01)	Acc 0.719727 (0.691523)
Epoch: [0][600/616]	Loss 2.4991e-01 (2.8106e-01)	Acc 0.751953 (0.705060)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.738574)
Training Loss of Epoch 0: 0.28051987147428153
Training Acc of Epoch 0: 0.7057021722560975
Testing Acc of Epoch 0: 0.7385739130434783
Model with the best training loss saved! The loss is 0.28051987147428153
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.5910e-01 (2.5910e-01)	Acc 0.732422 (0.732422)
Epoch: [1][300/616]	Loss 2.7600e-01 (2.6625e-01)	Acc 0.722656 (0.723792)
Epoch: [1][600/616]	Loss 2.8067e-01 (2.6578e-01)	Acc 0.723633 (0.724994)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.731900)
Training Loss of Epoch 1: 0.26571269905179495
Training Acc of Epoch 1: 0.725098450203252
Testing Acc of Epoch 1: 0.7319
Model with the best training loss saved! The loss is 0.26571269905179495
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4903e-01 (2.4903e-01)	Acc 0.750000 (0.750000)
Epoch: [2][300/616]	Loss 3.1274e-01 (2.7283e-01)	Acc 0.678711 (0.717339)
Epoch: [2][600/616]	Loss 2.5665e-01 (2.7084e-01)	Acc 0.724609 (0.719397)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.724352)
Training Loss of Epoch 2: 0.27049497666397715
Training Acc of Epoch 2: 0.7197170350609756
Testing Acc of Epoch 2: 0.7243521739130435
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.6803e-01 (2.6803e-01)	Acc 0.722656 (0.722656)
Epoch: [3][300/616]	Loss 2.4748e-01 (2.7075e-01)	Acc 0.749023 (0.720606)
Epoch: [3][600/616]	Loss 2.5699e-01 (2.6777e-01)	Acc 0.745117 (0.723153)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.734674)
Training Loss of Epoch 3: 0.2675541637631936
Training Acc of Epoch 3: 0.7232485391260163
Testing Acc of Epoch 3: 0.7346739130434783
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.6769e-01 (2.6769e-01)	Acc 0.729492 (0.729492)
Epoch: [4][300/616]	Loss 2.4611e-01 (2.6440e-01)	Acc 0.768555 (0.727403)
Epoch: [4][600/616]	Loss 2.5057e-01 (2.6699e-01)	Acc 0.741211 (0.722562)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.733039)
Training Loss of Epoch 4: 0.2667935487942967
Training Acc of Epoch 4: 0.7227912220528455
Testing Acc of Epoch 4: 0.7330391304347826
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.4939e-01 (2.4939e-01)	Acc 0.746094 (0.746094)
Epoch: [5][300/616]	Loss 2.8825e-01 (2.6628e-01)	Acc 0.697266 (0.724162)
Epoch: [5][600/616]	Loss 2.7920e-01 (2.6384e-01)	Acc 0.713867 (0.727232)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.705128 (0.724974)
Training Loss of Epoch 5: 0.2637458697567141
Training Acc of Epoch 5: 0.7272897611788618
Testing Acc of Epoch 5: 0.7249739130434782
Model with the best training loss saved! The loss is 0.2637458697567141
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.8404e-01 (2.8404e-01)	Acc 0.696289 (0.696289)
Epoch: [6][300/616]	Loss 2.6603e-01 (2.6386e-01)	Acc 0.725586 (0.727218)
Epoch: [6][600/616]	Loss 2.4409e-01 (2.6338e-01)	Acc 0.759766 (0.727273)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.732600)
Training Loss of Epoch 6: 0.26336941389533564
Training Acc of Epoch 6: 0.7273485137195121
Testing Acc of Epoch 6: 0.7326
Model with the best training loss saved! The loss is 0.26336941389533564
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4687e-01 (2.4687e-01)	Acc 0.733398 (0.733398)
Epoch: [7][300/616]	Loss 2.7110e-01 (2.6228e-01)	Acc 0.709961 (0.728366)
Epoch: [7][600/616]	Loss 2.8369e-01 (2.6305e-01)	Acc 0.708984 (0.727393)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.727539)
Training Loss of Epoch 7: 0.2630534994892958
Training Acc of Epoch 7: 0.7274294969512195
Testing Acc of Epoch 7: 0.7275391304347826
Model with the best training loss saved! The loss is 0.2630534994892958
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5923e-01 (2.5923e-01)	Acc 0.719727 (0.719727)
Epoch: [8][300/616]	Loss 2.6637e-01 (2.6336e-01)	Acc 0.723633 (0.727812)
Epoch: [8][600/616]	Loss 4.5868e-01 (2.6600e-01)	Acc 0.421875 (0.725032)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.352564 (0.366304)
Training Loss of Epoch 8: 0.270197423589908
Training Acc of Epoch 8: 0.7170430005081301
Testing Acc of Epoch 8: 0.36630434782608695
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 4.4659e-01 (4.4659e-01)	Acc 0.353516 (0.353516)
Epoch: [9][300/616]	Loss 2.9592e-01 (3.4852e-01)	Acc 0.707031 (0.584413)
Epoch: [9][600/616]	Loss 3.3647e-01 (3.2434e-01)	Acc 0.633789 (0.618094)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.677885 (0.663674)
Training Loss of Epoch 9: 0.32444701555783184
Training Acc of Epoch 9: 0.6185213414634146
Testing Acc of Epoch 9: 0.6636739130434782
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 3.1668e-01 (3.1668e-01)	Acc 0.675781 (0.675781)
Epoch: [10][300/616]	Loss 2.7977e-01 (2.8238e-01)	Acc 0.716797 (0.696656)
Epoch: [10][600/616]	Loss 2.4171e-01 (2.7234e-01)	Acc 0.768555 (0.712421)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.731735)
Training Loss of Epoch 10: 0.2720972663745648
Training Acc of Epoch 10: 0.712793762703252
Testing Acc of Epoch 10: 0.7317347826086956
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5424e-01 (2.5424e-01)	Acc 0.734375 (0.734375)
Epoch: [11][300/616]	Loss 2.9420e-01 (2.6293e-01)	Acc 0.697266 (0.727335)
Epoch: [11][600/616]	Loss 2.5155e-01 (2.6328e-01)	Acc 0.740234 (0.727136)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.734600)
Training Loss of Epoch 11: 0.2633508777473031
Training Acc of Epoch 11: 0.727146849593496
Testing Acc of Epoch 11: 0.7346
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.5810e-01 (2.5810e-01)	Acc 0.730469 (0.730469)
Epoch: [12][300/616]	Loss 2.6073e-01 (2.6170e-01)	Acc 0.738281 (0.730407)
Epoch: [12][600/616]	Loss 2.5792e-01 (2.6204e-01)	Acc 0.735352 (0.729590)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.738896)
Training Loss of Epoch 12: 0.26201692451306474
Training Acc of Epoch 12: 0.7296001651422764
Testing Acc of Epoch 12: 0.7388956521739131
Model with the best training loss saved! The loss is 0.26201692451306474
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4942e-01 (2.4942e-01)	Acc 0.750000 (0.750000)
Epoch: [13][300/616]	Loss 3.9344e-01 (3.6427e-01)	Acc 0.447266 (0.547761)
Epoch: [13][600/616]	Loss 2.6755e-01 (3.3188e-01)	Acc 0.722656 (0.616445)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.698718 (0.707952)
Training Loss of Epoch 13: 0.3308011386694947
Training Acc of Epoch 13: 0.6185594512195122
Testing Acc of Epoch 13: 0.7079521739130434
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.8157e-01 (2.8157e-01)	Acc 0.703125 (0.703125)
Epoch: [14][300/616]	Loss 2.6638e-01 (2.7643e-01)	Acc 0.714844 (0.718821)
Epoch: [14][600/616]	Loss 2.7519e-01 (2.7601e-01)	Acc 0.730469 (0.717954)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.698052)
Training Loss of Epoch 14: 0.27583395451065
Training Acc of Epoch 14: 0.7180608485772357
Testing Acc of Epoch 14: 0.6980521739130435
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.8966e-01 (2.8966e-01)	Acc 0.684570 (0.684570)
Epoch: [15][300/616]	Loss 2.4965e-01 (2.6733e-01)	Acc 0.739258 (0.723351)
Epoch: [15][600/616]	Loss 2.6536e-01 (2.6620e-01)	Acc 0.738281 (0.725176)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.729257)
Training Loss of Epoch 15: 0.2661267610100227
Training Acc of Epoch 15: 0.7252334222560975
Testing Acc of Epoch 15: 0.7292565217391305
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.6574e-01 (2.6574e-01)	Acc 0.715820 (0.715820)
Epoch: [16][300/616]	Loss 2.5184e-01 (2.6159e-01)	Acc 0.747070 (0.729275)
Epoch: [16][600/616]	Loss 2.7269e-01 (2.6420e-01)	Acc 0.726562 (0.726541)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.618590 (0.603374)
Training Loss of Epoch 16: 0.26692164419143183
Training Acc of Epoch 16: 0.7231818470528455
Testing Acc of Epoch 16: 0.6033739130434783
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 3.2902e-01 (3.2902e-01)	Acc 0.589844 (0.589844)
Epoch: [17][300/616]	Loss 2.6012e-01 (2.7821e-01)	Acc 0.726562 (0.715184)
Epoch: [17][600/616]	Loss 2.6928e-01 (2.8464e-01)	Acc 0.730469 (0.707813)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.726487)
Training Loss of Epoch 17: 0.28412218752915297
Training Acc of Epoch 17: 0.7083508003048781
Testing Acc of Epoch 17: 0.7264869565217391
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.8325e-01 (2.8325e-01)	Acc 0.719727 (0.719727)
Epoch: [18][300/616]	Loss 2.5311e-01 (2.7700e-01)	Acc 0.747070 (0.713439)
Epoch: [18][600/616]	Loss 4.7049e-01 (3.3953e-01)	Acc 0.332031 (0.593859)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.309295 (0.337078)
Training Loss of Epoch 18: 0.34245628227547903
Training Acc of Epoch 18: 0.5879557291666667
Testing Acc of Epoch 18: 0.3370782608695652
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 4.6835e-01 (4.6835e-01)	Acc 0.336914 (0.336914)
Epoch: [19][300/616]	Loss 5.0055e-01 (5.0330e-01)	Acc 0.181641 (0.208222)
Epoch: [19][600/616]	Loss 5.0077e-01 (5.0321e-01)	Acc 0.187500 (0.205171)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 19: 0.5031529855437396
Training Acc of Epoch 19: 0.20509876778455285
Testing Acc of Epoch 19: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 5.0075e-01 (5.0075e-01)	Acc 0.192383 (0.192383)
Epoch: [20][300/616]	Loss 5.0094e-01 (5.0016e-01)	Acc 0.194336 (0.201016)
Epoch: [20][600/616]	Loss 4.9913e-01 (4.9978e-01)	Acc 0.205078 (0.201422)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 20: 0.49974332064147886
Training Acc of Epoch 20: 0.20144658282520325
Testing Acc of Epoch 20: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 5.0071e-01 (5.0071e-01)	Acc 0.199219 (0.199219)
Epoch: [21][300/616]	Loss 5.0040e-01 (5.0430e-01)	Acc 0.205078 (0.201383)
Epoch: [21][600/616]	Loss 5.0040e-01 (5.0235e-01)	Acc 0.203125 (0.201360)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.195513 (0.201217)
Training Loss of Epoch 21: 0.5023100022378006
Training Acc of Epoch 21: 0.20126873729674796
Testing Acc of Epoch 21: 0.20121739130434782
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 5.0040e-01 (5.0040e-01)	Acc 0.201172 (0.201172)
Epoch: [22][300/616]	Loss 4.8236e-01 (4.9252e-01)	Acc 0.215820 (0.202434)
Epoch: [22][600/616]	Loss 5.0045e-01 (4.9668e-01)	Acc 0.184570 (0.210796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194704)
Training Loss of Epoch 22: 0.4967682113976983
Training Acc of Epoch 22: 0.21043730945121952
Testing Acc of Epoch 22: 0.19470434782608695
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 5.0076e-01 (5.0076e-01)	Acc 0.169922 (0.169922)
Epoch: [23][300/616]	Loss 5.0040e-01 (5.0101e-01)	Acc 0.208008 (0.199913)
Epoch: [23][600/616]	Loss 4.8310e-01 (4.9875e-01)	Acc 0.291992 (0.210021)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.275641 (0.275952)
Training Loss of Epoch 23: 0.4983033154068924
Training Acc of Epoch 23: 0.21190294715447155
Testing Acc of Epoch 23: 0.2759521739130435
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 4.8375e-01 (4.8375e-01)	Acc 0.291016 (0.291016)
Epoch: [24][300/616]	Loss 3.7876e-01 (4.7048e-01)	Acc 0.540039 (0.332606)
Epoch: [24][600/616]	Loss 3.3111e-01 (3.9587e-01)	Acc 0.656250 (0.493344)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.709039)
Training Loss of Epoch 24: 0.3941049458534737
Training Acc of Epoch 24: 0.49760861280487806
Testing Acc of Epoch 24: 0.7090391304347826
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.9269e-01 (2.9269e-01)	Acc 0.711914 (0.711914)
Epoch: [25][300/616]	Loss 2.5851e-01 (2.7562e-01)	Acc 0.723633 (0.712651)
Epoch: [25][600/616]	Loss 2.5874e-01 (2.7223e-01)	Acc 0.723633 (0.716220)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.695513 (0.722335)
Training Loss of Epoch 25: 0.27196408161302893
Training Acc of Epoch 25: 0.7164348323170732
Testing Acc of Epoch 25: 0.7223347826086957
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.6819e-01 (2.6819e-01)	Acc 0.714844 (0.714844)
Epoch: [26][300/616]	Loss 3.8206e-01 (3.1422e-01)	Acc 0.511719 (0.648210)
Epoch: [26][600/616]	Loss 2.4395e-01 (3.0239e-01)	Acc 0.757812 (0.668297)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.703291)
Training Loss of Epoch 26: 0.3018083623996595
Training Acc of Epoch 26: 0.6692692454268293
Testing Acc of Epoch 26: 0.703291304347826
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.7789e-01 (2.7789e-01)	Acc 0.708008 (0.708008)
Epoch: [27][300/616]	Loss 3.1449e-01 (2.6257e-01)	Acc 0.676758 (0.727497)
Epoch: [27][600/616]	Loss 2.5727e-01 (2.6164e-01)	Acc 0.738281 (0.728225)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.739687)
Training Loss of Epoch 27: 0.2614600973158348
Training Acc of Epoch 27: 0.7284219385162601
Testing Acc of Epoch 27: 0.7396869565217391
Model with the best training loss saved! The loss is 0.2614600973158348
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.5060e-01 (2.5060e-01)	Acc 0.752930 (0.752930)
Epoch: [28][300/616]	Loss 3.7425e-01 (3.6603e-01)	Acc 0.528320 (0.540555)
Epoch: [28][600/616]	Loss 2.7345e-01 (3.4086e-01)	Acc 0.728516 (0.593752)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.719152)
Training Loss of Epoch 28: 0.3393995548409175
Training Acc of Epoch 28: 0.5967479674796748
Testing Acc of Epoch 28: 0.7191521739130434
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.8354e-01 (2.8354e-01)	Acc 0.721680 (0.721680)
Epoch: [29][300/616]	Loss 2.5205e-01 (2.7262e-01)	Acc 0.738281 (0.718137)
Epoch: [29][600/616]	Loss 2.4498e-01 (2.6683e-01)	Acc 0.750000 (0.723189)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.733722)
Training Loss of Epoch 29: 0.2666691104086434
Training Acc of Epoch 29: 0.7232548907520325
Testing Acc of Epoch 29: 0.7337217391304348
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4956e-01 (2.4956e-01)	Acc 0.734375 (0.734375)
Epoch: [30][300/616]	Loss 2.5418e-01 (2.5924e-01)	Acc 0.737305 (0.729706)
Epoch: [30][600/616]	Loss 2.5841e-01 (2.6024e-01)	Acc 0.724609 (0.729203)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.702865)
Training Loss of Epoch 30: 0.2602778291314598
Training Acc of Epoch 30: 0.729174606199187
Testing Acc of Epoch 30: 0.7028652173913044
Model with the best training loss saved! The loss is 0.2602778291314598
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.6013e-01 (2.6013e-01)	Acc 0.720703 (0.720703)
Epoch: [31][300/616]	Loss 2.5876e-01 (2.6178e-01)	Acc 0.734375 (0.727591)
Epoch: [31][600/616]	Loss 2.8945e-01 (2.6346e-01)	Acc 0.685547 (0.725175)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740143)
Training Loss of Epoch 31: 0.2633352260037166
Training Acc of Epoch 31: 0.725317581300813
Testing Acc of Epoch 31: 0.7401434782608696
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4234e-01 (2.4234e-01)	Acc 0.750000 (0.750000)
Epoch: [32][300/616]	Loss 2.4183e-01 (2.6005e-01)	Acc 0.759766 (0.728889)
Epoch: [32][600/616]	Loss 4.4985e-01 (2.9760e-01)	Acc 0.341797 (0.657503)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.354167 (0.340122)
Training Loss of Epoch 32: 0.30110993237514805
Training Acc of Epoch 32: 0.649633193597561
Testing Acc of Epoch 32: 0.3401217391304348
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 4.5602e-01 (4.5602e-01)	Acc 0.333008 (0.333008)
Epoch: [33][300/616]	Loss 3.1680e+01 (1.6941e+01)	Acc 0.208008 (0.273048)
Epoch: [33][600/616]	Loss 3.1406e+01 (2.4589e+01)	Acc 0.214844 (0.233314)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 33: 24.76552729684163
Training Acc of Epoch 33: 0.23235041920731708
Testing Acc of Epoch 33: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.2617e+01 (3.2617e+01)	Acc 0.184570 (0.184570)
Epoch: [34][300/616]	Loss 3.2070e+01 (3.2237e+01)	Acc 0.198242 (0.194073)
Epoch: [34][600/616]	Loss 3.2383e+01 (3.2246e+01)	Acc 0.190430 (0.193852)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 34: 32.247434780462
Training Acc of Epoch 34: 0.1938135162601626
Testing Acc of Epoch 34: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [35][300/616]	Loss 3.3945e+01 (3.2251e+01)	Acc 0.151367 (0.193723)
Epoch: [35][600/616]	Loss 3.1680e+01 (3.2252e+01)	Acc 0.208008 (0.193692)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 35: 32.24743483008408
Training Acc of Epoch 35: 0.1938135162601626
Testing Acc of Epoch 35: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [36][300/616]	Loss 3.2109e+01 (3.2236e+01)	Acc 0.197266 (0.194089)
Epoch: [36][600/616]	Loss 3.1602e+01 (3.2246e+01)	Acc 0.209961 (0.193848)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 36: 32.24775229973522
Training Acc of Epoch 36: 0.1938055767276423
Testing Acc of Epoch 36: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [37][300/616]	Loss 3.1719e+01 (3.2270e+01)	Acc 0.207031 (0.193252)
Epoch: [37][600/616]	Loss 3.1875e+01 (3.2251e+01)	Acc 0.203125 (0.193727)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 37: 32.24762529822869
Training Acc of Epoch 37: 0.19380875254065041
Testing Acc of Epoch 37: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.2578e+01 (3.2578e+01)	Acc 0.185547 (0.185547)
Epoch: [38][300/616]	Loss 3.2617e+01 (3.2213e+01)	Acc 0.184570 (0.194667)
Epoch: [38][600/616]	Loss 3.2250e+01 (3.2247e+01)	Acc 0.193359 (0.193832)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 38: 32.248196913556356
Training Acc of Epoch 38: 0.19379446138211381
Testing Acc of Epoch 38: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.1445e+01 (3.1445e+01)	Acc 0.213867 (0.213867)
Epoch: [39][300/616]	Loss 3.1758e+01 (3.2273e+01)	Acc 0.206055 (0.193165)
Epoch: [39][600/616]	Loss 3.2383e+01 (3.2249e+01)	Acc 0.190430 (0.193787)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 39: 32.24800625932895
Training Acc of Epoch 39: 0.193799225101626
Testing Acc of Epoch 39: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.2383e+01 (3.2383e+01)	Acc 0.190430 (0.190430)
Epoch: [40][300/616]	Loss 3.2344e+01 (3.2216e+01)	Acc 0.191406 (0.194589)
Epoch: [40][600/616]	Loss 3.2539e+01 (3.2246e+01)	Acc 0.186523 (0.193847)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 40: 32.24756152145262
Training Acc of Epoch 40: 0.19381034044715448
Testing Acc of Epoch 40: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.2969e+01 (3.2969e+01)	Acc 0.175781 (0.175781)
Epoch: [41][300/616]	Loss 3.2578e+01 (3.2281e+01)	Acc 0.185547 (0.192980)
Epoch: [41][600/616]	Loss 3.2539e+01 (3.2248e+01)	Acc 0.186523 (0.193790)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 41: 32.24743439589089
Training Acc of Epoch 41: 0.1938135162601626
Testing Acc of Epoch 41: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [42][300/616]	Loss 3.2305e+01 (3.2250e+01)	Acc 0.192383 (0.193749)
Epoch: [42][600/616]	Loss 3.1602e+01 (3.2251e+01)	Acc 0.209961 (0.193731)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 42: 32.24762496948242
Training Acc of Epoch 42: 0.19380875254065041
Testing Acc of Epoch 42: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [43][300/616]	Loss 3.2109e+01 (3.2267e+01)	Acc 0.197266 (0.193314)
Epoch: [43][600/616]	Loss 3.1914e+01 (3.2250e+01)	Acc 0.202148 (0.193761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 43: 32.24810758761274
Training Acc of Epoch 43: 0.19379604928861788
Testing Acc of Epoch 43: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [44][300/616]	Loss 3.2422e+01 (3.2263e+01)	Acc 0.189453 (0.193428)
Epoch: [44][600/616]	Loss 3.3320e+01 (3.2247e+01)	Acc 0.166992 (0.193814)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 44: 32.24752286585366
Training Acc of Epoch 44: 0.19381192835365854
Testing Acc of Epoch 44: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [45][300/616]	Loss 3.1914e+01 (3.2244e+01)	Acc 0.202148 (0.193911)
Epoch: [45][600/616]	Loss 3.1953e+01 (3.2250e+01)	Acc 0.201172 (0.193749)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 45: 32.24669715447155
Training Acc of Epoch 45: 0.1938325711382114
Testing Acc of Epoch 45: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.2773e+01 (3.2773e+01)	Acc 0.180664 (0.180664)
Epoch: [46][300/616]	Loss 3.1953e+01 (3.2204e+01)	Acc 0.201172 (0.194897)
Epoch: [46][600/616]	Loss 3.2383e+01 (3.2249e+01)	Acc 0.190430 (0.193779)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 46: 32.248094512195124
Training Acc of Epoch 46: 0.19379763719512194
Testing Acc of Epoch 46: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.2578e+01 (3.2578e+01)	Acc 0.185547 (0.185547)
Epoch: [47][300/616]	Loss 3.2422e+01 (3.2267e+01)	Acc 0.189453 (0.193330)
Epoch: [47][600/616]	Loss 3.2070e+01 (3.2249e+01)	Acc 0.198242 (0.193777)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 47: 32.24784044715447
Training Acc of Epoch 47: 0.19380398882113822
Testing Acc of Epoch 47: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [48][300/616]	Loss 3.2344e+01 (3.2253e+01)	Acc 0.191406 (0.193674)
Epoch: [48][600/616]	Loss 3.2031e+01 (3.2248e+01)	Acc 0.199219 (0.193795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 48: 32.24758638211382
Training Acc of Epoch 48: 0.19381034044715448
Testing Acc of Epoch 48: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.2539e+01 (3.2539e+01)	Acc 0.186523 (0.186523)
Epoch: [49][300/616]	Loss 3.1133e+01 (3.2272e+01)	Acc 0.221680 (0.193210)
Epoch: [49][600/616]	Loss 3.3359e+01 (3.2246e+01)	Acc 0.166016 (0.193839)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 49: 32.24784044715447
Training Acc of Epoch 49: 0.19380398882113822
Testing Acc of Epoch 49: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [50][300/616]	Loss 3.2617e+01 (3.2247e+01)	Acc 0.184570 (0.193814)
Epoch: [50][600/616]	Loss 3.3086e+01 (3.2243e+01)	Acc 0.172852 (0.193935)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 50: 32.24822154471545
Training Acc of Epoch 50: 0.19379446138211381
Testing Acc of Epoch 50: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [51][300/616]	Loss 3.2188e+01 (3.2257e+01)	Acc 0.195312 (0.193583)
Epoch: [51][600/616]	Loss 3.1250e+01 (3.2251e+01)	Acc 0.218750 (0.193730)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 51: 32.24701473577236
Training Acc of Epoch 51: 0.19382463160569105
Testing Acc of Epoch 51: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.2461e+01 (3.2461e+01)	Acc 0.188477 (0.188477)
Epoch: [52][300/616]	Loss 3.2617e+01 (3.2261e+01)	Acc 0.184570 (0.193476)
Epoch: [52][600/616]	Loss 3.2422e+01 (3.2251e+01)	Acc 0.189453 (0.193733)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 52: 32.247395833333336
Training Acc of Epoch 52: 0.19381510416666667
Testing Acc of Epoch 52: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.2891e+01 (3.2891e+01)	Acc 0.177734 (0.177734)
Epoch: [53][300/616]	Loss 3.2930e+01 (3.2257e+01)	Acc 0.176758 (0.193583)
Epoch: [53][600/616]	Loss 3.2188e+01 (3.2247e+01)	Acc 0.195312 (0.193827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 53: 32.24720528455285
Training Acc of Epoch 53: 0.19381986788617886
Testing Acc of Epoch 53: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 3.2773e+01 (3.2773e+01)	Acc 0.180664 (0.180664)
Epoch: [54][300/616]	Loss 3.2500e+01 (3.2270e+01)	Acc 0.187500 (0.193249)
Epoch: [54][600/616]	Loss 3.2383e+01 (3.2255e+01)	Acc 0.190430 (0.193632)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 54: 32.24733311102642
Training Acc of Epoch 54: 0.19381510416666667
Testing Acc of Epoch 54: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [55][300/616]	Loss 3.1836e+01 (3.2240e+01)	Acc 0.204102 (0.193995)
Epoch: [55][600/616]	Loss 3.2070e+01 (3.2247e+01)	Acc 0.198242 (0.193816)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 55: 32.24739742123984
Training Acc of Epoch 55: 0.19381192835365854
Testing Acc of Epoch 55: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [56][300/616]	Loss 3.2344e+01 (3.2232e+01)	Acc 0.191406 (0.194161)
Epoch: [56][600/616]	Loss 3.2617e+01 (3.2244e+01)	Acc 0.184570 (0.193884)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194652)
Training Loss of Epoch 56: 32.246779725609755
Training Acc of Epoch 56: 0.19380398882113822
Testing Acc of Epoch 56: 0.19465217391304349
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [57][300/616]	Loss 3.2266e+01 (3.2238e+01)	Acc 0.193359 (0.194044)
Epoch: [57][600/616]	Loss 3.2344e+01 (3.2248e+01)	Acc 0.190430 (0.193792)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 57: 32.24790634527439
Training Acc of Epoch 57: 0.19379763719512194
Testing Acc of Epoch 57: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [58][300/616]	Loss 2.0250e+01 (2.2013e+01)	Acc 0.303711 (0.273947)
Epoch: [58][600/616]	Loss 2.0459e+01 (2.1337e+01)	Acc 0.303711 (0.290613)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.317308 (0.311022)
Training Loss of Epoch 58: 21.316461144424068
Training Acc of Epoch 58: 0.291041031504065
Testing Acc of Epoch 58: 0.3110217391304348
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.0071e+01 (2.0071e+01)	Acc 0.317383 (0.317383)
Epoch: [59][300/616]	Loss 1.9923e+01 (2.0884e+01)	Acc 0.323242 (0.310881)
Epoch: [59][600/616]	Loss 3.2383e+01 (2.4771e+01)	Acc 0.190430 (0.271746)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 59: 24.93078464415015
Training Acc of Epoch 59: 0.27022357723577234
Testing Acc of Epoch 59: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [60][300/616]	Loss 3.1133e+01 (3.1974e+01)	Acc 0.221680 (0.200656)
Epoch: [60][600/616]	Loss 3.3203e+01 (3.2017e+01)	Acc 0.169922 (0.199527)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 60: 32.023400339266146
Training Acc of Epoch 60: 0.19936960111788618
Testing Acc of Epoch 60: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [61][300/616]	Loss 3.2617e+01 (3.2269e+01)	Acc 0.184570 (0.193269)
Epoch: [61][600/616]	Loss 3.1836e+01 (3.2244e+01)	Acc 0.204102 (0.193887)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 61: 32.247954974911075
Training Acc of Epoch 61: 0.19380081300813007
Testing Acc of Epoch 61: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 3.3320e+01 (3.3320e+01)	Acc 0.166992 (0.166992)
Epoch: [62][300/616]	Loss 3.1836e+01 (3.2239e+01)	Acc 0.204102 (0.194015)
Epoch: [62][600/616]	Loss 3.1953e+01 (3.2249e+01)	Acc 0.201172 (0.193759)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 62: 32.24689076431398
Training Acc of Epoch 62: 0.19382145579268292
Testing Acc of Epoch 62: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 3.1602e+01 (3.1602e+01)	Acc 0.209961 (0.209961)
Epoch: [63][300/616]	Loss 3.1719e+01 (3.2288e+01)	Acc 0.207031 (0.192762)
Epoch: [63][600/616]	Loss 3.2617e+01 (3.2246e+01)	Acc 0.184570 (0.193795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 63: 32.245133606205144
Training Acc of Epoch 63: 0.1938135162601626
Testing Acc of Epoch 63: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [64][300/616]	Loss 3.1523e+01 (3.2233e+01)	Acc 0.211914 (0.194047)
Epoch: [64][600/616]	Loss 3.1850e+01 (3.2237e+01)	Acc 0.202148 (0.193845)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 64: 32.23624184151006
Training Acc of Epoch 64: 0.193823043699187
Testing Acc of Epoch 64: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.2214e+01 (3.2214e+01)	Acc 0.192383 (0.192383)
Epoch: [65][300/616]	Loss 3.2631e+01 (3.2026e+01)	Acc 0.181641 (0.198966)
Epoch: [65][600/616]	Loss 2.7392e+01 (2.9949e+01)	Acc 0.300781 (0.223740)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.293269 (0.297096)
Training Loss of Epoch 65: 29.888471609596316
Training Acc of Epoch 65: 0.22413459095528454
Testing Acc of Epoch 65: 0.29709565217391304
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.7157e+01 (2.7157e+01)	Acc 0.306641 (0.306641)
Epoch: [66][300/616]	Loss 2.6778e+01 (2.6893e+01)	Acc 0.220703 (0.263175)
Epoch: [66][600/616]	Loss 2.5002e+01 (2.6707e+01)	Acc 0.337891 (0.263552)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.238270)
Training Loss of Epoch 66: 26.70575315584012
Training Acc of Epoch 66: 0.2625936864837398
Testing Acc of Epoch 66: 0.23826956521739132
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.6697e+01 (2.6697e+01)	Acc 0.193359 (0.193359)
Epoch: [67][300/616]	Loss 2.4620e+01 (2.6850e+01)	Acc 0.359375 (0.256022)
Epoch: [67][600/616]	Loss 2.5391e+01 (2.6260e+01)	Acc 0.333008 (0.264891)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.225657)
Training Loss of Epoch 67: 26.235773322252722
Training Acc of Epoch 67: 0.26558053861788616
Testing Acc of Epoch 67: 0.22565652173913042
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.4988e+01 (2.4988e+01)	Acc 0.203125 (0.203125)
Epoch: [68][300/616]	Loss 2.5204e+01 (2.4659e+01)	Acc 0.334961 (0.287220)
Epoch: [68][600/616]	Loss 2.4549e+01 (2.4653e+01)	Acc 0.227539 (0.289240)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.334936 (0.321917)
Training Loss of Epoch 68: 24.660068133594542
Training Acc of Epoch 68: 0.28927210365853656
Testing Acc of Epoch 68: 0.3219173913043478
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4691e+01 (2.4691e+01)	Acc 0.339844 (0.339844)
Epoch: [69][300/616]	Loss 2.5212e+01 (2.4865e+01)	Acc 0.212891 (0.290714)
Epoch: [69][600/616]	Loss 2.4895e+01 (2.4873e+01)	Acc 0.221680 (0.291864)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.339744 (0.343530)
Training Loss of Epoch 69: 24.864733148590336
Training Acc of Epoch 69: 0.29180005081300814
Testing Acc of Epoch 69: 0.3435304347826087
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4403e+01 (2.4403e+01)	Acc 0.357422 (0.357422)
Epoch: [70][300/616]	Loss 2.4372e+01 (2.4679e+01)	Acc 0.356445 (0.293945)
Epoch: [70][600/616]	Loss 2.4563e+01 (2.4770e+01)	Acc 0.246094 (0.292888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.313113)
Training Loss of Epoch 70: 24.766578333164617
Training Acc of Epoch 70: 0.2927115091463415
Testing Acc of Epoch 70: 0.31311304347826086
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4711e+01 (2.4711e+01)	Acc 0.217773 (0.217773)
Epoch: [71][300/616]	Loss 2.4238e+01 (2.4685e+01)	Acc 0.225586 (0.294873)
Epoch: [71][600/616]	Loss 2.5696e+01 (2.4723e+01)	Acc 0.221680 (0.294095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.269235)
Training Loss of Epoch 71: 24.725147712521437
Training Acc of Epoch 71: 0.29405170223577237
Testing Acc of Epoch 71: 0.26923478260869566
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4195e+01 (2.4195e+01)	Acc 0.235352 (0.235352)
Epoch: [72][300/616]	Loss 2.4812e+01 (2.4740e+01)	Acc 0.337891 (0.294477)
Epoch: [72][600/616]	Loss 2.4842e+01 (2.4737e+01)	Acc 0.338867 (0.293924)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.306861)
Training Loss of Epoch 72: 24.738172720312104
Training Acc of Epoch 72: 0.2937230055894309
Testing Acc of Epoch 72: 0.3068608695652174
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4387e+01 (2.4387e+01)	Acc 0.237305 (0.237305)
Epoch: [73][300/616]	Loss 2.4675e+01 (2.4631e+01)	Acc 0.207031 (0.294338)
Epoch: [73][600/616]	Loss 2.5360e+01 (2.4594e+01)	Acc 0.209961 (0.294223)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.251270)
Training Loss of Epoch 73: 24.613189892652557
Training Acc of Epoch 73: 0.29384209857723576
Testing Acc of Epoch 73: 0.2512695652173913
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.4468e+01 (2.4468e+01)	Acc 0.237305 (0.237305)
Epoch: [74][300/616]	Loss 2.4107e+01 (2.4704e+01)	Acc 0.353516 (0.294273)
Epoch: [74][600/616]	Loss 2.4233e+01 (2.4745e+01)	Acc 0.345703 (0.293947)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.252943)
Training Loss of Epoch 74: 24.745212964313787
Training Acc of Epoch 74: 0.2938325711382114
Testing Acc of Epoch 74: 0.25294347826086955
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4863e+01 (2.4863e+01)	Acc 0.214844 (0.214844)
Epoch: [75][300/616]	Loss 2.4539e+01 (2.4625e+01)	Acc 0.351562 (0.292940)
Epoch: [75][600/616]	Loss 2.5565e+01 (2.4569e+01)	Acc 0.322266 (0.295216)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.255961)
Training Loss of Epoch 75: 24.569655488177045
Training Acc of Epoch 75: 0.2950489075203252
Testing Acc of Epoch 75: 0.2559608695652174
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.4938e+01 (2.4938e+01)	Acc 0.222656 (0.222656)
Epoch: [76][300/616]	Loss 2.5040e+01 (2.4592e+01)	Acc 0.228516 (0.296083)
Epoch: [76][600/616]	Loss 2.4198e+01 (2.4654e+01)	Acc 0.228516 (0.294558)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.319522)
Training Loss of Epoch 76: 24.661734790336794
Training Acc of Epoch 76: 0.2950584349593496
Testing Acc of Epoch 76: 0.3195217391304348
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4435e+01 (2.4435e+01)	Acc 0.213867 (0.213867)
Epoch: [77][300/616]	Loss 2.4707e+01 (2.4805e+01)	Acc 0.343750 (0.293517)
Epoch: [77][600/616]	Loss 2.4157e+01 (2.4775e+01)	Acc 0.358398 (0.293547)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.257274)
Training Loss of Epoch 77: 24.767885446936134
Training Acc of Epoch 77: 0.29485994664634146
Testing Acc of Epoch 77: 0.25727391304347824
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.6446e+01 (2.6446e+01)	Acc 0.203125 (0.203125)
Epoch: [78][300/616]	Loss 2.4388e+01 (2.4847e+01)	Acc 0.208984 (0.292122)
Epoch: [78][600/616]	Loss 2.6022e+01 (2.4778e+01)	Acc 0.203125 (0.292967)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.308143)
Training Loss of Epoch 78: 24.77396412671097
Training Acc of Epoch 78: 0.2938405106707317
Testing Acc of Epoch 78: 0.3081434782608696
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.5052e+01 (2.5052e+01)	Acc 0.208008 (0.208008)
Epoch: [79][300/616]	Loss 2.5437e+01 (2.4772e+01)	Acc 0.223633 (0.293598)
Epoch: [79][600/616]	Loss 2.3033e+01 (2.4858e+01)	Acc 0.246094 (0.292712)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.312861)
Training Loss of Epoch 79: 24.85922685948814
Training Acc of Epoch 79: 0.29327045223577236
Testing Acc of Epoch 79: 0.3128608695652174
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.5320e+01 (2.5320e+01)	Acc 0.225586 (0.225586)
Epoch: [80][300/616]	Loss 2.4923e+01 (2.4754e+01)	Acc 0.341797 (0.294915)
Epoch: [80][600/616]	Loss 2.5241e+01 (2.4803e+01)	Acc 0.224609 (0.293494)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.225535)
Training Loss of Epoch 80: 24.80495034504712
Training Acc of Epoch 80: 0.29414221290650405
Testing Acc of Epoch 80: 0.22553478260869564
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4856e+01 (2.4856e+01)	Acc 0.214844 (0.214844)
Epoch: [81][300/616]	Loss 2.4438e+01 (2.4726e+01)	Acc 0.219727 (0.294490)
Epoch: [81][600/616]	Loss 2.5436e+01 (2.4732e+01)	Acc 0.203125 (0.294431)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.224359 (0.230683)
Training Loss of Epoch 81: 24.73151347772862
Training Acc of Epoch 81: 0.29497903963414634
Testing Acc of Epoch 81: 0.23068260869565219
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.4360e+01 (2.4360e+01)	Acc 0.212891 (0.212891)
Epoch: [82][300/616]	Loss 2.3750e+01 (2.4608e+01)	Acc 0.368164 (0.294448)
Epoch: [82][600/616]	Loss 2.4599e+01 (2.4721e+01)	Acc 0.350586 (0.293624)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.227509)
Training Loss of Epoch 82: 24.726539959170953
Training Acc of Epoch 82: 0.29391673018292686
Testing Acc of Epoch 82: 0.2275086956521739
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.5515e+01 (2.5515e+01)	Acc 0.211914 (0.211914)
Epoch: [83][300/616]	Loss 2.5010e+01 (2.4945e+01)	Acc 0.232422 (0.290672)
Epoch: [83][600/616]	Loss 2.4502e+01 (2.4789e+01)	Acc 0.243164 (0.290137)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.333333 (0.341548)
Training Loss of Epoch 83: 24.780288953703593
Training Acc of Epoch 83: 0.29110295985772355
Testing Acc of Epoch 83: 0.3415478260869565
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.5367e+01 (2.5367e+01)	Acc 0.323242 (0.323242)
Epoch: [84][300/616]	Loss 2.4755e+01 (2.4768e+01)	Acc 0.344727 (0.291424)
Epoch: [84][600/616]	Loss 2.4667e+01 (2.4684e+01)	Acc 0.216797 (0.286739)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.320348)
Training Loss of Epoch 84: 24.681253330882004
Training Acc of Epoch 84: 0.2877175431910569
Testing Acc of Epoch 84: 0.3203478260869565
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.3989e+01 (2.3989e+01)	Acc 0.356445 (0.356445)
Epoch: [85][300/616]	Loss 2.4428e+01 (2.4851e+01)	Acc 0.232422 (0.288754)
Epoch: [85][600/616]	Loss 2.4685e+01 (2.4866e+01)	Acc 0.337891 (0.288159)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.233248)
Training Loss of Epoch 85: 24.867399457605874
Training Acc of Epoch 85: 0.2880557672764228
Testing Acc of Epoch 85: 0.23324782608695652
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.4701e+01 (2.4701e+01)	Acc 0.228516 (0.228516)
Epoch: [86][300/616]	Loss 2.4796e+01 (2.4963e+01)	Acc 0.335938 (0.285516)
Epoch: [86][600/616]	Loss 2.3649e+01 (2.4950e+01)	Acc 0.249023 (0.282898)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.226413)
Training Loss of Epoch 86: 24.954080969337525
Training Acc of Epoch 86: 0.2833762068089431
Testing Acc of Epoch 86: 0.22641304347826086
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.5470e+01 (2.5470e+01)	Acc 0.230469 (0.230469)
Epoch: [87][300/616]	Loss 2.4977e+01 (2.5099e+01)	Acc 0.208984 (0.284303)
Epoch: [87][600/616]	Loss 2.5980e+01 (2.4857e+01)	Acc 0.205078 (0.283515)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.217949 (0.289535)
Training Loss of Epoch 87: 24.8525981313814
Training Acc of Epoch 87: 0.28358263465447153
Testing Acc of Epoch 87: 0.28953478260869564
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.4944e+01 (2.4944e+01)	Acc 0.328125 (0.328125)
Epoch: [88][300/616]	Loss 2.4829e+01 (2.5116e+01)	Acc 0.346680 (0.287434)
Epoch: [88][600/616]	Loss 2.5362e+01 (2.5171e+01)	Acc 0.217773 (0.287616)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.322115 (0.298191)
Training Loss of Epoch 88: 25.164726651199466
Training Acc of Epoch 88: 0.28778741107723577
Testing Acc of Epoch 88: 0.2981913043478261
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.3833e+01 (2.3833e+01)	Acc 0.360352 (0.360352)
Epoch: [89][300/616]	Loss 2.4736e+01 (2.4688e+01)	Acc 0.329102 (0.277804)
Epoch: [89][600/616]	Loss 2.4985e+01 (2.4630e+01)	Acc 0.335938 (0.286770)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.323718 (0.264030)
Training Loss of Epoch 89: 24.63826675104901
Training Acc of Epoch 89: 0.2873459730691057
Testing Acc of Epoch 89: 0.2640304347826087
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4989e+01 (2.4989e+01)	Acc 0.333008 (0.333008)
Epoch: [90][300/616]	Loss 2.5324e+01 (2.5128e+01)	Acc 0.214844 (0.296226)
Epoch: [90][600/616]	Loss 2.5464e+01 (2.4859e+01)	Acc 0.203125 (0.294412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.322115 (0.329052)
Training Loss of Epoch 90: 24.85068441561567
Training Acc of Epoch 90: 0.2943978658536585
Testing Acc of Epoch 90: 0.3290521739130435
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.4563e+01 (2.4563e+01)	Acc 0.332031 (0.332031)
Epoch: [91][300/616]	Loss 2.4159e+01 (2.4441e+01)	Acc 0.314453 (0.293738)
Epoch: [91][600/616]	Loss 2.6174e+01 (2.4658e+01)	Acc 0.312500 (0.297585)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.229183)
Training Loss of Epoch 91: 24.6758924623815
Training Acc of Epoch 91: 0.2983422256097561
Testing Acc of Epoch 91: 0.22918260869565218
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.7075e+01 (2.7075e+01)	Acc 0.213867 (0.213867)
Epoch: [92][300/616]	Loss 2.4346e+01 (2.5548e+01)	Acc 0.349609 (0.316098)
Epoch: [92][600/616]	Loss 2.6012e+01 (2.5328e+01)	Acc 0.326172 (0.318701)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.307692 (0.321009)
Training Loss of Epoch 92: 25.32538828345818
Training Acc of Epoch 92: 0.3190310594512195
Testing Acc of Epoch 92: 0.32100869565217394
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.6212e+01 (2.6212e+01)	Acc 0.312500 (0.312500)
Epoch: [93][300/616]	Loss 2.5893e+01 (2.5594e+01)	Acc 0.307617 (0.325679)
Epoch: [93][600/616]	Loss 2.3331e+01 (2.5245e+01)	Acc 0.355469 (0.325847)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.312500 (0.321535)
Training Loss of Epoch 93: 25.227661123508362
Training Acc of Epoch 93: 0.325658981199187
Testing Acc of Epoch 93: 0.32153478260869567
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3814e+01 (2.3814e+01)	Acc 0.333984 (0.333984)
Epoch: [94][300/616]	Loss 2.2847e+01 (2.3916e+01)	Acc 0.352539 (0.325429)
Epoch: [94][600/616]	Loss 2.2517e+01 (2.3523e+01)	Acc 0.329102 (0.329158)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.328526 (0.332070)
Training Loss of Epoch 94: 23.50552977275073
Training Acc of Epoch 94: 0.32933657266260163
Testing Acc of Epoch 94: 0.3320695652173913
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2529e+01 (2.2529e+01)	Acc 0.323242 (0.323242)
Epoch: [95][300/616]	Loss 3.2109e+01 (2.8488e+01)	Acc 0.197266 (0.248384)
Epoch: [95][600/616]	Loss 3.2185e+01 (3.0225e+01)	Acc 0.195312 (0.224613)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 95: 30.263128767556292
Training Acc of Epoch 95: 0.2241187118902439
Testing Acc of Epoch 95: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 3.1016e+01 (3.1016e+01)	Acc 0.224609 (0.224609)
Epoch: [96][300/616]	Loss 3.2732e+01 (3.1977e+01)	Acc 0.181641 (0.200435)
Epoch: [96][600/616]	Loss 3.2105e+01 (3.1953e+01)	Acc 0.197266 (0.201066)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 96: 31.957424796499858
Training Acc of Epoch 96: 0.2009670350609756
Testing Acc of Epoch 96: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.1396e+01 (3.1396e+01)	Acc 0.214844 (0.214844)
Epoch: [97][300/616]	Loss 3.1397e+01 (3.1985e+01)	Acc 0.214844 (0.200299)
Epoch: [97][600/616]	Loss 3.2256e+01 (3.1958e+01)	Acc 0.193359 (0.200907)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 97: 31.95569323097787
Training Acc of Epoch 97: 0.20095909552845528
Testing Acc of Epoch 97: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.2441e+01 (3.2441e+01)	Acc 0.188477 (0.188477)
Epoch: [98][300/616]	Loss 3.1127e+01 (3.1967e+01)	Acc 0.219727 (0.200001)
Epoch: [98][600/616]	Loss 3.1428e+01 (3.1670e+01)	Acc 0.213867 (0.200930)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 98: 31.67194083299094
Training Acc of Epoch 98: 0.20103213922764226
Testing Acc of Epoch 98: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.2121e+01 (3.2121e+01)	Acc 0.196289 (0.196289)
Epoch: [99][300/616]	Loss 3.3110e+01 (3.1927e+01)	Acc 0.170898 (0.201075)
Epoch: [99][600/616]	Loss 3.1584e+01 (3.1930e+01)	Acc 0.209961 (0.201024)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 99: 31.9299861597821
Training Acc of Epoch 99: 0.2009670350609756
Testing Acc of Epoch 99: 0.20128695652173914
Early stopping not satisfied.
