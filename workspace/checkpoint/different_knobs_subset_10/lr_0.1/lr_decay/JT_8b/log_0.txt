train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_8b
different_width False
resnet18_width 64
weight_precision 8
bias_precision 8
act_precision 11
batch_norm False
dropout False
exp_num 5
lr 0.1
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.1/lr_decay/JT_8b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_8b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=11, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=8, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.1
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0150e-01 (5.0150e-01)	Acc 0.251953 (0.251953)
Epoch: [0][300/616]	Loss 2.5925e-01 (3.1888e-01)	Acc 0.722656 (0.639068)
Epoch: [0][600/616]	Loss 2.8744e-01 (2.9450e-01)	Acc 0.689453 (0.678644)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.723804)
Training Loss of Epoch 0: 0.2937901110183902
Training Acc of Epoch 0: 0.6797557799796748
Testing Acc of Epoch 0: 0.7238043478260869
Model with the best training loss saved! The loss is 0.2937901110183902
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.6543e-01 (2.6543e-01)	Acc 0.725586 (0.725586)
Epoch: [1][300/616]	Loss 2.5682e-01 (2.6113e-01)	Acc 0.716797 (0.728327)
Epoch: [1][600/616]	Loss 2.9479e-01 (2.6636e-01)	Acc 0.687500 (0.722183)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.732670)
Training Loss of Epoch 1: 0.26656238654764686
Training Acc of Epoch 1: 0.7220020325203252
Testing Acc of Epoch 1: 0.7326695652173913
Model with the best training loss saved! The loss is 0.26656238654764686
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.7798e-01 (2.7798e-01)	Acc 0.724609 (0.724609)
Epoch: [2][300/616]	Loss 2.6706e-01 (2.5955e-01)	Acc 0.718750 (0.730663)
Epoch: [2][600/616]	Loss 4.7345e-01 (3.2907e-01)	Acc 0.322266 (0.581266)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.349359 (0.330235)
Training Loss of Epoch 2: 0.3322220490230777
Training Acc of Epoch 2: 0.5752778836382114
Testing Acc of Epoch 2: 0.33023478260869565
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 4.5652e-01 (4.5652e-01)	Acc 0.299805 (0.299805)
Epoch: [3][300/616]	Loss 3.8897e-01 (4.1440e-01)	Acc 0.427734 (0.408599)
Epoch: [3][600/616]	Loss 4.0903e-01 (4.0814e-01)	Acc 0.465820 (0.410398)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.487179 (0.476657)
Training Loss of Epoch 3: 0.40799055889369995
Training Acc of Epoch 3: 0.4115186737804878
Testing Acc of Epoch 3: 0.4766565217391304
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 4.0162e-01 (4.0162e-01)	Acc 0.473633 (0.473633)
Epoch: [4][300/616]	Loss 3.4690e-01 (3.6545e-01)	Acc 0.597656 (0.545986)
Epoch: [4][600/616]	Loss 3.3340e-01 (3.5170e-01)	Acc 0.598633 (0.571996)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.600962 (0.610135)
Training Loss of Epoch 4: 0.35135607007073194
Training Acc of Epoch 4: 0.5727880462398374
Testing Acc of Epoch 4: 0.6101347826086957
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 3.3831e-01 (3.3831e-01)	Acc 0.603516 (0.603516)
Epoch: [5][300/616]	Loss 3.4476e-01 (3.3444e-01)	Acc 0.604492 (0.608038)
Epoch: [5][600/616]	Loss 3.0066e-01 (3.3195e-01)	Acc 0.608398 (0.603951)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.628205 (0.614843)
Training Loss of Epoch 5: 0.33124216664128187
Training Acc of Epoch 5: 0.6042158917682927
Testing Acc of Epoch 5: 0.6148434782608696
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.9888e-01 (2.9888e-01)	Acc 0.607422 (0.607422)
Epoch: [6][300/616]	Loss 2.8650e-01 (3.0574e-01)	Acc 0.644531 (0.610267)
Epoch: [6][600/616]	Loss 2.8858e-01 (3.0122e-01)	Acc 0.718750 (0.645748)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.629808 (0.628983)
Training Loss of Epoch 6: 0.3012579864602748
Training Acc of Epoch 6: 0.6465050177845528
Testing Acc of Epoch 6: 0.6289826086956521
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 3.1109e-01 (3.1109e-01)	Acc 0.626953 (0.626953)
Epoch: [7][300/616]	Loss 2.9747e-01 (2.9873e-01)	Acc 0.722656 (0.665467)
Epoch: [7][600/616]	Loss 2.9666e-01 (2.9796e-01)	Acc 0.631836 (0.669376)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.610577 (0.614109)
Training Loss of Epoch 7: 0.2979413645054267
Training Acc of Epoch 7: 0.6689929496951219
Testing Acc of Epoch 7: 0.614108695652174
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 3.0590e-01 (3.0590e-01)	Acc 0.596680 (0.596680)
Epoch: [8][300/616]	Loss 3.3163e-01 (2.9770e-01)	Acc 0.694336 (0.650819)
Epoch: [8][600/616]	Loss 2.9197e-01 (2.9886e-01)	Acc 0.701172 (0.666384)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.671474 (0.672048)
Training Loss of Epoch 8: 0.2988739708574807
Training Acc of Epoch 8: 0.6672319613821138
Testing Acc of Epoch 8: 0.6720478260869566
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 3.1026e-01 (3.1026e-01)	Acc 0.670898 (0.670898)
Epoch: [9][300/616]	Loss 3.4858e-01 (3.0180e-01)	Acc 0.597656 (0.681021)
Epoch: [9][600/616]	Loss 2.8759e-01 (3.0220e-01)	Acc 0.726562 (0.684065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.671474 (0.673048)
Training Loss of Epoch 9: 0.30209791442243067
Training Acc of Epoch 9: 0.6845020325203252
Testing Acc of Epoch 9: 0.6730478260869566
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.9608e-01 (2.9608e-01)	Acc 0.664062 (0.664062)
Epoch: [10][300/616]	Loss 2.8464e-01 (2.9608e-01)	Acc 0.688477 (0.696146)
Epoch: [10][600/616]	Loss 3.6717e-01 (3.2713e-01)	Acc 0.553711 (0.622927)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.706635)
Training Loss of Epoch 10: 0.3272040006106462
Training Acc of Epoch 10: 0.6231405614837399
Testing Acc of Epoch 10: 0.7066347826086956
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.9161e-01 (2.9161e-01)	Acc 0.727539 (0.727539)
Epoch: [11][300/616]	Loss 3.3268e-01 (3.2945e-01)	Acc 0.601562 (0.568512)
Epoch: [11][600/616]	Loss 2.9793e-01 (3.1392e-01)	Acc 0.629883 (0.594829)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.621795 (0.625617)
Training Loss of Epoch 11: 0.31326267481819403
Training Acc of Epoch 11: 0.5954871697154471
Testing Acc of Epoch 11: 0.6256173913043478
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.9843e-01 (2.9843e-01)	Acc 0.593750 (0.593750)
Epoch: [12][300/616]	Loss 2.8800e-01 (2.8852e-01)	Acc 0.612305 (0.624367)
Epoch: [12][600/616]	Loss 3.5837e-01 (3.0227e-01)	Acc 0.566406 (0.608033)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201283)
Training Loss of Epoch 12: 0.6843655995721739
Training Acc of Epoch 12: 0.6012004573170732
Testing Acc of Epoch 12: 0.20128260869565218
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [13][300/616]	Loss 3.1758e+01 (3.1930e+01)	Acc 0.206055 (0.201759)
Epoch: [13][600/616]	Loss 3.2070e+01 (3.1961e+01)	Acc 0.198242 (0.200979)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 13: 31.961572662601625
Training Acc of Epoch 13: 0.20096068343495935
Testing Acc of Epoch 13: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [14][300/616]	Loss 3.2031e+01 (3.1946e+01)	Acc 0.199219 (0.201347)
Epoch: [14][600/616]	Loss 3.1484e+01 (3.1958e+01)	Acc 0.212891 (0.201042)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 14: 31.961890243902438
Training Acc of Epoch 14: 0.20095274390243903
Testing Acc of Epoch 14: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 3.2109e+01 (3.2109e+01)	Acc 0.197266 (0.197266)
Epoch: [15][300/616]	Loss 3.2461e+01 (3.1953e+01)	Acc 0.188477 (0.201165)
Epoch: [15][600/616]	Loss 3.1953e+01 (3.1960e+01)	Acc 0.201172 (0.200998)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 15: 31.9609375
Training Acc of Epoch 15: 0.2009765625
Testing Acc of Epoch 15: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 3.2773e+01 (3.2773e+01)	Acc 0.180664 (0.180664)
Epoch: [16][300/616]	Loss 3.1641e+01 (3.1973e+01)	Acc 0.208984 (0.200666)
Epoch: [16][600/616]	Loss 3.2383e+01 (3.1959e+01)	Acc 0.190430 (0.201016)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 16: 31.96119156504065
Training Acc of Epoch 16: 0.20097021087398373
Testing Acc of Epoch 16: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 3.3008e+01 (3.3008e+01)	Acc 0.174805 (0.174805)
Epoch: [17][300/616]	Loss 3.1094e+01 (3.1975e+01)	Acc 0.222656 (0.200630)
Epoch: [17][600/616]	Loss 3.2070e+01 (3.1964e+01)	Acc 0.198242 (0.200902)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 17: 31.9614456300813
Training Acc of Epoch 17: 0.20096385924796747
Testing Acc of Epoch 17: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 3.1641e+01 (3.1641e+01)	Acc 0.208984 (0.208984)
Epoch: [18][300/616]	Loss 3.2148e+01 (3.1952e+01)	Acc 0.196289 (0.201208)
Epoch: [18][600/616]	Loss 3.2344e+01 (3.1964e+01)	Acc 0.191406 (0.200909)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 18: 31.9609375
Training Acc of Epoch 18: 0.2009765625
Testing Acc of Epoch 18: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 3.3008e+01 (3.3008e+01)	Acc 0.174805 (0.174805)
Epoch: [19][300/616]	Loss 3.1992e+01 (3.1953e+01)	Acc 0.200195 (0.201175)
Epoch: [19][600/616]	Loss 3.2656e+01 (3.1959e+01)	Acc 0.183594 (0.201014)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 19: 31.96119156504065
Training Acc of Epoch 19: 0.20097021087398373
Testing Acc of Epoch 19: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 3.2812e+01 (3.2812e+01)	Acc 0.179688 (0.179688)
Epoch: [20][300/616]	Loss 3.1602e+01 (3.1995e+01)	Acc 0.209961 (0.200127)
Epoch: [20][600/616]	Loss 3.1758e+01 (3.1965e+01)	Acc 0.206055 (0.200883)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 20: 31.962080808189825
Training Acc of Epoch 20: 0.20094798018292684
Testing Acc of Epoch 20: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 3.1953e+01 (3.1953e+01)	Acc 0.201172 (0.201172)
Epoch: [21][300/616]	Loss 3.2422e+01 (3.1949e+01)	Acc 0.189453 (0.201263)
Epoch: [21][600/616]	Loss 3.1758e+01 (3.1964e+01)	Acc 0.206055 (0.200892)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 21: 31.961509164949742
Training Acc of Epoch 21: 0.2009622713414634
Testing Acc of Epoch 21: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [22][300/616]	Loss 3.1055e+01 (3.1990e+01)	Acc 0.223633 (0.200244)
Epoch: [22][600/616]	Loss 3.1133e+01 (3.1958e+01)	Acc 0.221680 (0.201044)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 22: 31.961382113821138
Training Acc of Epoch 22: 0.20096544715447154
Testing Acc of Epoch 22: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 3.1719e+01 (3.1719e+01)	Acc 0.207031 (0.207031)
Epoch: [23][300/616]	Loss 3.2344e+01 (3.1976e+01)	Acc 0.191406 (0.200604)
Epoch: [23][600/616]	Loss 3.1953e+01 (3.1961e+01)	Acc 0.201172 (0.200983)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 23: 31.960920675014094
Training Acc of Epoch 23: 0.2009765625
Testing Acc of Epoch 23: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 3.2266e+01 (3.2266e+01)	Acc 0.193359 (0.193359)
Epoch: [24][300/616]	Loss 3.1367e+01 (3.1934e+01)	Acc 0.215820 (0.201652)
Epoch: [24][600/616]	Loss 3.1797e+01 (3.1965e+01)	Acc 0.205078 (0.200883)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 24: 31.961554457501666
Training Acc of Epoch 24: 0.20096068343495935
Testing Acc of Epoch 24: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 3.2656e+01 (3.2656e+01)	Acc 0.183594 (0.183594)
Epoch: [25][300/616]	Loss 3.1992e+01 (3.1983e+01)	Acc 0.200195 (0.200422)
Epoch: [25][600/616]	Loss 3.1875e+01 (3.1962e+01)	Acc 0.203125 (0.200941)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 25: 31.961488959459754
Training Acc of Epoch 25: 0.2009622713414634
Testing Acc of Epoch 25: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 3.1914e+01 (3.1914e+01)	Acc 0.202148 (0.202148)
Epoch: [26][300/616]	Loss 3.1406e+01 (3.1941e+01)	Acc 0.214844 (0.201467)
Epoch: [26][600/616]	Loss 3.1602e+01 (3.1962e+01)	Acc 0.209961 (0.200938)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 26: 31.961315803217694
Training Acc of Epoch 26: 0.20096544715447154
Testing Acc of Epoch 26: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 3.2148e+01 (3.2148e+01)	Acc 0.196289 (0.196289)
Epoch: [27][300/616]	Loss 3.1953e+01 (3.1922e+01)	Acc 0.201172 (0.201947)
Epoch: [27][600/616]	Loss 3.1875e+01 (3.1958e+01)	Acc 0.203125 (0.201042)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 27: 31.961754459288063
Training Acc of Epoch 27: 0.2009543318089431
Testing Acc of Epoch 27: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [28][300/616]	Loss 3.1992e+01 (3.1922e+01)	Acc 0.200195 (0.201934)
Epoch: [28][600/616]	Loss 3.1406e+01 (3.1963e+01)	Acc 0.214844 (0.200927)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 28: 31.960855319635655
Training Acc of Epoch 28: 0.20097338668699186
Testing Acc of Epoch 28: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 3.1758e+01 (3.1758e+01)	Acc 0.206055 (0.206055)
Epoch: [29][300/616]	Loss 3.2578e+01 (3.1995e+01)	Acc 0.185547 (0.200101)
Epoch: [29][600/616]	Loss 3.1523e+01 (3.1958e+01)	Acc 0.211914 (0.201022)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201339)
Training Loss of Epoch 29: 31.959556871119553
Training Acc of Epoch 29: 0.20098291412601627
Testing Acc of Epoch 29: 0.2013391304347826
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 3.1484e+01 (3.1484e+01)	Acc 0.212891 (0.212891)
Epoch: [30][300/616]	Loss 3.2695e+01 (3.2001e+01)	Acc 0.182617 (0.199962)
Epoch: [30][600/616]	Loss 3.2305e+01 (3.1964e+01)	Acc 0.192383 (0.200853)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201291)
Training Loss of Epoch 30: 31.959730594914134
Training Acc of Epoch 30: 0.2009717987804878
Testing Acc of Epoch 30: 0.20129130434782608
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 3.1367e+01 (3.1367e+01)	Acc 0.215820 (0.215820)
Epoch: [31][300/616]	Loss 3.2422e+01 (3.1945e+01)	Acc 0.189453 (0.201302)
Epoch: [31][600/616]	Loss 3.1413e+01 (3.1955e+01)	Acc 0.213867 (0.201078)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201291)
Training Loss of Epoch 31: 31.959225265378876
Training Acc of Epoch 31: 0.20094798018292684
Testing Acc of Epoch 31: 0.20129130434782608
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 3.2006e+01 (3.2006e+01)	Acc 0.199219 (0.199219)
Epoch: [32][300/616]	Loss 3.2370e+01 (3.1887e+01)	Acc 0.187500 (0.200951)
Epoch: [32][600/616]	Loss 3.2017e+01 (3.1825e+01)	Acc 0.193359 (0.201089)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 32: 31.82741954152177
Training Acc of Epoch 32: 0.20096068343495935
Testing Acc of Epoch 32: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 3.2025e+01 (3.2025e+01)	Acc 0.193359 (0.193359)
Epoch: [33][300/616]	Loss 3.1794e+01 (3.1813e+01)	Acc 0.205078 (0.200815)
Epoch: [33][600/616]	Loss 3.3174e+01 (3.1823e+01)	Acc 0.168945 (0.201073)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 33: 31.828308502445378
Training Acc of Epoch 33: 0.20096385924796747
Testing Acc of Epoch 33: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 3.1604e+01 (3.1604e+01)	Acc 0.207031 (0.207031)
Epoch: [34][300/616]	Loss 3.1406e+01 (3.1958e+01)	Acc 0.214844 (0.200536)
Epoch: [34][600/616]	Loss 3.2305e+01 (3.1954e+01)	Acc 0.192383 (0.200886)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 34: 31.950281738653416
Training Acc of Epoch 34: 0.2009765625
Testing Acc of Epoch 34: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 3.2050e+01 (3.2050e+01)	Acc 0.198242 (0.198242)
Epoch: [35][300/616]	Loss 3.1050e+01 (3.1955e+01)	Acc 0.223633 (0.201104)
Epoch: [35][600/616]	Loss 3.2031e+01 (3.1957e+01)	Acc 0.199219 (0.200975)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 35: 31.957760948863456
Training Acc of Epoch 35: 0.20097021087398373
Testing Acc of Epoch 35: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 3.2188e+01 (3.2188e+01)	Acc 0.195312 (0.195312)
Epoch: [36][300/616]	Loss 3.1367e+01 (3.1968e+01)	Acc 0.215820 (0.200724)
Epoch: [36][600/616]	Loss 3.1670e+01 (3.1957e+01)	Acc 0.208008 (0.201011)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 36: 31.95961237961684
Training Acc of Epoch 36: 0.20095591971544716
Testing Acc of Epoch 36: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 3.1754e+01 (3.1754e+01)	Acc 0.206055 (0.206055)
Epoch: [37][300/616]	Loss 3.1432e+01 (3.1986e+01)	Acc 0.213867 (0.200250)
Epoch: [37][600/616]	Loss 3.2500e+01 (3.1944e+01)	Acc 0.187500 (0.200774)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 37: 31.954509725803284
Training Acc of Epoch 37: 0.20052559705284553
Testing Acc of Epoch 37: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [38][300/616]	Loss 3.3047e+01 (3.2224e+01)	Acc 0.173828 (0.194394)
Epoch: [38][600/616]	Loss 3.1680e+01 (3.2246e+01)	Acc 0.208008 (0.193852)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 38: 32.2474593495935
Training Acc of Epoch 38: 0.1938135162601626
Testing Acc of Epoch 38: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [39][300/616]	Loss 3.2031e+01 (3.2256e+01)	Acc 0.199219 (0.193612)
Epoch: [39][600/616]	Loss 3.3086e+01 (3.2247e+01)	Acc 0.172852 (0.193831)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 39: 32.24771341463415
Training Acc of Epoch 39: 0.19380716463414635
Testing Acc of Epoch 39: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 3.1523e+01 (3.1523e+01)	Acc 0.211914 (0.211914)
Epoch: [40][300/616]	Loss 3.2344e+01 (3.2243e+01)	Acc 0.191406 (0.193934)
Epoch: [40][600/616]	Loss 3.2734e+01 (3.2244e+01)	Acc 0.181641 (0.193891)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 40: 32.247395833333336
Training Acc of Epoch 40: 0.19381510416666667
Testing Acc of Epoch 40: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [41][300/616]	Loss 3.1328e+01 (3.2253e+01)	Acc 0.216797 (0.193684)
Epoch: [41][600/616]	Loss 3.1641e+01 (3.2248e+01)	Acc 0.208984 (0.193806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 41: 32.24777693089431
Training Acc of Epoch 41: 0.1938055767276423
Testing Acc of Epoch 41: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 3.2891e+01 (3.2891e+01)	Acc 0.177734 (0.177734)
Epoch: [42][300/616]	Loss 3.3008e+01 (3.2259e+01)	Acc 0.174805 (0.193535)
Epoch: [42][600/616]	Loss 3.1758e+01 (3.2247e+01)	Acc 0.206055 (0.193827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 42: 32.24752286585366
Training Acc of Epoch 42: 0.19381192835365854
Testing Acc of Epoch 42: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [43][300/616]	Loss 3.2383e+01 (3.2314e+01)	Acc 0.190430 (0.192139)
Epoch: [43][600/616]	Loss 3.2383e+01 (3.2249e+01)	Acc 0.190430 (0.193764)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 43: 32.24726880081301
Training Acc of Epoch 43: 0.1938182799796748
Testing Acc of Epoch 43: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 3.1289e+01 (3.1289e+01)	Acc 0.217773 (0.217773)
Epoch: [44][300/616]	Loss 3.2656e+01 (3.2235e+01)	Acc 0.183594 (0.194115)
Epoch: [44][600/616]	Loss 3.2617e+01 (3.2249e+01)	Acc 0.184570 (0.193770)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 44: 32.24777693089431
Training Acc of Epoch 44: 0.1938055767276423
Testing Acc of Epoch 44: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [45][300/616]	Loss 3.1719e+01 (3.2193e+01)	Acc 0.207031 (0.195186)
Epoch: [45][600/616]	Loss 3.1445e+01 (3.2242e+01)	Acc 0.213867 (0.193948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 45: 32.24784044715447
Training Acc of Epoch 45: 0.19380398882113822
Testing Acc of Epoch 45: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 3.2422e+01 (3.2422e+01)	Acc 0.189453 (0.189453)
Epoch: [46][300/616]	Loss 3.1914e+01 (3.2250e+01)	Acc 0.202148 (0.193762)
Epoch: [46][600/616]	Loss 3.2305e+01 (3.2248e+01)	Acc 0.192383 (0.193798)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 46: 32.24745845019333
Training Acc of Epoch 46: 0.1938135162601626
Testing Acc of Epoch 46: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [47][300/616]	Loss 3.1367e+01 (3.2226e+01)	Acc 0.215820 (0.194362)
Epoch: [47][600/616]	Loss 3.1719e+01 (3.2240e+01)	Acc 0.207031 (0.194001)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 47: 32.2475648244222
Training Acc of Epoch 47: 0.19381034044715448
Testing Acc of Epoch 47: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 3.2305e+01 (3.2305e+01)	Acc 0.192383 (0.192383)
Epoch: [48][300/616]	Loss 3.2344e+01 (3.2244e+01)	Acc 0.191406 (0.193895)
Epoch: [48][600/616]	Loss 3.1914e+01 (3.2253e+01)	Acc 0.202148 (0.193670)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 48: 32.24726980876147
Training Acc of Epoch 48: 0.1938135162601626
Testing Acc of Epoch 48: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 3.1836e+01 (3.1836e+01)	Acc 0.204102 (0.204102)
Epoch: [49][300/616]	Loss 3.1950e+01 (3.2276e+01)	Acc 0.201172 (0.193067)
Epoch: [49][600/616]	Loss 3.2500e+01 (3.2239e+01)	Acc 0.187500 (0.193923)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 49: 32.232575318871476
Training Acc of Epoch 49: 0.19399453760162602
Testing Acc of Epoch 49: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 3.1992e+01 (3.1992e+01)	Acc 0.200195 (0.200195)
Epoch: [50][300/616]	Loss 3.1250e+01 (3.1970e+01)	Acc 0.218750 (0.200757)
Epoch: [50][600/616]	Loss 3.1328e+01 (3.1957e+01)	Acc 0.216797 (0.201074)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 50: 31.96107541836374
Training Acc of Epoch 50: 0.20096862296747967
Testing Acc of Epoch 50: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 3.2500e+01 (3.2500e+01)	Acc 0.187500 (0.187500)
Epoch: [51][300/616]	Loss 3.2070e+01 (3.2002e+01)	Acc 0.198242 (0.199949)
Epoch: [51][600/616]	Loss 3.1797e+01 (3.1962e+01)	Acc 0.205078 (0.200943)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 51: 31.961072245652115
Training Acc of Epoch 51: 0.20096862296747967
Testing Acc of Epoch 51: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 3.1367e+01 (3.1367e+01)	Acc 0.215820 (0.215820)
Epoch: [52][300/616]	Loss 3.1719e+01 (3.2016e+01)	Acc 0.207031 (0.199595)
Epoch: [52][600/616]	Loss 3.1797e+01 (3.1960e+01)	Acc 0.205078 (0.200987)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 52: 31.96101095308133
Training Acc of Epoch 52: 0.20096862296747967
Testing Acc of Epoch 52: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 3.0898e+01 (3.0898e+01)	Acc 0.227539 (0.227539)
Epoch: [53][300/616]	Loss 3.1914e+01 (3.1958e+01)	Acc 0.202148 (0.201039)
Epoch: [53][600/616]	Loss 2.3631e+01 (3.1799e+01)	Acc 0.260742 (0.201731)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.288462 (0.288709)
Training Loss of Epoch 53: 31.571821544616203
Training Acc of Epoch 53: 0.20331872459349593
Testing Acc of Epoch 53: 0.2887086956521739
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.1559e+01 (2.1559e+01)	Acc 0.260742 (0.260742)
Epoch: [54][300/616]	Loss 1.3825e+01 (1.4954e+01)	Acc 0.303711 (0.302991)
Epoch: [54][600/616]	Loss 1.9893e+01 (1.6447e+01)	Acc 0.285156 (0.296015)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.283654 (0.292361)
Training Loss of Epoch 54: 16.52193911017441
Training Acc of Epoch 54: 0.2959905360772358
Testing Acc of Epoch 54: 0.2923608695652174
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 1.9736e+01 (1.9736e+01)	Acc 0.293945 (0.293945)
Epoch: [55][300/616]	Loss 1.9183e+01 (1.9455e+01)	Acc 0.313477 (0.298744)
Epoch: [55][600/616]	Loss 2.0365e+01 (2.0089e+01)	Acc 0.308594 (0.296379)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.282051 (0.279522)
Training Loss of Epoch 55: 20.134176939677417
Training Acc of Epoch 55: 0.2960715193089431
Testing Acc of Epoch 55: 0.27952173913043477
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.1571e+01 (2.1571e+01)	Acc 0.281250 (0.281250)
Epoch: [56][300/616]	Loss 2.2759e+01 (2.2679e+01)	Acc 0.311523 (0.272360)
Epoch: [56][600/616]	Loss 2.5823e+01 (2.3323e+01)	Acc 0.257812 (0.268290)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.254808 (0.248300)
Training Loss of Epoch 56: 23.346265321436935
Training Acc of Epoch 56: 0.2681529471544715
Testing Acc of Epoch 56: 0.2483
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5522e+01 (2.5522e+01)	Acc 0.228516 (0.228516)
Epoch: [57][300/616]	Loss 2.5189e+01 (2.5256e+01)	Acc 0.285156 (0.253449)
Epoch: [57][600/616]	Loss 2.7555e+01 (2.6005e+01)	Acc 0.245117 (0.246318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.232372 (0.238409)
Training Loss of Epoch 57: 26.034307532581856
Training Acc of Epoch 57: 0.2460619918699187
Testing Acc of Epoch 57: 0.2384086956521739
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.7438e+01 (2.7438e+01)	Acc 0.244141 (0.244141)
Epoch: [58][300/616]	Loss 2.7978e+01 (2.8079e+01)	Acc 0.229492 (0.226024)
Epoch: [58][600/616]	Loss 2.9066e+01 (2.8509e+01)	Acc 0.207031 (0.222388)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.217296)
Training Loss of Epoch 58: 28.523020587704046
Training Acc of Epoch 58: 0.22232120172764228
Testing Acc of Epoch 58: 0.21729565217391306
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.9411e+01 (2.9411e+01)	Acc 0.215820 (0.215820)
Epoch: [59][300/616]	Loss 2.9330e+01 (2.9473e+01)	Acc 0.217773 (0.214824)
Epoch: [59][600/616]	Loss 2.9949e+01 (2.9611e+01)	Acc 0.214844 (0.214130)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.221154 (0.211817)
Training Loss of Epoch 59: 29.6207883199056
Training Acc of Epoch 59: 0.2140529725609756
Testing Acc of Epoch 59: 0.21181739130434782
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.9481e+01 (2.9481e+01)	Acc 0.219727 (0.219727)
Epoch: [60][300/616]	Loss 2.9597e+01 (2.9992e+01)	Acc 0.215820 (0.212083)
Epoch: [60][600/616]	Loss 2.9008e+01 (3.0110e+01)	Acc 0.233398 (0.211102)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.222756 (0.213835)
Training Loss of Epoch 60: 30.112022030838137
Training Acc of Epoch 60: 0.2111153455284553
Testing Acc of Epoch 60: 0.21383478260869565
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 3.0137e+01 (3.0137e+01)	Acc 0.220703 (0.220703)
Epoch: [61][300/616]	Loss 3.0452e+01 (3.0278e+01)	Acc 0.213867 (0.211158)
Epoch: [61][600/616]	Loss 3.0217e+01 (3.0256e+01)	Acc 0.210938 (0.211753)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.224359 (0.215691)
Training Loss of Epoch 61: 30.256169503684937
Training Acc of Epoch 61: 0.2116917555894309
Testing Acc of Epoch 61: 0.21569130434782607
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.9515e+01 (2.9515e+01)	Acc 0.237305 (0.237305)
Epoch: [62][300/616]	Loss 2.9204e+01 (3.0120e+01)	Acc 0.235352 (0.214049)
Epoch: [62][600/616]	Loss 2.9525e+01 (2.9896e+01)	Acc 0.223633 (0.215214)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.216346 (0.211378)
Training Loss of Epoch 62: 29.887280661109987
Training Acc of Epoch 62: 0.21517244664634147
Testing Acc of Epoch 62: 0.2113782608695652
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 3.0412e+01 (3.0412e+01)	Acc 0.188477 (0.188477)
Epoch: [63][300/616]	Loss 2.9999e+01 (2.9423e+01)	Acc 0.192383 (0.216719)
Epoch: [63][600/616]	Loss 2.9207e+01 (2.9268e+01)	Acc 0.211914 (0.218115)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.212313)
Training Loss of Epoch 63: 29.25520310750822
Training Acc of Epoch 63: 0.2181053099593496
Testing Acc of Epoch 63: 0.21231304347826088
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.8999e+01 (2.8999e+01)	Acc 0.203125 (0.203125)
Epoch: [64][300/616]	Loss 2.7552e+01 (2.8206e+01)	Acc 0.224609 (0.220749)
Epoch: [64][600/616]	Loss 3.1953e+01 (2.8646e+01)	Acc 0.201172 (0.217598)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.201287)
Training Loss of Epoch 64: 28.71992270306843
Training Acc of Epoch 64: 0.21725736788617886
Testing Acc of Epoch 64: 0.20128695652173914
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 3.1875e+01 (3.1875e+01)	Acc 0.203125 (0.203125)
Epoch: [65][300/616]	Loss 3.2266e+01 (3.1939e+01)	Acc 0.193359 (0.201289)
Epoch: [65][600/616]	Loss 3.2696e+01 (3.1957e+01)	Acc 0.180664 (0.200925)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 65: 31.950647177347324
Training Acc of Epoch 65: 0.2007764862804878
Testing Acc of Epoch 65: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 3.2812e+01 (3.2812e+01)	Acc 0.179688 (0.179688)
Epoch: [66][300/616]	Loss 3.2148e+01 (3.1061e+01)	Acc 0.196289 (0.199443)
Epoch: [66][600/616]	Loss 3.2578e+01 (3.1652e+01)	Acc 0.185547 (0.196664)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 66: 31.666685851802672
Training Acc of Epoch 66: 0.19657647357723576
Testing Acc of Epoch 66: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 3.2070e+01 (3.2070e+01)	Acc 0.198242 (0.198242)
Epoch: [67][300/616]	Loss 3.1797e+01 (3.2243e+01)	Acc 0.205078 (0.193930)
Epoch: [67][600/616]	Loss 3.2031e+01 (3.2244e+01)	Acc 0.199219 (0.193899)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 67: 32.247395833333336
Training Acc of Epoch 67: 0.19381510416666667
Testing Acc of Epoch 67: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 3.2773e+01 (3.2773e+01)	Acc 0.180664 (0.180664)
Epoch: [68][300/616]	Loss 3.2695e+01 (3.2238e+01)	Acc 0.182617 (0.194041)
Epoch: [68][600/616]	Loss 3.2188e+01 (3.2245e+01)	Acc 0.195312 (0.193881)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 68: 32.24766631087637
Training Acc of Epoch 68: 0.19380240091463416
Testing Acc of Epoch 68: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 3.2227e+01 (3.2227e+01)	Acc 0.194336 (0.194336)
Epoch: [69][300/616]	Loss 3.1484e+01 (3.2263e+01)	Acc 0.212891 (0.193259)
Epoch: [69][600/616]	Loss 3.2617e+01 (3.2245e+01)	Acc 0.184570 (0.193796)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 69: 32.24412642688286
Training Acc of Epoch 69: 0.19381510416666667
Testing Acc of Epoch 69: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 3.2734e+01 (3.2734e+01)	Acc 0.181641 (0.181641)
Epoch: [70][300/616]	Loss 3.1680e+01 (3.2242e+01)	Acc 0.208008 (0.193911)
Epoch: [70][600/616]	Loss 3.1445e+01 (3.2251e+01)	Acc 0.213867 (0.193688)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.192308 (0.194648)
Training Loss of Epoch 70: 32.247102725021236
Training Acc of Epoch 70: 0.19378969766260162
Testing Acc of Epoch 70: 0.19464782608695652
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 3.3125e+01 (3.3125e+01)	Acc 0.171875 (0.171875)
Epoch: [71][300/616]	Loss 3.2539e+01 (3.2239e+01)	Acc 0.186523 (0.194002)
Epoch: [71][600/616]	Loss 2.4858e+01 (3.2073e+01)	Acc 0.213867 (0.194367)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.240385 (0.235270)
Training Loss of Epoch 71: 31.905581934859114
Training Acc of Epoch 71: 0.19619537601626016
Testing Acc of Epoch 71: 0.2352695652173913
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4557e+01 (2.4557e+01)	Acc 0.215820 (0.215820)
Epoch: [72][300/616]	Loss 2.3953e+01 (2.4448e+01)	Acc 0.320312 (0.251794)
Epoch: [72][600/616]	Loss 2.5276e+01 (2.4698e+01)	Acc 0.218750 (0.251940)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.229167 (0.232461)
Training Loss of Epoch 72: 24.69543143791881
Training Acc of Epoch 72: 0.25239456300813007
Testing Acc of Epoch 72: 0.2324608695652174
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4929e+01 (2.4929e+01)	Acc 0.215820 (0.215820)
Epoch: [73][300/616]	Loss 2.4307e+01 (2.5191e+01)	Acc 0.247070 (0.253877)
Epoch: [73][600/616]	Loss 2.5173e+01 (2.4847e+01)	Acc 0.198242 (0.253095)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.227564 (0.231130)
Training Loss of Epoch 73: 24.824863483459968
Training Acc of Epoch 73: 0.2534155868902439
Testing Acc of Epoch 73: 0.2311304347826087
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3704e+01 (2.3704e+01)	Acc 0.212891 (0.212891)
Epoch: [74][300/616]	Loss 2.4631e+01 (2.4352e+01)	Acc 0.317383 (0.268412)
Epoch: [74][600/616]	Loss 2.5087e+01 (2.4635e+01)	Acc 0.229492 (0.266056)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.290064 (0.269804)
Training Loss of Epoch 74: 24.63598779507769
Training Acc of Epoch 74: 0.2658822408536585
Testing Acc of Epoch 74: 0.269804347826087
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4555e+01 (2.4555e+01)	Acc 0.317383 (0.317383)
Epoch: [75][300/616]	Loss 2.5434e+01 (2.4487e+01)	Acc 0.220703 (0.262556)
Epoch: [75][600/616]	Loss 2.4366e+01 (2.4512e+01)	Acc 0.319336 (0.263800)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.269213)
Training Loss of Epoch 75: 24.516387046255716
Training Acc of Epoch 75: 0.2635114964430894
Testing Acc of Epoch 75: 0.26921304347826086
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.4603e+01 (2.4603e+01)	Acc 0.311523 (0.311523)
Epoch: [76][300/616]	Loss 2.4074e+01 (2.4490e+01)	Acc 0.241211 (0.264483)
Epoch: [76][600/616]	Loss 2.4091e+01 (2.4512e+01)	Acc 0.329102 (0.263870)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.237165)
Training Loss of Epoch 76: 24.515454470626707
Training Acc of Epoch 76: 0.264356262703252
Testing Acc of Epoch 76: 0.23716521739130436
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4429e+01 (2.4429e+01)	Acc 0.318359 (0.318359)
Epoch: [77][300/616]	Loss 2.4689e+01 (2.4545e+01)	Acc 0.302734 (0.262961)
Epoch: [77][600/616]	Loss 2.5290e+01 (2.4510e+01)	Acc 0.205078 (0.263394)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.293269 (0.272830)
Training Loss of Epoch 77: 24.512303995892285
Training Acc of Epoch 77: 0.2638528963414634
Testing Acc of Epoch 77: 0.2728304347826087
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4633e+01 (2.4633e+01)	Acc 0.307617 (0.307617)
Epoch: [78][300/616]	Loss 2.4437e+01 (2.4418e+01)	Acc 0.229492 (0.262546)
Epoch: [78][600/616]	Loss 2.4420e+01 (2.4374e+01)	Acc 0.299805 (0.262991)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.278657)
Training Loss of Epoch 78: 24.375111048008367
Training Acc of Epoch 78: 0.2639672256097561
Testing Acc of Epoch 78: 0.2786565217391304
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3269e+01 (2.3269e+01)	Acc 0.310547 (0.310547)
Epoch: [79][300/616]	Loss 2.4202e+01 (2.4240e+01)	Acc 0.224609 (0.263260)
Epoch: [79][600/616]	Loss 2.3975e+01 (2.4268e+01)	Acc 0.236328 (0.263212)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.273117)
Training Loss of Epoch 79: 24.2721963618829
Training Acc of Epoch 79: 0.2636512322154472
Testing Acc of Epoch 79: 0.2731173913043478
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.5015e+01 (2.5015e+01)	Acc 0.305664 (0.305664)
Epoch: [80][300/616]	Loss 2.4315e+01 (2.4306e+01)	Acc 0.237305 (0.262971)
Epoch: [80][600/616]	Loss 2.3896e+01 (2.4206e+01)	Acc 0.231445 (0.264239)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.293269 (0.278187)
Training Loss of Epoch 80: 24.193922982564786
Training Acc of Epoch 80: 0.2637083968495935
Testing Acc of Epoch 80: 0.27818695652173914
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.4609e+01 (2.4609e+01)	Acc 0.308594 (0.308594)
Epoch: [81][300/616]	Loss 2.3653e+01 (2.4076e+01)	Acc 0.236328 (0.265404)
Epoch: [81][600/616]	Loss 2.4169e+01 (2.4042e+01)	Acc 0.220703 (0.261779)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.291667 (0.281161)
Training Loss of Epoch 81: 24.045179463208207
Training Acc of Epoch 81: 0.261596481199187
Testing Acc of Epoch 81: 0.2811608695652174
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3523e+01 (2.3523e+01)	Acc 0.313477 (0.313477)
Epoch: [82][300/616]	Loss 2.3737e+01 (2.4158e+01)	Acc 0.327148 (0.259003)
Epoch: [82][600/616]	Loss 2.5132e+01 (2.4318e+01)	Acc 0.299805 (0.266210)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.229167 (0.271409)
Training Loss of Epoch 82: 24.32662348553417
Training Acc of Epoch 82: 0.26550908282520325
Testing Acc of Epoch 82: 0.2714086956521739
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4266e+01 (2.4266e+01)	Acc 0.207031 (0.207031)
Epoch: [83][300/616]	Loss 2.4485e+01 (2.4676e+01)	Acc 0.334961 (0.276601)
Epoch: [83][600/616]	Loss 2.4880e+01 (2.4838e+01)	Acc 0.320312 (0.275149)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.227564 (0.254422)
Training Loss of Epoch 83: 24.848026219809928
Training Acc of Epoch 83: 0.2749364837398374
Testing Acc of Epoch 83: 0.2544217391304348
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.4213e+01 (2.4213e+01)	Acc 0.265625 (0.265625)
Epoch: [84][300/616]	Loss 2.6280e+01 (2.5360e+01)	Acc 0.290039 (0.279183)
Epoch: [84][600/616]	Loss 2.5219e+01 (2.5556e+01)	Acc 0.322266 (0.275452)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.224359 (0.269117)
Training Loss of Epoch 84: 25.55803644443915
Training Acc of Epoch 84: 0.27464907266260163
Testing Acc of Epoch 84: 0.2691173913043478
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4826e+01 (2.4826e+01)	Acc 0.242188 (0.242188)
Epoch: [85][300/616]	Loss 2.7322e+01 (2.5981e+01)	Acc 0.273438 (0.271137)
Epoch: [85][600/616]	Loss 2.6920e+01 (2.6028e+01)	Acc 0.292969 (0.270696)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.218939)
Training Loss of Epoch 85: 26.030418042438786
Training Acc of Epoch 85: 0.2710873983739837
Testing Acc of Epoch 85: 0.2189391304347826
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.6344e+01 (2.6344e+01)	Acc 0.215820 (0.215820)
Epoch: [86][300/616]	Loss 2.6066e+01 (2.6481e+01)	Acc 0.310547 (0.269038)
Epoch: [86][600/616]	Loss 2.6264e+01 (2.6635e+01)	Acc 0.303711 (0.268594)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.219551 (0.277139)
Training Loss of Epoch 86: 26.645219644686073
Training Acc of Epoch 86: 0.2684784679878049
Testing Acc of Epoch 86: 0.2771391304347826
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.7273e+01 (2.7273e+01)	Acc 0.209961 (0.209961)
Epoch: [87][300/616]	Loss 2.7808e+01 (2.7290e+01)	Acc 0.278320 (0.270430)
Epoch: [87][600/616]	Loss 2.8422e+01 (2.7642e+01)	Acc 0.273438 (0.269718)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.201923 (0.227330)
Training Loss of Epoch 87: 27.661947436449005
Training Acc of Epoch 87: 0.26981072154471547
Testing Acc of Epoch 87: 0.2273304347826087
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.9558e+01 (2.9558e+01)	Acc 0.187500 (0.187500)
Epoch: [88][300/616]	Loss 2.7792e+01 (2.8618e+01)	Acc 0.292969 (0.267270)
Epoch: [88][600/616]	Loss 2.8683e+01 (2.8774e+01)	Acc 0.276367 (0.266572)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.248397 (0.258943)
Training Loss of Epoch 88: 28.783665906704538
Training Acc of Epoch 88: 0.2664888211382114
Testing Acc of Epoch 88: 0.25894347826086955
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 3.0404e+01 (3.0404e+01)	Acc 0.231445 (0.231445)
Epoch: [89][300/616]	Loss 2.8915e+01 (2.9576e+01)	Acc 0.273438 (0.254850)
Epoch: [89][600/616]	Loss 3.0321e+01 (2.9423e+01)	Acc 0.238281 (0.258337)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.250000 (0.263148)
Training Loss of Epoch 89: 29.420142628119244
Training Acc of Epoch 89: 0.2584127286585366
Testing Acc of Epoch 89: 0.2631478260869565
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.9155e+01 (2.9155e+01)	Acc 0.262695 (0.262695)
Epoch: [90][300/616]	Loss 2.9618e+01 (2.9496e+01)	Acc 0.254883 (0.257352)
Epoch: [90][600/616]	Loss 2.9420e+01 (2.9639e+01)	Acc 0.262695 (0.254351)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.243590 (0.251609)
Training Loss of Epoch 90: 29.646446172202506
Training Acc of Epoch 90: 0.25418572154471547
Testing Acc of Epoch 90: 0.2516086956521739
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.9538e+01 (2.9538e+01)	Acc 0.257812 (0.257812)
Epoch: [91][300/616]	Loss 3.0319e+01 (2.9704e+01)	Acc 0.239258 (0.253034)
Epoch: [91][600/616]	Loss 2.8214e+01 (2.9669e+01)	Acc 0.290039 (0.253692)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.248397 (0.255043)
Training Loss of Epoch 91: 29.66669717494065
Training Acc of Epoch 91: 0.25373475609756097
Testing Acc of Epoch 91: 0.25504347826086954
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.9229e+01 (2.9229e+01)	Acc 0.263672 (0.263672)
Epoch: [92][300/616]	Loss 2.9698e+01 (2.9579e+01)	Acc 0.250977 (0.255610)
Epoch: [92][600/616]	Loss 2.8693e+01 (2.9533e+01)	Acc 0.273438 (0.256443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.250000 (0.258130)
Training Loss of Epoch 92: 29.531713473312255
Training Acc of Epoch 92: 0.2564453125
Testing Acc of Epoch 92: 0.2581304347826087
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.8920e+01 (2.8920e+01)	Acc 0.269531 (0.269531)
Epoch: [93][300/616]	Loss 2.9234e+01 (2.9421e+01)	Acc 0.258789 (0.258604)
Epoch: [93][600/616]	Loss 2.8763e+01 (2.9397e+01)	Acc 0.274414 (0.258857)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.250000 (0.259787)
Training Loss of Epoch 93: 29.39871547357823
Training Acc of Epoch 93: 0.258814469004065
Testing Acc of Epoch 93: 0.2597869565217391
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.9502e+01 (2.9502e+01)	Acc 0.257812 (0.257812)
Epoch: [94][300/616]	Loss 2.9703e+01 (2.9349e+01)	Acc 0.248047 (0.259367)
Epoch: [94][600/616]	Loss 3.0053e+01 (2.9329e+01)	Acc 0.239258 (0.259564)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.251603 (0.259278)
Training Loss of Epoch 94: 29.33038498793191
Training Acc of Epoch 94: 0.2595290269308943
Testing Acc of Epoch 94: 0.25927826086956524
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.8732e+01 (2.8732e+01)	Acc 0.272461 (0.272461)
Epoch: [95][300/616]	Loss 2.8569e+01 (2.9292e+01)	Acc 0.280273 (0.259561)
Epoch: [95][600/616]	Loss 2.8723e+01 (2.9310e+01)	Acc 0.275391 (0.259103)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.250000 (0.257548)
Training Loss of Epoch 95: 29.3102715344933
Training Acc of Epoch 95: 0.25908282520325204
Testing Acc of Epoch 95: 0.25754782608695653
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.9779e+01 (2.9779e+01)	Acc 0.249023 (0.249023)
Epoch: [96][300/616]	Loss 3.0169e+01 (2.9372e+01)	Acc 0.237305 (0.257147)
Epoch: [96][600/616]	Loss 3.0484e+01 (2.9433e+01)	Acc 0.228516 (0.255409)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.248397 (0.253026)
Training Loss of Epoch 96: 29.438003546241823
Training Acc of Epoch 96: 0.2552829649390244
Testing Acc of Epoch 96: 0.25302608695652173
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 3.0015e+01 (3.0015e+01)	Acc 0.240234 (0.240234)
Epoch: [97][300/616]	Loss 3.0051e+01 (2.9621e+01)	Acc 0.242188 (0.250526)
Epoch: [97][600/616]	Loss 2.9469e+01 (2.9731e+01)	Acc 0.248047 (0.247987)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.235577 (0.243435)
Training Loss of Epoch 97: 29.736429850260418
Training Acc of Epoch 97: 0.24781186483739837
Testing Acc of Epoch 97: 0.24343478260869567
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 3.0099e+01 (3.0099e+01)	Acc 0.233398 (0.233398)
Epoch: [98][300/616]	Loss 3.1377e+01 (3.0083e+01)	Acc 0.208008 (0.239508)
Epoch: [98][600/616]	Loss 3.0161e+01 (3.0209e+01)	Acc 0.243164 (0.236651)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.224359 (0.230878)
Training Loss of Epoch 98: 30.215435539803853
Training Acc of Epoch 98: 0.2364821519308943
Testing Acc of Epoch 98: 0.23087826086956523
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 3.0438e+01 (3.0438e+01)	Acc 0.232422 (0.232422)
Epoch: [99][300/616]	Loss 3.1801e+01 (3.0645e+01)	Acc 0.202148 (0.226670)
Epoch: [99][600/616]	Loss 3.1216e+01 (3.0784e+01)	Acc 0.215820 (0.223595)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.211538 (0.217661)
Training Loss of Epoch 99: 30.790273948607407
Training Acc of Epoch 99: 0.22345179115853658
Testing Acc of Epoch 99: 0.21766086956521738
Early stopping not satisfied.
