train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.025
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.025/lr_decay/JT_6b/
file_prefix exp_3
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.025
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0222e-01 (5.0222e-01)	Acc 0.185547 (0.185547)
Epoch: [0][300/616]	Loss 2.7756e-01 (2.7823e-01)	Acc 0.719727 (0.705662)
Epoch: [0][600/616]	Loss 2.3122e-01 (2.6459e-01)	Acc 0.766602 (0.721966)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.737048)
Training Loss of Epoch 0: 0.2641790273470607
Training Acc of Epoch 0: 0.7223720147357724
Testing Acc of Epoch 0: 0.7370478260869565
Model with the best training loss saved! The loss is 0.2641790273470607
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4517e-01 (2.4517e-01)	Acc 0.747070 (0.747070)
Epoch: [1][300/616]	Loss 2.5575e-01 (2.5053e-01)	Acc 0.729492 (0.738346)
Epoch: [1][600/616]	Loss 2.4927e-01 (2.4903e-01)	Acc 0.729492 (0.739646)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.733461)
Training Loss of Epoch 1: 0.24916200662046914
Training Acc of Epoch 1: 0.7394753556910569
Testing Acc of Epoch 1: 0.7334608695652174
Model with the best training loss saved! The loss is 0.24916200662046914
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4365e-01 (2.4365e-01)	Acc 0.749023 (0.749023)
Epoch: [2][300/616]	Loss 2.5379e-01 (2.4932e-01)	Acc 0.740234 (0.738940)
Epoch: [2][600/616]	Loss 2.5458e-01 (2.4908e-01)	Acc 0.725586 (0.738915)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.737987)
Training Loss of Epoch 2: 0.24903653751059276
Training Acc of Epoch 2: 0.7389831046747968
Testing Acc of Epoch 2: 0.7379869565217392
Model with the best training loss saved! The loss is 0.24903653751059276
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.2700e-01 (2.2700e-01)	Acc 0.777344 (0.777344)
Epoch: [3][300/616]	Loss 2.6619e-01 (2.4972e-01)	Acc 0.715820 (0.738388)
Epoch: [3][600/616]	Loss 2.5402e-01 (2.4831e-01)	Acc 0.728516 (0.739758)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744087)
Training Loss of Epoch 3: 0.24824685577939196
Training Acc of Epoch 3: 0.7398469258130081
Testing Acc of Epoch 3: 0.7440869565217392
Model with the best training loss saved! The loss is 0.24824685577939196
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5206e-01 (2.5206e-01)	Acc 0.734375 (0.734375)
Epoch: [4][300/616]	Loss 2.3949e-01 (2.4864e-01)	Acc 0.757812 (0.739547)
Epoch: [4][600/616]	Loss 2.2711e-01 (2.4855e-01)	Acc 0.764648 (0.739526)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.743722)
Training Loss of Epoch 4: 0.24843100125227519
Training Acc of Epoch 4: 0.7396754319105691
Testing Acc of Epoch 4: 0.7437217391304348
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.2754e-01 (2.2754e-01)	Acc 0.745117 (0.745117)
Epoch: [5][300/616]	Loss 2.4438e-01 (2.4853e-01)	Acc 0.736328 (0.739410)
Epoch: [5][600/616]	Loss 2.4574e-01 (2.4850e-01)	Acc 0.744141 (0.739181)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745287)
Training Loss of Epoch 5: 0.24844673557010125
Training Acc of Epoch 5: 0.739283219004065
Testing Acc of Epoch 5: 0.7452869565217392
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4787e-01 (2.4787e-01)	Acc 0.750000 (0.750000)
Epoch: [6][300/616]	Loss 2.2894e-01 (2.4807e-01)	Acc 0.751953 (0.740085)
Epoch: [6][600/616]	Loss 2.2526e-01 (2.4667e-01)	Acc 0.762695 (0.741455)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.744387)
Training Loss of Epoch 6: 0.24665176839363284
Training Acc of Epoch 6: 0.7414919969512195
Testing Acc of Epoch 6: 0.7443869565217391
Model with the best training loss saved! The loss is 0.24665176839363284
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.5239e-01 (2.5239e-01)	Acc 0.723633 (0.723633)
Epoch: [7][300/616]	Loss 2.4668e-01 (2.4827e-01)	Acc 0.738281 (0.740179)
Epoch: [7][600/616]	Loss 2.6206e-01 (2.4781e-01)	Acc 0.717773 (0.740249)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.745170)
Training Loss of Epoch 7: 0.2476749982533416
Training Acc of Epoch 7: 0.7403264735772358
Testing Acc of Epoch 7: 0.7451695652173913
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.6578e-01 (2.6578e-01)	Acc 0.706055 (0.706055)
Epoch: [8][300/616]	Loss 2.4119e-01 (2.4965e-01)	Acc 0.749023 (0.738421)
Epoch: [8][600/616]	Loss 2.4548e-01 (2.4930e-01)	Acc 0.744141 (0.738761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.744687)
Training Loss of Epoch 8: 0.24931294103948082
Training Acc of Epoch 8: 0.7387544461382114
Testing Acc of Epoch 8: 0.7446869565217391
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3541e-01 (2.3541e-01)	Acc 0.756836 (0.756836)
Epoch: [9][300/616]	Loss 2.6245e-01 (2.4827e-01)	Acc 0.722656 (0.740228)
Epoch: [9][600/616]	Loss 2.4624e-01 (2.4851e-01)	Acc 0.753906 (0.739469)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.727635)
Training Loss of Epoch 9: 0.24851670737673598
Training Acc of Epoch 9: 0.7394848831300813
Testing Acc of Epoch 9: 0.7276347826086956
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.6015e-01 (2.6015e-01)	Acc 0.728516 (0.728516)
Epoch: [10][300/616]	Loss 2.4440e-01 (2.4873e-01)	Acc 0.747070 (0.739083)
Epoch: [10][600/616]	Loss 2.3900e-01 (2.4840e-01)	Acc 0.746094 (0.739549)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736191)
Training Loss of Epoch 10: 0.24849855376452934
Training Acc of Epoch 10: 0.7394991742886179
Testing Acc of Epoch 10: 0.7361913043478261
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.3908e-01 (2.3908e-01)	Acc 0.750977 (0.750977)
Epoch: [11][300/616]	Loss 2.3875e-01 (2.5083e-01)	Acc 0.753906 (0.736766)
Epoch: [11][600/616]	Loss 2.5195e-01 (2.5052e-01)	Acc 0.752930 (0.737186)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735235)
Training Loss of Epoch 11: 0.25054442923728043
Training Acc of Epoch 11: 0.737231643800813
Testing Acc of Epoch 11: 0.7352347826086957
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4717e-01 (2.4717e-01)	Acc 0.735352 (0.735352)
Epoch: [12][300/616]	Loss 2.2955e-01 (2.4922e-01)	Acc 0.757812 (0.738748)
Epoch: [12][600/616]	Loss 2.4395e-01 (2.5012e-01)	Acc 0.733398 (0.737989)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.738526)
Training Loss of Epoch 12: 0.25007746779337164
Training Acc of Epoch 12: 0.738012893800813
Testing Acc of Epoch 12: 0.7385260869565218
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.5697e-01 (2.5697e-01)	Acc 0.723633 (0.723633)
Epoch: [13][300/616]	Loss 2.6345e-01 (2.4880e-01)	Acc 0.737305 (0.738881)
Epoch: [13][600/616]	Loss 2.4195e-01 (2.4856e-01)	Acc 0.754883 (0.739217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.735383)
Training Loss of Epoch 13: 0.24855636852543528
Training Acc of Epoch 13: 0.7391641260162601
Testing Acc of Epoch 13: 0.7353826086956522
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.4932e-01 (2.4932e-01)	Acc 0.736328 (0.736328)
Epoch: [14][300/616]	Loss 2.4568e-01 (2.5079e-01)	Acc 0.752930 (0.737071)
Epoch: [14][600/616]	Loss 2.5233e-01 (2.5063e-01)	Acc 0.729492 (0.737139)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.743348)
Training Loss of Epoch 14: 0.25075180552354676
Training Acc of Epoch 14: 0.7369839303861788
Testing Acc of Epoch 14: 0.7433478260869565
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.3552e-01 (2.3552e-01)	Acc 0.761719 (0.761719)
Epoch: [15][300/616]	Loss 2.6319e-01 (2.5199e-01)	Acc 0.737305 (0.735390)
Epoch: [15][600/616]	Loss 2.4391e-01 (2.5109e-01)	Acc 0.757812 (0.736482)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.734148)
Training Loss of Epoch 15: 0.2509971092871534
Training Acc of Epoch 15: 0.7366202997967479
Testing Acc of Epoch 15: 0.7341478260869565
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4222e-01 (2.4222e-01)	Acc 0.749023 (0.749023)
Epoch: [16][300/616]	Loss 2.4723e-01 (2.4822e-01)	Acc 0.744141 (0.739521)
Epoch: [16][600/616]	Loss 2.7130e-01 (2.4990e-01)	Acc 0.714844 (0.737739)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740839)
Training Loss of Epoch 16: 0.2500472974244172
Training Acc of Epoch 16: 0.7376524390243903
Testing Acc of Epoch 16: 0.7408391304347826
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.5335e-01 (2.5335e-01)	Acc 0.729492 (0.729492)
Epoch: [17][300/616]	Loss 2.6888e-01 (2.4823e-01)	Acc 0.713867 (0.739092)
Epoch: [17][600/616]	Loss 2.5610e-01 (2.4982e-01)	Acc 0.718750 (0.737318)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736057)
Training Loss of Epoch 17: 0.24986769898635586
Training Acc of Epoch 17: 0.7372967479674797
Testing Acc of Epoch 17: 0.7360565217391304
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6636e-01 (2.6636e-01)	Acc 0.727539 (0.727539)
Epoch: [18][300/616]	Loss 2.3599e-01 (2.5238e-01)	Acc 0.746094 (0.734693)
Epoch: [18][600/616]	Loss 2.4674e-01 (2.5161e-01)	Acc 0.728516 (0.735810)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746443)
Training Loss of Epoch 18: 0.2515653507253988
Training Acc of Epoch 18: 0.7358072916666667
Testing Acc of Epoch 18: 0.7464434782608695
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.3354e-01 (2.3354e-01)	Acc 0.758789 (0.758789)
Epoch: [19][300/616]	Loss 2.4608e-01 (2.4928e-01)	Acc 0.754883 (0.738564)
Epoch: [19][600/616]	Loss 2.6136e-01 (2.4899e-01)	Acc 0.715820 (0.738788)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.736930)
Training Loss of Epoch 19: 0.24891470452634298
Training Acc of Epoch 19: 0.7388878302845528
Testing Acc of Epoch 19: 0.7369304347826087
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.5074e-01 (2.5074e-01)	Acc 0.734375 (0.734375)
Epoch: [20][300/616]	Loss 2.5725e-01 (2.5256e-01)	Acc 0.729492 (0.734975)
Epoch: [20][600/616]	Loss 2.4146e-01 (2.5201e-01)	Acc 0.750000 (0.735628)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.689103 (0.708870)
Training Loss of Epoch 20: 0.25196825525140376
Training Acc of Epoch 20: 0.7356723196138212
Testing Acc of Epoch 20: 0.7088695652173913
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.8188e-01 (2.8188e-01)	Acc 0.705078 (0.705078)
Epoch: [21][300/616]	Loss 2.5728e-01 (2.4998e-01)	Acc 0.721680 (0.737658)
Epoch: [21][600/616]	Loss 2.4762e-01 (2.5117e-01)	Acc 0.738281 (0.736276)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739470)
Training Loss of Epoch 21: 0.25115149890988825
Training Acc of Epoch 21: 0.7362614329268292
Testing Acc of Epoch 21: 0.7394695652173913
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4368e-01 (2.4368e-01)	Acc 0.750977 (0.750977)
Epoch: [22][300/616]	Loss 2.4671e-01 (2.5114e-01)	Acc 0.750977 (0.735822)
Epoch: [22][600/616]	Loss 2.4299e-01 (2.5048e-01)	Acc 0.744141 (0.736715)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.724509)
Training Loss of Epoch 22: 0.2505768795323566
Training Acc of Epoch 22: 0.736669524898374
Testing Acc of Epoch 22: 0.7245086956521739
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.8079e-01 (2.8079e-01)	Acc 0.717773 (0.717773)
Epoch: [23][300/616]	Loss 2.4467e-01 (2.4820e-01)	Acc 0.746094 (0.739449)
Epoch: [23][600/616]	Loss 2.4368e-01 (2.5063e-01)	Acc 0.741211 (0.736668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.738991)
Training Loss of Epoch 23: 0.25059841028073937
Training Acc of Epoch 23: 0.7367044588414634
Testing Acc of Epoch 23: 0.7389913043478261
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.2515e-01 (2.2515e-01)	Acc 0.754883 (0.754883)
Epoch: [24][300/616]	Loss 2.5779e-01 (2.4967e-01)	Acc 0.724609 (0.737558)
Epoch: [24][600/616]	Loss 2.6300e-01 (2.4987e-01)	Acc 0.726562 (0.737615)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.734013)
Training Loss of Epoch 24: 0.24978221763924854
Training Acc of Epoch 24: 0.7376826092479675
Testing Acc of Epoch 24: 0.7340130434782609
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.5358e-01 (2.5358e-01)	Acc 0.732422 (0.732422)
Epoch: [25][300/616]	Loss 2.3787e-01 (2.5279e-01)	Acc 0.751953 (0.733872)
Epoch: [25][600/616]	Loss 2.4819e-01 (2.5105e-01)	Acc 0.742188 (0.736270)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.737730)
Training Loss of Epoch 25: 0.2510998719833731
Training Acc of Epoch 25: 0.7362503175813008
Testing Acc of Epoch 25: 0.7377304347826087
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.3410e-01 (2.3410e-01)	Acc 0.755859 (0.755859)
Epoch: [26][300/616]	Loss 2.6200e-01 (2.5207e-01)	Acc 0.717773 (0.735040)
Epoch: [26][600/616]	Loss 2.4389e-01 (2.5111e-01)	Acc 0.744141 (0.736057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743987)
Training Loss of Epoch 26: 0.25100417008729486
Training Acc of Epoch 26: 0.7362455538617886
Testing Acc of Epoch 26: 0.7439869565217392
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.5399e-01 (2.5399e-01)	Acc 0.725586 (0.725586)
Epoch: [27][300/616]	Loss 2.6504e-01 (2.5192e-01)	Acc 0.710938 (0.735653)
Epoch: [27][600/616]	Loss 2.6310e-01 (2.5199e-01)	Acc 0.722656 (0.735293)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736691)
Training Loss of Epoch 27: 0.2519342262328156
Training Acc of Epoch 27: 0.7353722052845528
Testing Acc of Epoch 27: 0.736691304347826
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3962e-01 (2.3962e-01)	Acc 0.752930 (0.752930)
Epoch: [28][300/616]	Loss 2.6968e-01 (2.5290e-01)	Acc 0.715820 (0.733823)
Epoch: [28][600/616]	Loss 2.4284e-01 (2.5228e-01)	Acc 0.754883 (0.734591)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.735013)
Training Loss of Epoch 28: 0.25207646588484445
Training Acc of Epoch 28: 0.7348815421747967
Testing Acc of Epoch 28: 0.7350130434782609
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.5183e-01 (2.5183e-01)	Acc 0.713867 (0.713867)
Epoch: [29][300/616]	Loss 2.4487e-01 (2.5089e-01)	Acc 0.755859 (0.736753)
Epoch: [29][600/616]	Loss 2.3663e-01 (2.5081e-01)	Acc 0.770508 (0.736443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735117)
Training Loss of Epoch 29: 0.25073624645791404
Training Acc of Epoch 29: 0.7365806021341463
Testing Acc of Epoch 29: 0.7351173913043478
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.4962e-01 (2.4962e-01)	Acc 0.744141 (0.744141)
Epoch: [30][300/616]	Loss 2.5844e-01 (2.4882e-01)	Acc 0.743164 (0.738963)
Epoch: [30][600/616]	Loss 2.4833e-01 (2.5042e-01)	Acc 0.730469 (0.737466)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.738883)
Training Loss of Epoch 30: 0.2506639676123131
Training Acc of Epoch 30: 0.7371951219512195
Testing Acc of Epoch 30: 0.7388826086956521
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.4623e-01 (2.4623e-01)	Acc 0.724609 (0.724609)
Epoch: [31][300/616]	Loss 2.4263e-01 (2.5188e-01)	Acc 0.733398 (0.735147)
Epoch: [31][600/616]	Loss 2.6195e-01 (2.5167e-01)	Acc 0.737305 (0.735738)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734235)
Training Loss of Epoch 31: 0.25187849346699753
Training Acc of Epoch 31: 0.7355214684959349
Testing Acc of Epoch 31: 0.7342347826086957
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.6469e-01 (2.6469e-01)	Acc 0.702148 (0.702148)
Epoch: [32][300/616]	Loss 2.4561e-01 (2.5402e-01)	Acc 0.739258 (0.733677)
Epoch: [32][600/616]	Loss 2.5351e-01 (2.5296e-01)	Acc 0.740234 (0.734638)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.732096)
Training Loss of Epoch 32: 0.25292774368592397
Training Acc of Epoch 32: 0.7347259273373984
Testing Acc of Epoch 32: 0.732095652173913
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.5097e-01 (2.5097e-01)	Acc 0.745117 (0.745117)
Epoch: [33][300/616]	Loss 2.7029e-01 (2.5315e-01)	Acc 0.723633 (0.734180)
Epoch: [33][600/616]	Loss 2.5976e-01 (2.5275e-01)	Acc 0.732422 (0.734780)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.738900)
Training Loss of Epoch 33: 0.2527895853286836
Training Acc of Epoch 33: 0.7347624491869919
Testing Acc of Epoch 33: 0.7389
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.6242e-01 (2.6242e-01)	Acc 0.718750 (0.718750)
Epoch: [34][300/616]	Loss 2.5487e-01 (2.5078e-01)	Acc 0.750000 (0.736831)
Epoch: [34][600/616]	Loss 2.4520e-01 (2.5109e-01)	Acc 0.741211 (0.736255)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.742639)
Training Loss of Epoch 34: 0.2511099570892691
Training Acc of Epoch 34: 0.7361598069105691
Testing Acc of Epoch 34: 0.7426391304347826
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4562e-01 (2.4562e-01)	Acc 0.740234 (0.740234)
Epoch: [35][300/616]	Loss 2.3628e-01 (2.5118e-01)	Acc 0.759766 (0.736523)
Epoch: [35][600/616]	Loss 2.3999e-01 (2.5125e-01)	Acc 0.757812 (0.736538)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.737926)
Training Loss of Epoch 35: 0.2513384536029847
Training Acc of Epoch 35: 0.7363757621951219
Testing Acc of Epoch 35: 0.7379260869565217
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.5310e-01 (2.5310e-01)	Acc 0.728516 (0.728516)
Epoch: [36][300/616]	Loss 2.4858e-01 (2.5237e-01)	Acc 0.734375 (0.734914)
Epoch: [36][600/616]	Loss 2.4931e-01 (2.5202e-01)	Acc 0.742188 (0.735163)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.739465)
Training Loss of Epoch 36: 0.2519311060992683
Training Acc of Epoch 36: 0.7352404090447154
Testing Acc of Epoch 36: 0.7394652173913043
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.4914e-01 (2.4914e-01)	Acc 0.755859 (0.755859)
Epoch: [37][300/616]	Loss 2.5381e-01 (2.5116e-01)	Acc 0.743164 (0.736137)
Epoch: [37][600/616]	Loss 2.5124e-01 (2.5224e-01)	Acc 0.739258 (0.735166)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.720722)
Training Loss of Epoch 37: 0.2523162677036068
Training Acc of Epoch 37: 0.7350959095528455
Testing Acc of Epoch 37: 0.7207217391304348
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.4774e-01 (2.4774e-01)	Acc 0.751953 (0.751953)
Epoch: [38][300/616]	Loss 2.5460e-01 (2.5056e-01)	Acc 0.731445 (0.736312)
Epoch: [38][600/616]	Loss 2.5666e-01 (2.5060e-01)	Acc 0.736328 (0.736621)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.736991)
Training Loss of Epoch 38: 0.250501489857348
Training Acc of Epoch 38: 0.7367552718495934
Testing Acc of Epoch 38: 0.7369913043478261
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.4641e-01 (2.4641e-01)	Acc 0.748047 (0.748047)
Epoch: [39][300/616]	Loss 2.5262e-01 (2.5116e-01)	Acc 0.731445 (0.735595)
Epoch: [39][600/616]	Loss 2.6256e-01 (2.5153e-01)	Acc 0.716797 (0.735769)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.745096)
Training Loss of Epoch 39: 0.25164109071095786
Training Acc of Epoch 39: 0.735619918699187
Testing Acc of Epoch 39: 0.745095652173913
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.5258e-01 (2.5258e-01)	Acc 0.728516 (0.728516)
Epoch: [40][300/616]	Loss 2.4820e-01 (2.5245e-01)	Acc 0.733398 (0.735056)
Epoch: [40][600/616]	Loss 2.6092e-01 (2.5176e-01)	Acc 0.722656 (0.736041)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.738661)
Training Loss of Epoch 40: 0.2517969981441653
Training Acc of Epoch 40: 0.7359962525406504
Testing Acc of Epoch 40: 0.7386608695652174
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.4074e-01 (2.4074e-01)	Acc 0.747070 (0.747070)
Epoch: [41][300/616]	Loss 2.4877e-01 (2.5313e-01)	Acc 0.745117 (0.734670)
Epoch: [41][600/616]	Loss 2.4379e-01 (2.5165e-01)	Acc 0.749023 (0.735712)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.741070)
Training Loss of Epoch 41: 0.25162258705472557
Training Acc of Epoch 41: 0.7356913744918699
Testing Acc of Epoch 41: 0.7410695652173913
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.4170e-01 (2.4170e-01)	Acc 0.749023 (0.749023)
Epoch: [42][300/616]	Loss 2.4472e-01 (2.5283e-01)	Acc 0.750000 (0.734223)
Epoch: [42][600/616]	Loss 2.5459e-01 (2.5230e-01)	Acc 0.735352 (0.734775)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.690705 (0.721039)
Training Loss of Epoch 42: 0.2521521425828701
Training Acc of Epoch 42: 0.7349545858739838
Testing Acc of Epoch 42: 0.7210391304347826
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.6313e-01 (2.6313e-01)	Acc 0.715820 (0.715820)
Epoch: [43][300/616]	Loss 2.4607e-01 (2.5113e-01)	Acc 0.741211 (0.736130)
Epoch: [43][600/616]	Loss 2.4625e-01 (2.5159e-01)	Acc 0.743164 (0.735818)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.739574)
Training Loss of Epoch 43: 0.2516224481952869
Training Acc of Epoch 43: 0.7356723196138212
Testing Acc of Epoch 43: 0.7395739130434783
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4278e-01 (2.4278e-01)	Acc 0.737305 (0.737305)
Epoch: [44][300/616]	Loss 2.3152e-01 (2.5273e-01)	Acc 0.773438 (0.734524)
Epoch: [44][600/616]	Loss 2.3084e-01 (2.5282e-01)	Acc 0.765625 (0.734391)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.736048)
Training Loss of Epoch 44: 0.25289872579458283
Training Acc of Epoch 44: 0.7343051321138211
Testing Acc of Epoch 44: 0.7360478260869565
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3802e-01 (2.3802e-01)	Acc 0.762695 (0.762695)
Epoch: [45][300/616]	Loss 2.5088e-01 (2.5192e-01)	Acc 0.735352 (0.735251)
Epoch: [45][600/616]	Loss 2.2996e-01 (2.5244e-01)	Acc 0.777344 (0.735236)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.737509)
Training Loss of Epoch 45: 0.2523432328933623
Training Acc of Epoch 45: 0.7353515625
Testing Acc of Epoch 45: 0.7375086956521739
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.3700e-01 (2.3700e-01)	Acc 0.755859 (0.755859)
Epoch: [46][300/616]	Loss 2.3832e-01 (2.5120e-01)	Acc 0.752930 (0.736172)
Epoch: [46][600/616]	Loss 2.5526e-01 (2.5214e-01)	Acc 0.725586 (0.735324)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.739078)
Training Loss of Epoch 46: 0.2521175990259744
Training Acc of Epoch 46: 0.7352991615853659
Testing Acc of Epoch 46: 0.7390782608695652
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.6093e-01 (2.6093e-01)	Acc 0.723633 (0.723633)
Epoch: [47][300/616]	Loss 2.6706e-01 (2.5100e-01)	Acc 0.707031 (0.737736)
Epoch: [47][600/616]	Loss 2.5273e-01 (2.5151e-01)	Acc 0.747070 (0.736567)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.685897 (0.716735)
Training Loss of Epoch 47: 0.25144062236072573
Training Acc of Epoch 47: 0.7366377667682927
Testing Acc of Epoch 47: 0.7167347826086956
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.7489e-01 (2.7489e-01)	Acc 0.706055 (0.706055)
Epoch: [48][300/616]	Loss 2.4244e-01 (2.5096e-01)	Acc 0.752930 (0.736682)
Epoch: [48][600/616]	Loss 2.5559e-01 (2.5170e-01)	Acc 0.750000 (0.735620)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.736491)
Training Loss of Epoch 48: 0.2516924817629946
Training Acc of Epoch 48: 0.7356024517276423
Testing Acc of Epoch 48: 0.7364913043478261
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.3097e-01 (2.3097e-01)	Acc 0.757812 (0.757812)
Epoch: [49][300/616]	Loss 2.5927e-01 (2.5312e-01)	Acc 0.724609 (0.734070)
Epoch: [49][600/616]	Loss 2.6024e-01 (2.5218e-01)	Acc 0.717773 (0.735207)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.711538 (0.733891)
Training Loss of Epoch 49: 0.2521942136248922
Training Acc of Epoch 49: 0.7351832444105691
Testing Acc of Epoch 49: 0.7338913043478261
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.5679e-01 (2.5679e-01)	Acc 0.732422 (0.732422)
Epoch: [50][300/616]	Loss 2.5114e-01 (2.5064e-01)	Acc 0.739258 (0.736792)
Epoch: [50][600/616]	Loss 2.6494e-01 (2.5118e-01)	Acc 0.725586 (0.736099)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.692308 (0.718804)
Training Loss of Epoch 50: 0.251217187032467
Training Acc of Epoch 50: 0.736012131605691
Testing Acc of Epoch 50: 0.7188043478260869
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.6252e-01 (2.6252e-01)	Acc 0.711914 (0.711914)
Epoch: [51][300/616]	Loss 2.5756e-01 (2.5161e-01)	Acc 0.732422 (0.735890)
Epoch: [51][600/616]	Loss 2.4703e-01 (2.5201e-01)	Acc 0.720703 (0.735517)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735370)
Training Loss of Epoch 51: 0.25201410110888445
Training Acc of Epoch 51: 0.7355198805894309
Testing Acc of Epoch 51: 0.7353695652173913
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.5466e-01 (2.5466e-01)	Acc 0.727539 (0.727539)
Epoch: [52][300/616]	Loss 2.6771e-01 (2.5140e-01)	Acc 0.718750 (0.736571)
Epoch: [52][600/616]	Loss 2.6361e-01 (2.5174e-01)	Acc 0.718750 (0.736062)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738674)
Training Loss of Epoch 52: 0.25174589627157384
Training Acc of Epoch 52: 0.7359851371951219
Testing Acc of Epoch 52: 0.7386739130434783
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5621e-01 (2.5621e-01)	Acc 0.736328 (0.736328)
Epoch: [53][300/616]	Loss 2.6867e-01 (2.5335e-01)	Acc 0.713867 (0.733752)
Epoch: [53][600/616]	Loss 2.4464e-01 (2.5238e-01)	Acc 0.742188 (0.735137)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.744283)
Training Loss of Epoch 53: 0.25242651497445456
Training Acc of Epoch 53: 0.7351356072154471
Testing Acc of Epoch 53: 0.7442826086956522
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3964e-01 (2.3964e-01)	Acc 0.741211 (0.741211)
Epoch: [54][300/616]	Loss 2.4454e-01 (2.5126e-01)	Acc 0.738281 (0.736896)
Epoch: [54][600/616]	Loss 2.7143e-01 (2.5317e-01)	Acc 0.715820 (0.734591)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740343)
Training Loss of Epoch 54: 0.25310176094857656
Training Acc of Epoch 54: 0.734690993394309
Testing Acc of Epoch 54: 0.7403434782608695
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.4075e-01 (2.4075e-01)	Acc 0.739258 (0.739258)
Epoch: [55][300/616]	Loss 2.7787e-01 (2.5256e-01)	Acc 0.703125 (0.734865)
Epoch: [55][600/616]	Loss 2.3963e-01 (2.5325e-01)	Acc 0.737305 (0.733782)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.741704)
Training Loss of Epoch 55: 0.25321911326268826
Training Acc of Epoch 55: 0.7338287601626017
Testing Acc of Epoch 55: 0.741704347826087
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4944e-01 (2.4944e-01)	Acc 0.750000 (0.750000)
Epoch: [56][300/616]	Loss 2.5636e-01 (2.5225e-01)	Acc 0.741211 (0.735546)
Epoch: [56][600/616]	Loss 2.4860e-01 (2.5262e-01)	Acc 0.734375 (0.734874)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745313)
Training Loss of Epoch 56: 0.25255116204905315
Training Acc of Epoch 56: 0.7349164761178861
Testing Acc of Epoch 56: 0.7453130434782609
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3749e-01 (2.3749e-01)	Acc 0.761719 (0.761719)
Epoch: [57][300/616]	Loss 2.4820e-01 (2.5168e-01)	Acc 0.744141 (0.735559)
Epoch: [57][600/616]	Loss 2.5891e-01 (2.5153e-01)	Acc 0.730469 (0.735514)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.728439)
Training Loss of Epoch 57: 0.25140662721502105
Training Acc of Epoch 57: 0.7356596163617887
Testing Acc of Epoch 57: 0.7284391304347826
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.8207e-01 (2.8207e-01)	Acc 0.719727 (0.719727)
Epoch: [58][300/616]	Loss 2.4194e-01 (2.5106e-01)	Acc 0.748047 (0.736442)
Epoch: [58][600/616]	Loss 2.4926e-01 (2.5064e-01)	Acc 0.749023 (0.736481)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.740522)
Training Loss of Epoch 58: 0.2504861671507843
Training Acc of Epoch 58: 0.7366838160569106
Testing Acc of Epoch 58: 0.7405217391304347
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.5320e-01 (2.5320e-01)	Acc 0.730469 (0.730469)
Epoch: [59][300/616]	Loss 2.4231e-01 (2.4971e-01)	Acc 0.733398 (0.737662)
Epoch: [59][600/616]	Loss 2.4956e-01 (2.5010e-01)	Acc 0.738281 (0.737423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.738622)
Training Loss of Epoch 59: 0.25005652143703244
Training Acc of Epoch 59: 0.7374539507113821
Testing Acc of Epoch 59: 0.7386217391304348
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3786e-01 (2.3786e-01)	Acc 0.756836 (0.756836)
Epoch: [60][300/616]	Loss 2.5323e-01 (2.5278e-01)	Acc 0.721680 (0.734842)
Epoch: [60][600/616]	Loss 2.4585e-01 (2.5314e-01)	Acc 0.747070 (0.734728)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.727583)
Training Loss of Epoch 60: 0.253075322557271
Training Acc of Epoch 60: 0.7347005208333334
Testing Acc of Epoch 60: 0.7275826086956522
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.6420e-01 (2.6420e-01)	Acc 0.733398 (0.733398)
Epoch: [61][300/616]	Loss 2.1865e-01 (2.5235e-01)	Acc 0.777344 (0.735277)
Epoch: [61][600/616]	Loss 2.4484e-01 (2.5346e-01)	Acc 0.742188 (0.733650)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737096)
Training Loss of Epoch 61: 0.25329546872677844
Training Acc of Epoch 61: 0.7338255843495934
Testing Acc of Epoch 61: 0.737095652173913
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.7273e-01 (2.7273e-01)	Acc 0.713867 (0.713867)
Epoch: [62][300/616]	Loss 2.7035e-01 (2.5269e-01)	Acc 0.715820 (0.734330)
Epoch: [62][600/616]	Loss 2.6141e-01 (2.5191e-01)	Acc 0.723633 (0.735412)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.737578)
Training Loss of Epoch 62: 0.25206738426917935
Training Acc of Epoch 62: 0.7352404090447154
Testing Acc of Epoch 62: 0.7375782608695652
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.6391e-01 (2.6391e-01)	Acc 0.731445 (0.731445)
Epoch: [63][300/616]	Loss 2.6331e-01 (2.5217e-01)	Acc 0.723633 (0.735381)
Epoch: [63][600/616]	Loss 2.4136e-01 (2.5049e-01)	Acc 0.738281 (0.737032)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.735030)
Training Loss of Epoch 63: 0.2504042853184832
Training Acc of Epoch 63: 0.7371347815040651
Testing Acc of Epoch 63: 0.7350304347826087
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.5156e-01 (2.5156e-01)	Acc 0.723633 (0.723633)
Epoch: [64][300/616]	Loss 2.4952e-01 (2.5060e-01)	Acc 0.729492 (0.737337)
Epoch: [64][600/616]	Loss 2.5941e-01 (2.4972e-01)	Acc 0.723633 (0.737932)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.742939)
Training Loss of Epoch 64: 0.2498827380378072
Training Acc of Epoch 64: 0.7377540650406504
Testing Acc of Epoch 64: 0.7429391304347827
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.3379e-01 (2.3379e-01)	Acc 0.762695 (0.762695)
Epoch: [65][300/616]	Loss 2.5382e-01 (2.5216e-01)	Acc 0.721680 (0.735783)
Epoch: [65][600/616]	Loss 2.5982e-01 (2.5206e-01)	Acc 0.727539 (0.735525)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.743283)
Training Loss of Epoch 65: 0.2519398079170444
Training Acc of Epoch 65: 0.735645325203252
Testing Acc of Epoch 65: 0.7432826086956522
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.4576e-01 (2.4576e-01)	Acc 0.732422 (0.732422)
Epoch: [66][300/616]	Loss 2.4525e-01 (2.5138e-01)	Acc 0.745117 (0.736279)
Epoch: [66][600/616]	Loss 2.8528e-01 (2.5083e-01)	Acc 0.701172 (0.737199)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.736074)
Training Loss of Epoch 66: 0.25103034711465605
Training Acc of Epoch 66: 0.7369474085365854
Testing Acc of Epoch 66: 0.7360739130434782
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.2806e-01 (2.2806e-01)	Acc 0.763672 (0.763672)
Epoch: [67][300/616]	Loss 2.4300e-01 (2.5220e-01)	Acc 0.738281 (0.734764)
Epoch: [67][600/616]	Loss 2.6162e-01 (2.5225e-01)	Acc 0.708008 (0.734763)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.736709)
Training Loss of Epoch 67: 0.2523593960254173
Training Acc of Epoch 67: 0.7346878175813009
Testing Acc of Epoch 67: 0.7367086956521739
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.5046e-01 (2.5046e-01)	Acc 0.739258 (0.739258)
Epoch: [68][300/616]	Loss 2.6705e-01 (2.5221e-01)	Acc 0.709961 (0.735712)
Epoch: [68][600/616]	Loss 2.3410e-01 (2.5189e-01)	Acc 0.754883 (0.735335)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.731974)
Training Loss of Epoch 68: 0.2519928479097723
Training Acc of Epoch 68: 0.7352022992886179
Testing Acc of Epoch 68: 0.7319739130434783
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.6943e-01 (2.6943e-01)	Acc 0.713867 (0.713867)
Epoch: [69][300/616]	Loss 2.4147e-01 (2.5288e-01)	Acc 0.758789 (0.734417)
Epoch: [69][600/616]	Loss 2.6655e-01 (2.5232e-01)	Acc 0.722656 (0.734988)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.697115 (0.715135)
Training Loss of Epoch 69: 0.252531491086735
Training Acc of Epoch 69: 0.734741806402439
Testing Acc of Epoch 69: 0.7151347826086957
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.6801e-01 (2.6801e-01)	Acc 0.714844 (0.714844)
Epoch: [70][300/616]	Loss 2.7438e-01 (2.5151e-01)	Acc 0.703125 (0.735893)
Epoch: [70][600/616]	Loss 2.6035e-01 (2.5221e-01)	Acc 0.726562 (0.735490)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.733126)
Training Loss of Epoch 70: 0.25221908123997167
Training Acc of Epoch 70: 0.7353896722560975
Testing Acc of Epoch 70: 0.7331260869565217
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5598e-01 (2.5598e-01)	Acc 0.726562 (0.726562)
Epoch: [71][300/616]	Loss 2.5945e-01 (2.5238e-01)	Acc 0.720703 (0.735027)
Epoch: [71][600/616]	Loss 2.5748e-01 (2.5114e-01)	Acc 0.725586 (0.736193)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742252)
Training Loss of Epoch 71: 0.25118525088802585
Training Acc of Epoch 71: 0.7361137576219512
Testing Acc of Epoch 71: 0.7422521739130434
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4003e-01 (2.4003e-01)	Acc 0.754883 (0.754883)
Epoch: [72][300/616]	Loss 2.4823e-01 (2.5080e-01)	Acc 0.741211 (0.736883)
Epoch: [72][600/616]	Loss 2.5041e-01 (2.5113e-01)	Acc 0.746094 (0.736167)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.738283)
Training Loss of Epoch 72: 0.2511139664708114
Training Acc of Epoch 72: 0.7362376143292683
Testing Acc of Epoch 72: 0.7382826086956522
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.3512e-01 (2.3512e-01)	Acc 0.763672 (0.763672)
Epoch: [73][300/616]	Loss 2.6288e-01 (2.5087e-01)	Acc 0.728516 (0.736558)
Epoch: [73][600/616]	Loss 2.6692e-01 (2.5134e-01)	Acc 0.712891 (0.736258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.734639)
Training Loss of Epoch 73: 0.2512668611315208
Training Acc of Epoch 73: 0.7363598831300813
Testing Acc of Epoch 73: 0.7346391304347826
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.5730e-01 (2.5730e-01)	Acc 0.732422 (0.732422)
Epoch: [74][300/616]	Loss 2.5893e-01 (2.5296e-01)	Acc 0.728516 (0.733924)
Epoch: [74][600/616]	Loss 2.3942e-01 (2.5119e-01)	Acc 0.758789 (0.736247)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.742896)
Training Loss of Epoch 74: 0.25113119054131394
Training Acc of Epoch 74: 0.7363440040650406
Testing Acc of Epoch 74: 0.7428956521739131
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.3323e-01 (2.3323e-01)	Acc 0.754883 (0.754883)
Epoch: [75][300/616]	Loss 2.4738e-01 (2.4129e-01)	Acc 0.729492 (0.746084)
Epoch: [75][600/616]	Loss 2.4078e-01 (2.4162e-01)	Acc 0.760742 (0.745644)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746826)
Training Loss of Epoch 75: 0.24178632308797138
Training Acc of Epoch 75: 0.7454442962398374
Testing Acc of Epoch 75: 0.7468260869565218
Model with the best training loss saved! The loss is 0.24178632308797138
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3785e-01 (2.3785e-01)	Acc 0.743164 (0.743164)
Epoch: [76][300/616]	Loss 2.4213e-01 (2.4321e-01)	Acc 0.748047 (0.743297)
Epoch: [76][600/616]	Loss 2.4476e-01 (2.4281e-01)	Acc 0.740234 (0.743905)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742987)
Training Loss of Epoch 76: 0.24277247110518013
Training Acc of Epoch 76: 0.7438817962398374
Testing Acc of Epoch 76: 0.7429869565217392
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.5267e-01 (2.5267e-01)	Acc 0.741211 (0.741211)
Epoch: [77][300/616]	Loss 2.4350e-01 (2.4351e-01)	Acc 0.739258 (0.743414)
Epoch: [77][600/616]	Loss 2.3198e-01 (2.4241e-01)	Acc 0.761719 (0.744677)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744739)
Training Loss of Epoch 77: 0.24243169286386754
Training Acc of Epoch 77: 0.7446709857723577
Testing Acc of Epoch 77: 0.7447391304347826
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.5134e-01 (2.5134e-01)	Acc 0.736328 (0.736328)
Epoch: [78][300/616]	Loss 2.5936e-01 (2.4273e-01)	Acc 0.714844 (0.744176)
Epoch: [78][600/616]	Loss 2.4276e-01 (2.4219e-01)	Acc 0.738281 (0.744612)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747378)
Training Loss of Epoch 78: 0.24220585619531027
Training Acc of Epoch 78: 0.7445344258130081
Testing Acc of Epoch 78: 0.7473782608695653
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3505e-01 (2.3505e-01)	Acc 0.758789 (0.758789)
Epoch: [79][300/616]	Loss 2.5419e-01 (2.4257e-01)	Acc 0.728516 (0.744426)
Epoch: [79][600/616]	Loss 2.2652e-01 (2.4252e-01)	Acc 0.767578 (0.744258)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747152)
Training Loss of Epoch 79: 0.24241070689224614
Training Acc of Epoch 79: 0.7444534425813009
Testing Acc of Epoch 79: 0.7471521739130434
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.4153e-01 (2.4153e-01)	Acc 0.734375 (0.734375)
Epoch: [80][300/616]	Loss 2.4336e-01 (2.4358e-01)	Acc 0.741211 (0.742995)
Epoch: [80][600/616]	Loss 2.4511e-01 (2.4285e-01)	Acc 0.739258 (0.743986)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743222)
Training Loss of Epoch 80: 0.24291399445960193
Training Acc of Epoch 80: 0.7439659552845529
Testing Acc of Epoch 80: 0.7432217391304348
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.5687e-01 (2.5687e-01)	Acc 0.726562 (0.726562)
Epoch: [81][300/616]	Loss 2.3896e-01 (2.4288e-01)	Acc 0.758789 (0.744050)
Epoch: [81][600/616]	Loss 2.3857e-01 (2.4250e-01)	Acc 0.755859 (0.744415)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745770)
Training Loss of Epoch 81: 0.242547139913086
Training Acc of Epoch 81: 0.7442501905487805
Testing Acc of Epoch 81: 0.7457695652173914
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.3599e-01 (2.3599e-01)	Acc 0.749023 (0.749023)
Epoch: [82][300/616]	Loss 2.5122e-01 (2.4361e-01)	Acc 0.731445 (0.742960)
Epoch: [82][600/616]	Loss 2.4244e-01 (2.4300e-01)	Acc 0.745117 (0.743920)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.740835)
Training Loss of Epoch 82: 0.24309572056541598
Training Acc of Epoch 82: 0.7438214557926829
Testing Acc of Epoch 82: 0.7408347826086956
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.5732e-01 (2.5732e-01)	Acc 0.725586 (0.725586)
Epoch: [83][300/616]	Loss 2.2382e-01 (2.4285e-01)	Acc 0.767578 (0.744352)
Epoch: [83][600/616]	Loss 2.5203e-01 (2.4308e-01)	Acc 0.727539 (0.743801)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743952)
Training Loss of Epoch 83: 0.24314528114911987
Training Acc of Epoch 83: 0.743824631605691
Testing Acc of Epoch 83: 0.7439521739130435
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3835e-01 (2.3835e-01)	Acc 0.749023 (0.749023)
Epoch: [84][300/616]	Loss 2.3600e-01 (2.4346e-01)	Acc 0.750000 (0.743339)
Epoch: [84][600/616]	Loss 2.3678e-01 (2.4335e-01)	Acc 0.750000 (0.743617)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745617)
Training Loss of Epoch 84: 0.2432666769599527
Training Acc of Epoch 84: 0.7437230055894309
Testing Acc of Epoch 84: 0.7456173913043478
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4589e-01 (2.4589e-01)	Acc 0.730469 (0.730469)
Epoch: [85][300/616]	Loss 2.6067e-01 (2.4418e-01)	Acc 0.715820 (0.742483)
Epoch: [85][600/616]	Loss 2.4951e-01 (2.4409e-01)	Acc 0.722656 (0.742568)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.744200)
Training Loss of Epoch 85: 0.24407548594280956
Training Acc of Epoch 85: 0.7425336636178862
Testing Acc of Epoch 85: 0.7442
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.5169e-01 (2.5169e-01)	Acc 0.734375 (0.734375)
Epoch: [86][300/616]	Loss 2.3503e-01 (2.4391e-01)	Acc 0.749023 (0.743271)
Epoch: [86][600/616]	Loss 2.4210e-01 (2.4357e-01)	Acc 0.744141 (0.743635)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.742022)
Training Loss of Epoch 86: 0.24362927319073097
Training Acc of Epoch 86: 0.743653137703252
Testing Acc of Epoch 86: 0.7420217391304348
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.6191e-01 (2.6191e-01)	Acc 0.733398 (0.733398)
Epoch: [87][300/616]	Loss 2.2322e-01 (2.4396e-01)	Acc 0.779297 (0.742765)
Epoch: [87][600/616]	Loss 2.3732e-01 (2.4389e-01)	Acc 0.763672 (0.742969)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.742700)
Training Loss of Epoch 87: 0.24387503823613732
Training Acc of Epoch 87: 0.7430132113821138
Testing Acc of Epoch 87: 0.7427
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.2334e-01 (2.2334e-01)	Acc 0.757812 (0.757812)
Epoch: [88][300/616]	Loss 2.4966e-01 (2.4415e-01)	Acc 0.741211 (0.742249)
Epoch: [88][600/616]	Loss 2.3502e-01 (2.4411e-01)	Acc 0.751953 (0.742503)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.737422)
Training Loss of Epoch 88: 0.2441080437685416
Training Acc of Epoch 88: 0.7425352515243903
Testing Acc of Epoch 88: 0.7374217391304347
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.5051e-01 (2.5051e-01)	Acc 0.741211 (0.741211)
Epoch: [89][300/616]	Loss 2.6222e-01 (2.4453e-01)	Acc 0.723633 (0.742145)
Epoch: [89][600/616]	Loss 2.6189e-01 (2.4373e-01)	Acc 0.713867 (0.742795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.744935)
Training Loss of Epoch 89: 0.2437929304876948
Training Acc of Epoch 89: 0.742674987296748
Testing Acc of Epoch 89: 0.7449347826086956
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.4288e-01 (2.4288e-01)	Acc 0.746094 (0.746094)
Epoch: [90][300/616]	Loss 2.2139e-01 (2.4445e-01)	Acc 0.771484 (0.741474)
Epoch: [90][600/616]	Loss 2.3356e-01 (2.4385e-01)	Acc 0.743164 (0.742577)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.746578)
Training Loss of Epoch 90: 0.24376041130806372
Training Acc of Epoch 90: 0.7427591463414634
Testing Acc of Epoch 90: 0.7465782608695652
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.4762e-01 (2.4762e-01)	Acc 0.726562 (0.726562)
Epoch: [91][300/616]	Loss 2.4460e-01 (2.4443e-01)	Acc 0.748047 (0.742441)
Epoch: [91][600/616]	Loss 2.3067e-01 (2.4398e-01)	Acc 0.752930 (0.742930)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745713)
Training Loss of Epoch 91: 0.24394221991542878
Training Acc of Epoch 91: 0.7429385797764227
Testing Acc of Epoch 91: 0.7457130434782608
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3842e-01 (2.3842e-01)	Acc 0.752930 (0.752930)
Epoch: [92][300/616]	Loss 2.4294e-01 (2.4465e-01)	Acc 0.754883 (0.742038)
Epoch: [92][600/616]	Loss 2.4393e-01 (2.4397e-01)	Acc 0.736328 (0.742774)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.746130)
Training Loss of Epoch 92: 0.24410161392960122
Training Acc of Epoch 92: 0.742552718495935
Testing Acc of Epoch 92: 0.7461304347826087
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2972e-01 (2.2972e-01)	Acc 0.750977 (0.750977)
Epoch: [93][300/616]	Loss 2.3926e-01 (2.4490e-01)	Acc 0.755859 (0.741921)
Epoch: [93][600/616]	Loss 2.3624e-01 (2.4461e-01)	Acc 0.734375 (0.742204)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746383)
Training Loss of Epoch 93: 0.24461498594865566
Training Acc of Epoch 93: 0.7421668572154472
Testing Acc of Epoch 93: 0.7463826086956522
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.3456e-01 (2.3456e-01)	Acc 0.757812 (0.757812)
Epoch: [94][300/616]	Loss 2.4924e-01 (2.4458e-01)	Acc 0.742188 (0.742638)
Epoch: [94][600/616]	Loss 2.4358e-01 (2.4480e-01)	Acc 0.742188 (0.742134)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742478)
Training Loss of Epoch 94: 0.2447413993075611
Training Acc of Epoch 94: 0.7421922637195122
Testing Acc of Epoch 94: 0.7424782608695653
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.5383e-01 (2.5383e-01)	Acc 0.728516 (0.728516)
Epoch: [95][300/616]	Loss 2.3129e-01 (2.4556e-01)	Acc 0.759766 (0.740796)
Epoch: [95][600/616]	Loss 2.3758e-01 (2.4494e-01)	Acc 0.766602 (0.741754)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.745217)
Training Loss of Epoch 95: 0.2449867235693505
Training Acc of Epoch 95: 0.7417222433943089
Testing Acc of Epoch 95: 0.7452173913043478
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.4162e-01 (2.4162e-01)	Acc 0.742188 (0.742188)
Epoch: [96][300/616]	Loss 2.4076e-01 (2.4420e-01)	Acc 0.751953 (0.742736)
Epoch: [96][600/616]	Loss 2.5413e-01 (2.4486e-01)	Acc 0.729492 (0.741967)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746322)
Training Loss of Epoch 96: 0.24487315302457266
Training Acc of Epoch 96: 0.7419096163617886
Testing Acc of Epoch 96: 0.7463217391304348
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.5314e-01 (2.5314e-01)	Acc 0.728516 (0.728516)
Epoch: [97][300/616]	Loss 2.4573e-01 (2.4394e-01)	Acc 0.740234 (0.743115)
Epoch: [97][600/616]	Loss 2.3364e-01 (2.4471e-01)	Acc 0.750977 (0.742051)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745222)
Training Loss of Epoch 97: 0.24467267539443038
Training Acc of Epoch 97: 0.7420302972560976
Testing Acc of Epoch 97: 0.7452217391304348
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.5733e-01 (2.5733e-01)	Acc 0.722656 (0.722656)
Epoch: [98][300/616]	Loss 2.4461e-01 (2.4394e-01)	Acc 0.744141 (0.743015)
Epoch: [98][600/616]	Loss 2.3777e-01 (2.4430e-01)	Acc 0.751953 (0.742452)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.741170)
Training Loss of Epoch 98: 0.2443849109537233
Training Acc of Epoch 98: 0.7423462906504065
Testing Acc of Epoch 98: 0.7411695652173913
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.5354e-01 (2.5354e-01)	Acc 0.727539 (0.727539)
Epoch: [99][300/616]	Loss 2.3586e-01 (2.4512e-01)	Acc 0.765625 (0.742655)
Epoch: [99][600/616]	Loss 2.5092e-01 (2.4541e-01)	Acc 0.722656 (0.741759)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.747104)
Training Loss of Epoch 99: 0.24534631049245353
Training Acc of Epoch 99: 0.7417873475609756
Testing Acc of Epoch 99: 0.7471043478260869
Early stopping not satisfied.
