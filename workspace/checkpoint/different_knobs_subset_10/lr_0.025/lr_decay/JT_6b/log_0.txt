train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.025
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.025/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.025
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 4.9951e-01 (4.9951e-01)	Acc 0.310547 (0.310547)
Epoch: [0][300/616]	Loss 2.4675e-01 (2.8126e-01)	Acc 0.735352 (0.702765)
Epoch: [0][600/616]	Loss 2.3557e-01 (2.6631e-01)	Acc 0.756836 (0.720648)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.741826)
Training Loss of Epoch 0: 0.2659128143777692
Training Acc of Epoch 0: 0.7210254700203252
Testing Acc of Epoch 0: 0.7418260869565217
Model with the best training loss saved! The loss is 0.2659128143777692
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4049e-01 (2.4049e-01)	Acc 0.750000 (0.750000)
Epoch: [1][300/616]	Loss 2.4929e-01 (2.4895e-01)	Acc 0.749023 (0.739563)
Epoch: [1][600/616]	Loss 2.5198e-01 (2.4830e-01)	Acc 0.734375 (0.740101)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.731674)
Training Loss of Epoch 1: 0.2484758207953073
Training Acc of Epoch 1: 0.7399247332317073
Testing Acc of Epoch 1: 0.7316739130434783
Model with the best training loss saved! The loss is 0.2484758207953073
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.6375e-01 (2.6375e-01)	Acc 0.698242 (0.698242)
Epoch: [2][300/616]	Loss 2.3012e-01 (2.4764e-01)	Acc 0.757812 (0.740896)
Epoch: [2][600/616]	Loss 2.4400e-01 (2.4696e-01)	Acc 0.744141 (0.741544)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.740191)
Training Loss of Epoch 2: 0.24686993992910153
Training Acc of Epoch 2: 0.7416206173780487
Testing Acc of Epoch 2: 0.7401913043478261
Model with the best training loss saved! The loss is 0.24686993992910153
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.4606e-01 (2.4606e-01)	Acc 0.740234 (0.740234)
Epoch: [3][300/616]	Loss 2.6970e-01 (2.5002e-01)	Acc 0.717773 (0.738729)
Epoch: [3][600/616]	Loss 2.4197e-01 (2.4902e-01)	Acc 0.745117 (0.739352)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745630)
Training Loss of Epoch 3: 0.24898617090733072
Training Acc of Epoch 3: 0.7393594385162602
Testing Acc of Epoch 3: 0.7456304347826087
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.5278e-01 (2.5278e-01)	Acc 0.732422 (0.732422)
Epoch: [4][300/616]	Loss 2.4707e-01 (2.4850e-01)	Acc 0.743164 (0.740098)
Epoch: [4][600/616]	Loss 2.6724e-01 (2.4969e-01)	Acc 0.730469 (0.738765)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.744065)
Training Loss of Epoch 4: 0.24973138890615323
Training Acc of Epoch 4: 0.7386798145325203
Testing Acc of Epoch 4: 0.7440652173913044
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3367e-01 (2.3367e-01)	Acc 0.757812 (0.757812)
Epoch: [5][300/616]	Loss 2.8079e-01 (2.5345e-01)	Acc 0.703125 (0.735186)
Epoch: [5][600/616]	Loss 2.4483e-01 (2.5293e-01)	Acc 0.740234 (0.735763)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.741674)
Training Loss of Epoch 5: 0.2527960080683716
Training Acc of Epoch 5: 0.7359311483739838
Testing Acc of Epoch 5: 0.7416739130434783
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.5460e-01 (2.5460e-01)	Acc 0.716797 (0.716797)
Epoch: [6][300/616]	Loss 2.3150e-01 (2.5304e-01)	Acc 0.771484 (0.735595)
Epoch: [6][600/616]	Loss 2.6194e-01 (2.5255e-01)	Acc 0.741211 (0.735680)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742957)
Training Loss of Epoch 6: 0.25245719748299295
Training Acc of Epoch 6: 0.7358025279471545
Testing Acc of Epoch 6: 0.7429565217391304
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.6057e-01 (2.6057e-01)	Acc 0.728516 (0.728516)
Epoch: [7][300/616]	Loss 2.3458e-01 (2.5138e-01)	Acc 0.761719 (0.737162)
Epoch: [7][600/616]	Loss 2.5745e-01 (2.5199e-01)	Acc 0.738281 (0.736700)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.738091)
Training Loss of Epoch 7: 0.2518396690366714
Training Acc of Epoch 7: 0.7368251397357723
Testing Acc of Epoch 7: 0.7380913043478261
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.2274e-01 (2.2274e-01)	Acc 0.769531 (0.769531)
Epoch: [8][300/616]	Loss 2.6554e-01 (2.5295e-01)	Acc 0.718750 (0.734745)
Epoch: [8][600/616]	Loss 2.4019e-01 (2.5325e-01)	Acc 0.745117 (0.734742)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.741170)
Training Loss of Epoch 8: 0.2531028599758458
Training Acc of Epoch 8: 0.7349244156504066
Testing Acc of Epoch 8: 0.7411695652173913
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.4387e-01 (2.4387e-01)	Acc 0.739258 (0.739258)
Epoch: [9][300/616]	Loss 2.5511e-01 (2.5026e-01)	Acc 0.734375 (0.738343)
Epoch: [9][600/616]	Loss 2.5355e-01 (2.5065e-01)	Acc 0.727539 (0.737664)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.743526)
Training Loss of Epoch 9: 0.25069446350500835
Training Acc of Epoch 9: 0.7375111153455285
Testing Acc of Epoch 9: 0.7435260869565218
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.2618e-01 (2.2618e-01)	Acc 0.767578 (0.767578)
Epoch: [10][300/616]	Loss 2.4231e-01 (2.5317e-01)	Acc 0.752930 (0.734683)
Epoch: [10][600/616]	Loss 2.6179e-01 (2.5233e-01)	Acc 0.736328 (0.735568)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.736983)
Training Loss of Epoch 10: 0.2522175029525912
Training Acc of Epoch 10: 0.735646913109756
Testing Acc of Epoch 10: 0.7369826086956521
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.5925e-01 (2.5925e-01)	Acc 0.723633 (0.723633)
Epoch: [11][300/616]	Loss 2.4976e-01 (2.5039e-01)	Acc 0.741211 (0.737211)
Epoch: [11][600/616]	Loss 2.5837e-01 (2.5210e-01)	Acc 0.728516 (0.735308)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.735622)
Training Loss of Epoch 11: 0.2520764057713796
Training Acc of Epoch 11: 0.7354277820121952
Testing Acc of Epoch 11: 0.7356217391304348
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.4147e-01 (2.4147e-01)	Acc 0.744141 (0.744141)
Epoch: [12][300/616]	Loss 2.4701e-01 (2.5259e-01)	Acc 0.734375 (0.735952)
Epoch: [12][600/616]	Loss 2.4422e-01 (2.5188e-01)	Acc 0.742188 (0.736655)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.739257)
Training Loss of Epoch 12: 0.2517137497905793
Training Acc of Epoch 12: 0.7368727769308943
Testing Acc of Epoch 12: 0.7392565217391305
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4799e-01 (2.4799e-01)	Acc 0.744141 (0.744141)
Epoch: [13][300/616]	Loss 2.2901e-01 (2.5167e-01)	Acc 0.765625 (0.736623)
Epoch: [13][600/616]	Loss 2.4276e-01 (2.5337e-01)	Acc 0.747070 (0.734734)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741000)
Training Loss of Epoch 13: 0.253212876848089
Training Acc of Epoch 13: 0.7349053607723577
Testing Acc of Epoch 13: 0.741
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.5811e-01 (2.5811e-01)	Acc 0.720703 (0.720703)
Epoch: [14][300/616]	Loss 2.6494e-01 (2.5162e-01)	Acc 0.708008 (0.736494)
Epoch: [14][600/616]	Loss 2.5846e-01 (2.5296e-01)	Acc 0.727539 (0.735057)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.713570)
Training Loss of Epoch 14: 0.2530246545386508
Training Acc of Epoch 14: 0.7349799923780488
Testing Acc of Epoch 14: 0.7135695652173913
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.8519e-01 (2.8519e-01)	Acc 0.698242 (0.698242)
Epoch: [15][300/616]	Loss 2.3850e-01 (2.5331e-01)	Acc 0.745117 (0.734738)
Epoch: [15][600/616]	Loss 2.4995e-01 (2.5276e-01)	Acc 0.734375 (0.735664)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.713141 (0.732161)
Training Loss of Epoch 15: 0.2530594369260276
Training Acc of Epoch 15: 0.7352991615853659
Testing Acc of Epoch 15: 0.7321608695652174
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4858e-01 (2.4858e-01)	Acc 0.746094 (0.746094)
Epoch: [16][300/616]	Loss 2.4699e-01 (2.5306e-01)	Acc 0.750000 (0.735426)
Epoch: [16][600/616]	Loss 2.5439e-01 (2.5277e-01)	Acc 0.732422 (0.735465)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.741017)
Training Loss of Epoch 16: 0.2526384219648392
Training Acc of Epoch 16: 0.7356389735772357
Testing Acc of Epoch 16: 0.7410173913043478
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4185e-01 (2.4185e-01)	Acc 0.748047 (0.748047)
Epoch: [17][300/616]	Loss 2.3947e-01 (2.5192e-01)	Acc 0.754883 (0.736296)
Epoch: [17][600/616]	Loss 2.4549e-01 (2.5142e-01)	Acc 0.740234 (0.736429)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.734874)
Training Loss of Epoch 17: 0.25141452403087927
Training Acc of Epoch 17: 0.7364710365853658
Testing Acc of Epoch 17: 0.7348739130434783
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.6065e-01 (2.6065e-01)	Acc 0.741211 (0.741211)
Epoch: [18][300/616]	Loss 2.5883e-01 (2.5427e-01)	Acc 0.724609 (0.733778)
Epoch: [18][600/616]	Loss 2.4712e-01 (2.5339e-01)	Acc 0.743164 (0.734705)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.745926)
Training Loss of Epoch 18: 0.2534656786094836
Training Acc of Epoch 18: 0.7346354166666667
Testing Acc of Epoch 18: 0.7459260869565217
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4657e-01 (2.4657e-01)	Acc 0.749023 (0.749023)
Epoch: [19][300/616]	Loss 2.3494e-01 (2.5199e-01)	Acc 0.757812 (0.736088)
Epoch: [19][600/616]	Loss 2.5314e-01 (2.5151e-01)	Acc 0.720703 (0.736377)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.742783)
Training Loss of Epoch 19: 0.2514802989194064
Training Acc of Epoch 19: 0.7364440421747968
Testing Acc of Epoch 19: 0.7427826086956522
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4841e-01 (2.4841e-01)	Acc 0.733398 (0.733398)
Epoch: [20][300/616]	Loss 2.6336e-01 (2.4998e-01)	Acc 0.724609 (0.738028)
Epoch: [20][600/616]	Loss 2.5284e-01 (2.5084e-01)	Acc 0.723633 (0.737007)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.727561)
Training Loss of Epoch 20: 0.2507676921724304
Training Acc of Epoch 20: 0.737061737804878
Testing Acc of Epoch 20: 0.7275608695652174
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4398e-01 (2.4398e-01)	Acc 0.759766 (0.759766)
Epoch: [21][300/616]	Loss 2.3712e-01 (2.5259e-01)	Acc 0.755859 (0.734988)
Epoch: [21][600/616]	Loss 2.5973e-01 (2.5386e-01)	Acc 0.727539 (0.734144)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740204)
Training Loss of Epoch 21: 0.2539448178153697
Training Acc of Epoch 21: 0.7341114075203252
Testing Acc of Epoch 21: 0.7402043478260869
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_6b
different_width False
resnet18_width 64
weight_precision 6
bias_precision 6
act_precision 9
batch_norm False
dropout False
exp_num 5
lr 0.025
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.025/lr_decay/JT_6b/
file_prefix exp_0
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_6b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=9, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=6, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.025
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0160e-01 (5.0160e-01)	Acc 0.082031 (0.082031)
Epoch: [0][300/616]	Loss 2.6454e-01 (2.7399e-01)	Acc 0.713867 (0.707109)
Epoch: [0][600/616]	Loss 2.4311e-01 (2.6200e-01)	Acc 0.742188 (0.722838)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.739917)
Training Loss of Epoch 0: 0.26166127852308074
Training Acc of Epoch 0: 0.723267594004065
Testing Acc of Epoch 0: 0.7399173913043479
Model with the best training loss saved! The loss is 0.26166127852308074
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4585e-01 (2.4585e-01)	Acc 0.744141 (0.744141)
Epoch: [1][300/616]	Loss 2.6101e-01 (2.4479e-01)	Acc 0.714844 (0.743446)
Epoch: [1][600/616]	Loss 2.4767e-01 (2.4709e-01)	Acc 0.732422 (0.741104)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.735143)
Training Loss of Epoch 1: 0.24708254002943272
Training Acc of Epoch 1: 0.7411283663617886
Testing Acc of Epoch 1: 0.7351434782608696
Model with the best training loss saved! The loss is 0.24708254002943272
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4082e-01 (2.4082e-01)	Acc 0.756836 (0.756836)
Epoch: [2][300/616]	Loss 2.4389e-01 (2.4920e-01)	Acc 0.758789 (0.739910)
Epoch: [2][600/616]	Loss 2.2209e-01 (2.4956e-01)	Acc 0.779297 (0.739172)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.747100)
Training Loss of Epoch 2: 0.24965270394232214
Training Acc of Epoch 2: 0.7389735772357724
Testing Acc of Epoch 2: 0.7471
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3872e-01 (2.3872e-01)	Acc 0.753906 (0.753906)
Epoch: [3][300/616]	Loss 2.3431e-01 (2.4996e-01)	Acc 0.756836 (0.739196)
Epoch: [3][600/616]	Loss 2.3676e-01 (2.4876e-01)	Acc 0.744141 (0.739960)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.741809)
Training Loss of Epoch 3: 0.2487629533056321
Training Acc of Epoch 3: 0.7399294969512196
Testing Acc of Epoch 3: 0.7418086956521739
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.4574e-01 (2.4574e-01)	Acc 0.754883 (0.754883)
Epoch: [4][300/616]	Loss 2.4693e-01 (2.4978e-01)	Acc 0.731445 (0.738203)
Epoch: [4][600/616]	Loss 2.4783e-01 (2.4965e-01)	Acc 0.748047 (0.738215)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.741409)
Training Loss of Epoch 4: 0.24957957262915326
Training Acc of Epoch 4: 0.7383003048780488
Testing Acc of Epoch 4: 0.7414086956521739
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.3290e-01 (2.3290e-01)	Acc 0.756836 (0.756836)
Epoch: [5][300/616]	Loss 2.4264e-01 (2.4923e-01)	Acc 0.739258 (0.739683)
Epoch: [5][600/616]	Loss 2.4080e-01 (2.5026e-01)	Acc 0.759766 (0.738244)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.744270)
Training Loss of Epoch 5: 0.2500877582445377
Training Acc of Epoch 5: 0.7385146722560976
Testing Acc of Epoch 5: 0.7442695652173913
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4819e-01 (2.4819e-01)	Acc 0.732422 (0.732422)
Epoch: [6][300/616]	Loss 2.4756e-01 (2.4920e-01)	Acc 0.753906 (0.739264)
Epoch: [6][600/616]	Loss 2.7170e-01 (2.4948e-01)	Acc 0.713867 (0.739256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.742630)
Training Loss of Epoch 6: 0.24951029402938316
Training Acc of Epoch 6: 0.7392022357723578
Testing Acc of Epoch 6: 0.7426304347826087
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.3773e-01 (2.3773e-01)	Acc 0.746094 (0.746094)
Epoch: [7][300/616]	Loss 2.6266e-01 (2.5254e-01)	Acc 0.728516 (0.736166)
Epoch: [7][600/616]	Loss 2.4664e-01 (2.5191e-01)	Acc 0.739258 (0.737035)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.743070)
Training Loss of Epoch 7: 0.25205961860776915
Training Acc of Epoch 7: 0.7367600355691057
Testing Acc of Epoch 7: 0.7430695652173913
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.5108e-01 (2.5108e-01)	Acc 0.735352 (0.735352)
Epoch: [8][300/616]	Loss 2.5716e-01 (2.5057e-01)	Acc 0.717773 (0.738236)
Epoch: [8][600/616]	Loss 2.3427e-01 (2.5085e-01)	Acc 0.758789 (0.737839)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.744287)
Training Loss of Epoch 8: 0.25095316772538473
Training Acc of Epoch 8: 0.7377334222560976
Testing Acc of Epoch 8: 0.7442869565217392
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.3421e-01 (2.3421e-01)	Acc 0.757812 (0.757812)
Epoch: [9][300/616]	Loss 2.4591e-01 (2.5067e-01)	Acc 0.735352 (0.737954)
Epoch: [9][600/616]	Loss 2.4949e-01 (2.5092e-01)	Acc 0.742188 (0.738067)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.746813)
Training Loss of Epoch 9: 0.25091986154637685
Training Acc of Epoch 9: 0.7379938389227643
Testing Acc of Epoch 9: 0.7468130434782608
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5107e-01 (2.5107e-01)	Acc 0.728516 (0.728516)
Epoch: [10][300/616]	Loss 2.4704e-01 (2.5380e-01)	Acc 0.747070 (0.733833)
Epoch: [10][600/616]	Loss 2.6507e-01 (2.5234e-01)	Acc 0.719727 (0.735933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.727183)
Training Loss of Epoch 10: 0.2525179200540713
Training Acc of Epoch 10: 0.735692962398374
Testing Acc of Epoch 10: 0.7271826086956522
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.8718e-01 (2.8718e-01)	Acc 0.705078 (0.705078)
Epoch: [11][300/616]	Loss 2.3776e-01 (2.5353e-01)	Acc 0.758789 (0.735056)
Epoch: [11][600/616]	Loss 2.3633e-01 (2.5309e-01)	Acc 0.758789 (0.735228)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.737435)
Training Loss of Epoch 11: 0.25307006799593207
Training Acc of Epoch 11: 0.7352594639227642
Testing Acc of Epoch 11: 0.7374347826086957
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.5143e-01 (2.5143e-01)	Acc 0.735352 (0.735352)
Epoch: [12][300/616]	Loss 2.6221e-01 (2.5326e-01)	Acc 0.716797 (0.734797)
Epoch: [12][600/616]	Loss 2.5969e-01 (2.5263e-01)	Acc 0.733398 (0.735280)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745896)
Training Loss of Epoch 12: 0.2524617757496795
Training Acc of Epoch 12: 0.7354674796747968
Testing Acc of Epoch 12: 0.7458956521739131
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.3619e-01 (2.3619e-01)	Acc 0.758789 (0.758789)
Epoch: [13][300/616]	Loss 2.4682e-01 (2.5237e-01)	Acc 0.738281 (0.735673)
Epoch: [13][600/616]	Loss 2.3272e-01 (2.5171e-01)	Acc 0.760742 (0.736658)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.736970)
Training Loss of Epoch 13: 0.2517178881701415
Training Acc of Epoch 13: 0.7366282393292682
Testing Acc of Epoch 13: 0.7369695652173913
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.7200e-01 (2.7200e-01)	Acc 0.716797 (0.716797)
Epoch: [14][300/616]	Loss 2.4504e-01 (2.5060e-01)	Acc 0.749023 (0.738119)
Epoch: [14][600/616]	Loss 2.5135e-01 (2.5108e-01)	Acc 0.730469 (0.737340)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.732922)
Training Loss of Epoch 14: 0.2510963164693941
Training Acc of Epoch 14: 0.7373904344512195
Testing Acc of Epoch 14: 0.7329217391304348
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.7094e-01 (2.7094e-01)	Acc 0.708984 (0.708984)
Epoch: [15][300/616]	Loss 2.4888e-01 (2.5284e-01)	Acc 0.736328 (0.735439)
Epoch: [15][600/616]	Loss 2.5453e-01 (2.5196e-01)	Acc 0.735352 (0.736612)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.743943)
Training Loss of Epoch 15: 0.2518875775540747
Training Acc of Epoch 15: 0.7366599974593496
Testing Acc of Epoch 15: 0.7439434782608696
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4499e-01 (2.4499e-01)	Acc 0.730469 (0.730469)
Epoch: [16][300/616]	Loss 2.4433e-01 (2.5153e-01)	Acc 0.747070 (0.736954)
Epoch: [16][600/616]	Loss 2.7734e-01 (2.5192e-01)	Acc 0.704102 (0.736037)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.738248)
Training Loss of Epoch 16: 0.2520464121810789
Training Acc of Epoch 16: 0.7358612804878049
Testing Acc of Epoch 16: 0.7382478260869565
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4928e-01 (2.4928e-01)	Acc 0.745117 (0.745117)
Epoch: [17][300/616]	Loss 2.7122e-01 (2.5043e-01)	Acc 0.706055 (0.737743)
Epoch: [17][600/616]	Loss 2.4383e-01 (2.5130e-01)	Acc 0.753906 (0.737108)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.742065)
Training Loss of Epoch 17: 0.2513936476009648
Training Acc of Epoch 17: 0.7370776168699187
Testing Acc of Epoch 17: 0.7420652173913044
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.5364e-01 (2.5364e-01)	Acc 0.731445 (0.731445)
Epoch: [18][300/616]	Loss 2.5062e-01 (2.5005e-01)	Acc 0.736328 (0.738320)
Epoch: [18][600/616]	Loss 2.4403e-01 (2.4981e-01)	Acc 0.733398 (0.738492)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.742817)
Training Loss of Epoch 18: 0.2497598621661101
Training Acc of Epoch 18: 0.7385321392276423
Testing Acc of Epoch 18: 0.7428173913043479
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.5454e-01 (2.5454e-01)	Acc 0.732422 (0.732422)
Epoch: [19][300/616]	Loss 2.7000e-01 (2.4956e-01)	Acc 0.707031 (0.738602)
Epoch: [19][600/616]	Loss 2.4247e-01 (2.5100e-01)	Acc 0.747070 (0.737256)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744183)
Training Loss of Epoch 19: 0.25087988267584543
Training Acc of Epoch 19: 0.7373269181910569
Testing Acc of Epoch 19: 0.7441826086956522
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.5010e-01 (2.5010e-01)	Acc 0.745117 (0.745117)
Epoch: [20][300/616]	Loss 2.3795e-01 (2.5076e-01)	Acc 0.752930 (0.736987)
Epoch: [20][600/616]	Loss 2.4260e-01 (2.5110e-01)	Acc 0.757812 (0.737345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.737291)
Training Loss of Epoch 20: 0.25122577722479655
Training Acc of Epoch 20: 0.7372030614837398
Testing Acc of Epoch 20: 0.7372913043478261
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.5889e-01 (2.5889e-01)	Acc 0.725586 (0.725586)
Epoch: [21][300/616]	Loss 2.5598e-01 (2.5015e-01)	Acc 0.738281 (0.738466)
Epoch: [21][600/616]	Loss 2.5033e-01 (2.5044e-01)	Acc 0.732422 (0.737948)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.731778)
Training Loss of Epoch 21: 0.25046402443715227
Training Acc of Epoch 21: 0.7379334984756097
Testing Acc of Epoch 21: 0.7317782608695652
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4578e-01 (2.4578e-01)	Acc 0.761719 (0.761719)
Epoch: [22][300/616]	Loss 2.7909e-01 (2.4982e-01)	Acc 0.704102 (0.737529)
Epoch: [22][600/616]	Loss 2.2912e-01 (2.4884e-01)	Acc 0.754883 (0.739204)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.741996)
Training Loss of Epoch 22: 0.24875244447370856
Training Acc of Epoch 22: 0.7394007240853658
Testing Acc of Epoch 22: 0.7419956521739131
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.2854e-01 (2.2854e-01)	Acc 0.769531 (0.769531)
Epoch: [23][300/616]	Loss 2.5360e-01 (2.4951e-01)	Acc 0.744141 (0.739122)
Epoch: [23][600/616]	Loss 2.5995e-01 (2.4887e-01)	Acc 0.721680 (0.739437)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746522)
Training Loss of Epoch 23: 0.24884714452716394
Training Acc of Epoch 23: 0.7393880208333333
Testing Acc of Epoch 23: 0.7465217391304347
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.5144e-01 (2.5144e-01)	Acc 0.737305 (0.737305)
Epoch: [24][300/616]	Loss 2.7144e-01 (2.4942e-01)	Acc 0.717773 (0.738615)
Epoch: [24][600/616]	Loss 2.4346e-01 (2.4994e-01)	Acc 0.734375 (0.738207)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.741652)
Training Loss of Epoch 24: 0.24974432931682927
Training Acc of Epoch 24: 0.7384511559959349
Testing Acc of Epoch 24: 0.7416521739130435
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4842e-01 (2.4842e-01)	Acc 0.738281 (0.738281)
Epoch: [25][300/616]	Loss 2.4641e-01 (2.4884e-01)	Acc 0.748047 (0.738667)
Epoch: [25][600/616]	Loss 2.6055e-01 (2.4888e-01)	Acc 0.730469 (0.739588)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.739283)
Training Loss of Epoch 25: 0.24891539351726935
Training Acc of Epoch 25: 0.739553163109756
Testing Acc of Epoch 25: 0.7392826086956522
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.5445e-01 (2.5445e-01)	Acc 0.729492 (0.729492)
Epoch: [26][300/616]	Loss 2.5667e-01 (2.4866e-01)	Acc 0.738281 (0.739478)
Epoch: [26][600/616]	Loss 2.4879e-01 (2.4883e-01)	Acc 0.737305 (0.739263)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.744787)
Training Loss of Epoch 26: 0.2488048561462542
Training Acc of Epoch 26: 0.7393118013211382
Testing Acc of Epoch 26: 0.7447869565217391
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.5300e-01 (2.5300e-01)	Acc 0.740234 (0.740234)
Epoch: [27][300/616]	Loss 2.4469e-01 (2.4905e-01)	Acc 0.746094 (0.739177)
Epoch: [27][600/616]	Loss 2.3719e-01 (2.4916e-01)	Acc 0.744141 (0.738673)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.741696)
Training Loss of Epoch 27: 0.2493087731483506
Training Acc of Epoch 27: 0.7385861280487804
Testing Acc of Epoch 27: 0.7416956521739131
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.4426e-01 (2.4426e-01)	Acc 0.755859 (0.755859)
Epoch: [28][300/616]	Loss 2.3240e-01 (2.5046e-01)	Acc 0.755859 (0.737347)
Epoch: [28][600/616]	Loss 2.6342e-01 (2.4983e-01)	Acc 0.726562 (0.738475)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.708333 (0.717487)
Training Loss of Epoch 28: 0.2499935187217666
Training Acc of Epoch 28: 0.7382034425813008
Testing Acc of Epoch 28: 0.7174869565217391
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.7644e-01 (2.7644e-01)	Acc 0.701172 (0.701172)
Epoch: [29][300/616]	Loss 2.3726e-01 (2.4951e-01)	Acc 0.766602 (0.739417)
Epoch: [29][600/616]	Loss 2.2239e-01 (2.4946e-01)	Acc 0.769531 (0.738634)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.741548)
Training Loss of Epoch 29: 0.24932568235610558
Training Acc of Epoch 29: 0.738768737296748
Testing Acc of Epoch 29: 0.7415478260869566
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.5457e-01 (2.5457e-01)	Acc 0.736328 (0.736328)
Epoch: [30][300/616]	Loss 2.3771e-01 (2.4776e-01)	Acc 0.765625 (0.741078)
Epoch: [30][600/616]	Loss 2.5509e-01 (2.4783e-01)	Acc 0.723633 (0.740525)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.743217)
Training Loss of Epoch 30: 0.24791544091895343
Training Acc of Epoch 30: 0.7404852642276423
Testing Acc of Epoch 30: 0.7432173913043478
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.4523e-01 (2.4523e-01)	Acc 0.746094 (0.746094)
Epoch: [31][300/616]	Loss 2.4969e-01 (2.4760e-01)	Acc 0.744141 (0.741049)
Epoch: [31][600/616]	Loss 2.5221e-01 (2.4797e-01)	Acc 0.730469 (0.740649)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.741326)
Training Loss of Epoch 31: 0.24797012658138584
Training Acc of Epoch 31: 0.7406837525406504
Testing Acc of Epoch 31: 0.7413260869565217
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4703e-01 (2.4703e-01)	Acc 0.745117 (0.745117)
Epoch: [32][300/616]	Loss 2.6007e-01 (2.4634e-01)	Acc 0.728516 (0.741925)
Epoch: [32][600/616]	Loss 2.4803e-01 (2.4838e-01)	Acc 0.749023 (0.739945)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745965)
Training Loss of Epoch 32: 0.24825264460187618
Training Acc of Epoch 32: 0.7401279852642276
Testing Acc of Epoch 32: 0.7459652173913044
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.3534e-01 (2.3534e-01)	Acc 0.752930 (0.752930)
Epoch: [33][300/616]	Loss 2.4263e-01 (2.4866e-01)	Acc 0.761719 (0.739706)
Epoch: [33][600/616]	Loss 2.5587e-01 (2.4921e-01)	Acc 0.736328 (0.738918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.742448)
Training Loss of Epoch 33: 0.24917767714194167
Training Acc of Epoch 33: 0.7389370553861788
Testing Acc of Epoch 33: 0.7424478260869565
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.4206e-01 (2.4206e-01)	Acc 0.749023 (0.749023)
Epoch: [34][300/616]	Loss 2.5317e-01 (2.4777e-01)	Acc 0.731445 (0.740711)
Epoch: [34][600/616]	Loss 2.3508e-01 (2.4846e-01)	Acc 0.752930 (0.739711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.727057)
Training Loss of Epoch 34: 0.24887535996068783
Training Acc of Epoch 34: 0.7391815929878048
Testing Acc of Epoch 34: 0.7270565217391305
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.6024e-01 (2.6024e-01)	Acc 0.730469 (0.730469)
Epoch: [35][300/616]	Loss 2.5585e-01 (2.5086e-01)	Acc 0.728516 (0.737726)
Epoch: [35][600/616]	Loss 2.5014e-01 (2.5073e-01)	Acc 0.755859 (0.737670)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.723026)
Training Loss of Epoch 35: 0.25091580145242737
Training Acc of Epoch 35: 0.7374491869918699
Testing Acc of Epoch 35: 0.7230260869565217
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.7128e-01 (2.7128e-01)	Acc 0.721680 (0.721680)
Epoch: [36][300/616]	Loss 2.4646e-01 (2.5047e-01)	Acc 0.749023 (0.738077)
Epoch: [36][600/616]	Loss 2.2876e-01 (2.5089e-01)	Acc 0.754883 (0.737345)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.693910 (0.711961)
Training Loss of Epoch 36: 0.2509918492983996
Training Acc of Epoch 36: 0.7372713414634147
Testing Acc of Epoch 36: 0.7119608695652174
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.6806e-01 (2.6806e-01)	Acc 0.716797 (0.716797)
Epoch: [37][300/616]	Loss 2.6272e-01 (2.5368e-01)	Acc 0.739258 (0.734586)
Epoch: [37][600/616]	Loss 2.4605e-01 (2.5258e-01)	Acc 0.763672 (0.735826)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.740917)
Training Loss of Epoch 37: 0.2525722084006643
Training Acc of Epoch 37: 0.7358771595528455
Testing Acc of Epoch 37: 0.7409173913043479
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.3434e-01 (2.3434e-01)	Acc 0.755859 (0.755859)
Epoch: [38][300/616]	Loss 2.6062e-01 (2.5295e-01)	Acc 0.721680 (0.735098)
Epoch: [38][600/616]	Loss 2.6223e-01 (2.5354e-01)	Acc 0.712891 (0.734708)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.744748)
Training Loss of Epoch 38: 0.25355164980500694
Training Acc of Epoch 38: 0.7346846417682927
Testing Acc of Epoch 38: 0.7447478260869566
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.3689e-01 (2.3689e-01)	Acc 0.751953 (0.751953)
Epoch: [39][300/616]	Loss 2.5690e-01 (2.5292e-01)	Acc 0.742188 (0.735446)
Epoch: [39][600/616]	Loss 2.5638e-01 (2.5362e-01)	Acc 0.736328 (0.734820)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737148)
Training Loss of Epoch 39: 0.25368319735294437
Training Acc of Epoch 39: 0.7347021087398374
Testing Acc of Epoch 39: 0.7371478260869565
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.3745e-01 (2.3745e-01)	Acc 0.753906 (0.753906)
Epoch: [40][300/616]	Loss 2.6433e-01 (2.5260e-01)	Acc 0.717773 (0.736448)
Epoch: [40][600/616]	Loss 2.2707e-01 (2.5392e-01)	Acc 0.773438 (0.734890)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.739452)
Training Loss of Epoch 40: 0.25386917879426385
Training Acc of Epoch 40: 0.7348751905487805
Testing Acc of Epoch 40: 0.7394521739130435
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.5722e-01 (2.5722e-01)	Acc 0.744141 (0.744141)
Epoch: [41][300/616]	Loss 2.4922e-01 (2.5223e-01)	Acc 0.752930 (0.735760)
Epoch: [41][600/616]	Loss 2.5924e-01 (2.5291e-01)	Acc 0.726562 (0.734999)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.739830)
Training Loss of Epoch 41: 0.2530005106596443
Training Acc of Epoch 41: 0.7350165142276422
Testing Acc of Epoch 41: 0.7398304347826087
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.5156e-01 (2.5156e-01)	Acc 0.729492 (0.729492)
Epoch: [42][300/616]	Loss 2.4435e-01 (2.5487e-01)	Acc 0.738281 (0.734018)
Epoch: [42][600/616]	Loss 2.2898e-01 (2.5372e-01)	Acc 0.763672 (0.734804)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.734883)
Training Loss of Epoch 42: 0.25355991917412457
Training Acc of Epoch 42: 0.7350323932926829
Testing Acc of Epoch 42: 0.7348826086956521
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.5625e-01 (2.5625e-01)	Acc 0.749023 (0.749023)
Epoch: [43][300/616]	Loss 2.5386e-01 (2.5310e-01)	Acc 0.740234 (0.735507)
Epoch: [43][600/616]	Loss 2.8345e-01 (2.5369e-01)	Acc 0.707031 (0.734281)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.742126)
Training Loss of Epoch 43: 0.2536314493514658
Training Acc of Epoch 43: 0.7344115218495935
Testing Acc of Epoch 43: 0.7421260869565217
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.4378e-01 (2.4378e-01)	Acc 0.756836 (0.756836)
Epoch: [44][300/616]	Loss 2.5524e-01 (2.5270e-01)	Acc 0.730469 (0.735030)
Epoch: [44][600/616]	Loss 2.6810e-01 (2.5254e-01)	Acc 0.703125 (0.735563)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.741300)
Training Loss of Epoch 44: 0.25248518348709353
Training Acc of Epoch 44: 0.7356056275406504
Testing Acc of Epoch 44: 0.7413
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.4024e-01 (2.4024e-01)	Acc 0.746094 (0.746094)
Epoch: [45][300/616]	Loss 2.3496e-01 (2.5231e-01)	Acc 0.757812 (0.735874)
Epoch: [45][600/616]	Loss 2.4507e-01 (2.5266e-01)	Acc 0.735352 (0.735945)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.709936 (0.731091)
Training Loss of Epoch 45: 0.25267933091012446
Training Acc of Epoch 45: 0.7358263465447155
Testing Acc of Epoch 45: 0.7310913043478261
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.5971e-01 (2.5971e-01)	Acc 0.730469 (0.730469)
Epoch: [46][300/616]	Loss 2.3379e-01 (2.5318e-01)	Acc 0.764648 (0.735069)
Epoch: [46][600/616]	Loss 2.2512e-01 (2.5260e-01)	Acc 0.771484 (0.735506)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.742561)
Training Loss of Epoch 46: 0.25252284305851636
Training Acc of Epoch 46: 0.7355976880081301
Testing Acc of Epoch 46: 0.7425608695652174
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.5636e-01 (2.5636e-01)	Acc 0.733398 (0.733398)
Epoch: [47][300/616]	Loss 2.4661e-01 (2.5157e-01)	Acc 0.747070 (0.737477)
Epoch: [47][600/616]	Loss 2.5931e-01 (2.5230e-01)	Acc 0.732422 (0.736219)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.734978)
Training Loss of Epoch 47: 0.25243104776231257
Training Acc of Epoch 47: 0.7361105818089431
Testing Acc of Epoch 47: 0.7349782608695652
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.6482e-01 (2.6482e-01)	Acc 0.720703 (0.720703)
Epoch: [48][300/616]	Loss 2.5748e-01 (2.5465e-01)	Acc 0.729492 (0.733905)
Epoch: [48][600/616]	Loss 2.4830e-01 (2.5361e-01)	Acc 0.738281 (0.734529)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.740570)
Training Loss of Epoch 48: 0.2536970535914103
Training Acc of Epoch 48: 0.7344877413617886
Testing Acc of Epoch 48: 0.7405695652173913
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4503e-01 (2.4503e-01)	Acc 0.755859 (0.755859)
Epoch: [49][300/616]	Loss 2.5207e-01 (2.5326e-01)	Acc 0.725586 (0.735024)
Epoch: [49][600/616]	Loss 2.4563e-01 (2.5359e-01)	Acc 0.739258 (0.734975)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.742657)
Training Loss of Epoch 49: 0.25348614047213297
Training Acc of Epoch 49: 0.7351022611788618
Testing Acc of Epoch 49: 0.7426565217391304
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.5130e-01 (2.5130e-01)	Acc 0.730469 (0.730469)
Epoch: [50][300/616]	Loss 2.4705e-01 (2.5349e-01)	Acc 0.744141 (0.734372)
Epoch: [50][600/616]	Loss 2.7366e-01 (2.5310e-01)	Acc 0.719727 (0.735350)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735187)
Training Loss of Epoch 50: 0.2532856561546403
Training Acc of Epoch 50: 0.7351467225609756
Testing Acc of Epoch 50: 0.7351869565217392
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.6085e-01 (2.6085e-01)	Acc 0.720703 (0.720703)
Epoch: [51][300/616]	Loss 2.7733e-01 (2.5447e-01)	Acc 0.704102 (0.734190)
Epoch: [51][600/616]	Loss 2.9215e-01 (2.5453e-01)	Acc 0.692383 (0.733917)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.703526 (0.720752)
Training Loss of Epoch 51: 0.2546466416702038
Training Acc of Epoch 51: 0.733814469004065
Testing Acc of Epoch 51: 0.7207521739130435
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.7754e-01 (2.7754e-01)	Acc 0.710938 (0.710938)
Epoch: [52][300/616]	Loss 2.3525e-01 (2.5406e-01)	Acc 0.752930 (0.734492)
Epoch: [52][600/616]	Loss 2.4397e-01 (2.5321e-01)	Acc 0.746094 (0.735025)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.739361)
Training Loss of Epoch 52: 0.25329490049583153
Training Acc of Epoch 52: 0.7348910696138211
Testing Acc of Epoch 52: 0.7393608695652174
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.5252e-01 (2.5252e-01)	Acc 0.738281 (0.738281)
Epoch: [53][300/616]	Loss 2.5733e-01 (2.5259e-01)	Acc 0.737305 (0.735533)
Epoch: [53][600/616]	Loss 2.8500e-01 (2.5315e-01)	Acc 0.697266 (0.735014)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.737857)
Training Loss of Epoch 53: 0.25315703667276274
Training Acc of Epoch 53: 0.7350085746951219
Testing Acc of Epoch 53: 0.7378565217391304
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.4265e-01 (2.4265e-01)	Acc 0.744141 (0.744141)
Epoch: [54][300/616]	Loss 2.6072e-01 (2.5467e-01)	Acc 0.728516 (0.733577)
Epoch: [54][600/616]	Loss 2.3455e-01 (2.5345e-01)	Acc 0.746094 (0.734793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.743974)
Training Loss of Epoch 54: 0.2534722668126347
Training Acc of Epoch 54: 0.734692581300813
Testing Acc of Epoch 54: 0.7439739130434783
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.5240e-01 (2.5240e-01)	Acc 0.734375 (0.734375)
Epoch: [55][300/616]	Loss 2.6335e-01 (2.5337e-01)	Acc 0.737305 (0.735459)
Epoch: [55][600/616]	Loss 2.5812e-01 (2.5320e-01)	Acc 0.724609 (0.735441)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.739917)
Training Loss of Epoch 55: 0.25316993991533915
Training Acc of Epoch 55: 0.7354674796747968
Testing Acc of Epoch 55: 0.7399173913043479
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.5243e-01 (2.5243e-01)	Acc 0.745117 (0.745117)
Epoch: [56][300/616]	Loss 2.4950e-01 (2.5268e-01)	Acc 0.736328 (0.735446)
Epoch: [56][600/616]	Loss 2.4885e-01 (2.5284e-01)	Acc 0.750000 (0.735355)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.740474)
Training Loss of Epoch 56: 0.2528601902044886
Training Acc of Epoch 56: 0.7353706173780488
Testing Acc of Epoch 56: 0.7404739130434783
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.5577e-01 (2.5577e-01)	Acc 0.730469 (0.730469)
Epoch: [57][300/616]	Loss 2.3837e-01 (2.5320e-01)	Acc 0.750000 (0.734839)
Epoch: [57][600/616]	Loss 2.4623e-01 (2.5483e-01)	Acc 0.746094 (0.733140)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.741643)
Training Loss of Epoch 57: 0.25472638129703395
Training Acc of Epoch 57: 0.7332428226626017
Testing Acc of Epoch 57: 0.7416434782608695
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.2390e-01 (2.2390e-01)	Acc 0.751953 (0.751953)
Epoch: [58][300/616]	Loss 2.3540e-01 (2.5155e-01)	Acc 0.753906 (0.737058)
Epoch: [58][600/616]	Loss 2.4259e-01 (2.5158e-01)	Acc 0.735352 (0.736738)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734691)
Training Loss of Epoch 58: 0.25162386097074524
Training Acc of Epoch 58: 0.7366806402439025
Testing Acc of Epoch 58: 0.734691304347826
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.6704e-01 (2.6704e-01)	Acc 0.722656 (0.722656)
Epoch: [59][300/616]	Loss 2.6721e-01 (2.5375e-01)	Acc 0.724609 (0.734959)
Epoch: [59][600/616]	Loss 2.5684e-01 (2.5309e-01)	Acc 0.737305 (0.735548)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.737474)
Training Loss of Epoch 59: 0.25302006507792124
Training Acc of Epoch 59: 0.735524644308943
Testing Acc of Epoch 59: 0.7374739130434783
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.5638e-01 (2.5638e-01)	Acc 0.720703 (0.720703)
Epoch: [60][300/616]	Loss 2.5727e-01 (2.5083e-01)	Acc 0.729492 (0.737389)
Epoch: [60][600/616]	Loss 2.6588e-01 (2.5293e-01)	Acc 0.713867 (0.735571)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.727483)
Training Loss of Epoch 60: 0.25285959626600996
Training Acc of Epoch 60: 0.7355611661585366
Testing Acc of Epoch 60: 0.7274826086956522
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.6239e-01 (2.6239e-01)	Acc 0.715820 (0.715820)
Epoch: [61][300/616]	Loss 2.5012e-01 (2.5333e-01)	Acc 0.737305 (0.734414)
Epoch: [61][600/616]	Loss 2.4400e-01 (2.5303e-01)	Acc 0.748047 (0.734984)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.737765)
Training Loss of Epoch 61: 0.25303435354697995
Training Acc of Epoch 61: 0.7349323551829269
Testing Acc of Epoch 61: 0.7377652173913043
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4769e-01 (2.4769e-01)	Acc 0.731445 (0.731445)
Epoch: [62][300/616]	Loss 2.4707e-01 (2.5017e-01)	Acc 0.741211 (0.737756)
Epoch: [62][600/616]	Loss 2.4536e-01 (2.5055e-01)	Acc 0.751953 (0.737711)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.735357)
Training Loss of Epoch 62: 0.25044234707588103
Training Acc of Epoch 62: 0.7377334222560976
Testing Acc of Epoch 62: 0.7353565217391305
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.4293e-01 (2.4293e-01)	Acc 0.755859 (0.755859)
Epoch: [63][300/616]	Loss 2.5167e-01 (2.5229e-01)	Acc 0.746094 (0.736189)
Epoch: [63][600/616]	Loss 2.5944e-01 (2.5248e-01)	Acc 0.726562 (0.735795)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.735139)
Training Loss of Epoch 63: 0.2524677034316024
Training Acc of Epoch 63: 0.7357930005081301
Testing Acc of Epoch 63: 0.7351391304347826
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.4061e-01 (2.4061e-01)	Acc 0.756836 (0.756836)
Epoch: [64][300/616]	Loss 2.5231e-01 (2.5028e-01)	Acc 0.743164 (0.737970)
Epoch: [64][600/616]	Loss 2.5075e-01 (2.5091e-01)	Acc 0.728516 (0.737535)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.742730)
Training Loss of Epoch 64: 0.2509232280700187
Training Acc of Epoch 64: 0.7375015879065041
Testing Acc of Epoch 64: 0.7427304347826087
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.2395e-01 (2.2395e-01)	Acc 0.766602 (0.766602)
Epoch: [65][300/616]	Loss 2.7352e-01 (2.5029e-01)	Acc 0.713867 (0.737866)
Epoch: [65][600/616]	Loss 2.5362e-01 (2.5053e-01)	Acc 0.740234 (0.738012)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.734591)
Training Loss of Epoch 65: 0.2504677432339366
Training Acc of Epoch 65: 0.7380589430894309
Testing Acc of Epoch 65: 0.734591304347826
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.6499e-01 (2.6499e-01)	Acc 0.722656 (0.722656)
Epoch: [66][300/616]	Loss 2.4685e-01 (2.5106e-01)	Acc 0.738281 (0.738009)
Epoch: [66][600/616]	Loss 2.4366e-01 (2.5101e-01)	Acc 0.749023 (0.737678)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740826)
Training Loss of Epoch 66: 0.25098839268936374
Training Acc of Epoch 66: 0.7376667301829268
Testing Acc of Epoch 66: 0.7408260869565217
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.5076e-01 (2.5076e-01)	Acc 0.732422 (0.732422)
Epoch: [67][300/616]	Loss 2.6862e-01 (2.5378e-01)	Acc 0.711914 (0.734261)
Epoch: [67][600/616]	Loss 2.6581e-01 (2.5304e-01)	Acc 0.722656 (0.735160)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.726530)
Training Loss of Epoch 67: 0.2530139328260732
Training Acc of Epoch 67: 0.7352467606707317
Testing Acc of Epoch 67: 0.7265304347826087
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.5688e-01 (2.5688e-01)	Acc 0.722656 (0.722656)
Epoch: [68][300/616]	Loss 2.6064e-01 (2.5092e-01)	Acc 0.740234 (0.737159)
Epoch: [68][600/616]	Loss 2.5366e-01 (2.5185e-01)	Acc 0.734375 (0.736554)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.737713)
Training Loss of Epoch 68: 0.2517349960115867
Training Acc of Epoch 68: 0.7366345909552846
Testing Acc of Epoch 68: 0.7377130434782608
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.6258e-01 (2.6258e-01)	Acc 0.714844 (0.714844)
Epoch: [69][300/616]	Loss 2.4175e-01 (2.5130e-01)	Acc 0.749023 (0.737626)
Epoch: [69][600/616]	Loss 2.5921e-01 (2.5066e-01)	Acc 0.718750 (0.737639)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.742265)
Training Loss of Epoch 69: 0.2507109116247999
Training Acc of Epoch 69: 0.7375254065040651
Testing Acc of Epoch 69: 0.7422652173913044
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.4863e-01 (2.4863e-01)	Acc 0.744141 (0.744141)
Epoch: [70][300/616]	Loss 2.4921e-01 (2.4998e-01)	Acc 0.732422 (0.738301)
Epoch: [70][600/616]	Loss 2.5880e-01 (2.4938e-01)	Acc 0.728516 (0.738769)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740935)
Training Loss of Epoch 70: 0.24928948036054285
Training Acc of Epoch 70: 0.7389354674796748
Testing Acc of Epoch 70: 0.7409347826086956
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.5194e-01 (2.5194e-01)	Acc 0.746094 (0.746094)
Epoch: [71][300/616]	Loss 2.7269e-01 (2.5220e-01)	Acc 0.706055 (0.736526)
Epoch: [71][600/616]	Loss 2.4229e-01 (2.5146e-01)	Acc 0.734375 (0.736471)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.740943)
Training Loss of Epoch 71: 0.2513279260900932
Training Acc of Epoch 71: 0.7366012449186992
Testing Acc of Epoch 71: 0.7409434782608696
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.5663e-01 (2.5663e-01)	Acc 0.733398 (0.733398)
Epoch: [72][300/616]	Loss 2.5082e-01 (2.4792e-01)	Acc 0.743164 (0.739611)
Epoch: [72][600/616]	Loss 2.6088e-01 (2.4903e-01)	Acc 0.740234 (0.738806)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.735896)
Training Loss of Epoch 72: 0.24922983951684904
Training Acc of Epoch 72: 0.7386305894308943
Testing Acc of Epoch 72: 0.7358956521739131
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.5629e-01 (2.5629e-01)	Acc 0.749023 (0.749023)
Epoch: [73][300/616]	Loss 2.6022e-01 (2.5075e-01)	Acc 0.722656 (0.737658)
Epoch: [73][600/616]	Loss 2.3043e-01 (2.5056e-01)	Acc 0.762695 (0.737284)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.731348)
Training Loss of Epoch 73: 0.2504688305825722
Training Acc of Epoch 73: 0.7373523246951219
Testing Acc of Epoch 73: 0.7313478260869565
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.5552e-01 (2.5552e-01)	Acc 0.725586 (0.725586)
Epoch: [74][300/616]	Loss 2.7014e-01 (2.5118e-01)	Acc 0.723633 (0.737324)
Epoch: [74][600/616]	Loss 2.4214e-01 (2.5004e-01)	Acc 0.747070 (0.738301)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.743387)
Training Loss of Epoch 74: 0.24993212259397274
Training Acc of Epoch 74: 0.7384590955284552
Testing Acc of Epoch 74: 0.7433869565217391
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.6636e-01 (2.6636e-01)	Acc 0.709961 (0.709961)
Epoch: [75][300/616]	Loss 2.4799e-01 (2.4124e-01)	Acc 0.747070 (0.746266)
Epoch: [75][600/616]	Loss 2.4377e-01 (2.4191e-01)	Acc 0.740234 (0.745694)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749539)
Training Loss of Epoch 75: 0.24183172280710888
Training Acc of Epoch 75: 0.7458047510162602
Testing Acc of Epoch 75: 0.7495391304347826
Model with the best training loss saved! The loss is 0.24183172280710888
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3799e-01 (2.3799e-01)	Acc 0.761719 (0.761719)
Epoch: [76][300/616]	Loss 2.5404e-01 (2.4265e-01)	Acc 0.734375 (0.744442)
Epoch: [76][600/616]	Loss 2.3660e-01 (2.4250e-01)	Acc 0.752930 (0.745065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.727564 (0.742087)
Training Loss of Epoch 76: 0.2424929126733687
Training Acc of Epoch 76: 0.7450600228658537
Testing Acc of Epoch 76: 0.7420869565217392
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.4078e-01 (2.4078e-01)	Acc 0.742188 (0.742188)
Epoch: [77][300/616]	Loss 2.5349e-01 (2.4271e-01)	Acc 0.730469 (0.744462)
Epoch: [77][600/616]	Loss 2.5185e-01 (2.4191e-01)	Acc 0.720703 (0.745493)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.746113)
Training Loss of Epoch 77: 0.24195175657912
Training Acc of Epoch 77: 0.7454173018292682
Testing Acc of Epoch 77: 0.7461130434782609
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.2748e-01 (2.2748e-01)	Acc 0.767578 (0.767578)
Epoch: [78][300/616]	Loss 2.5828e-01 (2.4232e-01)	Acc 0.726562 (0.744725)
Epoch: [78][600/616]	Loss 2.4902e-01 (2.4185e-01)	Acc 0.737305 (0.745216)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.748170)
Training Loss of Epoch 78: 0.2418381836113891
Training Acc of Epoch 78: 0.7453521976626016
Testing Acc of Epoch 78: 0.7481695652173913
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.2552e-01 (2.2552e-01)	Acc 0.769531 (0.769531)
Epoch: [79][300/616]	Loss 2.4975e-01 (2.4228e-01)	Acc 0.742188 (0.746204)
Epoch: [79][600/616]	Loss 2.4057e-01 (2.4203e-01)	Acc 0.753906 (0.745933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.742965)
Training Loss of Epoch 79: 0.24208840085723535
Training Acc of Epoch 79: 0.7458412728658537
Testing Acc of Epoch 79: 0.7429652173913044
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.3645e-01 (2.3645e-01)	Acc 0.764648 (0.764648)
Epoch: [80][300/616]	Loss 2.5455e-01 (2.4226e-01)	Acc 0.751953 (0.744890)
Epoch: [80][600/616]	Loss 2.4765e-01 (2.4148e-01)	Acc 0.738281 (0.745858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.747374)
Training Loss of Epoch 80: 0.24147460126295323
Training Acc of Epoch 80: 0.745947662601626
Testing Acc of Epoch 80: 0.7473739130434782
Model with the best training loss saved! The loss is 0.24147460126295323
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.5325e-01 (2.5325e-01)	Acc 0.724609 (0.724609)
Epoch: [81][300/616]	Loss 2.4321e-01 (2.4222e-01)	Acc 0.736328 (0.744991)
Epoch: [81][600/616]	Loss 2.4698e-01 (2.4120e-01)	Acc 0.725586 (0.746383)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.744661)
Training Loss of Epoch 81: 0.24121002891683965
Training Acc of Epoch 81: 0.7463239964430894
Testing Acc of Epoch 81: 0.7446608695652174
Model with the best training loss saved! The loss is 0.24121002891683965
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.5312e-01 (2.5312e-01)	Acc 0.747070 (0.747070)
Epoch: [82][300/616]	Loss 2.5352e-01 (2.4092e-01)	Acc 0.732422 (0.746178)
Epoch: [82][600/616]	Loss 2.3534e-01 (2.4108e-01)	Acc 0.765625 (0.746355)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.747926)
Training Loss of Epoch 82: 0.24106080973051427
Training Acc of Epoch 82: 0.7463462271341463
Testing Acc of Epoch 82: 0.7479260869565217
Model with the best training loss saved! The loss is 0.24106080973051427
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.4444e-01 (2.4444e-01)	Acc 0.733398 (0.733398)
Epoch: [83][300/616]	Loss 2.3664e-01 (2.3979e-01)	Acc 0.747070 (0.748478)
Epoch: [83][600/616]	Loss 2.3759e-01 (2.4012e-01)	Acc 0.754883 (0.747423)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.746257)
Training Loss of Epoch 83: 0.24013244211673737
Training Acc of Epoch 83: 0.7474164761178862
Testing Acc of Epoch 83: 0.7462565217391305
Model with the best training loss saved! The loss is 0.24013244211673737
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.3442e-01 (2.3442e-01)	Acc 0.754883 (0.754883)
Epoch: [84][300/616]	Loss 2.3013e-01 (2.4063e-01)	Acc 0.771484 (0.746366)
Epoch: [84][600/616]	Loss 2.3711e-01 (2.4104e-01)	Acc 0.750000 (0.746329)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.750657)
Training Loss of Epoch 84: 0.24107907402321574
Training Acc of Epoch 84: 0.7462890625
Testing Acc of Epoch 84: 0.7506565217391304
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.4306e-01 (2.4306e-01)	Acc 0.748047 (0.748047)
Epoch: [85][300/616]	Loss 2.2867e-01 (2.4009e-01)	Acc 0.762695 (0.747080)
Epoch: [85][600/616]	Loss 2.2541e-01 (2.4061e-01)	Acc 0.764648 (0.746614)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748813)
Training Loss of Epoch 85: 0.24061328218719824
Training Acc of Epoch 85: 0.746606643800813
Testing Acc of Epoch 85: 0.7488130434782608
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3955e-01 (2.3955e-01)	Acc 0.754883 (0.754883)
Epoch: [86][300/616]	Loss 2.2342e-01 (2.4004e-01)	Acc 0.768555 (0.747301)
Epoch: [86][600/616]	Loss 2.5720e-01 (2.4066e-01)	Acc 0.725586 (0.746890)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.751148)
Training Loss of Epoch 86: 0.2406724066269107
Training Acc of Epoch 86: 0.746973450203252
Testing Acc of Epoch 86: 0.7511478260869565
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.5431e-01 (2.5431e-01)	Acc 0.729492 (0.729492)
Epoch: [87][300/616]	Loss 2.1858e-01 (2.4217e-01)	Acc 0.766602 (0.745594)
Epoch: [87][600/616]	Loss 2.2334e-01 (2.4143e-01)	Acc 0.772461 (0.746250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747765)
Training Loss of Epoch 87: 0.2414139070161959
Training Acc of Epoch 87: 0.7462715955284552
Testing Acc of Epoch 87: 0.7477652173913043
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.1961e-01 (2.1961e-01)	Acc 0.773438 (0.773438)
Epoch: [88][300/616]	Loss 2.4537e-01 (2.4094e-01)	Acc 0.735352 (0.746726)
Epoch: [88][600/616]	Loss 2.3170e-01 (2.4109e-01)	Acc 0.766602 (0.746586)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.743787)
Training Loss of Epoch 88: 0.241040442387263
Training Acc of Epoch 88: 0.7466511051829269
Testing Acc of Epoch 88: 0.7437869565217391
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.5146e-01 (2.5146e-01)	Acc 0.741211 (0.741211)
Epoch: [89][300/616]	Loss 2.3620e-01 (2.4006e-01)	Acc 0.750000 (0.747424)
Epoch: [89][600/616]	Loss 2.4872e-01 (2.4090e-01)	Acc 0.739258 (0.746454)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.749909)
Training Loss of Epoch 89: 0.24099664470044577
Training Acc of Epoch 89: 0.7464065675813009
Testing Acc of Epoch 89: 0.7499086956521739
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2064e-01 (2.2064e-01)	Acc 0.775391 (0.775391)
Epoch: [90][300/616]	Loss 2.5595e-01 (2.4093e-01)	Acc 0.739258 (0.746798)
Epoch: [90][600/616]	Loss 2.4676e-01 (2.4079e-01)	Acc 0.752930 (0.746718)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.744987)
Training Loss of Epoch 90: 0.24095564368294506
Training Acc of Epoch 90: 0.7465478912601626
Testing Acc of Epoch 90: 0.7449869565217392
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.4495e-01 (2.4495e-01)	Acc 0.740234 (0.740234)
Epoch: [91][300/616]	Loss 2.3755e-01 (2.4086e-01)	Acc 0.744141 (0.746574)
Epoch: [91][600/616]	Loss 2.3651e-01 (2.4082e-01)	Acc 0.751953 (0.746508)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747643)
Training Loss of Epoch 91: 0.24075042806505187
Training Acc of Epoch 91: 0.7465240726626017
Testing Acc of Epoch 91: 0.7476434782608695
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3770e-01 (2.3770e-01)	Acc 0.758789 (0.758789)
Epoch: [92][300/616]	Loss 2.3565e-01 (2.4039e-01)	Acc 0.756836 (0.746811)
Epoch: [92][600/616]	Loss 2.5796e-01 (2.4038e-01)	Acc 0.714844 (0.747052)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.748397 (0.749022)
Training Loss of Epoch 92: 0.24033866839680246
Training Acc of Epoch 92: 0.7470353785569106
Testing Acc of Epoch 92: 0.7490217391304348
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.4817e-01 (2.4817e-01)	Acc 0.724609 (0.724609)
Epoch: [93][300/616]	Loss 2.4759e-01 (2.4198e-01)	Acc 0.739258 (0.745172)
Epoch: [93][600/616]	Loss 2.3946e-01 (2.4163e-01)	Acc 0.745117 (0.745933)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750243)
Training Loss of Epoch 93: 0.24169797960335646
Training Acc of Epoch 93: 0.7457666412601626
Testing Acc of Epoch 93: 0.7502434782608696
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.6082e-01 (2.6082e-01)	Acc 0.718750 (0.718750)
Epoch: [94][300/616]	Loss 2.5927e-01 (2.4099e-01)	Acc 0.731445 (0.746126)
Epoch: [94][600/616]	Loss 2.6177e-01 (2.4080e-01)	Acc 0.724609 (0.746658)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.749565)
Training Loss of Epoch 94: 0.24076776194378613
Training Acc of Epoch 94: 0.7466558689024391
Testing Acc of Epoch 94: 0.7495652173913043
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.2430e-01 (2.2430e-01)	Acc 0.753906 (0.753906)
Epoch: [95][300/616]	Loss 2.5996e-01 (2.4183e-01)	Acc 0.725586 (0.744741)
Epoch: [95][600/616]	Loss 2.2295e-01 (2.4147e-01)	Acc 0.766602 (0.745571)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.750143)
Training Loss of Epoch 95: 0.24143156188774884
Training Acc of Epoch 95: 0.7456793064024391
Testing Acc of Epoch 95: 0.7501434782608696
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.4652e-01 (2.4652e-01)	Acc 0.737305 (0.737305)
Epoch: [96][300/616]	Loss 2.1713e-01 (2.4099e-01)	Acc 0.778320 (0.746363)
Epoch: [96][600/616]	Loss 2.3249e-01 (2.4095e-01)	Acc 0.749023 (0.746463)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747278)
Training Loss of Epoch 96: 0.2409368174347451
Training Acc of Epoch 96: 0.746460556402439
Testing Acc of Epoch 96: 0.7472782608695652
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.2781e-01 (2.2781e-01)	Acc 0.763672 (0.763672)
Epoch: [97][300/616]	Loss 2.6050e-01 (2.4142e-01)	Acc 0.740234 (0.746207)
Epoch: [97][600/616]	Loss 2.3174e-01 (2.4157e-01)	Acc 0.759766 (0.745741)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.740517)
Training Loss of Epoch 97: 0.24164187486578778
Training Acc of Epoch 97: 0.7457094766260163
Testing Acc of Epoch 97: 0.7405173913043478
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3730e-01 (2.3730e-01)	Acc 0.743164 (0.743164)
Epoch: [98][300/616]	Loss 2.2858e-01 (2.4059e-01)	Acc 0.755859 (0.746983)
Epoch: [98][600/616]	Loss 2.5014e-01 (2.4064e-01)	Acc 0.742188 (0.747026)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.746952)
Training Loss of Epoch 98: 0.24070731185800662
Training Acc of Epoch 98: 0.7469274009146342
Testing Acc of Epoch 98: 0.7469521739130435
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.3264e-01 (2.3264e-01)	Acc 0.772461 (0.772461)
Epoch: [99][300/616]	Loss 2.3641e-01 (2.4115e-01)	Acc 0.750000 (0.747031)
Epoch: [99][600/616]	Loss 2.3832e-01 (2.4031e-01)	Acc 0.758789 (0.747451)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749709)
Training Loss of Epoch 99: 0.24025493097014544
Training Acc of Epoch 99: 0.7474831681910569
Testing Acc of Epoch 99: 0.7497086956521739
Early stopping not satisfied.
