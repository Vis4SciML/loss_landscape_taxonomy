train_bs 1024
test_bs 1024
training_type lr_decay
presplit_train False
train_data_path ../../data/jet_data/train.h5
val_data_bath ../../data/jet_data/val.h5
test_data_bath ../../data/jet_data/test.h5
random_labels False
shuffle_random_data False
num_classes 10
label_corrupt_prob 1.0
random_label_path ../../data/random_labels/random_label.pkl
random_label_path_test ../../data/random_labels/random_label_test.pkl
test_on_noise False
data_subset True
subset 1.0
subset_noisy False
arch JT_7b
different_width False
resnet18_width 64
weight_precision 7
bias_precision 7
act_precision 10
batch_norm False
dropout False
exp_num 5
lr 0.025
epochs 100
weight_decay 0.0005
no_lr_decay False
one_lr_decay True
stop_epoch 100
resume None
lr_decay_epoch [100, 150]
save_early_stop False
min_delta 0
patience 0
l1-enable False
l2-enable False
ignore_incomplete_batch True
only_exploration False
save_final False
save_middle False
save_best True
save_frequency 10
saving_folder ../checkpoint/different_knobs_subset_10/lr_0.025/lr_decay/JT_7b/
file_prefix exp_2
mixup_alpha 16.0
------------------------------------------------------
Experiement: lr_decay training for JT_7b
------------------------------------------------------
using quant model
---------------------- Model -------------------------
QThreeLayer(
  (quant_input): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_1): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_1): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_2): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_2): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_3): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (quant_act_3): QuantAct(activation_bit=10, full_precision_flag=False, quant_mode=symmetric, Act_min: 0.00, Act_max: 0.00)
  (dense_4): (QuantLinear() weight_bit=7, full_precision_flag=False, quantize_fn=symmetric)
  (act): ReLU()
  (softmax): Softmax(dim=1)
)
------------------------------------------------------
Loading Datasets
Could not load file: ../../data/train/jetImage_7_100p_0_10000.h5
Could not load file: ../../data/test/jetImage_7_100p_50000_60000.h5

Dataset loading complete!
The base learning rate is 0.025
---------------------
Start epoch 0
---------------------
Epoch: [0][  0/616]	Loss 5.0044e-01 (5.0044e-01)	Acc 0.209961 (0.209961)
Epoch: [0][300/616]	Loss 2.4608e-01 (2.7379e-01)	Acc 0.744141 (0.709513)
Epoch: [0][600/616]	Loss 2.4981e-01 (2.6116e-01)	Acc 0.726562 (0.724811)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.743096)
Training Loss of Epoch 0: 0.260899914789006
Training Acc of Epoch 0: 0.7251889608739838
Testing Acc of Epoch 0: 0.743095652173913
Model with the best training loss saved! The loss is 0.260899914789006
Early stopping not satisfied.
---------------------
Start epoch 1
---------------------
Epoch: [1][  0/616]	Loss 2.4952e-01 (2.4952e-01)	Acc 0.732422 (0.732422)
Epoch: [1][300/616]	Loss 2.4262e-01 (2.4496e-01)	Acc 0.734375 (0.743102)
Epoch: [1][600/616]	Loss 2.3510e-01 (2.4493e-01)	Acc 0.758789 (0.743076)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.745452)
Training Loss of Epoch 1: 0.244743986270292
Training Acc of Epoch 1: 0.7432910950203252
Testing Acc of Epoch 1: 0.7454521739130435
Model with the best training loss saved! The loss is 0.244743986270292
Early stopping not satisfied.
---------------------
Start epoch 2
---------------------
Epoch: [2][  0/616]	Loss 2.4751e-01 (2.4751e-01)	Acc 0.738281 (0.738281)
Epoch: [2][300/616]	Loss 2.4997e-01 (2.4298e-01)	Acc 0.719727 (0.744828)
Epoch: [2][600/616]	Loss 2.4845e-01 (2.4233e-01)	Acc 0.736328 (0.745666)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748609)
Training Loss of Epoch 2: 0.24228505838692674
Training Acc of Epoch 2: 0.7457475863821138
Testing Acc of Epoch 2: 0.7486086956521739
Model with the best training loss saved! The loss is 0.24228505838692674
Early stopping not satisfied.
---------------------
Start epoch 3
---------------------
Epoch: [3][  0/616]	Loss 2.3447e-01 (2.3447e-01)	Acc 0.762695 (0.762695)
Epoch: [3][300/616]	Loss 2.3322e-01 (2.4181e-01)	Acc 0.765625 (0.746717)
Epoch: [3][600/616]	Loss 2.4095e-01 (2.4169e-01)	Acc 0.739258 (0.746154)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.748796)
Training Loss of Epoch 3: 0.24172577819203941
Training Acc of Epoch 3: 0.7460572281504065
Testing Acc of Epoch 3: 0.7487956521739131
Model with the best training loss saved! The loss is 0.24172577819203941
Early stopping not satisfied.
---------------------
Start epoch 4
---------------------
Epoch: [4][  0/616]	Loss 2.3526e-01 (2.3526e-01)	Acc 0.748047 (0.748047)
Epoch: [4][300/616]	Loss 2.3521e-01 (2.4164e-01)	Acc 0.762695 (0.746477)
Epoch: [4][600/616]	Loss 2.4543e-01 (2.4119e-01)	Acc 0.734375 (0.746918)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.748522)
Training Loss of Epoch 4: 0.24127993779938395
Training Acc of Epoch 4: 0.7467400279471544
Testing Acc of Epoch 4: 0.7485217391304347
Model with the best training loss saved! The loss is 0.24127993779938395
Early stopping not satisfied.
---------------------
Start epoch 5
---------------------
Epoch: [5][  0/616]	Loss 2.5160e-01 (2.5160e-01)	Acc 0.736328 (0.736328)
Epoch: [5][300/616]	Loss 2.3847e-01 (2.4137e-01)	Acc 0.753906 (0.745980)
Epoch: [5][600/616]	Loss 2.4739e-01 (2.4080e-01)	Acc 0.747070 (0.746685)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749922)
Training Loss of Epoch 5: 0.24079016413145918
Training Acc of Epoch 5: 0.7466495172764228
Testing Acc of Epoch 5: 0.7499217391304348
Model with the best training loss saved! The loss is 0.24079016413145918
Early stopping not satisfied.
---------------------
Start epoch 6
---------------------
Epoch: [6][  0/616]	Loss 2.4566e-01 (2.4566e-01)	Acc 0.734375 (0.734375)
Epoch: [6][300/616]	Loss 2.4288e-01 (2.4049e-01)	Acc 0.748047 (0.746966)
Epoch: [6][600/616]	Loss 2.2646e-01 (2.4049e-01)	Acc 0.765625 (0.747078)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747387)
Training Loss of Epoch 6: 0.24051842694360065
Training Acc of Epoch 6: 0.7470417301829269
Testing Acc of Epoch 6: 0.7473869565217391
Model with the best training loss saved! The loss is 0.24051842694360065
Early stopping not satisfied.
---------------------
Start epoch 7
---------------------
Epoch: [7][  0/616]	Loss 2.4494e-01 (2.4494e-01)	Acc 0.735352 (0.735352)
Epoch: [7][300/616]	Loss 2.4908e-01 (2.3964e-01)	Acc 0.733398 (0.747521)
Epoch: [7][600/616]	Loss 2.4091e-01 (2.3965e-01)	Acc 0.745117 (0.747647)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.743696)
Training Loss of Epoch 7: 0.23978563321799767
Training Acc of Epoch 7: 0.7475022230691057
Testing Acc of Epoch 7: 0.7436956521739131
Model with the best training loss saved! The loss is 0.23978563321799767
Early stopping not satisfied.
---------------------
Start epoch 8
---------------------
Epoch: [8][  0/616]	Loss 2.3907e-01 (2.3907e-01)	Acc 0.762695 (0.762695)
Epoch: [8][300/616]	Loss 2.4953e-01 (2.4093e-01)	Acc 0.732422 (0.746976)
Epoch: [8][600/616]	Loss 2.4136e-01 (2.4009e-01)	Acc 0.740234 (0.747813)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.717949 (0.738670)
Training Loss of Epoch 8: 0.2402257421636969
Training Acc of Epoch 8: 0.7476768927845528
Testing Acc of Epoch 8: 0.7386695652173914
Early stopping not satisfied.
---------------------
Start epoch 9
---------------------
Epoch: [9][  0/616]	Loss 2.5375e-01 (2.5375e-01)	Acc 0.725586 (0.725586)
Epoch: [9][300/616]	Loss 2.5164e-01 (2.4000e-01)	Acc 0.720703 (0.746905)
Epoch: [9][600/616]	Loss 2.6113e-01 (2.4019e-01)	Acc 0.712891 (0.747355)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748609)
Training Loss of Epoch 9: 0.2401949469393831
Training Acc of Epoch 9: 0.7473164380081301
Testing Acc of Epoch 9: 0.7486086956521739
Early stopping not satisfied.
---------------------
Start epoch 10
---------------------
Epoch: [10][  0/616]	Loss 2.5119e-01 (2.5119e-01)	Acc 0.738281 (0.738281)
Epoch: [10][300/616]	Loss 2.4239e-01 (2.4076e-01)	Acc 0.752930 (0.747009)
Epoch: [10][600/616]	Loss 2.4265e-01 (2.4100e-01)	Acc 0.748047 (0.746443)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751022)
Training Loss of Epoch 10: 0.24094075044480764
Training Acc of Epoch 10: 0.7465240726626017
Testing Acc of Epoch 10: 0.7510217391304348
Early stopping not satisfied.
---------------------
Start epoch 11
---------------------
Epoch: [11][  0/616]	Loss 2.2445e-01 (2.2445e-01)	Acc 0.761719 (0.761719)
Epoch: [11][300/616]	Loss 2.2849e-01 (2.3985e-01)	Acc 0.761719 (0.747408)
Epoch: [11][600/616]	Loss 2.2240e-01 (2.4046e-01)	Acc 0.748047 (0.746888)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751161)
Training Loss of Epoch 11: 0.24026650224274737
Training Acc of Epoch 11: 0.7472021087398374
Testing Acc of Epoch 11: 0.7511608695652174
Early stopping not satisfied.
---------------------
Start epoch 12
---------------------
Epoch: [12][  0/616]	Loss 2.2662e-01 (2.2662e-01)	Acc 0.754883 (0.754883)
Epoch: [12][300/616]	Loss 2.5836e-01 (2.3958e-01)	Acc 0.718750 (0.747995)
Epoch: [12][600/616]	Loss 2.3739e-01 (2.3980e-01)	Acc 0.734375 (0.747785)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.735087)
Training Loss of Epoch 12: 0.2400344072560954
Training Acc of Epoch 12: 0.7475514481707317
Testing Acc of Epoch 12: 0.7350869565217392
Early stopping not satisfied.
---------------------
Start epoch 13
---------------------
Epoch: [13][  0/616]	Loss 2.4264e-01 (2.4264e-01)	Acc 0.748047 (0.748047)
Epoch: [13][300/616]	Loss 2.4112e-01 (2.4090e-01)	Acc 0.766602 (0.747096)
Epoch: [13][600/616]	Loss 2.3849e-01 (2.4044e-01)	Acc 0.759766 (0.747304)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.751130)
Training Loss of Epoch 13: 0.24034328010024095
Training Acc of Epoch 13: 0.7473577235772357
Testing Acc of Epoch 13: 0.7511304347826087
Early stopping not satisfied.
---------------------
Start epoch 14
---------------------
Epoch: [14][  0/616]	Loss 2.1275e-01 (2.1275e-01)	Acc 0.794922 (0.794922)
Epoch: [14][300/616]	Loss 2.3725e-01 (2.3992e-01)	Acc 0.750000 (0.747443)
Epoch: [14][600/616]	Loss 2.5843e-01 (2.3997e-01)	Acc 0.719727 (0.747389)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.740957)
Training Loss of Epoch 14: 0.24006639765045507
Training Acc of Epoch 14: 0.7472846798780488
Testing Acc of Epoch 14: 0.7409565217391304
Early stopping not satisfied.
---------------------
Start epoch 15
---------------------
Epoch: [15][  0/616]	Loss 2.4835e-01 (2.4835e-01)	Acc 0.730469 (0.730469)
Epoch: [15][300/616]	Loss 2.4991e-01 (2.4196e-01)	Acc 0.721680 (0.745909)
Epoch: [15][600/616]	Loss 2.4867e-01 (2.4108e-01)	Acc 0.738281 (0.746428)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747209)
Training Loss of Epoch 15: 0.24098605523264505
Training Acc of Epoch 15: 0.7465748856707317
Testing Acc of Epoch 15: 0.747208695652174
Early stopping not satisfied.
---------------------
Start epoch 16
---------------------
Epoch: [16][  0/616]	Loss 2.4815e-01 (2.4815e-01)	Acc 0.735352 (0.735352)
Epoch: [16][300/616]	Loss 2.4719e-01 (2.4007e-01)	Acc 0.719727 (0.747589)
Epoch: [16][600/616]	Loss 2.3431e-01 (2.3975e-01)	Acc 0.745117 (0.747914)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749078)
Training Loss of Epoch 16: 0.23981358147248988
Training Acc of Epoch 16: 0.7478198043699187
Testing Acc of Epoch 16: 0.7490782608695652
Early stopping not satisfied.
---------------------
Start epoch 17
---------------------
Epoch: [17][  0/616]	Loss 2.4415e-01 (2.4415e-01)	Acc 0.741211 (0.741211)
Epoch: [17][300/616]	Loss 2.5968e-01 (2.4048e-01)	Acc 0.726562 (0.747220)
Epoch: [17][600/616]	Loss 2.4943e-01 (2.4016e-01)	Acc 0.742188 (0.747091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747265)
Training Loss of Epoch 17: 0.24016368973061322
Training Acc of Epoch 17: 0.7471306529471544
Testing Acc of Epoch 17: 0.7472652173913044
Early stopping not satisfied.
---------------------
Start epoch 18
---------------------
Epoch: [18][  0/616]	Loss 2.5004e-01 (2.5004e-01)	Acc 0.729492 (0.729492)
Epoch: [18][300/616]	Loss 2.5737e-01 (2.4200e-01)	Acc 0.714844 (0.745776)
Epoch: [18][600/616]	Loss 2.3188e-01 (2.4123e-01)	Acc 0.747070 (0.746734)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.721154 (0.739900)
Training Loss of Epoch 18: 0.24124044230798394
Training Acc of Epoch 18: 0.7467924288617886
Testing Acc of Epoch 18: 0.7399
Early stopping not satisfied.
---------------------
Start epoch 19
---------------------
Epoch: [19][  0/616]	Loss 2.4547e-01 (2.4547e-01)	Acc 0.741211 (0.741211)
Epoch: [19][300/616]	Loss 2.4235e-01 (2.4001e-01)	Acc 0.745117 (0.748066)
Epoch: [19][600/616]	Loss 2.4214e-01 (2.3991e-01)	Acc 0.744141 (0.747639)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.744148)
Training Loss of Epoch 19: 0.23993411866145406
Training Acc of Epoch 19: 0.7475609756097561
Testing Acc of Epoch 19: 0.7441478260869565
Early stopping not satisfied.
---------------------
Start epoch 20
---------------------
Epoch: [20][  0/616]	Loss 2.4436e-01 (2.4436e-01)	Acc 0.756836 (0.756836)
Epoch: [20][300/616]	Loss 2.4714e-01 (2.4000e-01)	Acc 0.736328 (0.747109)
Epoch: [20][600/616]	Loss 2.4609e-01 (2.4014e-01)	Acc 0.734375 (0.747499)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.748830)
Training Loss of Epoch 20: 0.24020796565990138
Training Acc of Epoch 20: 0.7474688770325203
Testing Acc of Epoch 20: 0.7488304347826087
Early stopping not satisfied.
---------------------
Start epoch 21
---------------------
Epoch: [21][  0/616]	Loss 2.4151e-01 (2.4151e-01)	Acc 0.745117 (0.745117)
Epoch: [21][300/616]	Loss 2.4386e-01 (2.3922e-01)	Acc 0.739258 (0.748530)
Epoch: [21][600/616]	Loss 2.4439e-01 (2.3936e-01)	Acc 0.747070 (0.748032)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.746174)
Training Loss of Epoch 21: 0.23944153056396703
Training Acc of Epoch 21: 0.7480405233739837
Testing Acc of Epoch 21: 0.7461739130434782
Model with the best training loss saved! The loss is 0.23944153056396703
Early stopping not satisfied.
---------------------
Start epoch 22
---------------------
Epoch: [22][  0/616]	Loss 2.4460e-01 (2.4460e-01)	Acc 0.743164 (0.743164)
Epoch: [22][300/616]	Loss 2.6131e-01 (2.4019e-01)	Acc 0.719727 (0.747057)
Epoch: [22][600/616]	Loss 2.4012e-01 (2.4019e-01)	Acc 0.747070 (0.747217)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.743535)
Training Loss of Epoch 22: 0.24028525752265278
Training Acc of Epoch 22: 0.7471163617886178
Testing Acc of Epoch 22: 0.7435347826086957
Early stopping not satisfied.
---------------------
Start epoch 23
---------------------
Epoch: [23][  0/616]	Loss 2.3547e-01 (2.3547e-01)	Acc 0.755859 (0.755859)
Epoch: [23][300/616]	Loss 2.2750e-01 (2.4058e-01)	Acc 0.760742 (0.746538)
Epoch: [23][600/616]	Loss 2.3230e-01 (2.4028e-01)	Acc 0.765625 (0.747090)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.749339)
Training Loss of Epoch 23: 0.24030792209191051
Training Acc of Epoch 23: 0.747167174796748
Testing Acc of Epoch 23: 0.7493391304347826
Early stopping not satisfied.
---------------------
Start epoch 24
---------------------
Epoch: [24][  0/616]	Loss 2.3171e-01 (2.3171e-01)	Acc 0.752930 (0.752930)
Epoch: [24][300/616]	Loss 2.5642e-01 (2.3870e-01)	Acc 0.725586 (0.748371)
Epoch: [24][600/616]	Loss 2.3425e-01 (2.3919e-01)	Acc 0.745117 (0.748206)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.748743)
Training Loss of Epoch 24: 0.23922287700622064
Training Acc of Epoch 24: 0.748192962398374
Testing Acc of Epoch 24: 0.7487434782608695
Model with the best training loss saved! The loss is 0.23922287700622064
Early stopping not satisfied.
---------------------
Start epoch 25
---------------------
Epoch: [25][  0/616]	Loss 2.4360e-01 (2.4360e-01)	Acc 0.741211 (0.741211)
Epoch: [25][300/616]	Loss 2.3656e-01 (2.4049e-01)	Acc 0.762695 (0.746996)
Epoch: [25][600/616]	Loss 2.2806e-01 (2.3978e-01)	Acc 0.766602 (0.747743)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747761)
Training Loss of Epoch 25: 0.23987640365352475
Training Acc of Epoch 25: 0.7477213541666666
Testing Acc of Epoch 25: 0.7477608695652174
Early stopping not satisfied.
---------------------
Start epoch 26
---------------------
Epoch: [26][  0/616]	Loss 2.4409e-01 (2.4409e-01)	Acc 0.747070 (0.747070)
Epoch: [26][300/616]	Loss 2.3708e-01 (2.4119e-01)	Acc 0.767578 (0.746882)
Epoch: [26][600/616]	Loss 2.3930e-01 (2.4060e-01)	Acc 0.733398 (0.746960)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.722756 (0.746848)
Training Loss of Epoch 26: 0.24053889890996422
Training Acc of Epoch 26: 0.7470750762195122
Testing Acc of Epoch 26: 0.7468478260869565
Early stopping not satisfied.
---------------------
Start epoch 27
---------------------
Epoch: [27][  0/616]	Loss 2.4893e-01 (2.4893e-01)	Acc 0.747070 (0.747070)
Epoch: [27][300/616]	Loss 2.3027e-01 (2.3977e-01)	Acc 0.761719 (0.748154)
Epoch: [27][600/616]	Loss 2.3651e-01 (2.3959e-01)	Acc 0.748047 (0.747891)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749157)
Training Loss of Epoch 27: 0.2395448800509538
Training Acc of Epoch 27: 0.7479881224593496
Testing Acc of Epoch 27: 0.7491565217391304
Early stopping not satisfied.
---------------------
Start epoch 28
---------------------
Epoch: [28][  0/616]	Loss 2.3731e-01 (2.3731e-01)	Acc 0.753906 (0.753906)
Epoch: [28][300/616]	Loss 2.4601e-01 (2.3895e-01)	Acc 0.751953 (0.748673)
Epoch: [28][600/616]	Loss 2.5927e-01 (2.3993e-01)	Acc 0.723633 (0.747668)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750909)
Training Loss of Epoch 28: 0.24002824815792767
Training Acc of Epoch 28: 0.7475149263211383
Testing Acc of Epoch 28: 0.7509086956521739
Early stopping not satisfied.
---------------------
Start epoch 29
---------------------
Epoch: [29][  0/616]	Loss 2.2380e-01 (2.2380e-01)	Acc 0.772461 (0.772461)
Epoch: [29][300/616]	Loss 2.2211e-01 (2.3948e-01)	Acc 0.768555 (0.748079)
Epoch: [29][600/616]	Loss 2.5012e-01 (2.3986e-01)	Acc 0.717773 (0.747702)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.749765)
Training Loss of Epoch 29: 0.23989428313282446
Training Acc of Epoch 29: 0.7476213160569106
Testing Acc of Epoch 29: 0.7497652173913043
Early stopping not satisfied.
---------------------
Start epoch 30
---------------------
Epoch: [30][  0/616]	Loss 2.1763e-01 (2.1763e-01)	Acc 0.770508 (0.770508)
Epoch: [30][300/616]	Loss 2.2789e-01 (2.4016e-01)	Acc 0.754883 (0.747278)
Epoch: [30][600/616]	Loss 2.3253e-01 (2.4077e-01)	Acc 0.773438 (0.747018)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.730769 (0.747017)
Training Loss of Epoch 30: 0.24075955254275624
Training Acc of Epoch 30: 0.7470687245934959
Testing Acc of Epoch 30: 0.7470173913043479
Early stopping not satisfied.
---------------------
Start epoch 31
---------------------
Epoch: [31][  0/616]	Loss 2.3550e-01 (2.3550e-01)	Acc 0.759766 (0.759766)
Epoch: [31][300/616]	Loss 2.3616e-01 (2.4045e-01)	Acc 0.750977 (0.746441)
Epoch: [31][600/616]	Loss 2.4683e-01 (2.4007e-01)	Acc 0.729492 (0.746827)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.748587)
Training Loss of Epoch 31: 0.24005204141139985
Training Acc of Epoch 31: 0.7468638846544715
Testing Acc of Epoch 31: 0.7485869565217391
Early stopping not satisfied.
---------------------
Start epoch 32
---------------------
Epoch: [32][  0/616]	Loss 2.4044e-01 (2.4044e-01)	Acc 0.748047 (0.748047)
Epoch: [32][300/616]	Loss 2.2729e-01 (2.3956e-01)	Acc 0.770508 (0.747774)
Epoch: [32][600/616]	Loss 2.4604e-01 (2.3977e-01)	Acc 0.743164 (0.747522)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.751791)
Training Loss of Epoch 32: 0.23961219269085707
Training Acc of Epoch 32: 0.7477134146341463
Testing Acc of Epoch 32: 0.751791304347826
Early stopping not satisfied.
---------------------
Start epoch 33
---------------------
Epoch: [33][  0/616]	Loss 2.4165e-01 (2.4165e-01)	Acc 0.750977 (0.750977)
Epoch: [33][300/616]	Loss 2.2686e-01 (2.3865e-01)	Acc 0.751953 (0.748767)
Epoch: [33][600/616]	Loss 2.5216e-01 (2.3967e-01)	Acc 0.738281 (0.748061)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.749478)
Training Loss of Epoch 33: 0.2396209506242256
Training Acc of Epoch 33: 0.748118330792683
Testing Acc of Epoch 33: 0.7494782608695653
Early stopping not satisfied.
---------------------
Start epoch 34
---------------------
Epoch: [34][  0/616]	Loss 2.3399e-01 (2.3399e-01)	Acc 0.747070 (0.747070)
Epoch: [34][300/616]	Loss 2.3015e-01 (2.3981e-01)	Acc 0.755859 (0.748255)
Epoch: [34][600/616]	Loss 2.2806e-01 (2.4007e-01)	Acc 0.770508 (0.747498)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.706731 (0.732478)
Training Loss of Epoch 34: 0.2401609971513593
Training Acc of Epoch 34: 0.7474085365853659
Testing Acc of Epoch 34: 0.7324782608695652
Early stopping not satisfied.
---------------------
Start epoch 35
---------------------
Epoch: [35][  0/616]	Loss 2.4978e-01 (2.4978e-01)	Acc 0.738281 (0.738281)
Epoch: [35][300/616]	Loss 2.3989e-01 (2.3946e-01)	Acc 0.746094 (0.748216)
Epoch: [35][600/616]	Loss 2.4010e-01 (2.4013e-01)	Acc 0.747070 (0.747277)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745570)
Training Loss of Epoch 35: 0.24020057909372378
Training Acc of Epoch 35: 0.7472084603658536
Testing Acc of Epoch 35: 0.7455695652173913
Early stopping not satisfied.
---------------------
Start epoch 36
---------------------
Epoch: [36][  0/616]	Loss 2.3265e-01 (2.3265e-01)	Acc 0.767578 (0.767578)
Epoch: [36][300/616]	Loss 2.2806e-01 (2.3905e-01)	Acc 0.761719 (0.748936)
Epoch: [36][600/616]	Loss 2.3442e-01 (2.3980e-01)	Acc 0.752930 (0.747511)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.746630)
Training Loss of Epoch 36: 0.23973327162789135
Training Acc of Epoch 36: 0.7475768546747967
Testing Acc of Epoch 36: 0.7466304347826087
Early stopping not satisfied.
---------------------
Start epoch 37
---------------------
Epoch: [37][  0/616]	Loss 2.3049e-01 (2.3049e-01)	Acc 0.752930 (0.752930)
Epoch: [37][300/616]	Loss 2.4890e-01 (2.4076e-01)	Acc 0.739258 (0.745951)
Epoch: [37][600/616]	Loss 2.3914e-01 (2.3985e-01)	Acc 0.744141 (0.747725)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.725962 (0.745122)
Training Loss of Epoch 37: 0.2398877059298802
Training Acc of Epoch 37: 0.7477102388211382
Testing Acc of Epoch 37: 0.7451217391304348
Early stopping not satisfied.
---------------------
Start epoch 38
---------------------
Epoch: [38][  0/616]	Loss 2.4426e-01 (2.4426e-01)	Acc 0.748047 (0.748047)
Epoch: [38][300/616]	Loss 2.2791e-01 (2.4040e-01)	Acc 0.755859 (0.747018)
Epoch: [38][600/616]	Loss 2.5713e-01 (2.4012e-01)	Acc 0.733398 (0.747361)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.716346 (0.735648)
Training Loss of Epoch 38: 0.240136002961213
Training Acc of Epoch 38: 0.7473291412601626
Testing Acc of Epoch 38: 0.7356478260869566
Early stopping not satisfied.
---------------------
Start epoch 39
---------------------
Epoch: [39][  0/616]	Loss 2.5133e-01 (2.5133e-01)	Acc 0.728516 (0.728516)
Epoch: [39][300/616]	Loss 2.3884e-01 (2.4021e-01)	Acc 0.750000 (0.747304)
Epoch: [39][600/616]	Loss 2.3805e-01 (2.4024e-01)	Acc 0.750000 (0.747005)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.748996)
Training Loss of Epoch 39: 0.24019749336610965
Training Acc of Epoch 39: 0.7470115599593496
Testing Acc of Epoch 39: 0.7489956521739131
Early stopping not satisfied.
---------------------
Start epoch 40
---------------------
Epoch: [40][  0/616]	Loss 2.4207e-01 (2.4207e-01)	Acc 0.736328 (0.736328)
Epoch: [40][300/616]	Loss 2.3874e-01 (2.4057e-01)	Acc 0.749023 (0.747534)
Epoch: [40][600/616]	Loss 2.2729e-01 (2.3981e-01)	Acc 0.756836 (0.747789)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.745213)
Training Loss of Epoch 40: 0.23988003568436073
Training Acc of Epoch 40: 0.7476610137195122
Testing Acc of Epoch 40: 0.7452130434782609
Early stopping not satisfied.
---------------------
Start epoch 41
---------------------
Epoch: [41][  0/616]	Loss 2.5188e-01 (2.5188e-01)	Acc 0.730469 (0.730469)
Epoch: [41][300/616]	Loss 2.4605e-01 (2.3927e-01)	Acc 0.737305 (0.748839)
Epoch: [41][600/616]	Loss 2.4044e-01 (2.4036e-01)	Acc 0.747070 (0.747546)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.748726)
Training Loss of Epoch 41: 0.24031631302058212
Training Acc of Epoch 41: 0.7475895579268292
Testing Acc of Epoch 41: 0.7487260869565218
Early stopping not satisfied.
---------------------
Start epoch 42
---------------------
Epoch: [42][  0/616]	Loss 2.4127e-01 (2.4127e-01)	Acc 0.762695 (0.762695)
Epoch: [42][300/616]	Loss 2.3417e-01 (2.3961e-01)	Acc 0.750000 (0.747791)
Epoch: [42][600/616]	Loss 2.4400e-01 (2.3947e-01)	Acc 0.752930 (0.747858)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749339)
Training Loss of Epoch 42: 0.23944319871867575
Training Acc of Epoch 42: 0.7478880843495935
Testing Acc of Epoch 42: 0.7493391304347826
Early stopping not satisfied.
---------------------
Start epoch 43
---------------------
Epoch: [43][  0/616]	Loss 2.3060e-01 (2.3060e-01)	Acc 0.749023 (0.749023)
Epoch: [43][300/616]	Loss 2.4131e-01 (2.3993e-01)	Acc 0.738281 (0.747641)
Epoch: [43][600/616]	Loss 2.3555e-01 (2.3998e-01)	Acc 0.766602 (0.747793)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750265)
Training Loss of Epoch 43: 0.2399439519982997
Training Acc of Epoch 43: 0.7478118648373984
Testing Acc of Epoch 43: 0.7502652173913044
Early stopping not satisfied.
---------------------
Start epoch 44
---------------------
Epoch: [44][  0/616]	Loss 2.3376e-01 (2.3376e-01)	Acc 0.755859 (0.755859)
Epoch: [44][300/616]	Loss 2.4412e-01 (2.4059e-01)	Acc 0.735352 (0.746979)
Epoch: [44][600/616]	Loss 2.4309e-01 (2.4005e-01)	Acc 0.747070 (0.747607)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.749248)
Training Loss of Epoch 44: 0.2400079828694584
Training Acc of Epoch 44: 0.7475974974593496
Testing Acc of Epoch 44: 0.7492478260869565
Early stopping not satisfied.
---------------------
Start epoch 45
---------------------
Epoch: [45][  0/616]	Loss 2.3695e-01 (2.3695e-01)	Acc 0.742188 (0.742188)
Epoch: [45][300/616]	Loss 2.4672e-01 (2.4094e-01)	Acc 0.742188 (0.746159)
Epoch: [45][600/616]	Loss 2.4572e-01 (2.4103e-01)	Acc 0.734375 (0.746536)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.714744 (0.740522)
Training Loss of Epoch 45: 0.24105491139055268
Training Acc of Epoch 45: 0.7465002540650406
Testing Acc of Epoch 45: 0.7405217391304347
Early stopping not satisfied.
---------------------
Start epoch 46
---------------------
Epoch: [46][  0/616]	Loss 2.4841e-01 (2.4841e-01)	Acc 0.751953 (0.751953)
Epoch: [46][300/616]	Loss 2.2995e-01 (2.3954e-01)	Acc 0.746094 (0.748167)
Epoch: [46][600/616]	Loss 2.5554e-01 (2.4044e-01)	Acc 0.722656 (0.747101)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.719551 (0.740326)
Training Loss of Epoch 46: 0.24046047577043858
Training Acc of Epoch 46: 0.7471497078252033
Testing Acc of Epoch 46: 0.7403260869565217
Early stopping not satisfied.
---------------------
Start epoch 47
---------------------
Epoch: [47][  0/616]	Loss 2.3728e-01 (2.3728e-01)	Acc 0.752930 (0.752930)
Epoch: [47][300/616]	Loss 2.4891e-01 (2.4008e-01)	Acc 0.742188 (0.747706)
Epoch: [47][600/616]	Loss 2.3450e-01 (2.4032e-01)	Acc 0.763672 (0.747415)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.724359 (0.746409)
Training Loss of Epoch 47: 0.24021368041271116
Training Acc of Epoch 47: 0.7474990472560976
Testing Acc of Epoch 47: 0.7464086956521739
Early stopping not satisfied.
---------------------
Start epoch 48
---------------------
Epoch: [48][  0/616]	Loss 2.4383e-01 (2.4383e-01)	Acc 0.742188 (0.742188)
Epoch: [48][300/616]	Loss 2.4260e-01 (2.4049e-01)	Acc 0.731445 (0.747323)
Epoch: [48][600/616]	Loss 2.3760e-01 (2.4010e-01)	Acc 0.742188 (0.747478)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.746187)
Training Loss of Epoch 48: 0.24019907204116264
Training Acc of Epoch 48: 0.7473831300813009
Testing Acc of Epoch 48: 0.7461869565217392
Early stopping not satisfied.
---------------------
Start epoch 49
---------------------
Epoch: [49][  0/616]	Loss 2.4113e-01 (2.4113e-01)	Acc 0.757812 (0.757812)
Epoch: [49][300/616]	Loss 2.4537e-01 (2.4063e-01)	Acc 0.736328 (0.747284)
Epoch: [49][600/616]	Loss 2.6319e-01 (2.3985e-01)	Acc 0.710938 (0.747784)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.745265)
Training Loss of Epoch 49: 0.23992915223769057
Training Acc of Epoch 49: 0.7476848323170732
Testing Acc of Epoch 49: 0.7452652173913044
Early stopping not satisfied.
---------------------
Start epoch 50
---------------------
Epoch: [50][  0/616]	Loss 2.4117e-01 (2.4117e-01)	Acc 0.738281 (0.738281)
Epoch: [50][300/616]	Loss 2.4499e-01 (2.4034e-01)	Acc 0.731445 (0.746950)
Epoch: [50][600/616]	Loss 2.5389e-01 (2.4057e-01)	Acc 0.706055 (0.746978)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.748796)
Training Loss of Epoch 50: 0.24066630727876492
Training Acc of Epoch 50: 0.7468130716463415
Testing Acc of Epoch 50: 0.7487956521739131
Early stopping not satisfied.
---------------------
Start epoch 51
---------------------
Epoch: [51][  0/616]	Loss 2.3232e-01 (2.3232e-01)	Acc 0.760742 (0.760742)
Epoch: [51][300/616]	Loss 2.5346e-01 (2.3957e-01)	Acc 0.731445 (0.747826)
Epoch: [51][600/616]	Loss 2.5411e-01 (2.4001e-01)	Acc 0.725586 (0.747629)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.750061)
Training Loss of Epoch 51: 0.24001251127177137
Training Acc of Epoch 51: 0.7476149644308943
Testing Acc of Epoch 51: 0.7500608695652173
Early stopping not satisfied.
---------------------
Start epoch 52
---------------------
Epoch: [52][  0/616]	Loss 2.4837e-01 (2.4837e-01)	Acc 0.745117 (0.745117)
Epoch: [52][300/616]	Loss 2.5428e-01 (2.4167e-01)	Acc 0.721680 (0.745763)
Epoch: [52][600/616]	Loss 2.4527e-01 (2.4089e-01)	Acc 0.739258 (0.746510)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.749183)
Training Loss of Epoch 52: 0.24087800604056536
Training Acc of Epoch 52: 0.7464399136178862
Testing Acc of Epoch 52: 0.7491826086956521
Early stopping not satisfied.
---------------------
Start epoch 53
---------------------
Epoch: [53][  0/616]	Loss 2.3694e-01 (2.3694e-01)	Acc 0.753906 (0.753906)
Epoch: [53][300/616]	Loss 2.4285e-01 (2.4237e-01)	Acc 0.753906 (0.745474)
Epoch: [53][600/616]	Loss 2.2774e-01 (2.4159e-01)	Acc 0.760742 (0.746011)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.746887)
Training Loss of Epoch 53: 0.2414353809221004
Training Acc of Epoch 53: 0.7462779471544716
Testing Acc of Epoch 53: 0.7468869565217391
Early stopping not satisfied.
---------------------
Start epoch 54
---------------------
Epoch: [54][  0/616]	Loss 2.3661e-01 (2.3661e-01)	Acc 0.758789 (0.758789)
Epoch: [54][300/616]	Loss 2.5406e-01 (2.4034e-01)	Acc 0.732422 (0.747369)
Epoch: [54][600/616]	Loss 2.4928e-01 (2.4061e-01)	Acc 0.727539 (0.747046)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.749052)
Training Loss of Epoch 54: 0.24052694410812564
Training Acc of Epoch 54: 0.7471719385162602
Testing Acc of Epoch 54: 0.7490521739130435
Early stopping not satisfied.
---------------------
Start epoch 55
---------------------
Epoch: [55][  0/616]	Loss 2.4722e-01 (2.4722e-01)	Acc 0.734375 (0.734375)
Epoch: [55][300/616]	Loss 2.3607e-01 (2.4133e-01)	Acc 0.744141 (0.745974)
Epoch: [55][600/616]	Loss 2.4042e-01 (2.4059e-01)	Acc 0.743164 (0.747065)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.743717)
Training Loss of Epoch 55: 0.24060204804428226
Training Acc of Epoch 55: 0.7470480818089431
Testing Acc of Epoch 55: 0.7437173913043478
Early stopping not satisfied.
---------------------
Start epoch 56
---------------------
Epoch: [56][  0/616]	Loss 2.4224e-01 (2.4224e-01)	Acc 0.759766 (0.759766)
Epoch: [56][300/616]	Loss 2.5432e-01 (2.4053e-01)	Acc 0.726562 (0.747310)
Epoch: [56][600/616]	Loss 2.5203e-01 (2.4089e-01)	Acc 0.744141 (0.746682)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.747609)
Training Loss of Epoch 56: 0.2410082670731273
Training Acc of Epoch 56: 0.746555830792683
Testing Acc of Epoch 56: 0.7476086956521739
Early stopping not satisfied.
---------------------
Start epoch 57
---------------------
Epoch: [57][  0/616]	Loss 2.3600e-01 (2.3600e-01)	Acc 0.760742 (0.760742)
Epoch: [57][300/616]	Loss 2.3758e-01 (2.4058e-01)	Acc 0.744141 (0.747028)
Epoch: [57][600/616]	Loss 2.4810e-01 (2.4084e-01)	Acc 0.729492 (0.746615)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750652)
Training Loss of Epoch 57: 0.24088394111249506
Training Acc of Epoch 57: 0.7465907647357723
Testing Acc of Epoch 57: 0.7506521739130435
Early stopping not satisfied.
---------------------
Start epoch 58
---------------------
Epoch: [58][  0/616]	Loss 2.4298e-01 (2.4298e-01)	Acc 0.746094 (0.746094)
Epoch: [58][300/616]	Loss 2.3250e-01 (2.3961e-01)	Acc 0.743164 (0.748186)
Epoch: [58][600/616]	Loss 2.3932e-01 (2.4014e-01)	Acc 0.748047 (0.747761)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.749435)
Training Loss of Epoch 58: 0.24012493236762722
Training Acc of Epoch 58: 0.7478134527439024
Testing Acc of Epoch 58: 0.7494347826086957
Early stopping not satisfied.
---------------------
Start epoch 59
---------------------
Epoch: [59][  0/616]	Loss 2.4527e-01 (2.4527e-01)	Acc 0.744141 (0.744141)
Epoch: [59][300/616]	Loss 2.2994e-01 (2.4097e-01)	Acc 0.744141 (0.747119)
Epoch: [59][600/616]	Loss 2.3876e-01 (2.4094e-01)	Acc 0.754883 (0.746697)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.745230)
Training Loss of Epoch 59: 0.2409977171721497
Training Acc of Epoch 59: 0.7466256986788617
Testing Acc of Epoch 59: 0.7452304347826086
Early stopping not satisfied.
---------------------
Start epoch 60
---------------------
Epoch: [60][  0/616]	Loss 2.3968e-01 (2.3968e-01)	Acc 0.763672 (0.763672)
Epoch: [60][300/616]	Loss 2.3417e-01 (2.4038e-01)	Acc 0.748047 (0.747025)
Epoch: [60][600/616]	Loss 2.4447e-01 (2.4073e-01)	Acc 0.747070 (0.747103)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.751726)
Training Loss of Epoch 60: 0.240661949912707
Training Acc of Epoch 60: 0.7471735264227642
Testing Acc of Epoch 60: 0.7517260869565218
Early stopping not satisfied.
---------------------
Start epoch 61
---------------------
Epoch: [61][  0/616]	Loss 2.3081e-01 (2.3081e-01)	Acc 0.757812 (0.757812)
Epoch: [61][300/616]	Loss 2.3900e-01 (2.4128e-01)	Acc 0.754883 (0.746538)
Epoch: [61][600/616]	Loss 2.3782e-01 (2.4102e-01)	Acc 0.756836 (0.747018)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.749817)
Training Loss of Epoch 61: 0.24096672992880752
Training Acc of Epoch 61: 0.7470893673780488
Testing Acc of Epoch 61: 0.7498173913043479
Early stopping not satisfied.
---------------------
Start epoch 62
---------------------
Epoch: [62][  0/616]	Loss 2.4631e-01 (2.4631e-01)	Acc 0.750977 (0.750977)
Epoch: [62][300/616]	Loss 2.4995e-01 (2.4037e-01)	Acc 0.736328 (0.747304)
Epoch: [62][600/616]	Loss 2.5368e-01 (2.3989e-01)	Acc 0.733398 (0.747813)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.747896)
Training Loss of Epoch 62: 0.23996575062837058
Training Acc of Epoch 62: 0.7476435467479675
Testing Acc of Epoch 62: 0.7478956521739131
Early stopping not satisfied.
---------------------
Start epoch 63
---------------------
Epoch: [63][  0/616]	Loss 2.3947e-01 (2.3947e-01)	Acc 0.744141 (0.744141)
Epoch: [63][300/616]	Loss 2.4574e-01 (2.4239e-01)	Acc 0.746094 (0.745137)
Epoch: [63][600/616]	Loss 2.3567e-01 (2.4060e-01)	Acc 0.728516 (0.747251)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747930)
Training Loss of Epoch 63: 0.2405744834886334
Training Acc of Epoch 63: 0.7472878556910569
Testing Acc of Epoch 63: 0.7479304347826087
Early stopping not satisfied.
---------------------
Start epoch 64
---------------------
Epoch: [64][  0/616]	Loss 2.4428e-01 (2.4428e-01)	Acc 0.738281 (0.738281)
Epoch: [64][300/616]	Loss 2.3874e-01 (2.4016e-01)	Acc 0.744141 (0.747051)
Epoch: [64][600/616]	Loss 2.4240e-01 (2.4043e-01)	Acc 0.743164 (0.746786)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.745574)
Training Loss of Epoch 64: 0.2404317361794836
Training Acc of Epoch 64: 0.7468416539634146
Testing Acc of Epoch 64: 0.7455739130434783
Early stopping not satisfied.
---------------------
Start epoch 65
---------------------
Epoch: [65][  0/616]	Loss 2.4874e-01 (2.4874e-01)	Acc 0.740234 (0.740234)
Epoch: [65][300/616]	Loss 2.4671e-01 (2.3994e-01)	Acc 0.735352 (0.747995)
Epoch: [65][600/616]	Loss 2.3889e-01 (2.3939e-01)	Acc 0.745117 (0.748138)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.749191)
Training Loss of Epoch 65: 0.23938733540414794
Training Acc of Epoch 65: 0.7481262703252033
Testing Acc of Epoch 65: 0.7491913043478261
Early stopping not satisfied.
---------------------
Start epoch 66
---------------------
Epoch: [66][  0/616]	Loss 2.3272e-01 (2.3272e-01)	Acc 0.772461 (0.772461)
Epoch: [66][300/616]	Loss 2.4129e-01 (2.3959e-01)	Acc 0.743164 (0.747852)
Epoch: [66][600/616]	Loss 2.4948e-01 (2.4030e-01)	Acc 0.735352 (0.747132)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.746887)
Training Loss of Epoch 66: 0.24037258329430247
Training Acc of Epoch 66: 0.7470401422764228
Testing Acc of Epoch 66: 0.7468869565217391
Early stopping not satisfied.
---------------------
Start epoch 67
---------------------
Epoch: [67][  0/616]	Loss 2.4982e-01 (2.4982e-01)	Acc 0.725586 (0.725586)
Epoch: [67][300/616]	Loss 2.3089e-01 (2.4025e-01)	Acc 0.766602 (0.746639)
Epoch: [67][600/616]	Loss 2.1545e-01 (2.4060e-01)	Acc 0.775391 (0.746799)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.729167 (0.748570)
Training Loss of Epoch 67: 0.2405109493955364
Training Acc of Epoch 67: 0.7468241869918699
Testing Acc of Epoch 67: 0.7485695652173913
Early stopping not satisfied.
---------------------
Start epoch 68
---------------------
Epoch: [68][  0/616]	Loss 2.1681e-01 (2.1681e-01)	Acc 0.781250 (0.781250)
Epoch: [68][300/616]	Loss 2.4842e-01 (2.4011e-01)	Acc 0.742188 (0.746996)
Epoch: [68][600/616]	Loss 2.3174e-01 (2.4027e-01)	Acc 0.752930 (0.747225)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.749591)
Training Loss of Epoch 68: 0.24043482050178497
Training Acc of Epoch 68: 0.7470607850609756
Testing Acc of Epoch 68: 0.7495913043478261
Early stopping not satisfied.
---------------------
Start epoch 69
---------------------
Epoch: [69][  0/616]	Loss 2.4516e-01 (2.4516e-01)	Acc 0.744141 (0.744141)
Epoch: [69][300/616]	Loss 2.3146e-01 (2.4123e-01)	Acc 0.775391 (0.746613)
Epoch: [69][600/616]	Loss 2.2542e-01 (2.4099e-01)	Acc 0.759766 (0.746359)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.747404)
Training Loss of Epoch 69: 0.24091671748859125
Training Acc of Epoch 69: 0.7464573805894309
Testing Acc of Epoch 69: 0.747404347826087
Early stopping not satisfied.
---------------------
Start epoch 70
---------------------
Epoch: [70][  0/616]	Loss 2.5144e-01 (2.5144e-01)	Acc 0.747070 (0.747070)
Epoch: [70][300/616]	Loss 2.5672e-01 (2.4141e-01)	Acc 0.706055 (0.746003)
Epoch: [70][600/616]	Loss 2.3146e-01 (2.4105e-01)	Acc 0.754883 (0.746472)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.747978)
Training Loss of Epoch 70: 0.2411312507177756
Training Acc of Epoch 70: 0.7463176448170732
Testing Acc of Epoch 70: 0.7479782608695652
Early stopping not satisfied.
---------------------
Start epoch 71
---------------------
Epoch: [71][  0/616]	Loss 2.4389e-01 (2.4389e-01)	Acc 0.737305 (0.737305)
Epoch: [71][300/616]	Loss 2.5254e-01 (2.4074e-01)	Acc 0.741211 (0.746078)
Epoch: [71][600/616]	Loss 2.4036e-01 (2.4026e-01)	Acc 0.754883 (0.746974)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.746822)
Training Loss of Epoch 71: 0.2403233599856617
Training Acc of Epoch 71: 0.7469385162601626
Testing Acc of Epoch 71: 0.7468217391304348
Early stopping not satisfied.
---------------------
Start epoch 72
---------------------
Epoch: [72][  0/616]	Loss 2.4157e-01 (2.4157e-01)	Acc 0.736328 (0.736328)
Epoch: [72][300/616]	Loss 2.3329e-01 (2.4088e-01)	Acc 0.758789 (0.746275)
Epoch: [72][600/616]	Loss 2.3338e-01 (2.4077e-01)	Acc 0.753906 (0.746968)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.746800)
Training Loss of Epoch 72: 0.24081737244032264
Training Acc of Epoch 72: 0.7468845274390243
Testing Acc of Epoch 72: 0.7468
Early stopping not satisfied.
---------------------
Start epoch 73
---------------------
Epoch: [73][  0/616]	Loss 2.4762e-01 (2.4762e-01)	Acc 0.742188 (0.742188)
Epoch: [73][300/616]	Loss 2.3600e-01 (2.4006e-01)	Acc 0.761719 (0.747395)
Epoch: [73][600/616]	Loss 2.3882e-01 (2.4050e-01)	Acc 0.750000 (0.746866)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.750700)
Training Loss of Epoch 73: 0.2405128965775172
Training Acc of Epoch 73: 0.7469321646341464
Testing Acc of Epoch 73: 0.7507
Early stopping not satisfied.
---------------------
Start epoch 74
---------------------
Epoch: [74][  0/616]	Loss 2.3784e-01 (2.3784e-01)	Acc 0.749023 (0.749023)
Epoch: [74][300/616]	Loss 2.3400e-01 (2.4266e-01)	Acc 0.743164 (0.745419)
Epoch: [74][600/616]	Loss 2.4765e-01 (2.4176e-01)	Acc 0.748047 (0.746131)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.745048)
Training Loss of Epoch 74: 0.24173152589701055
Training Acc of Epoch 74: 0.7461858485772358
Testing Acc of Epoch 74: 0.7450478260869565
Early stopping not satisfied.
---------------------
Start epoch 75
---------------------
Epoch: [75][  0/616]	Loss 2.4460e-01 (2.4460e-01)	Acc 0.739258 (0.739258)
Epoch: [75][300/616]	Loss 2.3247e-01 (2.3357e-01)	Acc 0.743164 (0.752378)
Epoch: [75][600/616]	Loss 2.3831e-01 (2.3415e-01)	Acc 0.754883 (0.752138)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.735577 (0.753461)
Training Loss of Epoch 75: 0.23428894351652968
Training Acc of Epoch 75: 0.7519547129065041
Testing Acc of Epoch 75: 0.7534608695652174
Model with the best training loss saved! The loss is 0.23428894351652968
Early stopping not satisfied.
---------------------
Start epoch 76
---------------------
Epoch: [76][  0/616]	Loss 2.3372e-01 (2.3372e-01)	Acc 0.764648 (0.764648)
Epoch: [76][300/616]	Loss 2.4151e-01 (2.3376e-01)	Acc 0.733398 (0.752612)
Epoch: [76][600/616]	Loss 2.4716e-01 (2.3390e-01)	Acc 0.742188 (0.752441)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.752574)
Training Loss of Epoch 76: 0.23385810401381515
Training Acc of Epoch 76: 0.7525422383130081
Testing Acc of Epoch 76: 0.7525739130434783
Model with the best training loss saved! The loss is 0.23385810401381515
Early stopping not satisfied.
---------------------
Start epoch 77
---------------------
Epoch: [77][  0/616]	Loss 2.3662e-01 (2.3662e-01)	Acc 0.754883 (0.754883)
Epoch: [77][300/616]	Loss 2.3547e-01 (2.3390e-01)	Acc 0.764648 (0.752492)
Epoch: [77][600/616]	Loss 2.4031e-01 (2.3409e-01)	Acc 0.731445 (0.752510)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.753957)
Training Loss of Epoch 77: 0.234138729683752
Training Acc of Epoch 77: 0.7525168318089431
Testing Acc of Epoch 77: 0.7539565217391304
Early stopping not satisfied.
---------------------
Start epoch 78
---------------------
Epoch: [78][  0/616]	Loss 2.4073e-01 (2.4073e-01)	Acc 0.743164 (0.743164)
Epoch: [78][300/616]	Loss 2.4899e-01 (2.3367e-01)	Acc 0.734375 (0.753092)
Epoch: [78][600/616]	Loss 2.2060e-01 (2.3415e-01)	Acc 0.774414 (0.752320)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.752974)
Training Loss of Epoch 78: 0.23409954542066994
Training Acc of Epoch 78: 0.7523501016260162
Testing Acc of Epoch 78: 0.7529739130434783
Early stopping not satisfied.
---------------------
Start epoch 79
---------------------
Epoch: [79][  0/616]	Loss 2.3693e-01 (2.3693e-01)	Acc 0.750977 (0.750977)
Epoch: [79][300/616]	Loss 2.4025e-01 (2.3414e-01)	Acc 0.751953 (0.752677)
Epoch: [79][600/616]	Loss 2.3536e-01 (2.3418e-01)	Acc 0.751953 (0.752174)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.753352)
Training Loss of Epoch 79: 0.2341789410850866
Training Acc of Epoch 79: 0.7520976244918699
Testing Acc of Epoch 79: 0.7533521739130434
Early stopping not satisfied.
---------------------
Start epoch 80
---------------------
Epoch: [80][  0/616]	Loss 2.2519e-01 (2.2519e-01)	Acc 0.762695 (0.762695)
Epoch: [80][300/616]	Loss 2.3546e-01 (2.3429e-01)	Acc 0.745117 (0.752015)
Epoch: [80][600/616]	Loss 2.2489e-01 (2.3420e-01)	Acc 0.755859 (0.752135)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.754165)
Training Loss of Epoch 80: 0.2342330022798321
Training Acc of Epoch 80: 0.7520722179878049
Testing Acc of Epoch 80: 0.7541652173913044
Early stopping not satisfied.
---------------------
Start epoch 81
---------------------
Epoch: [81][  0/616]	Loss 2.2486e-01 (2.2486e-01)	Acc 0.755859 (0.755859)
Epoch: [81][300/616]	Loss 2.4752e-01 (2.3378e-01)	Acc 0.746094 (0.752586)
Epoch: [81][600/616]	Loss 2.3487e-01 (2.3393e-01)	Acc 0.750977 (0.752584)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.733974 (0.753639)
Training Loss of Epoch 81: 0.23398654022352483
Training Acc of Epoch 81: 0.7525120680894309
Testing Acc of Epoch 81: 0.7536391304347826
Early stopping not satisfied.
---------------------
Start epoch 82
---------------------
Epoch: [82][  0/616]	Loss 2.2580e-01 (2.2580e-01)	Acc 0.769531 (0.769531)
Epoch: [82][300/616]	Loss 2.4330e-01 (2.3416e-01)	Acc 0.745117 (0.752333)
Epoch: [82][600/616]	Loss 2.3307e-01 (2.3359e-01)	Acc 0.750000 (0.752866)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754648)
Training Loss of Epoch 82: 0.23359827006735454
Training Acc of Epoch 82: 0.7528042428861789
Testing Acc of Epoch 82: 0.7546478260869566
Model with the best training loss saved! The loss is 0.23359827006735454
Early stopping not satisfied.
---------------------
Start epoch 83
---------------------
Epoch: [83][  0/616]	Loss 2.3046e-01 (2.3046e-01)	Acc 0.762695 (0.762695)
Epoch: [83][300/616]	Loss 2.2190e-01 (2.3333e-01)	Acc 0.762695 (0.752842)
Epoch: [83][600/616]	Loss 2.3383e-01 (2.3332e-01)	Acc 0.750977 (0.753297)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753117)
Training Loss of Epoch 83: 0.23336396239152768
Training Acc of Epoch 83: 0.7532679115853659
Testing Acc of Epoch 83: 0.7531173913043478
Model with the best training loss saved! The loss is 0.23336396239152768
Early stopping not satisfied.
---------------------
Start epoch 84
---------------------
Epoch: [84][  0/616]	Loss 2.2698e-01 (2.2698e-01)	Acc 0.761719 (0.761719)
Epoch: [84][300/616]	Loss 2.3433e-01 (2.3253e-01)	Acc 0.739258 (0.754633)
Epoch: [84][600/616]	Loss 2.3167e-01 (2.3362e-01)	Acc 0.750000 (0.753004)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755235)
Training Loss of Epoch 84: 0.23359369160683174
Training Acc of Epoch 84: 0.7530678353658536
Testing Acc of Epoch 84: 0.7552347826086957
Early stopping not satisfied.
---------------------
Start epoch 85
---------------------
Epoch: [85][  0/616]	Loss 2.1901e-01 (2.1901e-01)	Acc 0.767578 (0.767578)
Epoch: [85][300/616]	Loss 2.4788e-01 (2.3349e-01)	Acc 0.731445 (0.753024)
Epoch: [85][600/616]	Loss 2.3207e-01 (2.3347e-01)	Acc 0.762695 (0.753250)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.753726)
Training Loss of Epoch 85: 0.233452975362297
Training Acc of Epoch 85: 0.7531821646341463
Testing Acc of Epoch 85: 0.7537260869565218
Early stopping not satisfied.
---------------------
Start epoch 86
---------------------
Epoch: [86][  0/616]	Loss 2.3528e-01 (2.3528e-01)	Acc 0.761719 (0.761719)
Epoch: [86][300/616]	Loss 2.2789e-01 (2.3411e-01)	Acc 0.758789 (0.752424)
Epoch: [86][600/616]	Loss 2.5861e-01 (2.3361e-01)	Acc 0.708008 (0.752837)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.732372 (0.753709)
Training Loss of Epoch 86: 0.23358681393348105
Training Acc of Epoch 86: 0.7528042428861789
Testing Acc of Epoch 86: 0.7537086956521739
Early stopping not satisfied.
---------------------
Start epoch 87
---------------------
Epoch: [87][  0/616]	Loss 2.5115e-01 (2.5115e-01)	Acc 0.740234 (0.740234)
Epoch: [87][300/616]	Loss 2.2020e-01 (2.3330e-01)	Acc 0.767578 (0.752962)
Epoch: [87][600/616]	Loss 2.3313e-01 (2.3343e-01)	Acc 0.756836 (0.753300)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.754400)
Training Loss of Epoch 87: 0.23338522499169762
Training Acc of Epoch 87: 0.7533076092479675
Testing Acc of Epoch 87: 0.7544
Early stopping not satisfied.
---------------------
Start epoch 88
---------------------
Epoch: [88][  0/616]	Loss 2.3240e-01 (2.3240e-01)	Acc 0.754883 (0.754883)
Epoch: [88][300/616]	Loss 2.2734e-01 (2.3310e-01)	Acc 0.755859 (0.753682)
Epoch: [88][600/616]	Loss 2.2105e-01 (2.3351e-01)	Acc 0.766602 (0.753055)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.754430)
Training Loss of Epoch 88: 0.23349758195198647
Training Acc of Epoch 88: 0.7530932418699187
Testing Acc of Epoch 88: 0.7544304347826087
Early stopping not satisfied.
---------------------
Start epoch 89
---------------------
Epoch: [89][  0/616]	Loss 2.2735e-01 (2.2735e-01)	Acc 0.764648 (0.764648)
Epoch: [89][300/616]	Loss 2.2874e-01 (2.3347e-01)	Acc 0.760742 (0.753572)
Epoch: [89][600/616]	Loss 2.3809e-01 (2.3319e-01)	Acc 0.753906 (0.753643)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.741987 (0.755096)
Training Loss of Epoch 89: 0.2332388474446971
Training Acc of Epoch 89: 0.7534536966463414
Testing Acc of Epoch 89: 0.7550956521739131
Model with the best training loss saved! The loss is 0.2332388474446971
Early stopping not satisfied.
---------------------
Start epoch 90
---------------------
Epoch: [90][  0/616]	Loss 2.2981e-01 (2.2981e-01)	Acc 0.752930 (0.752930)
Epoch: [90][300/616]	Loss 2.4338e-01 (2.3324e-01)	Acc 0.741211 (0.753647)
Epoch: [90][600/616]	Loss 2.1812e-01 (2.3342e-01)	Acc 0.782227 (0.753091)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.740385 (0.751491)
Training Loss of Epoch 90: 0.2334370923720724
Training Acc of Epoch 90: 0.7530900660569105
Testing Acc of Epoch 90: 0.7514913043478261
Early stopping not satisfied.
---------------------
Start epoch 91
---------------------
Epoch: [91][  0/616]	Loss 2.5795e-01 (2.5795e-01)	Acc 0.729492 (0.729492)
Epoch: [91][300/616]	Loss 2.3833e-01 (2.3364e-01)	Acc 0.752930 (0.752803)
Epoch: [91][600/616]	Loss 2.2259e-01 (2.3317e-01)	Acc 0.768555 (0.753299)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754896)
Training Loss of Epoch 91: 0.23319090175919416
Training Acc of Epoch 91: 0.7533473069105691
Testing Acc of Epoch 91: 0.7548956521739131
Model with the best training loss saved! The loss is 0.23319090175919416
Early stopping not satisfied.
---------------------
Start epoch 92
---------------------
Epoch: [92][  0/616]	Loss 2.3822e-01 (2.3822e-01)	Acc 0.756836 (0.756836)
Epoch: [92][300/616]	Loss 2.3318e-01 (2.3328e-01)	Acc 0.755859 (0.753679)
Epoch: [92][600/616]	Loss 2.3864e-01 (2.3282e-01)	Acc 0.745117 (0.753778)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.738782 (0.755626)
Training Loss of Epoch 92: 0.23295857208531076
Training Acc of Epoch 92: 0.753661712398374
Testing Acc of Epoch 92: 0.7556260869565218
Model with the best training loss saved! The loss is 0.23295857208531076
Early stopping not satisfied.
---------------------
Start epoch 93
---------------------
Epoch: [93][  0/616]	Loss 2.2568e-01 (2.2568e-01)	Acc 0.752930 (0.752930)
Epoch: [93][300/616]	Loss 2.2857e-01 (2.3239e-01)	Acc 0.767578 (0.754224)
Epoch: [93][600/616]	Loss 2.3046e-01 (2.3271e-01)	Acc 0.741211 (0.753786)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.745192 (0.754600)
Training Loss of Epoch 93: 0.2326973782322271
Training Acc of Epoch 93: 0.7537935086382114
Testing Acc of Epoch 93: 0.7546
Model with the best training loss saved! The loss is 0.2326973782322271
Early stopping not satisfied.
---------------------
Start epoch 94
---------------------
Epoch: [94][  0/616]	Loss 2.2546e-01 (2.2546e-01)	Acc 0.774414 (0.774414)
Epoch: [94][300/616]	Loss 2.3073e-01 (2.3320e-01)	Acc 0.760742 (0.753390)
Epoch: [94][600/616]	Loss 2.2434e-01 (2.3299e-01)	Acc 0.765625 (0.753448)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.754309)
Training Loss of Epoch 94: 0.23286946633966957
Training Acc of Epoch 94: 0.7536299542682927
Testing Acc of Epoch 94: 0.754308695652174
Early stopping not satisfied.
---------------------
Start epoch 95
---------------------
Epoch: [95][  0/616]	Loss 2.4734e-01 (2.4734e-01)	Acc 0.730469 (0.730469)
Epoch: [95][300/616]	Loss 2.3347e-01 (2.3287e-01)	Acc 0.734375 (0.753514)
Epoch: [95][600/616]	Loss 2.4655e-01 (2.3310e-01)	Acc 0.733398 (0.753310)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.743590 (0.755065)
Training Loss of Epoch 95: 0.23306755765182216
Training Acc of Epoch 95: 0.7533727134146342
Testing Acc of Epoch 95: 0.7550652173913044
Early stopping not satisfied.
---------------------
Start epoch 96
---------------------
Epoch: [96][  0/616]	Loss 2.3965e-01 (2.3965e-01)	Acc 0.743164 (0.743164)
Epoch: [96][300/616]	Loss 2.4840e-01 (2.3264e-01)	Acc 0.750977 (0.753222)
Epoch: [96][600/616]	Loss 2.3564e-01 (2.3309e-01)	Acc 0.745117 (0.753385)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.746795 (0.755639)
Training Loss of Epoch 96: 0.23309810413577692
Training Acc of Epoch 96: 0.7533615980691057
Testing Acc of Epoch 96: 0.7556391304347826
Early stopping not satisfied.
---------------------
Start epoch 97
---------------------
Epoch: [97][  0/616]	Loss 2.1516e-01 (2.1516e-01)	Acc 0.792969 (0.792969)
Epoch: [97][300/616]	Loss 2.1402e-01 (2.3281e-01)	Acc 0.774414 (0.754318)
Epoch: [97][600/616]	Loss 2.4240e-01 (2.3307e-01)	Acc 0.743164 (0.753721)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754600)
Training Loss of Epoch 97: 0.23308874904140225
Training Acc of Epoch 97: 0.7537426956300813
Testing Acc of Epoch 97: 0.7546
Early stopping not satisfied.
---------------------
Start epoch 98
---------------------
Epoch: [98][  0/616]	Loss 2.3107e-01 (2.3107e-01)	Acc 0.763672 (0.763672)
Epoch: [98][300/616]	Loss 2.2729e-01 (2.3285e-01)	Acc 0.773438 (0.753682)
Epoch: [98][600/616]	Loss 2.4620e-01 (2.3272e-01)	Acc 0.736328 (0.753749)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.753957)
Training Loss of Epoch 98: 0.23277870651667681
Training Acc of Epoch 98: 0.7537109375
Testing Acc of Epoch 98: 0.7539565217391304
Early stopping not satisfied.
---------------------
Start epoch 99
---------------------
Epoch: [99][  0/616]	Loss 2.5053e-01 (2.5053e-01)	Acc 0.732422 (0.732422)
Epoch: [99][300/616]	Loss 2.3955e-01 (2.3214e-01)	Acc 0.737305 (0.754909)
Epoch: [99][600/616]	Loss 2.3339e-01 (2.3260e-01)	Acc 0.762695 (0.754381)
Neglect the last batch so that num samples/batch size = int
Testing
Acc 0.737179 (0.754030)
Training Loss of Epoch 99: 0.2326613244244723
Training Acc of Epoch 99: 0.7542841717479675
Testing Acc of Epoch 99: 0.7540304347826087
Model with the best training loss saved! The loss is 0.2326613244244723
Early stopping not satisfied.
